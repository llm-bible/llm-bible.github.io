---
layout: publication
title: Decoding Stumpers Large Language Models Vs. Human Problem45;solvers
authors: Goldstein Alon, Havin Miriam, Reichart Roi, Goldstein Ariel
conference: "Arxiv"
year: 2023
bibkey: goldstein2023decoding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.16411"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
This paper investigates the problem45;solving capabilities of Large Language Models (LLMs) by evaluating their performance on stumpers unique single45;step intuition problems that pose challenges for human solvers but are easily verifiable. We compare the performance of four state45;of45;the45;art LLMs (Davinci45;2 Davinci45;3 GPT45;3.545;Turbo GPT45;4) to human participants. Our findings reveal that the new45;generation LLMs excel in solving stumpers and surpass human performance. However humans exhibit superior skills in verifying solutions to the same problems. This research enhances our understanding of LLMs cognitive abilities and provides insights for enhancing their problem45;solving potential across various domains.
