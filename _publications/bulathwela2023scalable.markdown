---
layout: publication
title: Scalable Educational Question Generation With Pre-trained Language Models
authors: Sahan Bulathwela, Hamze Muse, Emine Yilmaz
conference: Arxiv
year: 2023
citations: 21
bibkey: bulathwela2023scalable
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2305.07871'}]
tags: [Pre-Training, Fine-Tuning]
---
The automatic generation of educational questions will play a key role in
scaling online education, enabling self-assessment at scale when a global
population is manoeuvring their personalised learning journeys. We develop
\textit\{EduQG\}, a novel educational question generation model built by adapting
a large language model. Our extensive experiments demonstrate that
\textit\{EduQG\} can produce superior educational questions by further
pre-training and fine-tuning a pre-trained language model on the scientific
text and science question data.