---
layout: publication
title: 'Personalizing Multimodal Large Language Models For Image Captioning: An Experimental Analysis'
authors: Davide Bucciarelli, Nicholas Moratelli, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
conference: "Arxiv"
year: 2024
bibkey: bucciarelli2024personalizing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.03665'}
tags: ['Training Techniques', 'Model Architecture', 'GPT', 'Prompting', 'Fine-Tuning', 'Multimodal Models', 'Pretraining Methods']
---
The task of image captioning demands an algorithm to generate natural
language descriptions of visual inputs. Recent advancements have seen a
convergence between image captioning research and the development of Large
Language Models (LLMs) and Multimodal LLMs -- like GPT-4V and Gemini -- which
extend the capabilities of text-only LLMs to multiple modalities. This paper
investigates whether Multimodal LLMs can supplant traditional image captioning
networks by evaluating their performance on various image description
benchmarks. We explore both the zero-shot capabilities of these models and
their adaptability to different semantic domains through fine-tuning methods,
including prompt learning, prefix tuning, and low-rank adaptation. Our results
demonstrate that while Multimodal LLMs achieve impressive zero-shot
performance, fine-tuning for specific domains while maintaining their
generalization capabilities intact remains challenging. We discuss the
implications of these findings for future research in image captioning and the
development of more adaptable Multimodal LLMs.
