---
layout: publication
title: 'Learning To Reason Over Time: Timeline Self-reflection For Improved Temporal Reasoning In Language Models'
authors: Adrián Bazaga, Rexhina Blloshmi, Bill Byrne, Adrià De Gispert
conference: "Arxiv"
year: 2025
bibkey: bazaga2025learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.05258"}
tags: ['RAG', 'Tools', 'Applications']
---
Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information such as event sequencing, durations, and inter-temporal relationships. These capabilities are critical for applications including question answering, scheduling, and historical analysis. In this paper, we introduce TISER, a novel framework that enhances the temporal reasoning abilities of LLMs through a multi-stage process that combines timeline construction with iterative self-reflection. Our approach leverages test-time scaling to extend the length of reasoning traces, enabling models to capture complex temporal dependencies more effectively. This strategy not only boosts reasoning accuracy but also improves the traceability of the inference process. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, including out-of-distribution test sets, and reveal that TISER enables smaller open-source models to surpass larger closed-weight models on challenging temporal reasoning tasks.
