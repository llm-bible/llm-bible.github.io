---
layout: publication
title: 'Towards Pedagogical Llms With Supervised Fine Tuning For Computing Education'
authors: Alexandra Vassar, Jake Renzella, Emily Ross, Andrew Taylor
conference: "Arxiv"
year: 2024
bibkey: vassar2024towards
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.01765'}
tags: ['Fine-Tuning', 'Training Techniques', 'Pretraining Methods']
---
This paper investigates supervised fine-tuning of large language models
(LLMs) to improve their pedagogical alignment in computing education,
addressing concerns that LLMs may hinder learning outcomes. The project
utilised a proprietary dataset of 2,500 high quality question/answer pairs from
programming course forums, and explores two research questions: the suitability
of university course forums in contributing to fine-tuning datasets, and how
supervised fine-tuning can improve LLMs' alignment with educational principles
such as constructivism. Initial findings suggest benefits in pedagogical
alignment of LLMs, with deeper evaluations required.
