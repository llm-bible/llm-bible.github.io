---
layout: publication
title: 'A Systematic Examination Of Preference Learning Through The Lens Of Instruction-following'
authors: Joongwon Kim, Anirudh Goyal, Aston Zhang, Bo Xiong, Rui Hou, Melanie Kambadur, Dhruv Mahajan, Hannaneh Hajishirzi, Liang Tan
conference: "Arxiv"
year: 2024
bibkey: kim2024systematic
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.15282'}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Tools', 'Prompting', 'Reinforcement Learning']
---
Preference learning is a widely adopted post-training technique that aligns
large language models (LLMs) to human preferences and improves specific
downstream task capabilities. In this work we systematically investigate how
specific attributes of preference datasets affect the alignment and downstream
performance of LLMs in instruction-following tasks. We use a novel synthetic
data generation pipeline to generate 48,000 unique instruction-following
prompts with combinations of 23 verifiable constraints that enable fine-grained
and automated quality assessments of model responses. With our synthetic
prompts, we use two preference dataset curation methods - rejection sampling
(RS) and Monte Carlo Tree Search (MCTS) - to obtain pairs of (chosen, rejected)
responses. Then, we perform experiments investigating the effects of (1) the
presence of shared prefixes between the chosen and rejected responses, (2) the
contrast and quality of the chosen, rejected responses and (3) the complexity
of the training prompts. Our experiments reveal that shared prefixes in
preference pairs, as generated by MCTS, provide marginal but consistent
improvements and greater stability across challenging training configurations.
High-contrast preference pairs generally outperform low-contrast pairs;
however, combining both often yields the best performance by balancing
diversity and learning efficiency. Additionally, training on prompts of
moderate difficulty leads to better generalization across tasks, even for more
complex evaluation scenarios, compared to overly challenging prompts. Our
findings provide actionable insights into optimizing preference data curation
for instruction-following tasks, offering a scalable and effective framework
for enhancing LLM training and alignment.
