---
layout: publication
title: Xlam A Family Of Large Action Models To Empower AI Agent Systems
authors: Zhang Jianguo, Lan Tian, Zhu Ming, Liu Zuxin, Hoang Thai, Kokane Shirley, Yao Weiran, Tan Juntao, Prabhakar Akshara, Chen Haolin, Liu Zhiwei, Feng Yihao, Awalgaonkar Tulika, Murthy Rithesh, Hu Eric, Chen Zeyuan, Xu Ran, Niebles Juan Carlos, Heinecke Shelby, Wang Huan, Savarese Silvio, Xiong Caiming
conference: "Arxiv"
year: 2024
bibkey: zhang2024family
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.03215"}
tags: ['Agent', 'Agentic', 'GPT', 'Model Architecture']
---
Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However the open45;source community faces many challenges in developing specialized models for agent tasks driven by the scarcity of high45;quality agent datasets and the absence of standard protocols in this area. We introduce and publicly release xLAM a series of large action models designed for AI agent tasks. The xLAM series includes five models with both dense and mixture45;of45;expert architectures ranging from 1B to 8x22B parameters trained using a scalable flexible pipeline that unifies augments and synthesizes diverse datasets to enhance AI agents generalizability and performance across varied environments. Our experimental results demonstrate that xLAM consistently delivers exceptional performance across multiple agent ability benchmarks notably securing the 1st position on the Berkeley Function45;Calling Leaderboard outperforming GPT45;4 Claude45;3 and many other models in terms of tool use. By releasing the xLAM series we aim to advance the performance of open45;source LLMs for autonomous AI agents potentially accelerating progress and democratizing access to high45;performance models for agent tasks. Models are available at https://huggingface.co/collections/Salesforce/xlam&#45;models&#45;65f00e2a0a63bbcd1c2dade4
