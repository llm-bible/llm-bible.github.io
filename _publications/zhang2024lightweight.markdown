---
layout: publication
title: 'Omnievalkit: A Modular, Lightweight Toolbox For Evaluating Large Language Model And Its Omni-extensions'
authors: Yi-kai Zhang, Xu-xiang Zhong, Shiyin Lu, Qing-guo Chen, De-chuan Zhan, Han-jia Ye
conference: "Arxiv"
year: 2024
bibkey: zhang2024lightweight
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.06693"}
tags: ['Tools', 'Applications', 'Model Architecture', 'Reinforcement Learning', 'Multimodal Models']
---
The rapid advancements in Large Language Models (LLMs) have significantly
expanded their applications, ranging from multilingual support to
domain-specific tasks and multimodal integration. In this paper, we present
OmniEvalKit, a novel benchmarking toolbox designed to evaluate LLMs and their
omni-extensions across multilingual, multidomain, and multimodal capabilities.
Unlike existing benchmarks that often focus on a single aspect, OmniEvalKit
provides a modular, lightweight, and automated evaluation system. It is
structured with a modular architecture comprising a Static Builder and Dynamic
Data Flow, promoting the seamless integration of new models and datasets.
OmniEvalKit supports over 100 LLMs and 50 evaluation datasets, covering
comprehensive evaluations across thousands of model-dataset combinations.
OmniEvalKit is dedicated to creating an ultra-lightweight and fast-deployable
evaluation framework, making downstream applications more convenient and
versatile for the AI community.
