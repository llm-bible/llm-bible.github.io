---
layout: publication
title: 'Small Language Models: Survey, Measurements, And Insights'
authors: Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, Mengwei Xu
conference: "Arxiv"
year: 2024
bibkey: lu2024small
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.15790"}
tags: ['Training Techniques', 'Model Architecture', 'Survey Paper', 'In-Context Learning', 'Pretraining Methods', 'Transformer', 'Prompting', 'Attention Mechanism']
---
Small language models (SLMs), despite their widespread adoption in modern
smart devices, have received significantly less academic attention compared to
their large language model (LLM) counterparts, which are predominantly deployed
in data centers and cloud environments. While researchers continue to improve
the capabilities of LLMs in the pursuit of artificial general intelligence, SLM
research aims to make machine intelligence more accessible, affordable, and
efficient for everyday tasks. Focusing on transformer-based, decoder-only
language models with 100M-5B parameters, we survey 70 state-of-the-art
open-source SLMs, analyzing their technical innovations across three axes:
architectures, training datasets, and training algorithms. In addition, we
evaluate their capabilities in various domains, including commonsense
reasoning, mathematics, in-context learning, and long context. To gain further
insight into their on-device runtime costs, we benchmark their inference
latency and memory footprints. Through in-depth analysis of our benchmarking
data, we offer valuable insights to advance research in this field.
