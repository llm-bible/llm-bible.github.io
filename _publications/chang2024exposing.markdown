---
layout: publication
title: 'Exposing LLM Vulnerabilities: Adversarial Scam Detection And Performance'
authors: Chen-wei Chang, Shailik Sarkar, Shutonu Mitra, Qi Zhang, Hossein Salemi, Hemant Purohit, Fengxiu Zhang, Michin Hong, Jin-hee Cho, Chang-tien Lu
conference: "Arxiv"
year: 2024
bibkey: chang2024exposing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.00621'}
tags: ['Security']
---
Can we trust Large Language Models (LLMs) to accurately predict scam? This
paper investigates the vulnerabilities of LLMs when facing adversarial scam
messages for the task of scam detection. We addressed this issue by creating a
comprehensive dataset with fine-grained labels of scam messages, including both
original and adversarial scam messages. The dataset extended traditional binary
classes for the scam detection task into more nuanced scam types. Our analysis
showed how adversarial examples took advantage of vulnerabilities of a LLM,
leading to high misclassification rate. We evaluated the performance of LLMs on
these adversarial scam messages and proposed strategies to improve their
robustness.
