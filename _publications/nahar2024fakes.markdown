---
layout: publication
title: Fakes of Varying Shades How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations
authors: Nahar Mahjabin, Seo Haeseung, Lee Eun-ju, Xiong Aiping, Lee Dongwon
conference: "Arxiv"
year: 2024
bibkey: nahar2024fakes
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.03745"}
  - {name: "Code", url: "https://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials"}
tags: ['ARXIV', 'Has Code', 'LLM', 'Model Architecture', 'Survey Paper', 'Tools']
---
The widespread adoption and transformative effects of large language models (LLMs) have sparked concerns regarding their capacity to produce inaccurate and fictitious content referred to as hallucinations. Given the potential risks associated with hallucinations humans should be able to identify them. This research aims to understand the human perception of LLM hallucinations by systematically varying the degree of hallucination (genuine minor hallucination major hallucination) and examining its interaction with warning (i.e. a warning of potential inaccuracies absent vs. present). Participants (N=419) from Prolific rated the perceived accuracy and engaged with content (e.g. like dislike share) in a Q/A format. Participants ranked content as truthful in the order of genuine minor hallucination and major hallucination and user engagement behaviors mirrored this pattern. More importantly we observed that warning improved the detection of hallucination without significantly affecting the perceived truthfulness of genuine content. We conclude by offering insights for future tools to aid human detection of hallucinations. All survey materials demographic questions and post-session questions are available at https://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials
