---
layout: publication
title: Scaling Vision-language Models With Sparse Mixture Of Experts
authors: Sheng Shen et al.
conference: Arxiv
year: 2023
citations: 17
bibkey: shen2023scaling
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2303.07226'}]
tags: [Multimodal Models, Reinforcement Learning, Interpretability and Explainability,
  Applications]
---
The field of natural language processing (NLP) has made significant strides
in recent years, particularly in the development of large-scale vision-language
models (VLMs). These models aim to bridge the gap between text and visual
information, enabling a more comprehensive understanding of multimedia data.
However, as these models become larger and more complex, they also become more
challenging to train and deploy. One approach to addressing this challenge is
the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the
model into smaller, specialized sub-models that can jointly solve a task. In
this paper, we explore the effectiveness of MoE in scaling vision-language
models, demonstrating its potential to achieve state-of-the-art performance on
a range of benchmarks over dense models of equivalent computational cost. Our
research offers valuable insights into stabilizing the training of MoE models,
understanding the impact of MoE on model interpretability, and balancing the
trade-offs between compute performance when scaling VLMs. We hope our work will
inspire further research into the use of MoE for scaling large-scale
vision-language models and other multimodal machine learning applications.