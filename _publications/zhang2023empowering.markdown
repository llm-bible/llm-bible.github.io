---
layout: publication
title: Speechgpt Empowering Large Language Models With Intrinsic Cross45;modal Conversational Abilities
authors: Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, Xipeng Qiu
conference: "Arxiv"
year: 2023
bibkey: zhang2023empowering
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2305.11000v2"}
tags: ['GPT', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
Multi45;modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However current speech45;language models typically adopt the cascade paradigm preventing inter45;modal knowledge transfer. In this paper we propose SpeechGPT a large language model with intrinsic cross45;modal conversational abilities capable of perceiving and generating multi45;model content. With discrete speech representations we first construct SpeechInstruct a large45;scale cross45;modal speech instruction dataset. Additionally we employ a three45;stage training strategy that includes modality45;adaptation pre45;training cross45;modal instruction fine45;tuning and chain45;of45;modality instruction fine45;tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi45;modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.
