---
layout: publication
title: 'Process-supervised Reward Models For Verifying Clinical Note Generation: A Scalable Approach Guided By Domain Expertise'
authors: Hanyin Wang, Chufan Gao, Qiping Xu, Bolun Liu, Guleid Hussein, Hariprasad Korsapati, Mohamad El Labban, Kingsley Iheasirim, Mohamed Hassan, Gokhan Anil, Brian Bartlett, Jimeng Sun
conference: "Arxiv"
year: 2024
bibkey: wang2024process
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.12583"}
tags: ['Reinforcement Learning']
---
Process-supervised reward models (PRMs), which verify large language model
(LLM) outputs step-by-step, have achieved significant success in mathematical
and coding problems. However, their application to other domains remains
largely unexplored. In this work, we train a PRM to provide step-level reward
signals for clinical notes generated by LLMs from patient-doctor dialogues.
Guided by real-world clinician expertise, we carefully designed step
definitions for clinical notes and utilized Gemini-Pro 1.5 to automatically
generate process supervision data at scale. Our proposed PRM, trained on the
LLaMA-3.1 8B instruct model, outperformed both Gemini-Pro 1.5 and the vanilla
outcome-supervised reward model (ORM) in two key evaluations: (1) selecting
gold-reference samples from error-containing ones, achieving 98.8% accuracy
(versus 70.0% for the vanilla ORM and 93.8% for Gemini-Pro 1.5), and (2)
selecting physician-preferred notes, achieving 56.2% accuracy (compared to
37.5% for the vanilla ORM and 50.0% for Gemini-Pro 1.5). Additionally, we
conducted ablation studies to determine optimal loss functions and data
selection strategies, along with physician reader studies to explore predictors
of downstream Best-of-N performance. Our promising results suggest the
potential of PRMs to extend beyond the clinical domain, offering a scalable and
effective solution for diverse generative tasks.
