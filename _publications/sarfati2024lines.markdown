---
layout: publication
title: 'Lines Of Thought In Large Language Models'
authors: Raphaël Sarfati, Toni J. B. Liu, Nicolas Boullé, Christopher J. Earls
conference: "Arxiv"
year: 2024
bibkey: sarfati2024lines
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.01545"}
tags: ['Model Architecture', 'Merging', 'Pretraining Methods', 'Transformer', 'Prompting']
---
Large Language Models achieve next-token prediction by transporting a
vectorized piece of text (prompt) across an accompanying embedding space under
the action of successive transformer layers. The resulting high-dimensional
trajectories realize different contextualization, or 'thinking', steps, and
fully determine the output probability distribution. We aim to characterize the
statistical properties of ensembles of these 'lines of thought.' We observe
that independent trajectories cluster along a low-dimensional, non-Euclidean
manifold, and that their path can be well approximated by a stochastic equation
with few parameters extracted from data. We find it remarkable that the vast
complexity of such large models can be reduced to a much simpler form, and we
reflect on implications.
