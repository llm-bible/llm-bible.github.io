---
layout: publication
title: 'Llm-powered Preference Elicitation In Combinatorial Assignment'
authors: Ermis Soumalias, Yanchen Jiang, Kehang Zhu, Michael Curry, Sven Seuken, David C. Parkes
conference: "Arxiv"
year: 2025
bibkey: soumalias2025llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.10308"}
tags: ['Tools', 'Efficiency and Optimization']
---
We study the potential of large language models (LLMs) as proxies for humans
to simplify preference elicitation (PE) in combinatorial assignment. While
traditional PE methods rely on iterative queries to capture preferences, LLMs
offer a one-shot alternative with reduced human effort. We propose a framework
for LLM proxies that can work in tandem with SOTA ML-powered preference
elicitation schemes. Our framework handles the novel challenges introduced by
LLMs, such as response variability and increased computational costs. We
experimentally evaluate the efficiency of LLM proxies against human queries in
the well-studied course allocation domain, and we investigate the model
capabilities required for success. We find that our approach improves
allocative efficiency by up to 20%, and these results are robust across
different LLMs and to differences in quality and accuracy of reporting.
