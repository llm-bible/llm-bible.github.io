---
layout: publication
title: PLATO Pre45;trained Dialogue Generation Model With Discrete Latent Variable
authors: Bao Siqi, He Huang, Wang Fan, Wu Hua, Wang Haifeng
conference: "Arxiv"
year: 2019
bibkey: bao2019pre
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1910.07931"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques', 'Transformer']
---
Pre45;training models have been proved effective for a wide range of natural language processing tasks. Inspired by this we propose a novel dialogue generation pre45;training framework to support various kinds of conversations including chit45;chat knowledge grounded dialogues and conversational question answering. In this framework we adopt flexible attention mechanisms to fully leverage the bi45;directional context and the uni45;directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one45;to45;many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.
