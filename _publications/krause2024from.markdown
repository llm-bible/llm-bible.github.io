---
layout: publication
title: 'From Data To Commonsense Reasoning: The Use Of Large Language Models For Explainable AI'
authors: Stefanie Krause, Frieder Stolzenburg
conference: "Arxiv"
year: 2024
bibkey: krause2024from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.03778"}
tags: ['Model Architecture', 'RAG', 'GPT', 'Interpretability', 'Interpretability and Explainability']
---
Commonsense reasoning is a difficult task for a computer, but a critical
skill for an artificial intelligence (AI). It can enhance the explainability of
AI models by enabling them to provide intuitive and human-like explanations for
their decisions. This is necessary in many areas especially in question
answering (QA), which is one of the most important tasks of natural language
processing (NLP). Over time, a multitude of methods have emerged for solving
commonsense reasoning problems such as knowledge-based approaches using formal
logic or linguistic analysis. In this paper, we investigate the effectiveness
of large language models (LLMs) on different QA tasks with a focus on their
abilities in reasoning and explainability. We study three LLMs: GPT-3.5, Gemma
and Llama 3. We further evaluate the LLM results by means of a questionnaire.
We demonstrate the ability of LLMs to reason with commonsense as the models
outperform humans on different datasets. While GPT-3.5's accuracy ranges from
56% to 93% on various QA benchmarks, Llama 3 achieved a mean accuracy of 90% on
all eleven datasets. Thereby Llama 3 is outperforming humans on all datasets
with an average 21% higher accuracy over ten datasets. Furthermore, we can
appraise that, in the sense of explainable artificial intelligence (XAI),
GPT-3.5 provides good explanations for its decisions. Our questionnaire
revealed that 66% of participants rated GPT-3.5's explanations as either "good"
or "excellent". Taken together, these findings enrich our understanding of
current LLMs and pave the way for future investigations of reasoning and
explainability.
