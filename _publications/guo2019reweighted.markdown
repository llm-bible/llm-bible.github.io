---
layout: publication
title: Reweighted Proximal Pruning For Large45;scale Language Representation
authors: Guo Fu-ming, Liu Sijia, Mungall Finlay S., Lin Xue, Wang Yanzhi
conference: "Arxiv"
year: 2019
bibkey: guo2019reweighted
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1909.12486"}
tags: ['Applications', 'BERT', 'Efficiency And Optimization', 'Fine Tuning', 'Model Architecture', 'Pruning', 'Tools', 'Training Techniques']
---
Recently pre45;trained language representation flourishes as the mainstay of the natural language understanding community e.g. BERT. These pre45;trained language representations can create state45;of45;the45;art results on a wide range of downstream tasks. Along with continuous significant performance improvement the size and complexity of these pre45;trained neural models continue to increase rapidly. Is it possible to compress these large45;scale language representation models How will the pruned language representation affect the downstream multi45;task transfer learning objectives In this paper we propose Reweighted Proximal Pruning (RPP) a new pruning method specifically designed for a large45;scale language representation model. Through experiments on SQuAD and the GLUE benchmark suite we show that proximal pruned BERT keeps high accuracy for both the pre45;training task and the downstream multiple fine45;tuning tasks at high prune ratio. RPP provides a new perspective to help us analyze what large45;scale language representation might learn. Additionally RPP makes it possible to deploy a large state45;of45;the45;art language representation model such as BERT on a series of distinct devices (e.g. online servers mobile phones and edge devices).
