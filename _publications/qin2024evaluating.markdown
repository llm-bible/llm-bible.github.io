---
layout: publication
title: Infobench Evaluating Instruction Following Ability In Large Language Models
authors: Qin Yiwei, Song Kaiqiang, Hu Yebowen, Yao Wenlin, Cho Sangwoo, Wang Xiaoyang, Wu Xuansheng, Liu Fei, Liu Pengfei, Yu Dong
conference: "Arxiv"
year: 2024
bibkey: qin2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.03601"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Tools']
---
This paper introduces the Decomposed Requirements Following Ratio (DRFR) a new metric for evaluating Large Language Models (LLMs) ability to follow instructions. Addressing a gap in current methodologies DRFR breaks down complex instructions into simpler criteria facilitating a detailed analysis of LLMs compliance with various aspects of tasks. Alongside this metric we present InFoBench a benchmark comprising 500 diverse instructions and 2250 decomposed questions across multiple constraint categories. Our experiments compare DRFR with traditional scoring methods and explore annotation sources including human experts crowd45;sourced workers and GPT45;4. The findings demonstrate DRFRs higher reliability and the effectiveness of using GPT45;4 as a cost45;efficient annotator. The evaluation of several advanced LLMs using this framework reveals their strengths and areas needing improvement particularly in complex instruction45;following. This study contributes a novel metric and benchmark offering insights for future LLM development and evaluation.
