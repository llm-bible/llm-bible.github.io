---
layout: publication
title: 'Jsontuning: Towards Generalizable, Robust, And Controllable Instruction Tuning'
authors: Chang Gao, Wenxuan Zhang, Guizhen Chen, Wai Lam
conference: "Arxiv"
year: 2023
bibkey: gao2023towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.02953"}
tags: ['Security']
---
Instruction tuning is vital for enhancing the performance of large language
models (LLMs), but existing text-to-text methods, referred to as TextTuning,
struggle with issues such as generalization, robustness, and controllability
due to their lack of explicit task structures. We introduce JsonTuning, a
structure-to-structure approach that uses JSON structures to represent tasks.
This method improves generalization by clarifying task elements and their
relations, boosts robustness by minimizing ambiguity, and enhances
controllability by allowing precise control over outputs. We conduct an
extensive comparative analysis between JsonTuning and TextTuning using various
language models and benchmarks. Our findings reveal that JsonTuning
consistently surpasses TextTuning in terms of performance, robustness, and
controllability across different scenarios. By overcoming the limitations of
TextTuning, JsonTuning demonstrates significant potential for developing more
effective and reliable LLMs capable of handling diverse scenarios.
