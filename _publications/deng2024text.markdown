---
layout: publication
title: Text45;tuple45;table Towards Information Integration In Text45;to45;table Generation Via Global Tuple Extraction
authors: Deng Zheye, Chan Chunkit, Wang Weiqi, Sun Yuxi, Fan Wei, Zheng Tianshi, Yim Yauwai, Song Yangqiu
conference: "Arxiv"
year: 2024
bibkey: deng2024text
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.14215"}
  - {name: "Code", url: "https://github.com/HKUST&#45;KnowComp/LiveSum&#45;TTT"}
tags: ['Applications', 'Attention Mechanism', 'Has Code', 'Model Architecture', 'Training Techniques']
---
The task of condensing large chunks of textual information into concise and structured tables has gained attention recently due to the emergence of Large Language Models (LLMs) and their potential benefit for downstream tasks such as text summarization and text mining. Previous approaches often generate tables that directly replicate information from the text limiting their applicability in broader contexts as text45;to45;table generation in real45;life scenarios necessitates information extraction reasoning and integration. However there is a lack of both datasets and methodologies towards this task. In this paper we introduce LiveSum a new benchmark dataset created for generating summary tables of competitions based on real45;time commentary texts. We evaluate the performances of state45;of45;the45;art LLMs on this task in both fine45;tuning and zero45;shot settings and additionally propose a novel pipeline called T^3(Text45;Tuple45;Table) to improve their performances. Extensive experimental results demonstrate that LLMs still struggle with this task even after fine45;tuning while our approach can offer substantial performance gains without explicit training. Further analyses demonstrate that our method exhibits strong generalization abilities surpassing previous approaches on several other text45;to45;table datasets. Our code and data can be found at https://github.com/HKUST&#45;KnowComp/LiveSum&#45;TTT.
