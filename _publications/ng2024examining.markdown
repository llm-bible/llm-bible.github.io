---
layout: publication
title: Examining the Influence of Political Bias on Large Language Model Performance in Stance Classification
authors: Ng Lynnette Hui Xian, Cruickshank Iain, Lee Roy Ka-wei
conference: "Arxiv"
year: 2024
bibkey: ng2024examining
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.17688"}
  - {name: "Paper", url: "http://doi.org/10.5281/zenodo.12938478"}
tags: ['Ethics And Bias', 'Prompting']
---
Large Language Models (LLMs) have demonstrated remarkable capabilities in executing tasks based on natural language queries. However these models trained on curated datasets inherently embody biases ranging from racial to national and gender biases. It remains uncertain whether these biases impact the performance of LLMs for certain tasks. In this study we investigate the political biases of LLMs within the stance classification task specifically examining whether these models exhibit a tendency to more accurately classify politically-charged stances. Utilizing three datasets seven LLMs and four distinct prompting schemes we analyze the performance of LLMs on politically oriented statements and targets. Our findings reveal a statistically significant difference in the performance of LLMs across various politically oriented stance classification tasks. Furthermore we observe that this difference primarily manifests at the dataset level with models and prompting schemes showing statistically similar performances across different stance classification datasets. Lastly we observe that when there is greater ambiguity in the target the statement is directed towards LLMs have poorer stance classification accuracy. Code Dataset http://doi.org/10.5281/zenodo.12938478
