---
layout: publication
title: 'Mateval: A Multi-agent Discussion Framework For Advancing Open-ended Text Evaluation'
authors: Li Yu, Zhang Shenyu, Wu Rui, Huang Xiutian, Chen Yongrui, Xu Wenhao, Qi Guilin, Min Dehai
conference: "Arxiv"
year: 2024
bibkey: li2024multi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.19305"}
tags: ['Agentic', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Tools']
---
"Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using LLMs as evaluators. While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A Multi-Agent Text Evaluation framework where all agents are played by LLMs like GPT-4. The MATEval framework emulates human collaborative discussion methods, integrating multiple agents' interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and breadth of the evaluation process and guiding discussions towards consensus, while the framework generates comprehensive evaluation reports, including error localization, error types and scoring. Experimental results show that our framework outperforms existing open-ended text evaluation methods and achieves the highest correlation with human evaluation, which confirms the effectiveness and advancement of our framework in addressing the uncertainties and instabilities in evaluating LLMs-generated text. Furthermore, our framework significantly improves the efficiency of text evaluation and model iteration in industrial scenarios."
