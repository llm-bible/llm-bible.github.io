---
layout: publication
title: 'Unitoken: Harmonizing Multimodal Understanding And Generation Through Unified Visual Encoding'
authors: Yang Jiao, Haibo Qiu, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin Ma, Yu-gang Jiang
conference: "Arxiv"
year: 2025
bibkey: jiao2025harmonizing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.04423'}
  - {name: "Code", url: 'https://github.com/SxJyJay/UniToken'}
tags: ['Has Code', 'Multimodal Models', 'Tools']
---
We introduce UniToken, an auto-regressive generation model that encodes
visual inputs through a combination of discrete and continuous representations,
enabling seamless integration of unified visual understanding and image
generation tasks. Unlike previous approaches that rely on unilateral visual
representations, our unified visual encoding framework captures both high-level
semantics and low-level details, delivering multidimensional information that
empowers heterogeneous tasks to selectively assimilate domain-specific
knowledge based on their inherent characteristics. Through in-depth
experiments, we uncover key principles for developing a unified model capable
of both visual understanding and image generation. Extensive evaluations across
a diverse range of prominent benchmarks demonstrate that UniToken achieves
state-of-the-art performance, surpassing existing approaches. These results
establish UniToken as a robust foundation for future research in this domain.
The code and models are available at https://github.com/SxJyJay/UniToken.
