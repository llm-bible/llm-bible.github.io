---
layout: publication
title: BMIKE45;53 Investigating Cross45;lingual Knowledge Editing With In45;context Learning
authors: Nie Ercong, Shao Bo, Ding Zifeng, Wang Mingyang, Schmid Helmut, Sch√ºtze Hinrich
conference: "Arxiv"
year: 2024
bibkey: nie2024bmike
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.17764"}
  - {name: "Code", url: "https://anonymous.4open.science/r/MIKE"}
tags: ['Has Code', 'Prompting', 'Tools', 'Training Techniques']
---
Large language models (LLMs) possess extensive parametric knowledge but this knowledge is difficult to update with new information because retraining is very expensive and infeasible for closed45;source models. Knowledge editing (KE) has emerged as a viable solution for updating the knowledge of LLMs without compromising their overall performance. On45;the45;fly KE methods inspired by in45;context learning (ICL) have shown great promise and allow LLMs to be treated as black boxes. In the past KE was primarily employed in English contexts whereas the potential for cross45;lingual KE in current English45;centric LLMs has not been fully explored. To foster more research in this direction we introduce the BMIKE45;53 benchmark for evaluating cross45;lingual KE on 53 diverse languages across three KE task types. We also propose a gradient45;free KE method called Multilingual In45;context Knowledge Editing (MIKE) and evaluate it on BMIKE45;53. Our evaluation focuses on cross45;lingual knowledge transfer in terms of reliability generality locality and portability offering valuable insights and a framework for future research in cross45;lingual KE. Our code and data are publicly accessible via the anonymous repository at https://anonymous.4open.science/r/MIKE.
