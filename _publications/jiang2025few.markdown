---
layout: publication
title: 'Few-shot Multilingual Open-domain QA From 5 Examples'
authors: Fan Jiang, Tom Drummond, Trevor Cohn
conference: "Arxiv"
year: 2025
bibkey: jiang2025few
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.19722'}
tags: ['Few-Shot', 'Applications', 'Training Techniques', 'Prompting', 'Pre-Training']
---
Recent approaches to multilingual open-domain question answering (MLODQA)
have achieved promising results given abundant language-specific training data.
However, the considerable annotation cost limits the application of these
methods for underrepresented languages. We introduce a *few-shot learning*
approach to synthesise large-scale multilingual data from large language models
(LLMs). Our method begins with large-scale self-supervised pre-training using
WikiData, followed by training on high-quality synthetic multilingual data
generated by prompting LLMs with few-shot supervision. The final model,
\textsc\{FsModQA\}, significantly outperforms existing few-shot and supervised
baselines in MLODQA and cross-lingual and monolingual retrieval. We further
show our method can be extended for effective zero-shot adaptation to new
languages through a *cross-lingual prompting* strategy with only
English-supervised data, making it a general and applicable solution for MLODQA
tasks without costly large-scale annotation.
