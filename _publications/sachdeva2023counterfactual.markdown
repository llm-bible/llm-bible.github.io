---
layout: publication
title: Catfood Counterfactual Augmented Training For Improving Out45;of45;domain Performance And Calibration
authors: Sachdeva Rachneet, Tutek Martin, Gurevych Iryna
conference: "Arxiv"
year: 2023
bibkey: sachdeva2023counterfactual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.07822"}
tags: ['Applications', 'Interpretability And Explainability', 'Prompting', 'Reinforcement Learning', 'Training Techniques']
---
In recent years large language models (LLMs) have shown remarkable capabilities at scale particularly at generating text conditioned on a prompt. In our work we investigate the use of LLMs to augment training data of small language models~(SLMs) with automatically generated counterfactual~(CF) instances 45;45; i.e. minimally altered inputs 45;45; in order to improve out45;of45;domain~(OOD) performance of SLMs in the extractive question answering~(QA) setup. We show that across various LLM generators such data augmentation consistently enhances OOD performance and improves model calibration for both confidence45;based and rationale45;augmented calibrator models. Furthermore these performance improvements correlate with higher diversity of CF instances in terms of their surface form and semantic content. Finally we show that CF augmented models which are easier to calibrate also exhibit much lower entropy when assigning importance indicating that rationale45;augmented calibrators prefer concise explanations.
