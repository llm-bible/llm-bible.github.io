---
layout: publication
title: Turkishmmlu Measuring Massive Multitask Language Understanding In Turkish
authors: Yüksel Arda, Köksal Abdullatif, Şenel Lütfi Kerem, Korhonen Anna, Schütze Hinrich
conference: "Arxiv"
year: 2024
bibkey: yüksel2024measuring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.12402"}
  - {name: "Code", url: "https://github.com/ArdaYueksel/TurkishMMLU"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Has Code', 'Model Architecture']
---
Multiple choice question answering tasks evaluate the reasoning comprehension and mathematical abilities of Large Language Models (LLMs). While existing benchmarks employ automatic translation for multilingual evaluation this approach is error45;prone and potentially introduces culturally biased questions especially in social sciences. We introduce the first multitask multiple45;choice Turkish QA benchmark TurkishMMLU to evaluate LLMs understanding of the Turkish language. TurkishMMLU includes over 10000 questions covering 9 different subjects from Turkish high45;school education curricula. These questions are written by curriculum experts suitable for the high45;school curricula in Turkey covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic. We evaluate over 20 LLMs including multilingual open45;source (e.g. Gemma Llama MT5) closed45;source (GPT 4o Claude Gemini) and Turkish45;adapted (e.g. Trendyol) models. We provide an extensive evaluation including zero45;shot and few45;shot evaluation of LLMs chain45;of45;thought reasoning and question difficulty analysis along with model performance. We provide an in45;depth analysis of the Turkish capabilities and limitations of current LLMs to provide insights for future LLMs for the Turkish language. We publicly release our code for the dataset and evaluation https://github.com/ArdaYueksel/TurkishMMLU.
