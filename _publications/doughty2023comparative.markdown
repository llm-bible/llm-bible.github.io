---
layout: publication
title: A Comparative Study Of Ai45;generated (GPT45;4) And Human45;crafted Mcqs In Programming Education
authors: Doughty Jacob, Wan Zipiao, Bompelli Anishka, Qayum Jubahed, Wang Taozhi, Zhang Juran, Zheng Yujia, Doyle Aidan, Sridhar Pragnya, Agarwal Arav, Bogart Christopher, Keylor Eric, Kultur Can, Savelka Jaromir, Sakr Majd
conference: "Arxiv"
year: 2023
bibkey: doughty2023comparative
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.03173"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning']
---
There is a constant need for educators to develop and maintain effective up45;to45;date assessments. While there is a growing body of research in computing education on utilizing large language models (LLMs) in generation and engagement with coding exercises the use of LLMs for generating programming MCQs has not been extensively explored. We analyzed the capability of GPT45;4 to produce multiple45;choice questions (MCQs) aligned with specific learning objectives (LOs) from Python programming classes in higher education. Specifically we developed an LLM45;powered (GPT45;4) system for generation of MCQs from high45;level course context and module45;level LOs. We evaluated 651 LLM45;generated and 449 human45;crafted MCQs aligned to 246 LOs from 6 Python courses. We found that GPT45;4 was capable of producing MCQs with clear language a single correct choice and high45;quality distractors. We also observed that the generated MCQs appeared to be well45;aligned with the LOs. Our findings can be leveraged by educators wishing to take advantage of the state45;of45;the45;art generative models to support MCQ authoring efforts.
