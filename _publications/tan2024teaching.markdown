---
layout: publication
title: 'Teaching-inspired Integrated Prompting Framework: A Novel Approach For Enhancing Reasoning In Large Language Models'
authors: Wenting Tan, Dongxiao Chen, Jieting Xue, Zihao Wang, Taijie Chen
conference: "Arxiv"
year: 2024
bibkey: tan2024teaching
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.08068"}
  - {name: "Code", url: "https://github.com/SallyTan13/Teaching-Inspired-Prompting"}
tags: ['Tools', 'GPT', 'Interpretability and Explainability', 'Model Architecture', 'Reinforcement Learning', 'Has Code', 'Prompting']
---
Large Language Models (LLMs) exhibit impressive performance across various
domains but still struggle with arithmetic reasoning tasks. Recent work shows
the effectiveness of prompt design methods in enhancing reasoning capabilities.
However, these approaches overlook crucial requirements for prior knowledge of
specific concepts, theorems, and tricks to tackle most arithmetic reasoning
problems successfully. To address this issue, we propose a novel and effective
Teaching-Inspired Integrated Framework, which emulates the instructional
process of a teacher guiding students. This method equips LLMs with essential
concepts, relevant theorems, and similar problems with analogous solution
approaches, facilitating the enhancement of reasoning abilities. Additionally,
we introduce two new Chinese datasets, MathMC and MathToF, both with detailed
explanations and answers. Experiments are conducted on nine benchmarks which
demonstrates that our approach improves the reasoning accuracy of LLMs. With
GPT-4 and our framework, we achieve new state-of-the-art performance on four
math benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2%
(+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%). Our data and code are
available at https://github.com/SallyTan13/Teaching-Inspired-Prompting.
