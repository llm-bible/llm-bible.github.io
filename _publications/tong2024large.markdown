---
layout: publication
title: Cpsdbench A Large Language Model Evaluation Benchmark And Baseline For Chinese Public Security Domain
authors: Tong Xin, Jin Bo, Lin Zhi, Wang Binjun, Yu Ting, Cheng Qiang
conference: "Arxiv"
year: 2024
bibkey: tong2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.07234"}
tags: ['Applications', 'Language Modeling', 'Reinforcement Learning', 'Security']
---
Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream LLMs in public security tasks this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain45;45;CPSDbench. CPSDbench integrates datasets related to public security collected from real45;world scenarios supporting a comprehensive assessment of LLMs across four key dimensions text classification information extraction question answering and text generation. Furthermore this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security. Through the in45;depth analysis and evaluation conducted in this research we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the future development of more accurate and customized LLM models targeted at applications in this field.
