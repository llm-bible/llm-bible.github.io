---
layout: publication
title: Novelqa Benchmarking Question Answering On Documents Exceeding 200K Tokens
authors: Wang Cunxiang, Ning Ruoxi, Pan Boqi, Wu Tonghui, Guo Qipeng, Deng Cheng, Bao Guangsheng, Hu Xiangkun, Zhang Zheng, Wang Qian, Zhang Yue
conference: "Arxiv"
year: 2024
bibkey: wang2024novelqa
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.12766"}
tags: ['Applications', 'RAG', 'Reinforcement Learning', 'Tools']
---
The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing particularly in understanding and processing long-context information. However the evaluation of these models long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap we introduce NovelQA a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels NovelQA offers a unique blend of complexity length and narrative coherence making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA highlighting its manual annotation and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models performance particularly emphasizing the challenges they face with multi-hop reasoning detail-oriented questions and extremely long input with an average length more than 200000 tokens. The results underscore the necessity for further advancements in LLMs to improve their long-context comprehension.
