---
layout: publication
title: 'QUENCH: Measuring The Gap Between Indic And Non-indic Contextual General Reasoning In Llms'
authors: Mohammad Aflah Khan, Neemesh Yadav, Sarah Masud, Md. Shad Akhtar
conference: "Arxiv"
year: 2024
bibkey: khan2024measuring
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.11763'}
tags: ['Reinforcement Learning', 'Prompting']
---
The rise of large language models (LLMs) has created a need for advanced
benchmarking systems beyond traditional setups. To this end, we introduce
QUENCH, a novel text-based English Quizzing Benchmark manually curated and
transcribed from YouTube quiz videos. QUENCH possesses masked entities and
rationales for the LLMs to predict via generation. At the intersection of
geographical context and common sense reasoning, QUENCH helps assess world
knowledge and deduction capabilities of LLMs via a zero-shot, open-domain
quizzing setup. We perform an extensive evaluation on 7 LLMs and 4 metrics,
investigating the influence of model size, prompting style, geographical
context, and gold-labeled rationale generation. The benchmarking concludes with
an error analysis to which the LLMs are prone.
