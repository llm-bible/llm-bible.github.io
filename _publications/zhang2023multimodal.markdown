---
layout: publication
title: Multimodal Chain45;of45;thought Reasoning In Language Models
authors: Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola
conference: "Arxiv"
year: 2023
bibkey: zhang2023multimodal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2302.00923v5"}
  - {name: "Code", url: "https://github.com/amazon&#45;science/mm&#45;cot"}
tags: ['Has Code', 'Multimodal Models', 'Prompting', 'RAG', 'Tools']
---
Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain45;of45;thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However existing CoT studies have primarily focused on the language modality. We propose Multimodal45;CoT that incorporates language (text) and vision (images) modalities into a two45;stage framework that separates rationale generation and answer inference. In this way answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A45;OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal45;CoT our model under 1 billion parameters achieves state45;of45;the45;art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal45;CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon&#45;science/mm&#45;cot.
