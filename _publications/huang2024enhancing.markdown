---
layout: publication
title: 'Orca: Enhancing Role-playing Abilities Of Large Language Models By Integrating Personality Traits'
authors: Yuxuan Huang
conference: "Arxiv"
year: 2024
bibkey: huang2024enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.10006"}
  - {name: "Code", url: "https://github.com/Aipura/Orca"}
tags: ['Agentic', 'Training Techniques', 'Tools', 'RAG', 'Has Code', 'Prompting']
---
Large language models has catalyzed the development of personalized dialogue
systems, numerous role-playing conversational agents have emerged. While
previous research predominantly focused on enhancing the model's capability to
follow instructions by designing character profiles, neglecting the
psychological factors that drive human conversations. In this paper, we propose
Orca, a framework for data processing and training LLMs of custom characters by
integrating personality traits. Orca comprises four stages: (1) Personality
traits inferring, leverage LLMs to infer user's BigFive personality trait
reports and scores. (2) Data Augment, simulate user's profile, background
story, and psychological activities. (3) Dataset construction,
personality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4)
Modeling and Training, personality-conditioned instruction tuning (PTIT and
PSIT), using the generated data to enhance existing open-source LLMs. We
introduce OrcaBench, the first benchmark for evaluating the quality of content
generated by LLMs on social platforms across multiple scales. Our experiments
demonstrate that our proposed model achieves superior performance on this
benchmark, demonstrating its excellence and effectiveness in perceiving
personality traits that significantly improve role-playing abilities. Our Code
is available at https://github.com/Aipura/Orca.
