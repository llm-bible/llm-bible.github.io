---
layout: publication
title: 'Single- Vs. Dual-prompt Dialogue Generation With Llms For Job Interviews In Human Resources'
authors: Joachim De Baer, A. Seza Doğruöz, Thomas Demeester, Chris Develder
conference: "Arxiv"
year: 2025
bibkey: debaer2025single
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.18650'}
tags: ['Agentic', 'GPT', 'Prompting', 'Model Architecture']
---
Optimizing language models for use in conversational agents requires large
quantities of example dialogues. Increasingly, these dialogues are
synthetically generated by using powerful large language models (LLMs),
especially in domains with challenges to obtain authentic human data. One such
domain is human resources (HR). In this context, we compare two LLM-based
dialogue generation methods for the use case of generating HR job interviews,
and assess whether one method generates higher-quality dialogues that are more
challenging to distinguish from genuine human discourse. The first method uses
a single prompt to generate the complete interview dialog. The second method
uses two agents that converse with each other. To evaluate dialogue quality
under each method, we ask a judge LLM to determine whether AI was used for
interview generation, using pairwise interview comparisons. We demonstrate that
despite a sixfold increase in token cost, interviews generated with the
dual-prompt method achieve a win rate up to ten times higher than those
generated with the single-prompt method. This difference remains consistent
regardless of whether GPT-4o or Llama 3.3 70B is used for either interview
generation or judging quality.
