---
layout: publication
title: 'Multi-way, Multilingual Neural Machine Translation With A Shared Attention Mechanism'
authors: Firat Orhan, Cho Kyunghyun, Bengio Yoshua
conference: "Arxiv"
year: 2016
citations: 550
bibkey: firat2016multi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1601.01073"}
tags: ['Model Architecture', 'Attention Mechanism', 'Transformer', 'Applications', 'Reinforcement Learning', 'WMT']
---
We propose multi-way, multilingual neural machine translation. The proposed
approach enables a single neural translation model to translate between
multiple languages, with a number of parameters that grows only linearly with
the number of languages. This is made possible by having a single attention
mechanism that is shared across all language pairs. We train the proposed
multi-way, multilingual model on ten language pairs from WMT'15 simultaneously
and observe clear performance improvements over models trained on only one
language pair. In particular, we observe that the proposed model significantly
improves the translation quality of low-resource language pairs.
