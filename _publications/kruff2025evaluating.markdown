---
layout: publication
title: 'Evaluating Contrastive Feedback For Effective User Simulations'
authors: Andreas Konstantin Kruff, Timo Breuer, Philipp Schaer
conference: "Arxiv"
year: 2025
bibkey: kruff2025evaluating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.02560'}
tags: ['Agentic', 'RAG', 'Training Techniques', 'Applications', 'Fine-Tuning', 'Prompting', 'Reinforcement Learning', 'Pretraining Methods']
---
The use of Large Language Models (LLMs) for simulating user behavior in the
domain of Interactive Information Retrieval has recently gained significant
popularity. However, their application and capabilities remain highly debated
and understudied. This study explores whether the underlying principles of
contrastive training techniques, which have been effective for fine-tuning
LLMs, can also be applied beneficially in the area of prompt engineering for
user simulations.
  Previous research has shown that LLMs possess comprehensive world knowledge,
which can be leveraged to provide accurate estimates of relevant documents.
This study attempts to simulate a knowledge state by enhancing the model with
additional implicit contextual information gained during the simulation. This
approach enables the model to refine the scope of desired documents further.
The primary objective of this study is to analyze how different modalities of
contextual information influence the effectiveness of user simulations.
  Various user configurations were tested, where models are provided with
summaries of already judged relevant, irrelevant, or both types of documents in
a contrastive manner. The focus of this study is the assessment of the impact
of the prompting techniques on the simulated user agent performance. We hereby
lay the foundations for leveraging LLMs as part of more realistic simulated
users.
