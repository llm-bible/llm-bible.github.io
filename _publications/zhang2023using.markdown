---
layout: publication
title: LLM4DV Using Large Language Models For Hardware Test Stimuli Generation
authors: Zhang Zixi, Chadwick Greg, Mcnally Hugo, Zhao Yiren, Mullins Robert
conference: "Arxiv"
year: 2023
bibkey: zhang2023using
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.04535"}
tags: ['Applications', 'Efficiency And Optimization', 'Prompting', 'RAG', 'Reinforcement Learning', 'Tools']
---
Test stimuli generation has been a crucial but labor45;intensive task in hardware design verification. In this paper we revolutionize this process by harnessing the power of large language models (LLMs) and present a novel benchmarking framework LLM4DV. This framework introduces a prompt template for interactively eliciting test stimuli from the LLM along with four innovative prompting improvements to support the pipeline execution and further enhance its performance. We compare LLM4DV to traditional constrained45;random testing (CRT) using three self45;designed design45;under45;test (DUT) modules. Experiments demonstrate that LLM4DV excels in efficiently handling straightforward DUT scenarios leveraging its ability to employ basic mathematical reasoning and pre45;trained knowledge. While it exhibits reduced efficiency in complex task settings it still outperforms CRT in relative terms. The proposed framework and the DUT modules used in our experiments will be open45;sourced upon publication.
