---
layout: publication
title: History45;aware Hierarchical Transformer For Multi45;session Open45;domain Dialogue System
authors: Zhang Tong, Liu Yong, Li Boyang, Zeng Zhiwei, Wang Pengwei, You Yuan, Miao Chunyan, Cui Lizhen
conference: "Arxiv"
year: 2023
bibkey: zhang2023history
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2302.00907"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Transformer']
---
With the evolution of pre45;trained language models current open45;domain dialogue systems have achieved great progress in conducting one45;session conversations. In contrast Multi45;Session Conversation (MSC) which consists of multiple sessions over a long term with the same user is under45;investigated. In this paper we propose History45;Aware Hierarchical Transformer (HAHT) for multi45;session open45;domain dialogue. HAHT maintains a long45;term memory of history conversations and utilizes history information to understand current conversation context and generate well45;informed and context45;relevant responses. Specifically HAHT first encodes history conversation sessions hierarchically into a history memory. Then HAHT leverages historical information to facilitate the understanding of the current conversation context by encoding the history memory together with the current context with attention45;based mechanisms. Finally to explicitly utilize historical information HAHT uses a history45;aware response generator that switches between a generic vocabulary and a history45;aware vocabulary. Experimental results on a large45;scale MSC dataset suggest that the proposed HAHT model consistently outperforms baseline models. Human evaluation results support that HAHT generates more human45;like context45;relevant and history45;relevant responses than baseline models.
