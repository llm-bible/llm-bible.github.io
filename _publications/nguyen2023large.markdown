---
layout: publication
title: Large Language Models For In45;context Student Modeling Synthesizing Students Behavior In Visual Programming
authors: Nguyen Manh Hung, Tschiatschek Sebastian, Singla Adish
conference: "Arxiv"
year: 2023
bibkey: nguyen2023large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.10690"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Tools']
---
Student modeling is central to many educational technologies as it enables predicting future learning outcomes and designing targeted instructional strategies. However open45;ended learning domains pose challenges for accurately modeling students due to the diverse behaviors and a large space of possible misconceptions. To approach these challenges we explore the application of large language models (LLMs) for in45;context student modeling in open45;ended learning domains. More concretely given a particular students attempt on a reference task as observation the objective is to synthesize the students attempt on a target task. We introduce a novel framework LLM for Student Synthesis (LLM45;SS) that leverages LLMs for synthesizing a students behavior. Our framework can be combined with different LLMs; moreover we fine45;tune LLMs to boost their student modeling capabilities. We instantiate several methods based on LLM45;SS framework and evaluate them using an existing benchmark StudentSyn for student attempt synthesis in a visual programming domain. Experimental results show that our methods perform significantly better than the baseline method NeurSS provided in the StudentSyn benchmark. Furthermore our method using a fine45;tuned version of the GPT45;3.5 model is significantly better than using the base GPT45;3.5 model and gets close to human tutors performance.
