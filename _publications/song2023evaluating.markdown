---
layout: publication
title: Nlpbench Evaluating Large Language Models On Solving NLP Problems
authors: Song Linxin, Zhang Jieyu, Cheng Lechao, Zhou Pengyuan, Zhou Tianyi, Li Irene
conference: "Arxiv"
year: 2023
bibkey: song2023evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.15630"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting']
---
Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes there remains a dearth of research dedicated to the NLP problem45;solving abilities of LLMs. To fill the gap in this area we present a unique benchmarking dataset NLPBench comprising 378 college45;level NLP questions spanning various NLP topics sourced from Yale Universitys prior final exams. NLPBench includes questions with context in which multiple sub45;questions share the same public information and diverse question types including multiple choice short answer and math. Our evaluation centered on LLMs such as GPT45;3.5/4 PaLM45;2 and LLAMA45;2 incorporates advanced prompting strategies like the chain45;of45;thought (CoT) and tree45;of45;thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent occasionally damaging LLM performance especially in smaller models like the LLAMA45;2 (13b). Furthermore our manual assessment illuminated specific shortcomings in LLMs scientific problem45;solving skills with weaknesses in logical decomposition and reasoning notably affecting results.
