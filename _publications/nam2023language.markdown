---
layout: publication
title: 'Language-oriented Communication With Semantic Coding And Knowledge Distillation For Text-to-image Generation'
authors: Hyelin Nam, Jihong Park, Jinho Choi, Mehdi Bennis, Seong-lyun Kim
conference: "Arxiv"
year: 2023
bibkey: nam2023language
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2309.11127'}
tags: ['Efficiency and Optimization', 'Distillation', 'Tools', 'Security', 'Merging', 'Prompting']
---
By integrating recent advances in large language models (LLMs) and generative
models into the emerging semantic communication (SC) paradigm, in this article
we put forward to a novel framework of language-oriented semantic communication
(LSC). In LSC, machines communicate using human language messages that can be
interpreted and manipulated via natural language processing (NLP) techniques
for SC efficiency. To demonstrate LSC's potential, we introduce three
innovative algorithms: 1) semantic source coding (SSC) which compresses a text
prompt into its key head words capturing the prompt's syntactic essence while
maintaining their appearance order to keep the prompt's context; 2) semantic
channel coding (SCC) that improves robustness against errors by substituting
head words with their lenghthier synonyms; and 3) semantic knowledge
distillation (SKD) that produces listener-customized prompts via in-context
learning the listener's language style. In a communication task for progressive
text-to-image generation, the proposed methods achieve higher perceptual
similarities with fewer transmissions while enhancing robustness in noisy
communication channels.
