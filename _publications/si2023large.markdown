---
layout: publication
title: Large Language Models Help Humans Verify Truthfulness 45;45; Except When They Are Convincingly Wrong
authors: Si Chenglei, Goyal Navita, Wu Sherry Tongshuang, Zhao Chen, Feng Shi, Daum√© Hal Iii, Boyd-graber Jordan
conference: "Arxiv"
year: 2023
bibkey: si2023large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.12558"}
tags: ['Applications', 'Interpretability And Explainability', 'Prompting']
---
Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they get LLMs should not only provide information but also help users fact45;check it. Our experiments with 80 crowdworkers compare language models with search engines (information retrieval systems) at facilitating fact45;checking. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than those using search engines while achieving similar accuracy. However they over45;rely on the LLMs when the explanation is wrong. To reduce over45;reliance on LLMs we ask LLMs to provide contrastive information 45; explain both why the claim is true and false and then we present both sides of the explanation to users. This contrastive explanation mitigates users over45;reliance on LLMs but cannot significantly outperform search engines. Further showing both search engine results and LLM explanations offers no complementary benefits compared to search engines alone. Taken together our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages especially in high45;stakes settings where over45;relying on wrong AI explanations could lead to critical consequences.
