---
layout: publication
title: 'Semeval-2025 Task 4: Unlearning Sensitive Content From Large Language Models'
authors: Anil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-wei Chang, Zhiqi Bu, Bhanukiran Vinzamuri, Volkan Cevher, Mingyi Hong, Rahul Gupta
conference: "Arxiv"
year: 2025
bibkey: ramakrishna2025semeval
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.02883'}
tags: ['Training Techniques', 'Applications']
---
We introduce SemEval-2025 Task 4: unlearning sensitive content from Large
Language Models (LLMs). The task features 3 subtasks for LLM unlearning
spanning different use cases: (1) unlearn long form synthetic creative
documents spanning different genres; (2) unlearn short form synthetic
biographies containing personally identifiable information (PII), including
fake names, phone number, SSN, email and home addresses, and (3) unlearn real
documents sampled from the target model's training dataset. We received over
100 submissions from over 30 institutions and we summarize the key techniques
and lessons in this paper.
