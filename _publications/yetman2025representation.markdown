---
layout: publication
title: 'Representation In Large Language Models'
authors: Cameron C. Yetman
conference: "Arxiv"
year: 2025
bibkey: yetman2025representation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.00885"}
tags: ['Interpretability and Explainability']
---
The extraordinary success of recent Large Language Models (LLMs) on a diverse
array of tasks has led to an explosion of scientific and philosophical
theorizing aimed at explaining how they do what they do. Unfortunately,
disagreement over fundamental theoretical issues has led to stalemate, with
entrenched camps of LLM optimists and pessimists often committed to very
different views of how these systems work. Overcoming stalemate requires
agreement on fundamental questions, and the goal of this paper is to address
one such question, namely: is LLM behavior driven partly by
representation-based information processing of the sort implicated in
biological cognition, or is it driven entirely by processes of memorization and
stochastic table look-up? This is a question about what kind of algorithm LLMs
implement, and the answer carries serious implications for higher level
questions about whether these systems have beliefs, intentions, concepts,
knowledge, and understanding. I argue that LLM behavior is partially driven by
representation-based information processing, and then I describe and defend a
series of practical techniques for investigating these representations and
developing explanations on their basis. The resulting account provides a
groundwork for future theorizing about language models and their successors.
