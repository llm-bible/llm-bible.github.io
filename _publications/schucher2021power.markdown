---
layout: publication
title: The Power Of Prompt Tuning For Low45;resource Semantic Parsing
authors: Schucher Nathan, Reddy Siva, De Vries Harm
conference: "Arxiv"
year: 2021
bibkey: schucher2021power
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2110.08525"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Training Techniques']
---
Prompt tuning has recently emerged as an effective method for adapting pre45;trained language models to a number of language understanding and generation tasks. In this paper we investigate prompt tuning for semantic parsing 45;45; the task of mapping natural language utterances onto formal meaning representations. On the low45;resource splits of Overnight and TOPv2 we find that a prompt tuned T545;xl significantly outperforms its fine45;tuned counterpart as well as strong GPT45;3 and BART baselines. We also conduct ablation studies across different model scales and target representations finding that with increasing model scale prompt tuned T5 models improve at generating target representations that are far from the pre45;training distribution.
