---
layout: publication
title: Augmented Large Language Models With Parametric Knowledge Guiding
authors: Luo Ziyang, Xu Can, Zhao Pu, Geng Xiubo, Tao Chongyang, Ma Jing, Lin Qingwei, Jiang Daxin
conference: "Arxiv"
year: 2023
bibkey: luo2023augmented
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.04757"}
tags: ['Ethics And Bias', 'Multimodal Models', 'Tools']
---
Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However their performance may be suboptimal for domain45;specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally the lack of transparency of most state45;of45;the45;art (SOTA) LLMs which can only be accessed via APIs impedes further fine45;tuning with domain custom data. Moreover providing private data to the LLMs owner leads to data privacy problems. To address these challenges we propose the novel Parametric Knowledge Guiding (PKG) framework which equips LLMs with a knowledge45;guiding module to access relevant knowledge without altering the LLMs parameters. Our PKG is based on open45;source white45;box language models allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of black45;box LLMs on a range of domain knowledge45;intensive tasks that require factual (+7.937;) tabular (+11.937;) medical (+3.037;) and multimodal (+8.137;) knowledge.
