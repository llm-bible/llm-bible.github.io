---
layout: publication
title: 'Realitysummary: Exploring On-demand Mixed Reality Text Summarization And Question Answering Using Large Language Models'
authors: Aditya Gunturu, Shivesh Jadon, Nandi Zhang, Morteza Faraji, Jarin Thundathil, Tafreed Ahmad, Wesley Willett, Ryo Suzuki
conference: "Arxiv"
year: 2024
bibkey: gunturu2024exploring
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2405.18620'}
tags: ['Reinforcement Learning', 'Applications', 'Tools']
---
Large Language Models (LLMs) are gaining popularity as tools for reading and
summarization aids. However, little is known about their potential benefits
when integrated with mixed reality (MR) interfaces to support everyday reading
assistants. We developed RealitySummary, an MR reading assistant that
seamlessly integrates LLMs with always-on camera access, OCR-based text
extraction, and augmented spatial and visual responses in MR interfaces.
Developed iteratively, RealitySummary evolved across three versions, each
shaped by user feedback and reflective analysis: 1) a preliminary user study to
understand user perceptions (N=12), 2) an in-the-wild deployment to explore
real-world usage (N=11), and 3) a diary study to capture insights from
real-world work contexts (N=5). Our findings highlight the unique advantages of
combining AI and MR, including an always-on implicit assistant, minimal context
switching, and spatial affordances, demonstrating significant potential for
future LLM-MR interfaces beyond traditional screen-based interactions.
