---
layout: publication
title: Prompting Large Vision45;language Models For Compositional Reasoning
authors: Ossowski Timothy, Jiang Ming, Hu Junjie
conference: "Arxiv"
year: 2024
bibkey: ossowski2024prompting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.11337"}
tags: ['Fine Tuning', 'GPT', 'Model Architecture', 'Multimodal Models', 'Prompting']
---
Vision45;language models such as CLIP have shown impressive capabilities in encoding texts and images into aligned embeddings enabling the retrieval of multimodal data in a shared embedding space. However these embedding45;based models still face challenges in effectively matching images and texts with similar visio45;linguistic compositionality as evidenced by their performance on the recent Winoground dataset. In this paper we argue that this limitation stems from two factors the use of single vector representations for complex multimodal data and the absence of step45;by45;step reasoning in these embedding45;based methods. To address this issue we make an exploratory step using a novel generative method that prompts large vision45;language models (e.g. GPT45;4) to depict images and perform compositional reasoning. Our method outperforms other embedding45;based methods on the Winoground dataset and obtains further improvement of up to 1037; accuracy when enhanced with the optimal description.
