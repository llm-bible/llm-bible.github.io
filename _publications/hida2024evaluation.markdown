---
layout: publication
title: Evaluation Of Instruction45;following Ability For Large Language Models On Story45;ending Generation
authors: Hida Rem, Ohmura Junki, Sekiya Toshiyuki
conference: "Arxiv"
year: 2024
bibkey: hida2024evaluation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.16356"}
tags: ['Applications', 'GPT', 'Model Architecture']
---
Instruction45;tuned Large Language Models (LLMs) have achieved remarkable performance across various benchmark tasks. While providing instructions to LLMs for guiding their generations is user45;friendly assessing their instruction45;following capabilities is still unclarified due to a lack of evaluation metrics. In this paper we focus on evaluating the instruction45;following ability of LLMs in the context of story45;ending generation which requires diverse and context45;specific instructions. We propose an automatic evaluation pipeline that utilizes a machine reading comprehension (MRC) model to determine whether the generated story45;ending reflects instruction. Our findings demonstrate that our proposed metric aligns with human evaluation. Furthermore our experiments confirm that recent open45;source LLMs can achieve instruction45;following performance close to GPT45;3.5 as assessed through automatic evaluation.
