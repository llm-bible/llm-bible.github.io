---
layout: publication
title: SynCPKL Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
authors: Lin Kuan-yen
conference: "Arxiv"
year: 2024
bibkey: lin2024syncpkl
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.15281"}
  - {name: "Code", url: "https://github.com/irislin1006/CPKL"}
tags: ['ARXIV', 'Applications', 'BERT', 'Has Code', 'LLM', 'Model Architecture', 'NLP', 'RAG', 'Security', 'Tools']
---
Understanding rich dialogues often requires NLP systems to access relevant commonsense persona knowledge but retrieving this knowledge is challenging due to complex contexts and the implicit nature of commonsense. This paper presents our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge addressing the critical need for integrating persona and commonsense knowledge in open-domain dialogue systems. We introduce SynCPKL Pipeline a pipeline that leverages Large Language Models to generate high-quality synthetic datasets for training commonsense persona knowledge linkers. To demonstrate the efficacy of our approach we present SynCPKL a new dataset specifically designed for this task. Our experiments validate the effectiveness of SynCPKL for training commonsense persona knowledge linkers. Additionally our top-performing model Derberta-SynCPKL secured first place in the CPKL challenge by a 16 improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at https://github.com/irislin1006/CPKL.
