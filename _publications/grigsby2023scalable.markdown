---
layout: publication
title: AMAGO Scalable In45;context Reinforcement Learning For Adaptive Agents
authors: Grigsby Jake, Fan Linxi, Zhu Yuke
conference: "Arxiv"
year: 2023
bibkey: grigsby2023scalable
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.09971"}
tags: ['Agentic', 'Fine Tuning', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
We introduce AMAGO an in45;context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization long45;term memory and meta45;learning. Recent works have shown that off45;policy learning can make in45;context RL with recurrent policies viable. Nonetheless these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents memory capacity planning horizon and model size. AMAGO revisits and redesigns the off45;policy in45;context approach to successfully train long45;sequence Transformers over entire rollouts in parallel with end45;to45;end RL. Our agent is scalable and applicable to a wide range of problems and we demonstrate its strong performance empirically in meta45;RL and long45;term memory domains. AMAGOs focus on sparse rewards and off45;policy data also allows in45;context learning to extend to goal45;conditioned problems with challenging exploration. When combined with a multi45;goal hindsight relabeling scheme AMAGO can solve a previously difficult category of open45;world domains where agents complete many possible instructions in procedurally generated environments.
