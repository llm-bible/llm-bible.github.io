---
layout: publication
title: 'Trafficsafetygpt: Tuning A Pre-trained Large Language Model To A Domain-specific Expert In Transportation Safety'
authors: Ou Zheng, Mohamed Abdel-aty, Dongdong Wang, Chenzhu Wang, Shengxuan Ding
conference: "Arxiv"
year: 2023
bibkey: zheng2023tuning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.15311"}
  - {name: "Code", url: "https://github.com/ozheng1993/TrafficSafetyGPT"}
tags: ['Responsible AI', 'Training Techniques', 'Model Architecture', 'GPT', 'Pretraining Methods', 'Fine-Tuning', 'Has Code']
---
Large Language Models (LLMs) have shown remarkable effectiveness in various
general-domain natural language processing (NLP) tasks. However, their
performance in transportation safety domain tasks has been suboptimal,
primarily attributed to the requirement for specialized transportation safety
expertise in generating accurate responses [1]. To address this challenge, we
introduce TrafficSafetyGPT, a novel LLAMA-based model, which has undergone
supervised fine-tuning using TrafficSafety-2K dataset which has human labels
from government produced guiding books and ChatGPT-generated instruction-output
pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset
are accessible at https://github.com/ozheng1993/TrafficSafetyGPT.
