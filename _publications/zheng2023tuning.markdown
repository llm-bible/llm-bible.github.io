---
layout: publication
title: Trafficsafetygpt Tuning A Pre-trained Large Language Model To A Domain-specific Expert In Transportation Safety
authors: Zheng Ou, Abdel-aty Mohamed, Wang Dongdong, Wang Chenzhu, Ding Shengxuan
conference: "Arxiv"
year: 2023
bibkey: zheng2023tuning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.15311"}
  - {name: "Code", url: "https://github.com/ozheng1993/TrafficSafetyGPT"}
tags: ['Fine Tuning', 'GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Responsible AI', 'Training Techniques']
---
Large Language Models (LLMs) have shown remarkable effectiveness in various general-domain natural language processing (NLP) tasks. However their performance in transportation safety domain tasks has been suboptimal primarily attributed to the requirement for specialized transportation safety expertise in generating accurate responses 1. To address this challenge we introduce TrafficSafetyGPT a novel LLAMA-based model which has undergone supervised fine-tuning using TrafficSafety-2K dataset which has human labels from government produced guiding books and ChatGPT-generated instruction-output pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset are accessible at https://github.com/ozheng1993/TrafficSafetyGPT.
