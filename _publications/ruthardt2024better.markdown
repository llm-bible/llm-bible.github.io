---
layout: publication
title: 'Better Language Models Exhibit Higher Visual Alignment'
authors: Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano
conference: "Arxiv"
year: 2024
bibkey: ruthardt2024better
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.07173"}
tags: ['Security', 'Tools', 'Multimodal Models', 'Reinforcement Learning']
---
How well do text-only Large Language Models (LLMs) naturally align with the
visual world? We provide the first direct analysis by utilizing frozen text
representations in a discriminative vision-language model framework and
measuring zero-shot generalization on unseen classes. We find decoder-based
LLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs
reliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs
leads to strong gains in cross-lingual settings, where our approach surpasses
CLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves
both robustness and generalization and also significantly reduces the need for
paired data and compute, making vision-language models more accessible and
adaptable.
