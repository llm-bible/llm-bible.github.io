---
layout: publication
title: 'Adding Guardrails To Advanced Chatbots'
authors: Yanchen Wang, Lisa Singh
conference: "Arxiv"
year: 2023
bibkey: wang2023adding
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2306.07500'}
tags: ['Fairness', 'GPT', 'Model Architecture', 'Applications', 'Prompting', 'Survey Paper', 'Bias Mitigation', 'Reinforcement Learning', 'Ethics and Bias']
---
Generative AI models continue to become more powerful. The launch of ChatGPT
in November 2022 has ushered in a new era of AI. ChatGPT and other similar
chatbots have a range of capabilities, from answering student homework
questions to creating music and art. There are already concerns that humans may
be replaced by chatbots for a variety of jobs. Because of the wide spectrum of
data chatbots are built on, we know that they will have human errors and human
biases built into them. These biases may cause significant harm and/or inequity
toward different subpopulations. To understand the strengths and weakness of
chatbot responses, we present a position paper that explores different use
cases of ChatGPT to determine the types of questions that are answered fairly
and the types that still need improvement. We find that ChatGPT is a fair
search engine for the tasks we tested; however, it has biases on both text
generation and code generation. We find that ChatGPT is very sensitive to
changes in the prompt, where small changes lead to different levels of
fairness. This suggests that we need to immediately implement "corrections" or
mitigation strategies in order to improve fairness of these systems. We suggest
different strategies to improve chatbots and also advocate for an impartial
review panel that has access to the model parameters to measure the levels of
different types of biases and then recommends safeguards that move toward
responses that are less discriminatory and more accurate.
