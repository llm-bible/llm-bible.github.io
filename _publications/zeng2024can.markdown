---
layout: publication
title: Can Mllms Perform Text45;to45;image In45;context Learning
authors: Zeng Yuchen, Kang Wonjun, Chen Yicong, Koo Hyung Il, Lee Kangwook
conference: "Arxiv"
year: 2024
bibkey: zeng2024can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.01293"}
  - {name: "Code", url: "https://github.com/UW&#45;Madison&#45;Lee&#45;Lab/CoBSAT"}
tags: ['Applications', 'Has Code', 'Multimodal Models', 'Prompting']
---
The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In45;Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image45;to45;text ICL. However the Text45;to45;Image ICL (T2I45;ICL) with its unique characteristics and potential applications remains underexplored. To address this gap we formally define the task of T2I45;ICL and present CoBSAT the first T2I45;ICL benchmark dataset encompassing ten tasks. Utilizing our dataset to benchmark six state45;of45;the45;art MLLMs we uncover considerable difficulties MLLMs encounter in solving T2I45;ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation and show that strategies such as fine45;tuning and Chain45;of45;Thought prompting help to mitigate these difficulties leading to notable improvements in performance. Our code and dataset are available at https://github.com/UW&#45;Madison&#45;Lee&#45;Lab/CoBSAT.
