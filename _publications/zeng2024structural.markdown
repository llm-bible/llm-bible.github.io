---
layout: publication
title: 'On The Structural Memory Of LLM Agents'
authors: Ruihong Zeng, Jinyuan Fang, Siwei Liu, Zaiqiao Meng
conference: "Arxiv"
year: 2024
bibkey: zeng2024structural
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.15266"}
tags: ['Agentic', 'Applications']
---
Memory plays a pivotal role in enabling large language model~(LLM)-based
agents to engage in complex and long-term interactions, such as question
answering (QA) and dialogue systems. While various memory modules have been
proposed for these tasks, the impact of different memory structures across
tasks remains insufficiently explored. This paper investigates how memory
structures and memory retrieval methods affect the performance of LLM-based
agents. Specifically, we evaluate four types of memory structures, including
chunks, knowledge triples, atomic facts, and summaries, along with mixed memory
that combines these components. In addition, we evaluate three widely used
memory retrieval methods: single-step retrieval, reranking, and iterative
retrieval. Extensive experiments conducted across four tasks and six datasets
yield the following key insights: (1) Different memory structures offer
distinct advantages, enabling them to be tailored to specific tasks; (2) Mixed
memory structures demonstrate remarkable resilience in noisy environments; (3)
Iterative retrieval consistently outperforms other methods across various
scenarios. Our investigation aims to inspire further research into the design
of memory systems for LLM-based agents.
