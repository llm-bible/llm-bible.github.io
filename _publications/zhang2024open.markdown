---
layout: publication
title: Tinyllama An Open-source Small Language Model
authors: Zhang Peiyuan, Zeng Guangtao, Wang Tianduo, Lu Wei
conference: "Arxiv"
year: 2024
bibkey: zhang2024open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.02385"}
  - {name: "Code", url: "https://github.com/jzhang38/TinyLlama"}
tags: ['Attention Mechanism', 'Efficiency And Optimization', 'GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'RAG']
---
We present TinyLlama a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2 TinyLlama leverages various advances contributed by the open-source community (e.g. FlashAttention and Lit-GPT) achieving better computational efficiency. Despite its relatively small size TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.
