---
layout: publication
title: Transformers Go For The Lols Generating (humourous) Titles From Scientific Abstracts End45;to45;end
authors: Chen Yanran, Eger Steffen
conference: "Arxiv"
year: 2022
bibkey: chen2022transformers
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2212.10522"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
We consider the end45;to45;end abstract45;to45;title generation problem exploring seven recent transformer based models (including ChatGPT) fine45;tuned on more than 30k abstract45;title pairs from NLP and machine learning (ML) venues. As an extension we also consider the harder problem of generating humorous paper titles. For the latter we compile the first large45;scale humor annotated dataset for scientific papers in the NLP/ML domains comprising almost ~2.6k titles. We evaluate all models using human and automatic metrics. Our human evaluation suggests that our best end45;to45;end system performs similarly to human authors (but arguably slightly worse). Generating funny titles is more difficult however and our automatic systems clearly underperform relative to humans and often learn dataset artefacts of humor. Finally ChatGPT without any fine45;tuning performs on the level of our best fine45;tuned system.
