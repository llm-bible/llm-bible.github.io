---
layout: publication
title: Lateval&#58; An Interactive Llms Evaluation Benchmark With Incomplete Information From Lateral Thinking Puzzles
authors: Huang Shulin, Ma Shirong, Li Yinghui, Huang Mengzuo, Zou Wuhe, Zhang Weidong, Zheng Hai-tao
conference: "Arxiv"
year: 2023
bibkey: huang2023interactive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.10855"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
With the continuous evolution and refinement of LLMs they are endowed with impressive logical reasoning or vertical thinking capabilities. But can they think out of the box Do they possess proficient lateral thinking abilities Following the setup of Lateral Thinking Puzzles we propose a novel evaluation benchmark LatEval which assesses the models lateral thinking within an interactive framework. In our benchmark we challenge LLMs with 2 aspects the quality of questions posed by the model and the models capability to integrate information for problem-solving. We find that nearly all LLMs struggle with employing lateral thinking during interactions. For example even the most advanced model GPT-4 exhibits the advantage to some extent yet still maintain a noticeable gap when compared to human. This evaluation benchmark provides LLMs with a highly challenging and distinctive task that is crucial to an effective AI assistant.
