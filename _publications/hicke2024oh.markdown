---
layout: publication
title: '[lions: 1] And [tigers: 2] And [bears: 3], Oh My! Literary Coreference Annotation With Llms'
authors: Hicke Rebecca M. M., Mimno David
conference: "Arxiv"
year: 2024
bibkey: hicke2024oh
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.17922"}
tags: ['Reinforcement Learning', 'Training Techniques']
---
Coreference annotation and resolution is a vital component of computational
literary studies. However, it has previously been difficult to build high
quality systems for fiction. Coreference requires complicated structured
outputs, and literary text involves subtle inferences and highly varied
language. New language-model-based seq2seq systems present the opportunity to
solve both these problems by learning to directly generate a copy of an input
sentence with markdown-like annotations. We create, evaluate, and release
several trained models for coreference, as well as a workflow for training new
models.
