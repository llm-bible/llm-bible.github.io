---
layout: publication
title: 'Lumina-omnilv: A Unified Multimodal Framework For General Low-level Vision'
authors: Yuandong Pu, Le Zhuo, Kaiwen Zhu, Liangbin Xie, Wenlong Zhang, Xiangyu Chen, Peng Gao, Yu Qiao, Chao Dong, Yihao Liu
conference: "Arxiv"
year: 2025
bibkey: pu2025lumina
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.04903"}
tags: ['Transformer', 'Tools', 'RAG', 'Model Architecture', 'Reinforcement Learning', 'Merging', 'Training Techniques', 'Pretraining Methods', 'Multimodal Models', 'Prompting']
---
We present Lunima-OmniLV (abbreviated as OmniLV), a universal multimodal
multi-task framework for low-level vision that addresses over 100 sub-tasks
across four major categories: image restoration, image enhancement,
weak-semantic dense prediction, and stylization. OmniLV leverages both textual
and visual prompts to offer flexible and user-friendly interactions. Built on
Diffusion Transformer (DiT)-based generative priors, our framework supports
arbitrary resolutions -- achieving optimal performance at 1K resolution --
while preserving fine-grained details and high fidelity. Through extensive
experiments, we demonstrate that separately encoding text and visual
instructions, combined with co-training using shallow feature control, is
essential to mitigate task ambiguity and enhance multi-task generalization. Our
findings also reveal that integrating high-level generative tasks into
low-level vision models can compromise detail-sensitive restoration. These
insights pave the way for more robust and generalizable low-level vision
systems.
