---
layout: publication
title: Alexatm 20B Few45;shot Learning Using A Large45;scale Multilingual Seq2seq Model
authors: Saleh Soltan, Shankar Ananthakrishnan, Jack Fitzgerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan
conference: "Arxiv"
year: 2022
bibkey: soltan2022alexatm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2208.01448v2"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
In this work we demonstrate that multilingual large45;scale sequence45;to45;sequence (seq2seq) models pre45;trained on a mixture of denoising and Causal Language Modeling (CLM) tasks are more efficient few45;shot learners than decoder45;only models on various tasks. In particular we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state45;of45;the45;art (SOTA) performance on 145;shot summarization tasks outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 145;shot machine translation especially for low45;resource languages across almost all language pairs supported by the model (Arabic English French German Hindi Italian Japanese Marathi Portuguese Spanish Tamil and Telugu) on Flores45;101 dataset. We also show in zero45;shot setting AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI XCOPA Paws45;X and XWinograd. Overall our results present a compelling case for seq2seq models as a powerful alternative to decoder45;only models for Large45;scale Language Model (LLM) training.
