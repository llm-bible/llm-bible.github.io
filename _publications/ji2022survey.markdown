---
layout: publication
title: 'Survey Of Hallucination In Natural Language Generation'
authors: Ji Ziwei, Lee Nayeon, Frieske Rita, Yu Tiezheng, Su Dan, Xu Yan, Ishii Etsuko, Bang Yejin, Chen Delong, Dai Wenliang, Chan Ho Shu, Madotto Andrea, Fung Pascale
conference: "ACM Computing Surveys"
year: 2022
bibkey: ji2022survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2202.03629"}
tags: ['Applications', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Survey Paper', 'Transformer']
---
Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG leading to improved development in downstream tasks such as abstractive summarization dialogue generation and data-to-text generation. However it is also apparent that deep learning based generation is prone to hallucinate unintended text which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue many studies have been presented in measuring and mitigating hallucinated texts but these have never been reviewed in a comprehensive manner before. In this survey we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts (1) a general overview of metrics mitigation methods and future directions; (2) an overview of task-specific research progress on hallucinations in the following downstream tasks namely abstractive summarization dialogue generation generative question answering data-to-text generation machine translation and visual-language generation; and (3) hallucinations in large language models (LLMs). This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.
