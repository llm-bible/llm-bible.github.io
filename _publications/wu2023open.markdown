---
layout: publication
title: Openicl An Open45;source Framework For In45;context Learning
authors: Wu Zhenyu, Wang Yaoxiang, Ye Jiacheng, Feng Jiangtao, Xu Jingjing, Qiao Yu, Wu Zhiyong
conference: "Arxiv"
year: 2023
bibkey: wu2023open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2303.02913"}
  - {name: "Code", url: "https://github.com/Shark&#45;NLP/OpenICL"}
tags: ['Applications', 'Attention Mechanism', 'Has Code', 'Model Architecture', 'Tools']
---
In recent years In45;context Learning (ICL) has gained increasing attention and emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine45;tuning methods ICL instead adapts the pre45;trained models to unseen tasks without any parameter updates. However the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved as well as the varying pre45;processing requirements for different models datasets and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components. To facilitate ICL research we introduce OpenICL an open45;source toolkit for ICL and LLM evaluation. OpenICL is research45;friendly with a highly flexible architecture that users can easily combine different components to suit their needs. It also provides various state45;of45;the45;art retrieval and inference methods to streamline the process of adapting ICL to cutting45;edge research. The effectiveness of OpenICL has been validated on a wide range of NLP tasks including classification QA machine translation and semantic parsing. As a side45;product we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released at https://github.com/Shark&#45;NLP/OpenICL
