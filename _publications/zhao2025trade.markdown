---
layout: publication
title: 'Trade-offs In Large Reasoning Models: An Empirical Analysis Of Deliberative And Adaptive Reasoning Over Foundational Capabilities'
authors: Weixiang Zhao, Xingyu Sui, Jiahe Guo, Yulin Hu, Yang Deng, Yanyan Zhao, Bing Qin, Wanxiang Che, Tat-seng Chua, Ting Liu
conference: "Arxiv"
year: 2025
bibkey: zhao2025trade
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.17979'}
tags: ['Reinforcement Learning']
---
Recent advancements in Large Reasoning Models (LRMs), such as OpenAI's o1/o3
and DeepSeek-R1, have demonstrated remarkable performance in specialized
reasoning tasks through human-like deliberative thinking and long
chain-of-thought reasoning. However, our systematic evaluation across various
model families (DeepSeek, Qwen, and LLaMA) and scales (7B to 671B) reveals that
acquiring these deliberative reasoning capabilities significantly reduces the
foundational capabilities of LRMs, including notable declines in helpfulness
and harmlessness, alongside substantially increased inference costs.
Importantly, we demonstrate that adaptive reasoning -- employing modes like
Zero-Thinking, Less-Thinking, and Summary-Thinking -- can effectively alleviate
these drawbacks. Our empirical insights underline the critical need for
developing more versatile LRMs capable of dynamically allocating inference-time
compute according to specific task characteristics.
