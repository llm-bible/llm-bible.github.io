---
layout: publication
title: 'Position: Enough Of Scaling Llms! Lets Focus On Downscaling'
authors: Yash Goel, Ayan Sengupta, Tanmoy Chakraborty
conference: "Arxiv"
year: 2025
bibkey: goel2025enough
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.00985"}
tags: ['Efficiency and Optimization', 'Model Architecture', 'Tools', 'Scaling Laws', 'Reinforcement Learning', 'Large-Scale Training', 'Pre-Training']
---
We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.
