---
layout: publication
title: Text-to-text Pre-training For Data-to-text Tasks
authors: Mihir Kale, Abhinav Rastogi
conference: Arxiv
year: 2020
citations: 48
bibkey: kale2020text
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2005.10433'}]
tags: [Fine-Tuning, Transformer, Language Modeling, Pre-Training, GPT, BERT]
---
We study the pre-train + fine-tune strategy for data-to-text tasks. Our
experiments indicate that text-to-text pre-training in the form of T5, enables
simple, end-to-end transformer based models to outperform pipelined neural
architectures tailored for data-to-text generation, as well as alternative
language model based pre-training techniques such as BERT and GPT-2.
Importantly, T5 pre-training leads to better generalization, as evidenced by
large improvements on out-of-domain test sets. We hope our work serves as a
useful baseline for future research, as transfer learning becomes ever more
prevalent for data-to-text tasks.