---
layout: publication
title: Text45;to45;text Pre45;training For Data45;to45;text Tasks
authors: Kale Mihir, Rastogi Abhinav
conference: "Arxiv"
year: 2020
bibkey: kale2020text
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2005.10433"}
tags: ['Applications', 'BERT', 'Fine Tuning', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
We study the pre45;train + fine45;tune strategy for data45;to45;text tasks. Our experiments indicate that text45;to45;text pre45;training in the form of T5 enables simple end45;to45;end transformer based models to outperform pipelined neural architectures tailored for data45;to45;text generation as well as alternative language model based pre45;training techniques such as BERT and GPT45;2. Importantly T5 pre45;training leads to better generalization as evidenced by large improvements on out45;of45;domain test sets. We hope our work serves as a useful baseline for future research as transfer learning becomes ever more prevalent for data45;to45;text tasks.
