---
layout: publication
title: 'Hallucination Detection And Hallucination Mitigation: An Investigation'
authors: Luo Junliang, Li Tianyu, Wu Di, Jenkin Michael, Liu Steve, Dudek Gregory
conference: "Arxiv"
year: 2024
bibkey: luo2024hallucination
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.08358"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Survey Paper']
---
Large language models (LLMs), including ChatGPT, Bard, and Llama, have achieved remarkable successes over the last two years in a range of different applications. In spite of these successes, there exist concerns that limit the wide application of LLMs. A key problem is the problem of hallucination. Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses. This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation. We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks.
