---
layout: publication
title: 'Understanding Epistemic Language With A Language-augmented Bayesian Theory Of Mind'
authors: Lance Ying, Tan Zhi-xuan, Lionel Wong, Vikash Mansinghka, Joshua B. Tenenbaum
conference: "Arxiv"
year: 2024
bibkey: ying2024understanding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.12022"}
tags: ['Agentic', 'Multimodal Models', 'Model Architecture', 'GPT']
---
How do people understand and evaluate claims about others' beliefs, even
though these beliefs cannot be directly observed? In this paper, we introduce a
cognitive model of epistemic language interpretation, grounded in Bayesian
inferences about other agents' goals, beliefs, and intentions: a
language-augmented Bayesian theory-of-mind (LaBToM). By translating natural
language into an epistemic ``language-of-thought'' with grammar-constrained LLM
decoding, then evaluating these translations against the inferences produced by
inverting a generative model of rational action and perception, LaBToM captures
graded plausibility judgments of epistemic claims. We validate our model in an
experiment where participants watch an agent navigate a maze to find keys
hidden in boxes needed to reach their goal, then rate sentences about the
agent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and
ablated models, our model correlates highly with human judgments for a wide
range of expressions, including modal language, uncertainty expressions,
knowledge claims, likelihood comparisons, and attributions of false belief.
