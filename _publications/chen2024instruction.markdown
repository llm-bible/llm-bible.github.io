---
layout: publication
title: 'Graphwiz: An Instruction-following Language Model For Graph Problems'
authors: Chen Nuo, Li Yuhan, Tang Jianheng, Li Jia
conference: "Arxiv"
year: 2024
bibkey: chen2024instruction
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.16029"}
tags: ['Efficiency And Optimization', 'Fine Tuning', 'GPT', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Large language models (LLMs) have achieved impressive success across several
fields, but their proficiency in understanding and resolving complex graph
problems is less explored. To bridge this gap, we introduce GraphInstruct, a
novel and comprehensive instruction-tuning dataset designed to equip language
models with the ability to tackle a broad spectrum of graph problems using
explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an
open-source language model capable of resolving various graph problem types
while generating clear reasoning processes. To enhance the model's capability
and reliability, we incorporate the Direct Preference Optimization (DPO)
framework into the graph problem-solving context. The enhanced model,
GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with
different complexity levels, surpassing GPT-4 which has an average accuracy of
43.8%. Moreover, our research delves into the delicate balance between training
data volume and model performance, highlighting the potential for overfitting
with increased data. We also explore the transferability of the model's
reasoning ability across different graph tasks, indicating the model's
adaptability and practical application potential. Our investigation offers a
new blueprint and valuable insights for developing LLMs specialized in graph
reasoning and problem-solving.
