---
layout: publication
title: 'Fine-tuning Llms With Noisy Data For Political Argument Generation And Post Guidance'
authors: Svetlana Churina, Kokil Jaidka
conference: "Arxiv"
year: 2024
bibkey: churina2024fine
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.16813"}
tags: ['Security', 'Training Techniques', 'Model Architecture', 'Language Modeling', 'GPT', 'Pretraining Methods', 'Fine-Tuning', 'Prompting', 'Applications']
---
The incivility in social media discourse complicates the deployment of
automated text generation models for politically sensitive content. Fine-tuning
and prompting strategies are critical, but underexplored, solutions to mitigate
toxicity in such contexts. This study investigates the fine-tuning and
prompting effects on GPT-3.5 Turbo using subsets of the CLAPTON dataset of
political discussion posts, comprising Twitter and Reddit data labeled for
their justification, reciprocity and incivility. Fine-tuned models on Reddit
data scored highest on discussion quality, while combined noisy data led to
persistent toxicity. Prompting strategies reduced specific toxic traits, such
as personal attacks, but had limited broader impact. The findings emphasize
that high-quality data and well-crafted prompts are essential to reduce
incivility and improve rhetorical quality in automated political discourse
generation.
