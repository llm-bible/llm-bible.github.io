---
layout: publication
title: Are Small Language Models Ready To Compete With Large Language Models For Practical Applications
authors: Sinha Neelabh, Jain Vinija, Chadha Aman
conference: "Arxiv"
year: 2024
bibkey: sinha2024are
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.11402"}
tags: ['Applications', 'GPT', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG', 'Tools']
---
The rapid rise of Language Models (LMs) has expanded their use in several applications. Yet due to constraints of model size associated cost or proprietary restrictions utilizing state45;of45;the45;art (SOTA) LLMs is not always feasible. With open smaller LMs emerging more applications can leverage their capabilities but selecting the right LM can be challenging as smaller LMs dont perform well universally. This work tries to bridge this gap by proposing a framework to experimentally evaluate small open LMs in practical settings through measuring semantic correctness of outputs across three practical aspects task types application domains and reasoning types using diverse prompt styles. It also conducts an in45;depth comparison of 10 small open LMs to identify best LM and prompt style depending on specific application requirement using the proposed framework. We also show that if selected appropriately they can outperform SOTA LLMs like DeepSeek45;v2 GPT45;4o45;mini Gemini45;1.545;Pro and even compete with GPT45;4o.
