---
layout: publication
title: An Empirical Study Of Retrieval Augmented Generation With Chain45;of45;thought
authors: Zhao Yuetong, Cao Hongyu, Zhao Xianyu, Ou Zhijian
conference: "Arxiv"
year: 2024
bibkey: zhao2024empirical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.15569"}
tags: ['Fine Tuning', 'GPT', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools']
---
Since the launch of ChatGPT at the end of 2022 generative dialogue models represented by ChatGPT have quickly become essential tools in daily life. As user expectations increase enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Augmented Fine45;Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain45;of45;thought with model supervised fine45;tuning (SFT) and retrieval augmented generation (RAG) which significantly enhanced the models information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks including long45;form QA and short45;form QA tasks tasks in both Chinese and English and supportive and comparison reasoning tasks. Notably it addresses the gaps in previous research regarding long45;form QA tasks and Chinese datasets. Moreover we also evaluate the benefit of the chain45;of45;thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.
