---
layout: publication
title: 'Unlocking Scaling Law In Industrial Recommendation Systems With A Three-step Paradigm Based Large User Model'
authors: Bencheng Yan, Shilei Liu, Zhiyuan Zeng, Zihao Wang, Yizhen Zhang, Yujin Yuan, Langming Liu, Jiaqi Liu, Di Wang, Wenbo Su, Wang Pengjie, Jian Xu, Bo Zheng
conference: "Arxiv"
year: 2025
bibkey: yan2025unlocking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.08309"}
tags: ['RecSys', 'Pretraining Methods', 'Model Architecture', 'GPT']
---
Recent advancements in autoregressive Large Language Models (LLMs) have
achieved significant milestones, largely attributed to their scalability, often
referred to as the "scaling law". Inspired by these achievements, there has
been a growing interest in adapting LLMs for Recommendation Systems (RecSys) by
reformulating RecSys tasks into generative problems. However, these End-to-End
Generative Recommendation (E2E-GR) methods tend to prioritize idealized goals,
often at the expense of the practical advantages offered by traditional Deep
Learning based Recommendation Models (DLRMs) in terms of in features,
architecture, and practices. This disparity between idealized goals and
practical needs introduces several challenges and limitations, locking the
scaling law in industrial RecSys. In this paper, we introduce a large user
model (LUM) that addresses these limitations through a three-step paradigm,
designed to meet the stringent requirements of industrial settings while
unlocking the potential for scalable recommendations. Our extensive
experimental evaluations demonstrate that LUM outperforms both state-of-the-art
DLRMs and E2E-GR approaches. Notably, LUM exhibits excellent scalability, with
performance improvements observed as the model scales up to 7 billion
parameters. Additionally, we have successfully deployed LUM in an industrial
application, where it achieved significant gains in an A/B test, further
validating its effectiveness and practicality.
