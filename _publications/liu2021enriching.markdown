---
layout: publication
title: Enriching Non45;autoregressive Transformer With Syntactic And Semanticstructures For Neural Machine Translation
authors: Liu Ye, Wan Yao, Zhang Jian-guo, Zhao Wenting, Yu Philip S.
conference: "Arxiv"
year: 2021
bibkey: liu2021enriching
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2101.08942"}
tags: ['Applications', 'Efficiency And Optimization', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
The non45;autoregressive models have boosted the efficiency of neural machine translation through parallelized decoding at the cost of effectiveness when comparing with the autoregressive counterparts. In this paper we claim that the syntactic and semantic structures among natural language are critical for non45;autoregressive machine translation and can further improve the performance. However these structures are rarely considered in the existing non45;autoregressive models. Inspired by this intuition we propose to incorporate the explicit syntactic and semantic structures of languages into a non45;autoregressive Transformer for the task of neural machine translation. Moreover we also consider the intermediate latent alignment within target sentences to better learn the long45;term token dependencies. Experimental results on two real45;world datasets (i.e. WMT14 En45;De and WMT16 En45;Ro) show that our model achieves a significantly faster speed as well as keeps the translation quality when compared with several state45;of45;the45;art non45;autoregressive models.
