---
layout: publication
title: Self45;prompted Chain45;of45;thought On Large Language Models For Open45;domain Multi45;hop Reasoning
authors: Wang Jinyuan, Li Junlong, Zhao Hai
conference: "Arxiv"
year: 2023
bibkey: wang2023self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.13552"}
tags: ['Applications', 'Prompting', 'Reinforcement Learning', 'Tools']
---
In open45;domain question45;answering (ODQA) most existing questions require single45;hop reasoning on commonsense. To further extend this task we officially introduce open45;domain multi45;hop reasoning (ODMR) by answering multi45;hop questions with explicit reasoning steps in open45;domain setting. Recently large language models (LLMs) have found significant utility in facilitating ODQA without external corpus. Furthermore chain45;of45;thought (CoT) prompting boosts the reasoning capability of LLMs to a greater extent with manual or automated paradigms. However existing automated methods lack of quality assurance while manual approaches suffer from limited scalability and poor diversity hindering the capabilities of LLMs. In this paper we propose Self45;prompted Chain45;of45;Thought (SP45;CoT) an automated framework to mass45;produce high quality CoTs of LLMs by LLMs and for LLMs. SP45;CoT introduces an automated generation pipeline of high quality ODMR datasets an adaptive sampler for in45;context CoT selection and self45;prompted inference via in45;context learning. Extensive experiments on four multi45;hop question45;answering benchmarks show that our proposed SP45;CoT not only significantly surpasses the previous SOTA methods on large45;scale (175B) LLMs but also nearly doubles the zero45;shot performance of small45;scale (13B) LLMs. Further analysis reveals the remarkable capability of SP45;CoT to elicit direct and concise intermediate reasoning steps by recalling sim5037; of intermediate answers on MuSiQue45;Ans dataset.
