---
layout: publication
title: Neural Data45;to45;text Generation A Comparison Between Pipeline And End45;to45;end Architectures
authors: Ferreira Thiago Castro, Van Der Lee Chris, Van Miltenburg Emiel, Krahmer Emiel
conference: "Arxiv"
year: 2019
bibkey: ferreira2019neural
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1908.09022"}
tags: ['Applications', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Traditionally most data45;to45;text applications have been designed using a modular pipeline architecture in which non45;linguistic input data is converted into natural language through several intermediate transformations. In contrast recent neural models for data45;to45;text generation have been proposed as end45;to45;end approaches where the non45;linguistic input is rendered in natural language with much less explicit intermediate representations in45;between. This study introduces a systematic comparison between neural pipeline and end45;to45;end data45;to45;text approaches for the generation of text from RDF triples. Both architectures were implemented making use of state45;of45;the art deep learning methods as the encoder45;decoder Gated45;Recurrent Units (GRU) and Transformer. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end45;to45;end approaches. Moreover the pipeline models generalize better to unseen inputs. Data and code are publicly available.
