---
layout: publication
title: 'Exploring Memorization In Fine-tuned Language Models'
authors: Shenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han Xu, Pengfei He, Yue Xing, Shuaiqiang Wang, Jiliang Tang, Dawei Yin
conference: "Arxiv"
year: 2023
bibkey: zeng2023exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.06714"}
tags: ['Fine-Tuning', 'Pre-Training', 'Interpretability and Explainability', 'Model Architecture', 'Training Techniques', 'Attention Mechanism', 'Pretraining Methods']
---
Large language models (LLMs) have shown great capabilities in various tasks
but also exhibited memorization of training data, raising tremendous privacy
and copyright concerns. While prior works have studied memorization during
pre-training, the exploration of memorization during fine-tuning is rather
limited. Compared to pre-training, fine-tuning typically involves more
sensitive data and diverse objectives, thus may bring distinct privacy risks
and unique memorization behaviors. In this work, we conduct the first
comprehensive analysis to explore language models' (LMs) memorization during
fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned
LMs across various tasks indicate that memorization presents a strong disparity
among different fine-tuning tasks. We provide an intuitive explanation of this
task disparity via sparse coding theory and unveil a strong correlation between
memorization and attention score distribution.
