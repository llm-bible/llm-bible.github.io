---
layout: publication
title: Large Visual45;language Models Are Also Good Classifiers A Study Of In45;context Multimodal Fake News Detection
authors: Jiang Ye, Wang Yimin
conference: "Arxiv"
year: 2024
bibkey: jiang2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.12879"}
tags: ['Applications', 'BERT', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Multimodal Models', 'Prompting', 'Tools']
---
Large visual45;language models (LVLMs) exhibit exceptional performance in visual45;language reasoning across diverse cross45;modal benchmarks. Despite these advances recent research indicates that Large Language Models (LLMs) like GPT45;3.545;turbo underachieve compared to well45;trained smaller models such as BERT in Fake News Detection (FND) prompting inquiries into LVLMs efficacy in FND tasks. Although performance could improve through fine45;tuning LVLMs the substantial parameters and requisite pre45;trained weights render it a resource45;heavy endeavor for FND applications. This paper initially assesses the FND capabilities of two notable LVLMs CogVLM and GPT4V in comparison to a smaller yet adeptly trained CLIP model in a zero45;shot context. The findings demonstrate that LVLMs can attain performance competitive with that of the smaller model. Next we integrate standard in45;context learning (ICL) with LVLMs noting improvements in FND performance though limited in scope and consistency. To address this we introduce the textbf123;I125;n45;context textbf123;M125;ultimodal textbf123;F125;ake textbf123;N125;ews textbf123;D125;etection (IMFND) framework enriching in45;context examples and test inputs with predictions and corresponding probabilities from a well45;trained smaller model. This strategic integration directs the LVLMs focus towards news segments associated with higher probabilities thereby improving their analytical accuracy. The experimental results suggest that the IMFND framework significantly boosts the FND efficiency of LVLMs achieving enhanced accuracy over the standard ICL approach across three publicly available FND datasets.
