---
layout: publication
title: Autonomous Workflow For Multimodal Fine45;grained Training Assistants Towards Mixed Reality
authors: Pei Jiahuan, Viola Irene, Huang Haochen, Wang Junxiao, Ahsan Moonisa, Ye Fanghua, Yiming Jiang, Sai Yao, Wang Di, Chen Zhumin, Ren Pengjie, Cesar Pablo
conference: "Arxiv"
year: 2024
bibkey: pei2024autonomous
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.13034"}
tags: ['Agentic', 'Applications', 'Multimodal Models', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Autonomous artificial intelligence (AI) agents have emerged as promising protocols for automatically understanding the language45;based environment particularly with the exponential development of large language models (LLMs). However a fine45;grained comprehensive understanding of multimodal environments remains under45;explored. This work designs an autonomous workflow tailored for integrating AI agents seamlessly into extended reality (XR) applications for fine45;grained training. We present a demonstration of a multimodal fine45;grained training assistant for LEGO brick assembly in a pilot XR environment. Specifically we design a cerebral language agent that integrates LLM with memory planning and interaction with XR tools and a vision45;language agent enabling agents to decide their actions based on past experiences. Furthermore we introduce LEGO45;MRTA a multimodal fine45;grained assembly dialogue dataset synthesized automatically in the workflow served by a commercial LLM. This dataset comprises multimodal instruction manuals conversations XR responses and vision question answering. Last we present several prevailing open45;resource LLMs as benchmarks assessing their performance with and without fine45;tuning on the proposed dataset. We anticipate that the broader impact of this workflow will advance the development of smarter assistants for seamless user interaction in XR environments fostering research in both AI and HCI communities.
