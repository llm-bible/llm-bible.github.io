---
layout: publication
title: 'Code Readability In The Age Of Large Language Models: An Industrial Case Study From Atlassian'
authors: Wannita Takerngsaksiri, Micheal Fu, Chakkrit Tantithamthavorn, Jirat Pasuksmit, Kun Chen, Ming Wu
conference: "Arxiv"
year: 2025
bibkey: takerngsaksiri2025code
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.11264'}
tags: ['Reinforcement Learning', 'Agentic', 'Tools', 'Survey Paper']
---
Software engineers spend a significant amount of time reading code during the software development process. This trend is amplified by the emergence of large language models (LLMs) that automatically generate code. However, little is known about the readability of the LLM-generated code and whether it is still important from practitioners' perspectives in this new era. In this paper, we conduct a survey to explore the practitioners' perspectives on code readability in the age of LLMs and investigate the readability of our LLM-based software development agents framework, HULA, by comparing its generated code with human-written code in real-world scenarios. Overall, the findings underscore that (1) readability remains a critical aspect of software development; (2) the readability of our LLM-generated code is comparable to human-written code, fostering the establishment of appropriate trust and driving the broad adoption of our LLM-powered software development platform.
