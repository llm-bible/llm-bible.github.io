---
layout: publication
title: 'A Survey Of The Evolution Of Language Model-based Dialogue Systems'
authors: Hongru Wang, Lingzhi Wang, Yiming Du, Liang Chen, Jingyan Zhou, Yufei Wang, Kam-fai Wong
conference: "Arxiv"
year: 2023
bibkey: wang2023survey
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2311.16789'}
tags: ['Model Architecture', 'Applications', 'Merging', 'Fine-Tuning', 'Survey Paper', 'Reinforcement Learning']
---
Dialogue systems, including task-oriented_dialogue_system (TOD) and
open-domain_dialogue_system (ODD), have undergone significant transformations,
with language_models (LM) playing a central role. This survey delves into the
historical trajectory of dialogue systems, elucidating their intricate
relationship with advancements in language models by categorizing this
evolution into four distinct stages, each marked by pivotal LM breakthroughs:
1) Early_Stage: characterized by statistical LMs, resulting in rule-based or
machine-learning-driven dialogue_systems; 2) Independent development of TOD and
ODD based on neural_language_models (NLM; e.g., LSTM and GRU), since NLMs lack
intrinsic knowledge in their parameters; 3) fusion between different types of
dialogue systems with the advert of pre-trained_language_models (PLMs),
starting from the fusion between four_sub-tasks_within_TOD, and then
TOD_with_ODD; and 4) current LLM-based_dialogue_system, wherein LLMs can be
used to conduct TOD and ODD seamlessly. Thus, our survey provides a
chronological perspective aligned with LM breakthroughs, offering a
comprehensive review of state-of-the-art research outcomes. What's more, we
focus on emerging topics and discuss open challenges, providing valuable
insights into future directions for LLM-based_dialogue_systems. Through this
exploration, we pave the way for a deeper_comprehension of the evolution,
guiding future developments in LM-based dialogue_systems.
