---
layout: publication
title: A Survey Of The Evolution Of Language Model-based Dialogue Systems
authors: Wang Hongru, Wang Lingzhi, Du Yiming, Chen Liang, Zhou Jingyan, Wang Yufei, Wong Kam-fai
conference: "Arxiv"
year: 2023
bibkey: wang2023survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.16789"}
tags: ['Applications', 'Fine Tuning', 'Merging', 'Model Architecture', 'Reinforcement Learning', 'Survey Paper']
---
Dialogue systems including task-oriented_dialogue_system (TOD) and open-domain_dialogue_system (ODD) have undergone significant transformations with language_models (LM) playing a central role. This survey delves into the historical trajectory of dialogue systems elucidating their intricate relationship with advancements in language models by categorizing this evolution into four distinct stages each marked by pivotal LM breakthroughs 1) Early_Stage characterized by statistical LMs resulting in rule-based or machine-learning-driven dialogue_systems; 2) Independent development of TOD and ODD based on neural_language_models (NLM; e.g. LSTM and GRU) since NLMs lack intrinsic knowledge in their parameters; 3) fusion between different types of dialogue systems with the advert of pre-trained_language_models (PLMs) starting from the fusion between four_sub-tasks_within_TOD and then TOD_with_ODD; and 4) current LLM-based_dialogue_system wherein LLMs can be used to conduct TOD and ODD seamlessly. Thus our survey provides a chronological perspective aligned with LM breakthroughs offering a comprehensive review of state-of-the-art research outcomes. Whats more we focus on emerging topics and discuss open challenges providing valuable insights into future directions for LLM-based_dialogue_systems. Through this exploration we pave the way for a deeper_comprehension of the evolution guiding future developments in LM-based dialogue_systems.
