---
layout: publication
title: Towards Expert45;level Medical Question Answering With Large Language Models
authors: Singhal Karan, Tu Tao, Gottweis Juraj, Sayres Rory, Wulczyn Ellery, Hou Le, Clark Kevin, Pfohl Stephen, Cole-lewis Heather, Neal Darlene, Schaekermann Mike, Wang Amy, Amin Mohamed, Lachgar Sami, Mansfield Philip, Prakash Sushant, Green Bradley, Dominowska Ewa, Arcas Blaise Aguera Y, Tomasev Nenad, Liu Yun, Wong Renee, Semturs Christopher, Mahdavi S. Sara, Barral Joelle, Webster Dale, Corrado Greg S., Matias Yossi, Azizi Shekoofeh, Karthikesalingam Alan, Natarajan Vivek
conference: "Arxiv"
year: 2023
bibkey: singhal2023towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.09617"}
tags: ['Applications', 'Prompting', 'RAG', 'Reinforcement Learning', 'Security', 'Tools']
---
Recent artificial intelligence (AI) systems have reached milestones in grand challenges ranging from Go to protein45;folding. The capability to retrieve medical knowledge reason over it and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med45;PaLM was the first model to exceed a passing score in US Medical Licensing Examination (USMLE) style questions with a score of 67.237; on the MedQA dataset. However this and other prior work suggested significant room for improvement especially when models answers were compared to clinicians answers. Here we present Med45;PaLM 2 which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2) medical domain finetuning and prompting strategies including a novel ensemble refinement approach. Med45;PaLM 2 scored up to 86.537; on the MedQA dataset improving upon Med45;PaLM by over 1937; and setting a new state45;of45;the45;art. We also observed performance approaching or exceeding state45;of45;the45;art across MedMCQA PubMedQA and MMLU clinical topics datasets. We performed detailed human evaluations on long45;form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions physicians preferred Med45;PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p < 0.001). We also observed significant improvements compared to Med45;PaLM on every evaluation axis (p < 0.001) on newly introduced datasets of 240 long45;form adversarial questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real45;world settings these results highlight rapid progress towards physician45;level performance in medical question answering.
