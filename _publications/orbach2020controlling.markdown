---
layout: publication
title: Facts2story Controlling Text Generation By Key Facts
authors: Orbach Eyal Bar Ilan University, Goldberg Yoav Bar Ilan University And Allen Institute For Artificial Intelligence
conference: "Arxiv"
year: 2020
bibkey: orbach2020controlling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2012.04332"}
tags: ['Applications', 'Attention Mechanism', 'GPT', 'Language Modeling', 'Model Architecture', 'Training Techniques']
---
Recent advancements in self45;attention neural network architectures have raised the bar for open45;ended text generation. Yet while current methods are capable of producing a coherent text which is several hundred words long attaining control over the content that is being generated 45;45; as well as evaluating it 45;45; are still open questions. We propose a controlled generation task which is based on expanding a sequence of facts expressed in natural language into a longer narrative. We introduce human45;based evaluation metrics for this task as well as a method for deriving a large training dataset. We evaluate three methods on this task based on fine45;tuning pre45;trained models. We show that while auto45;regressive unidirectional Language Models such as GPT2 produce better fluency they struggle to adhere to the requested facts. We propose a plan45;and45;cloze model (using fine45;tuned XLNet) which produces competitive fluency while adhering to the requested content.
