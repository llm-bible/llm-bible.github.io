---
layout: publication
title: 'Do Llms Understand Ambiguity In Text? A Case Study In Open-world Question Answering'
authors: Aryan Keluskar, Amrita Bhattacharjee, Huan Liu
conference: "Arxiv"
year: 2024
bibkey: keluskar2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.12395"}
tags: ['Applications', 'Ethics and Bias', 'Reinforcement Learning', 'Training Techniques', 'Few-Shot']
---
Ambiguity in natural language poses significant challenges to Large Language
Models (LLMs) used for open-domain question answering. LLMs often struggle with
the inherent uncertainties of human communication, leading to
misinterpretations, miscommunications, hallucinations, and biased responses.
This significantly weakens their ability to be used for tasks like
fact-checking, question answering, feature extraction, and sentiment analysis.
Using open-domain question answering as a test case, we compare off-the-shelf
and few-shot LLM performance, focusing on measuring the impact of explicit
disambiguation strategies. We demonstrate how simple, training-free,
token-level disambiguation methods may be effectively used to improve LLM
performance for ambiguous question answering tasks. We empirically show our
findings and discuss best practices and broader impacts regarding ambiguity in
LLMs.
