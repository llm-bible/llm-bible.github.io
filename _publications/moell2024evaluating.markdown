---
layout: publication
title: 'Evaluating Large Language Models With Human Feedback: Establishing A Swedish Benchmark'
authors: Birger Moell
conference: "Arxiv"
year: 2024
bibkey: moell2024evaluating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2405.14006'}
tags: ['GPT', 'Tools', 'Applications', 'Model Architecture']
---
In the rapidly evolving field of artificial intelligence, large language
models (LLMs) have demonstrated significant capabilities across numerous
applications. However, the performance of these models in languages with fewer
resources, such as Swedish, remains under-explored. This study introduces a
comprehensive human benchmark to assess the efficacy of prominent LLMs in
understanding and generating Swedish language texts using forced choice
ranking. We employ a modified version of the ChatbotArena benchmark,
incorporating human feedback to evaluate eleven different models, including
GPT-4, GPT-3.5, various Claude and Llama models, and bespoke models like
Dolphin-2.9-llama3b-8b-flashback and BeagleCatMunin. These models were chosen
based on their performance on LMSYS chatbot arena and the Scandeval benchmarks.
We release the chatbotarena.se benchmark as a tool to improve our understanding
of language model performance in Swedish with the hopes that it will be widely
used. We aim to create a leaderboard once sufficient data has been collected
and analysed.
