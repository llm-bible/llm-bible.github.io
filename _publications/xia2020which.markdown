---
layout: publication
title: Which *BERT? A Survey Organizing Contextualized Encoders
authors: Patrick Xia, Shijie Wu, Benjamin Van Durme
conference: Arxiv
year: 2020
citations: 15
bibkey: xia2020which
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2010.00854'}]
tags: [Survey Paper, BERT]
---
Pretrained contextualized text encoders are now a staple of the NLP
community. We present a survey on language representation learning with the aim
of consolidating a series of shared lessons learned across a variety of recent
efforts. While significant advancements continue at a rapid pace, we find that
enough has now been discovered, in different directions, that we can begin to
organize advances according to common themes. Through this organization, we
highlight important considerations when interpreting recent contributions and
choosing which model to use.