---
layout: publication
title: 'Batayan: A Filipino NLP Benchmark For Evaluating Large Language Models'
authors: Jann Railey Montalan, Jimson Paulo Layacan, David Demitri Africa, Richell Isaiah Flores, Michael T. Ii Lopez, Theresa Denise Magsajo, Anjanette Cayabyab, William Chandra Tjhi
conference: "Arxiv"
year: 2025
bibkey: montalan2025filipino
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.14911'}
tags: ['Reinforcement Learning', 'Ethics and Bias', 'Training Techniques', 'Pretraining Methods']
---
Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities on widely benchmarked high-resource languages; however, linguistic
nuances of under-resourced languages remain unexplored. We introduce Batayan, a
holistic Filipino benchmark designed to systematically evaluate LLMs across
three key natural language processing (NLP) competencies: understanding,
reasoning, and generation. Batayan consolidates eight tasks, covering both
Tagalog and code-switched Taglish utterances. Our rigorous,
native-speaker-driven annotation process ensures fluency and authenticity to
the complex morphological and syntactic structures of Filipino, alleviating a
pervasive translationese bias in existing Filipino corpora. We report empirical
results on a variety of multilingual LLMs, highlighting significant performance
gaps that signal the under-representation of Filipino in pretraining corpora,
the unique hurdles in modeling Filipino's rich morphology and construction, and
the importance of explicit Filipino language support and instruction tuning.
Moreover, we discuss the practical challenges encountered in dataset
construction and propose principled solutions for building culturally and
linguistically-faithful resources in under-represented languages. We also
provide a public benchmark and leaderboard as a clear foundation for iterative,
community-driven progress in Filipino NLP.
