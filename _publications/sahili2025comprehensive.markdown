---
layout: publication
title: 'A Comprehensive Social Bias Audit Of Contrastive Vision Language Models'
authors: Zahraa Al Sahili, Ioannis Patras, Matthew Purver
conference: "Arxiv"
year: 2025
bibkey: sahili2025comprehensive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.13223"}
tags: ['Tools', 'Ethics and Bias', 'Bias Mitigation', 'Reinforcement Learning', 'Merging', 'Fairness', 'Training Techniques', 'Multimodal Models', 'Prompting']
---
In the domain of text-to-image generative models, biases inherent in training datasets often propagate into generated content, posing significant ethical challenges, particularly in socially sensitive contexts. We introduce FairCoT, a novel framework that enhances fairness in text-to-image models through Chain-of-Thought (CoT) reasoning within multimodal generative large language models. FairCoT employs iterative CoT refinement to systematically mitigate biases, and dynamically adjusts textual prompts in real time, ensuring diverse and equitable representation in generated images. By integrating iterative reasoning processes, FairCoT addresses the limitations of zero-shot CoT in sensitive scenarios, balancing creativity with ethical responsibility. Experimental evaluations across popular text-to-image systems--including DALL-E and various Stable Diffusion variants--demonstrate that FairCoT significantly enhances fairness and diversity without sacrificing image quality or semantic fidelity. By combining robust reasoning, lightweight deployment, and extensibility to multiple models, FairCoT represents a promising step toward more socially responsible and transparent AI-driven content generation.
