---
layout: publication
title: 'Llm-pros: Analyzing Large Language Models'' Performance In Competitive Problem Solving'
authors: Md Sifat Hossain, Anika Tabassum, Md. Fahim Arefin, Tarannum Shaila Zaman
conference: "Arxiv"
year: 2025
bibkey: hossain2025llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.04355"}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Tools', 'Reinforcement Learning', 'GPT']
---
The rapid advancement of large language models has opened new avenues for
automating complex problem-solving tasks such as algorithmic coding and
competitive programming. This paper introduces a novel evaluation technique,
LLM-ProS, to assess the performance of state-of-the-art LLMs on International
Collegiate Programming Contest (ICPC) problems. Using a curated dataset of 166
World Finals problems from 2011 to 2024, we benchmark the models' reasoning,
accuracy, and efficiency. We evaluate the five models-GPT-4o, Mistral Large,
Llama-3.1-405B, and the o1 family, consisting of o1-mini and o1-preview, across
critical metrics like correctness, resource utilization, and response
calibration. Our results reveal significant differences in the models'
abilities to generalize, adapt, and solve novel problems. We also investigated
the impact of training methodologies, dataset contamination, and
chain-of-thought reasoning on model performance. The findings provide new
insights into optimizing LLMs for algorithmic tasks, highlighting both
strengths and limitations of current models.
