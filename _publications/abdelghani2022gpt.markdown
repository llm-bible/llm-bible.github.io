---
layout: publication
title: 'Gpt-3-driven Pedagogical Agents For Training Children''s Curious Question-asking Skills'
authors: Abdelghani Rania, Wang Yen-hsiang, Yuan Xingdi, Wang Tong, Lucas Pauline, Sauzéon Hélène, Oudeyer Pierre-yves
conference: "Arxiv"
year: 2022
bibkey: abdelghani2022gpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.14228"}
tags: ['Agentic', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
In order to train children's ability to ask curiosity-driven questions, previous research has explored designing specific exercises relying on providing semantic and linguistic cues to help formulate such questions. But despite showing pedagogical efficiency, this method is still limited as it relies on generating the said cues by hand, which can be a very costly process. In this context, we propose to leverage advances in the natural language processing field (NLP) and investigate the efficiency of using a large language model (LLM) for automating the production of the pedagogical content of a curious question-asking (QA) training. We study generating the said content using the prompt-based method that consists of explaining the task to the LLM in natural text. We evaluate the output using human experts annotations and comparisons with hand-generated content. Results suggested indeed the relevance and usefulness of this content. We also conduct a field study in primary school (75 children aged 9-10), where we evaluate children's QA performance when having this training. We compare 3 types of content : 1) hand-generated content that proposes closed cues leading to predefined questions; 2) GPT-3-generated content that proposes the same type of cues; 3) GPT-3-generated content that proposes open cues leading to several possible questions. We see a similar QA performance between the two closed trainings (showing the scalability of the approach using GPT-3), and a better one for participants with the open training. These results suggest the efficiency of using LLMs to support children in generating more curious questions, using a natural language prompting approach that affords usability by teachers and other users not specialists of AI techniques. Furthermore, results also show that open-ended content may be more suitable for training curious question-asking skills.
