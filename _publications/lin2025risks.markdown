---
layout: publication
title: 'The Risks Of Using Large Language Models For Text Annotation In Social Science Research'
authors: Hao Lin, Yongjun Zhang
conference: "Arxiv"
year: 2025
bibkey: lin2025risks
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.22040'}
tags: ['Tools', 'Prompting', 'Reinforcement Learning', 'Ethics and Bias', 'Interpretability']
---
Generative artificial intelligence (GenAI) or large language models (LLMs)
have the potential to revolutionize computational social science, particularly
in automated textual analysis. In this paper, we conduct a systematic
evaluation of the promises and risks of using LLMs for diverse coding tasks,
with social movement studies serving as a case example. We propose a framework
for social scientists to incorporate LLMs into text annotation, either as the
primary coding decision-maker or as a coding assistant. This framework provides
tools for researchers to develop the optimal prompt, and to examine and report
the validity and reliability of LLMs as a methodological tool. Additionally, we
discuss the associated epistemic risks related to validity, reliability,
replicability, and transparency. We conclude with several practical guidelines
for using LLMs in text annotation tasks, and how we can better communicate the
epistemic risks in research.
