---
layout: publication
title: 'Diagnosing Moral Reasoning Acquisition In Language Models: Pragmatics And Generalization'
authors: Guangliang Liu, Lei Jiang, Xitong Zhang, Kristen Marie Johnson
conference: "Arxiv"
year: 2025
bibkey: liu2025diagnosing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.16600"}
tags: ['Responsible AI', 'Training Techniques', 'RAG', 'Ethics and Bias', 'Pretraining Methods', 'Fine-Tuning']
---
Ensuring that Large Language Models (LLMs) return just responses which adhere
to societal values is crucial for their broader application. Prior research has
shown that LLMs often fail to perform satisfactorily on tasks requiring moral
cognizance, such as ethics-based judgments. While current approaches have
focused on fine-tuning LLMs with curated datasets to improve their capabilities
on such tasks, choosing the optimal learning paradigm to enhance the ethical
responses of LLMs remains an open research debate. In this work, we aim to
address this fundamental question: can current learning paradigms enable LLMs
to acquire sufficient moral reasoning capabilities? Drawing from distributional
semantics theory and the pragmatic nature of moral discourse, our analysis
indicates that performance improvements follow a mechanism similar to that of
semantic-level tasks, and therefore remain affected by the pragmatic nature of
morals latent in discourse, a phenomenon we name the pragmatic dilemma. We
conclude that this pragmatic dilemma imposes significant limitations on the
generalization ability of current learning paradigms, making it the primary
bottleneck for moral reasoning acquisition in LLMs.
