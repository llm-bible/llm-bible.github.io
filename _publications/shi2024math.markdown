---
layout: publication
title: Math45;llava Bootstrapping Mathematical Reasoning For Multimodal Large Language Models
authors: Shi Wenhao, Hu Zhiqiang, Bin Yi, Liu Junhua, Yang Yang, Ng See-kiong, Bing Lidong, Lee Roy Ka-wei
conference: "Arxiv"
year: 2024
bibkey: shi2024math
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.17294"}
  - {name: "Code", url: "https://github.com/HZQ950419/Math&#45;LLaVA&#125;"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning']
---
Large language models (LLMs) have demonstrated impressive reasoning capabilities particularly in textual mathematical problem45;solving. However existing open45;source image instruction fine45;tuning datasets containing limited question45;answer pairs per image do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap we address the lack of high45;quality diverse multimodal mathematical datasets by collecting 40K high45;quality images with question45;answer pairs from 24 existing datasets and synthesizing 320K new pairs creating the MathV360K dataset which enhances both the breadth and depth of multimodal mathematical questions. We introduce Math45;LLaVA a LLaVA45;1.545;based model fine45;tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA45;1.5 achieving a 1945;point increase and comparable performance to GPT45;4V on MathVistas minitest split. Furthermore Math45;LLaVA demonstrates enhanced generalizability showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs mathematical reasoning abilities. The code and data are available at url123;https://github.com/HZQ950419/Math&#45;LLaVA&#125;.
