---
layout: publication
title: Instruction Tuning With GPT45;4
authors: Peng Baolin, Li Chunyuan, He Pengcheng, Galley Michel, Gao Jianfeng
conference: "Arxiv"
year: 2023
bibkey: peng2023instruction
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.03277"}
tags: ['GPT', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
Prior work has shown that finetuning large language models (LLMs) using machine45;generated instruction45;following data enables such models to achieve remarkable zero45;shot capabilities on new tasks and no human45;written instructions are needed. In this paper we present the first attempt to use GPT45;4 to generate instruction45;following data for LLM finetuning. Our early experiments on instruction45;tuned LLaMA models show that the 52K English and Chinese instruction45;following data generated by GPT45;4 leads to superior zero45;shot performance on new tasks to the instruction45;following data generated by previous state45;of45;the45;art models. We also collect feedback and comparison data from GPT45;4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT45;4 as well as our codebase publicly available.
