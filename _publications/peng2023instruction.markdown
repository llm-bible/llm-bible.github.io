---
layout: publication
title: 'Instruction Tuning With GPT-4'
authors: Peng Baolin, Li Chunyuan, He Pengcheng, Galley Michel, Gao Jianfeng
conference: "Arxiv"
year: 2023
bibkey: peng2023instruction
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.03277"}
tags: ['GPT', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
Prior work has shown that finetuning large language models (LLMs) using
machine-generated instruction-following data enables such models to achieve
remarkable zero-shot capabilities on new tasks, and no human-written
instructions are needed. In this paper, we present the first attempt to use
GPT-4 to generate instruction-following data for LLM finetuning. Our early
experiments on instruction-tuned LLaMA models show that the 52K English and
Chinese instruction-following data generated by GPT-4 leads to superior
zero-shot performance on new tasks to the instruction-following data generated
by previous state-of-the-art models. We also collect feedback and comparison
data from GPT-4 to enable a comprehensive evaluation and reward model training.
We make our data generated using GPT-4 as well as our codebase publicly
available.
