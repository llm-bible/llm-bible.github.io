---
layout: publication
title: Chatglm A Family Of Large Language Models From GLM45;130B To GLM45;4 All Tools
authors: Glm Team, :, Zeng Aohan, Xu Bin, Wang Bowen, Zhang Chenhui, Yin Da, Zhang Dan, Rojas Diego, Feng Guanyu, Zhao Hanlin, Lai Hanyu, Yu Hao, Wang Hongning, Sun Jiadai, Zhang Jiajie, Cheng Jiale, Gui Jiayi, Tang Jie, Zhang Jing, Sun Jingyu, Li Juanzi, Zhao Lei, Wu Lindong, Zhong Lucen, Liu Mingdao, Huang Minlie, Zhang Peng, Zheng Qinkai, Lu Rui, Duan Shuaiqi, Zhang Shudan, Cao Shulin, Yang Shuxun, Tam Weng Lam, Zhao Wenyi, Liu Xiao, Xia Xiao, Zhang Xiaohan, Gu Xiaotao, Lv Xin, Liu Xinghan, Liu Xinyi, Yang Xinyue, Song Xixuan, Zhang Xunkai, An Yifan, Xu Yifan, Niu Yilin, Yang Yuantao, Li Yueyan, Bai Yushi, Dong Yuxiao, Qi Zehan, Wang Zhaoyu, Yang Zhen, Du Zhengxiao, Hou Zhenyu, Wang Zihan
conference: "Arxiv"
year: 2024
bibkey: glm2024family
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.12793"}
  - {name: "Code", url: "https://github.com/THUDM"}
  - {name: "Code", url: "https://huggingface.co/THUDM"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Has Code', 'Model Architecture', 'Tools', 'Training Techniques']
---
We introduce ChatGLM an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM45;4 language series which includes GLM45;4 GLM45;445;Air and GLM45;445;9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date the GLM45;4 models are pre45;trained on ten trillions of tokens mostly in Chinese and English along with a small set of corpus from 24 languages and aligned primarily for Chinese and English usage. The high45;quality alignment is achieved via a multi45;stage post45;training process which involves supervised fine45;tuning and learning from human feedback. Evaluations show that GLM45;4 1) closely rivals or outperforms GPT45;4 in terms of general metrics such as MMLU GSM8K MATH BBH GPQA and HumanEval 2) gets close to GPT45;445;Turbo in instruction following as measured by IFEval 3) matches GPT45;4 Turbo (128K) and Claude 3 for long context tasks and 4) outperforms GPT45;4 in Chinese alignments as measured by AlignBench. The GLM45;4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse 45;45; including web browser Python interpreter text45;to45;image model and user45;defined functions 45;45; to effectively complete complex tasks. In practical applications it matches and even surpasses GPT45;4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course we have open45;sourced a series of models including ChatGLM45;6B (three generations) GLM45;445;9B (128K 1M) GLM45;4V45;9B WebGLM and CodeGeeX attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through https://github.com/THUDM and https://huggingface.co/THUDM.
