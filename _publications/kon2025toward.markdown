---
layout: publication
title: 'Curie: Toward Rigorous And Automated Scientific Experimentation With AI Agents'
authors: Patrick Tser Jern Kon, Jiachen Liu, Qiuyi Ding, Yiming Qiu, Zhenning Yang, Yibo Huang, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Ang Chen
conference: "Arxiv"
year: 2025
bibkey: kon2025toward
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.16069"}
  - {name: "Code", url: "https://github.com/Just-Curieous/Curie"}
tags: ['Agentic', 'Has Code', 'Tools', 'Interpretability and Explainability']
---
Scientific experimentation, a cornerstone of human progress, demands rigor in
reliability, methodical control, and interpretability to yield meaningful
results. Despite the growing capabilities of large language models (LLMs) in
automating different aspects of the scientific process, automating rigorous
experimentation remains a significant challenge. To address this gap, we
propose Curie, an AI agent framework designed to embed rigor into the
experimentation process through three key components: an intra-agent rigor
module to enhance reliability, an inter-agent rigor module to maintain
methodical control, and an experiment knowledge module to enhance
interpretability. To evaluate Curie, we design a novel experimental benchmark
composed of 46 questions across four computer science domains, derived from
influential research papers, and widely adopted open-source projects. Compared
to the strongest baseline tested, we achieve a 3.4\\(\times\\) improvement in
correctly answering experimental questions. Curie is open-sourced at
https://github.com/Just-Curieous/Curie.
