---
layout: publication
title: 'Mac-tuning: LLM Multi-compositional Problem Reasoning With Enhanced Knowledge Boundary Awareness'
authors: Junsheng Huang, Zhitao He, Sandeep Polisetty, Qingyun Wang, May Fung
conference: "Arxiv"
year: 2025
bibkey: huang2025mac
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.21773"}
tags: ['Fine-Tuning', 'RAG', 'Model Architecture', 'Training Techniques', 'Attention Mechanism', 'Pretraining Methods']
---
With the widespread application of large language models (LLMs), the issue of
generating non-existing facts, known as hallucination, has garnered increasing
attention. Previous research in enhancing LLM confidence estimation mainly
focuses on the single problem setting. However, LLM awareness of its internal
parameterized knowledge boundary under the more challenging multi-problem
setting, which requires answering multiple problems accurately simultaneously,
remains underexplored. To bridge this gap, we introduce a novel method,
Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates
the learning of answer prediction and confidence estimation during fine-tuning
on instruction data. Extensive experiments demonstrate that our method
outperforms baselines by up to 25% in average precision.
