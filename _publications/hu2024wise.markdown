---
layout: publication
title: Wilke Wise45;layer Knowledge Editor For Lifelong Knowledge Editing
authors: Hu Chenhui, Cao Pengfei, Chen Yubo, Liu Kang, Zhao Jun
conference: "Arxiv"
year: 2024
bibkey: hu2024wise
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.10987"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques']
---
Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However current knowledge editing methods primarily focus on single editing failing to meet the requirements for lifelong editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing characterized by toxicity buildup and toxicity flash with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named Wise45;Layer Knowledge Editor (WilKE) which selects editing layer based on the pattern matching degree of editing knowledge across different layers in language models. Experimental results demonstrate that in lifelong editing WilKE exhibits an average improvement of 46.237; and 67.837; on editing GPT245;XL and GPT45;J relative to state45;of45;the45;art knowledge editing methods.
