---
layout: publication
title: 'Wilke: Wise-layer Knowledge Editor For Lifelong Knowledge Editing'
authors: Hu Chenhui, Cao Pengfei, Chen Yubo, Liu Kang, Zhao Jun
conference: "Arxiv"
year: 2024
bibkey: hu2024wise
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.10987"}
tags: ['GPT', 'Model Architecture', 'RAG', 'Training Techniques', 'Uncategorized']
---
Knowledge editing aims to rectify inaccuracies in large language models
(LLMs) without costly retraining for outdated or erroneous knowledge. However,
current knowledge editing methods primarily focus on single editing, failing to
meet the requirements for lifelong editing. This study reveals a performance
degradation encountered by knowledge editing in lifelong editing, characterized
by toxicity buildup and toxicity flash, with the primary cause identified as
pattern unmatch. We introduce a knowledge editing approach named Wise-Layer
Knowledge Editor (WilKE), which selects editing layer based on the pattern
matching degree of editing knowledge across different layers in language
models. Experimental results demonstrate that, in lifelong editing, WilKE
exhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J
relative to state-of-the-art knowledge editing methods.
