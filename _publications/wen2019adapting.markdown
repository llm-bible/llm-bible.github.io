---
layout: publication
title: Adapting And Evaluating A Deep Learning Language Model For Clinical Why45;question Answering
authors: Wen Andrew, Elwazir Mohamed Y., Moon Sungrim, Fan Jungwei
conference: "Arxiv"
year: 2019
bibkey: wen2019adapting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1911.05604"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Tools', 'Training Techniques', 'Transformer']
---
Objectives To adapt and evaluate a deep learning language model for answering why45;questions based on patient45;specific clinical text. Materials and Methods Bidirectional encoder representations from transformers (BERT) models were trained with varying data sources to perform SQuAD 2.0 style why45;question answering (why45;QA) on clinical notes. The evaluation focused on 1) comparing the merits from different training data 2) error analysis. Results The best model achieved an accuracy of 0.707 (or 0.760 by partial match). Training toward customization for the clinical language helped increase 637; in accuracy. Discussion The error analysis suggested that the model did not really perform deep reasoning and that clinical why45;QA might warrant more sophisticated solutions. Conclusion The BERT model achieved moderate accuracy in clinical why45;QA and should benefit from the rapidly evolving technology. Despite the identified limitations it could serve as a competent proxy for question45;driven clinical information extraction.
