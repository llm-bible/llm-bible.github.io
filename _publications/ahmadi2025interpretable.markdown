---
layout: publication
title: 'Interpretable Locomotion Prediction In Construction Using A Memory-driven LLM Agent With Chain-of-thought Reasoning'
authors: Ehsan Ahmadi, Chao Wang
conference: "Arxiv"
year: 2025
bibkey: ahmadi2025interpretable
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.15263'}
tags: ['Agentic', 'RAG', 'Tools', 'Multimodal Models', 'Reinforcement Learning', 'Responsible AI']
---
Construction tasks are inherently unpredictable, with dynamic environments
and safety-critical demands posing significant risks to workers. Exoskeletons
offer potential assistance but falter without accurate intent recognition
across diverse locomotion modes. This paper presents a locomotion prediction
agent leveraging Large Language Models (LLMs) augmented with memory systems,
aimed at improving exoskeleton assistance in such settings. Using multimodal
inputs - spoken commands and visual data from smart glasses - the agent
integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory
(LTM), and Refinement Module to predict locomotion modes effectively.
Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising
to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague
and safety-critical commands. Calibration metrics, including a Brier Score drop
from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability.
This framework supports safer, high-level human-exoskeleton collaboration, with
promise for adaptive assistive systems in dynamic industries.
