---
layout: publication
title: 'Reasoning Beyond Language: A Comprehensive Survey On Latent Chain-of-thought Reasoning'
authors: Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, Xiaoyu Shen
conference: "Arxiv"
year: 2025
bibkey: chen2025reasoning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.16782"}
  - {name: "Code", url: "https://github.com/EIT-NLP/Awesome-Latent-CoT"}
tags: ['Survey Paper', 'Applications', 'Merging', 'Reinforcement Learning', 'Training Techniques', 'Has Code', 'Prompting']
---
Large Language Models (LLMs) have achieved impressive performance on complex reasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional CoT relies on reasoning steps explicitly verbalized in natural language, introducing inefficiencies and limiting its applicability to abstract reasoning. To address this, there has been growing research interest in latent CoT reasoning, where inference occurs within latent spaces. By decoupling reasoning from language, latent reasoning promises richer cognitive representations and more flexible, faster inference. Researchers have explored various directions in this promising field, including training methodologies, structural innovations, and internal reasoning mechanisms. This paper presents a comprehensive overview and analysis of this reasoning paradigm. We begin by proposing a unified taxonomy from four perspectives: token-wise strategies, internal mechanisms, analysis, and applications. We then provide in-depth discussions and comparative analyses of representative methods, highlighting their design patterns, strengths, and open challenges. We aim to provide a structured foundation for advancing this emerging direction in LLM reasoning. The relevant papers will be regularly updated at https://github.com/EIT-NLP/Awesome-Latent-CoT.
