---
layout: publication
title: 'Get Large Language Models Ready To Speak: A Late-fusion Approach For Speech Generation'
authors: Maohao Shen, Shun Zhang, Jilong Wu, Zhiping Xiu, Ehab Albadawy, Yiting Lu, Mike Seltzer, Qing He
conference: "Arxiv"
year: 2024
bibkey: shen2024get
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.20336"}
tags: ['Fine-Tuning', 'Model Architecture', 'Merging', 'Training Techniques', 'Pretraining Methods', 'Multimodal Models']
---
Large language models (LLMs) have revolutionized natural language processing
(NLP) with impressive performance across various text-based tasks. However, the
extension of text-dominant LLMs to with speech generation tasks remains
under-explored. In this work, we introduce a text-to-speech (TTS) system
powered by a fine-tuned Llama model, named TTS-Llama, that achieves
state-of-the-art speech synthesis performance. Building on TTS-Llama, we
further propose MoLE-Llama, a text-and-speech multimodal LLM developed through
purely late-fusion parameter-efficient fine-tuning (PEFT) and a
mixture-of-expert architecture. Extensive empirical results demonstrate
MoLE-Llama's competitive performance on both text-only question-answering (QA)
and TTS tasks, mitigating catastrophic forgetting issue in either modality.
Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks,
demonstrating its great potential as a multimodal dialog system capable of
speech generation.
