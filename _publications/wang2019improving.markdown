---
layout: publication
title: 'Improving Knowledge-aware Dialogue Generation Via Knowledge Base Question Answering'
authors: Jian Wang, Junhao Liu, Wei Bi, Xiaojiang Liu, Kejing He, Ruifeng Xu, Min Yang
conference: "Arxiv"
year: 2019
bibkey: wang2019improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1912.07491"}
  - {name: "Code", url: "https://github.com/siat-nlp/TransDG"}
tags: ['Attention Mechanism', 'Has Code', 'Applications', 'Model Architecture']
---
Neural network models usually suffer from the challenge of incorporating
commonsense knowledge into the open-domain dialogue systems. In this paper, we
propose a novel knowledge-aware dialogue generation model (called TransDG),
which transfers question representation and knowledge matching abilities from
knowledge base question answering (KBQA) task to facilitate the utterance
understanding and factual knowledge selection for dialogue generation. In
addition, we propose a response guiding attention and a multi-step decoding
strategy to steer our model to focus on relevant features for response
generation. Experiments on two benchmark datasets demonstrate that our model
has robust superiority over compared methods in generating informative and
fluent dialogues. Our code is available at https://github.com/siat-nlp/TransDG.
