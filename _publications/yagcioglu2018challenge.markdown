---
layout: publication
title: 'Recipeqa: A Challenge Dataset For Multimodal Comprehension Of Cooking Recipes'
authors: Semih Yagcioglu, Aykut Erdem, Erkut Erdem, Nazli Ikizler-cinbis
conference: Arxiv
year: 2018
citations: 52
bibkey: yagcioglu2018challenge
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1809.00812'}]
tags: [Multimodal Models]
---
Understanding and reasoning about cooking recipes is a fruitful research
direction towards enabling machines to interpret procedural text. In this work,
we introduce RecipeQA, a dataset for multimodal comprehension of cooking
recipes. It comprises of approximately 20K instructional recipes with multiple
modalities such as titles, descriptions and aligned set of images. With over
36K automatically generated question-answer pairs, we design a set of
comprehension and reasoning tasks that require joint understanding of images
and text, capturing the temporal flow of events and making sense of procedural
knowledge. Our preliminary results indicate that RecipeQA will serve as a
challenging test bed and an ideal benchmark for evaluating machine
comprehension systems. The data and leaderboard are available at
http://hucvl.github.io/recipeqa.