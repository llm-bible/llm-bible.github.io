---
layout: publication
title: 'Real: Efficient RLHF Training Of Large Language Models With Parameter Reallocation'
authors: Zhiyu Mei, Wei Fu, Kaiwei Li, Guangju Wang, Huanchen Zhang, Yi Wu
conference: "Arxiv"
year: 2024
bibkey: mei2024efficient
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.14088"}
  - {name: "Code", url: "https://github.com/openpsi-project/ReaLHF"}
tags: ['Agentic', 'Efficiency and Optimization', 'Applications', 'RAG', 'Reinforcement Learning', 'Large-Scale Training', 'Training Techniques', 'Has Code']
---
Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for
empowering large language model (LLM) applications. Compared with the
supervised training process of LLMs, the RLHF training process is much more
sophisticated, requiring a diverse range of computation workloads with
intricate dependencies between multiple LLM instances. Therefore, simply
adopting the fixed parallelization strategies from supervised training for LLMs
can be insufficient for RLHF and result in low training efficiency. To overcome
this limitation, we propose a novel technique named parameter ReaLlocation,
which dynamically adapts the parallelization strategies for different workloads
during training by redistributing LLM parameters across the training cluster.
Building upon this idea, we introduce ReaL, a pioneering system for efficient
RLHF training. ReaL introduces the concept of an execution plan, which defines
a fine-grained resource allocation and parallelization strategy particularly
designed for RLHF training. Based on this concept, ReaL employs a tailored
search algorithm with a lightweight run-time estimator to automatically
discover an efficient execution plan for an instance of RLHF experiment.
Subsequently, the runtime engine deploys the selected plan by effectively
parallelizing computations and redistributing parameters. We evaluate ReaL on
the LLaMA models with up to 70 billion parameters and 128 GPUs. The
experimental results demonstrate that ReaL achieves speedups of up to
\\(3.58\times\\) compared to baseline methods. Furthermore, the execution plans
generated by ReaL exhibit an average of \\(81%\\) performance improvement over
heuristic approaches based on Megatron-LM in the long-context scenario. The
source code of ReaL is publicly available at
https://github.com/openpsi-project/ReaLHF .
