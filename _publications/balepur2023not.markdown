---
layout: publication
title: Its Not Easy Being Wrong Large Language Models Struggle With Process Of Elimination Reasoning
authors: Balepur Nishant, Palta Shramay, Rudinger Rachel
conference: "Arxiv"
year: 2023
bibkey: balepur2023not
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.07532"}
tags: ['GPT', 'Interpretability And Explainability', 'Model Architecture', 'Prompting']
---
Chain45;of45;thought (COT) prompting can help large language models (LLMs) reason toward correct answers but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE) when used with COT can enhance self45;consistency interpretability and tasks such as medical diagnoses of exclusion. Thus we propose PoE with COT where LLMs must reason toward incorrect options on multiple45;choice questions. We evaluate the ability of GPT45;3.5 LLaMA45;2 and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self45;consistency of each strategy. To study these issues further we conduct error analyses and give suggestions for future work.
