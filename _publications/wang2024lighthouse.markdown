---
layout: publication
title: LightHouse A Survey of AGI Hallucination
authors: Wang Feng
conference: "Arxiv"
year: 2024
bibkey: wang2024lighthouse
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.06792"}
tags: ['Fine Tuning', 'Multimodal Models', 'Reinforcement Learning', 'Survey Paper']
---
With the development of artificial intelligence large-scale models have become increasingly intelligent. However numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research. In the pursuit of achieving strong artificial intelligence a significant volume of research effort is being invested in the AGI (Artificial General Intelligence) hallucination research. Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models). As for multimodal AGI research on hallucinations is still in an early stage. To further the progress of research in the domain of hallucinatory phenomena we present a birds eye view of hallucinations in AGI summarizing the current work on AGI hallucinations and proposing some directions for future research.
