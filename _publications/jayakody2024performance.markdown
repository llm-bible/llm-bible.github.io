---
layout: publication
title: 'Performance Of Recent Large Language Models For A Low-resourced Language'
authors: Ravindu Jayakody, Gihan Dias
conference: "Arxiv"
year: 2024
bibkey: jayakody2024performance
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.21330"}
tags: ['Fine-Tuning', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods']
---
Large Language Models (LLMs) have shown significant advances in the past
year. In addition to new versions of GPT and Llama, several other LLMs have
been introduced recently. Some of these are open models available for download
and modification.
  Although multilingual large language models have been available for some
time, their performance on low-resourced languages such as Sinhala has been
poor. We evaluated four recent LLMs on their performance directly in the
Sinhala language, and by translation to and from English. We also evaluated
their fine-tunability with a small amount of fine-tuning data. Claude and GPT
4o perform well out-of-the-box and do significantly better than previous
versions. Llama and Mistral perform poorly but show some promise of improvement
with fine tuning.
