---
layout: publication
title: Performance Of Recent Large Language Models For A Low45;resourced Language
authors: Jayakody Ravindu, Dias Gihan
conference: "Arxiv"
year: 2024
bibkey: jayakody2024performance
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.21330"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Large Language Models (LLMs) have shown significant advances in the past year. In addition to new versions of GPT and Llama several other LLMs have been introduced recently. Some of these are open models available for download and modification. Although multilingual large language models have been available for some time their performance on low45;resourced languages such as Sinhala has been poor. We evaluated four recent LLMs on their performance directly in the Sinhala language and by translation to and from English. We also evaluated their fine45;tunability with a small amount of fine45;tuning data. Claude and GPT 4o perform well out45;of45;the45;box and do significantly better than previous versions. Llama and Mistral perform poorly but show some promise of improvement with fine tuning.
