---
layout: publication
title: PEFT45;U Parameter45;efficient Fine45;tuning For User Personalization
authors: Clarke Christopher, Heng Yuzhao, Tang Lingjia, Mars Jason
conference: "Arxiv"
year: 2024
bibkey: clarke2024peft
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.18078"}
tags: ['Applications', 'Efficiency And Optimization', 'GPT', 'Model Architecture']
---
The recent emergence of Large Language Models (LLMs) has heralded a new era of human45;AI interaction. These sophisticated models exemplified by Chat45;GPT and its successors have exhibited remarkable capabilities in language understanding. However as these LLMs have undergone exponential growth a crucial dimension that remains understudied is the personalization of these models. Large foundation models such as GPT45;3 etc. focus on creating a universal model that serves a broad range of tasks and users. This approach emphasizes the models generalization capabilities treating users as a collective rather than as distinct individuals. While practical for many common applications this one45;size45;fits45;all approach often fails to address the rich tapestry of human diversity and individual needs. To explore this issue we introduce the PEFT45;U Benchmark a new dataset for building and evaluating NLP models for user personalization. datasetname123;125; consists of a series of user45;centered tasks containing diverse and individualized expressions where the preferences of users can potentially differ for the same input. Using PEFT45;U we explore the challenge of efficiently personalizing LLMs to accommodate user45;specific preferences in the context of diverse user45;centered tasks.
