---
layout: publication
title: Swectrl45;mini A Data45;transparent Transformer45;based Large Language Model For Controllable Text Generation In Swedish
authors: Kalpakchi Dmytro, Boye Johan
conference: "Arxiv"
year: 2023
bibkey: kalpakchi2023swectrl
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.13994"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
We present SweCTRL45;Mini a large Swedish language model that can be used for inference and fine45;tuning on a single consumer45;grade GPU. The model is based on the CTRL architecture by Keskar McCann Varshney Xiong and Socher (2019) which means that users of the SweCTRL45;Mini model can control the genre of the generated text by inserting special tokens in the generation prompts. SweCTRL45;Mini is trained on a subset of the Swedish part of the mC4 corpus and a set of Swedish novels. In this article we provide (1) a detailed account of the utilized training data and text pre45;processing steps to the extent that it is possible to check whether a specific phrase/source was a part of the training data and (2) an evaluation of the model on both discriminative tasks using automatic evaluation methods and generative tasks using human referees. We also compare the generative capabilities of the model with those of GPT45;3. SweCTRL45;Mini is fully open and available for download.
