---
layout: publication
title: FANNO Augmenting High45;quality Instruction Data With Open45;sourced Llms Only
authors: Zhu He, Su Junyou, Lun Tianle, Tao Yicheng, Zhang Wenjia, Fan Zipei, Chen Guanhua
conference: "Arxiv"
year: 2024
bibkey: zhu2024augmenting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.01323"}
tags: ['Applications', 'GPT', 'Model Architecture', 'RAG', 'Tools']
---
Instruction fine45;tuning stands as a crucial advancement in leveraging large language models (LLMs) for enhanced task performance. However the annotation of instruction datasets has traditionally been expensive and laborious often relying on manual annotations or costly API calls of proprietary LLMs. To address these challenges we introduce FANNO a fully autonomous open45;sourced framework that revolutionizes the annotation process without the need for pre45;existing annotated data. Utilizing a Mistral45;7b45;instruct model FANNO efficiently produces diverse and high45;quality datasets through a structured process involving document pre45;screening instruction generation and response generation. Experiments on Open LLM Leaderboard and AlpacaEval benchmark show that the FANNO can generate high45;quality data with diversity and complexity for free comparable to human45;annotated or cleaned datasets like Alpaca45;GPT445;Cleaned.
