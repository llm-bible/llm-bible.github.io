---
layout: publication
title: 'EE-MLLM: A Data-efficient And Compute-efficient Multimodal Large Language Model'
authors: Ma Feipeng, Zhou Yizhou, Li Hebei, He Zilong, Wu Siying, Rao Fengyun, Zhang Yueyi, Sun Xiaoyan
conference: "Arxiv"
year: 2024
bibkey: ma2024ee
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.11795"}
tags: ['Attention Mechanism', 'Efficiency And Optimization', 'Model Architecture', 'Multimodal Models', 'RAG', 'Transformer']
---
In the realm of multimodal research, numerous studies leverage substantial
image-text pairs to conduct modal alignment learning, transforming Large
Language Models (LLMs) into Multimodal LLMs and excelling in a variety of
visual-language tasks. The prevailing methodologies primarily fall into two
categories: self-attention-based and cross-attention-based methods. While
self-attention-based methods offer superior data efficiency due to their simple
MLP architecture, they often suffer from lower computational efficiency due to
concatenating visual and textual tokens as input for LLM. Conversely,
cross-attention-based methods, although less data-efficient due to additional
learnable parameters, exhibit higher computational efficiency by avoiding long
sequence input for LLM. To address these trade-offs, we introduce the
Data-Efficient and Compute-Efficient Multimodal Large Language Model (EE-MLLM).
Without introducing additional modules or learnable parameters, EE-MLLM
achieves both data and compute efficiency. Specifically, we modify the original
self-attention mechanism in MLLM to a composite attention mechanism. This
mechanism has two key characteristics: 1) Eliminating the computational
overhead of self-attention within visual tokens to achieve compute efficiency,
and 2) Reusing the weights on each layer of LLM to facilitate effective
modality alignment between vision and language for data efficiency.
Experimental results demonstrate the effectiveness of EE-MLLM across a range of
benchmarks, including general-purpose datasets like MMBench and SeedBench, as
well as fine-grained tasks such as TextVQA and DocVQA.
