---
layout: publication
title: EE45;MLLM A Data45;efficient And Compute45;efficient Multimodal Large Language Model
authors: Ma Feipeng, Zhou Yizhou, Li Hebei, He Zilong, Wu Siying, Rao Fengyun, Zhang Yueyi, Sun Xiaoyan
conference: "Arxiv"
year: 2024
bibkey: ma2024ee
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.11795"}
tags: ['Attention Mechanism', 'Efficiency And Optimization', 'Model Architecture', 'Multimodal Models', 'RAG', 'Transformer']
---
In the realm of multimodal research numerous studies leverage substantial image45;text pairs to conduct modal alignment learning transforming Large Language Models (LLMs) into Multimodal LLMs and excelling in a variety of visual45;language tasks. The prevailing methodologies primarily fall into two categories self45;attention45;based and cross45;attention45;based methods. While self45;attention45;based methods offer superior data efficiency due to their simple MLP architecture they often suffer from lower computational efficiency due to concatenating visual and textual tokens as input for LLM. Conversely cross45;attention45;based methods although less data45;efficient due to additional learnable parameters exhibit higher computational efficiency by avoiding long sequence input for LLM. To address these trade45;offs we introduce the Data45;Efficient and Compute45;Efficient Multimodal Large Language Model (EE45;MLLM). Without introducing additional modules or learnable parameters EE45;MLLM achieves both data and compute efficiency. Specifically we modify the original self45;attention mechanism in MLLM to a composite attention mechanism. This mechanism has two key characteristics 1) Eliminating the computational overhead of self45;attention within visual tokens to achieve compute efficiency and 2) Reusing the weights on each layer of LLM to facilitate effective modality alignment between vision and language for data efficiency. Experimental results demonstrate the effectiveness of EE45;MLLM across a range of benchmarks including general45;purpose datasets like MMBench and SeedBench as well as fine45;grained tasks such as TextVQA and DocVQA.
