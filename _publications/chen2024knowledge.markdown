---
layout: publication
title: 'Knowledge Distillation Of Black-box Large Language Models'
authors: Hongzhan Chen, Ruijun Chen, Yuqi Yi, Xiaojun Quan, Chenliang Li, Ming Yan, Ji Zhang
conference: "Arxiv"
year: 2024
bibkey: chen2024knowledge
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.07013"}
tags: ['Efficiency and Optimization', 'GPT', 'RAG', 'Model Architecture', 'Distillation']
---
Given the exceptional performance of proprietary large language models (LLMs)
like GPT-4, recent research has increasingly focused on boosting the
capabilities of smaller models through knowledge distillation (KD) from these
powerful yet black-box teachers. While leveraging the high-quality outputs of
these teachers is advantageous, the inaccessibility of their internal states
often limits effective knowledge transfer. To overcome this limitation, we
introduce Proxy-KD, a novel method that uses a proxy model to facilitate the
efficient transfer of knowledge from black-box LLMs to smaller models. Our
experiments show that Proxy-KD not only enhances the performance of KD from
black-box teacher models but also surpasses traditional white-box KD
techniques.~This approach presents a compelling new avenue for distilling
knowledge from advanced LLMs.
