---
layout: publication
title: L45;TUNING Synchronized Label Tuning For Prompt And Prefix In Llms
authors: Kowsher Md., Sobuj Md. Shohanur Islam, Mahmud Asif, Prottasha Nusrat Jahan, Bhat Prakash
conference: "Arxiv"
year: 2023
bibkey: kowsher2023l
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.01643"}
tags: ['Applications', 'Efficiency And Optimization', 'Prompting', 'Tools', 'Training Techniques']
---
Efficiently fine45;tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods like prompt or prefix tuning typically rely on arbitrary tokens for training leading to prolonged training times and generalized token use across various class labels. To address these issues this paper introduces L45;Tuning an efficient fine45;tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods L45;Tuning focuses on the fine45;tuning of label tokens processed through a pre45;trained LLM thereby harnessing its pre45;existing semantic knowledge. This technique not only improves the fine45;tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class enhancing the models training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with L45;Tuning compared to traditional approaches marking a promising advancement in fine45;tuning LLMs for complex language tasks.
