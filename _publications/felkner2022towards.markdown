---
layout: publication
title: Towards Winoqueer Developing A Benchmark For Anti45;queer Bias In Large Language Models
authors: Felkner Virginia K., Chang Ho-chun Herbert, Jang Eugene, May Jonathan
conference: "Arxiv"
year: 2022
bibkey: felkner2022towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2206.11484"}
tags: ['BERT', 'Ethics And Bias', 'Fine Tuning', 'Model Architecture']
---
This paper presents exploratory work on whether and to what extent biases against queer and trans people are encoded in large language models (LLMs) such as BERT. We also propose a method for reducing these biases in downstream tasks finetuning the models on data written by and/or about queer people. To measure anti45;queer bias we introduce a new benchmark dataset WinoQueer modeled after other bias45;detection benchmarks but addressing homophobic and transphobic biases. We found that BERT shows significant homophobic bias but this bias can be mostly mitigated by finetuning BERT on a natural language corpus written by members of the LGBTQ+ community.
