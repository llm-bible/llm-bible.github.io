---
layout: publication
title: Evaluating Pretrained Transformer Models For Entity Linking In Task45;oriented Dialog
authors: Jayanthi Sai Muralidhar, Embar Varsha, Raghunathan Karthik
conference: "Arxiv"
year: 2021
bibkey: jayanthi2021evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2112.08327"}
  - {name: "Code", url: "https://github.com/murali1996/el&#95;tod"}
tags: ['Has Code', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
The wide applicability of pretrained transformer models (PTMs) for natural language tasks is well demonstrated but their ability to comprehend short phrases of text is less explored. To this end we evaluate different PTMs from the lens of unsupervised Entity Linking in task45;oriented dialog across 5 characteristics 45;45; syntactic semantic short45;forms numeric and phonetic. Our results demonstrate that several of the PTMs produce sub45;par results when compared to traditional techniques albeit competitive to other neural baselines. We find that some of their shortcomings can be addressed by using PTMs fine45;tuned for text45;similarity tasks which illustrate an improved ability in comprehending semantic and syntactic correspondences as well as some improvements for short45;forms numeric and phonetic variations in entity mentions. We perform qualitative analysis to understand nuances in their predictions and discuss scope for further improvements. Code can be found at https://github.com/murali1996/el&#95;tod
