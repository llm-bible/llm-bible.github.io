---
layout: publication
title: CROME Cross45;modal Adapters For Efficient Multimodal LLM
authors: Ebrahimi Sayna, Arik Sercan O., Nama Tejas, Pfister Tomas
conference: "Arxiv"
year: 2024
bibkey: ebrahimi2024cross
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.06610"}
tags: ['Applications', 'Efficiency And Optimization', 'Multimodal Models', 'Tools', 'Training Techniques']
---
Multimodal Large Language Models (MLLMs) demonstrate remarkable image45;language capabilities but their widespread use faces challenges in cost45;effective training and adaptation. Existing approaches often necessitate expensive language model retraining and limited adaptability. Additionally the current focus on zero45;shot performance improvements offers insufficient guidance for task45;specific tuning. We propose CROME an efficient vision45;language instruction tuning framework. It features a novel gated cross45;modal adapter that effectively combines visual and textual representations prior to input into a frozen LLM. This lightweight adapter trained with minimal parameters enables efficient cross45;modal understanding. Notably CROME demonstrates superior zero45;shot performance on standard visual question answering and instruction45;following benchmarks. Moreover it yields fine45;tuning with exceptional parameter efficiency competing with task45;specific specialist state45;of45;the45;art methods. CROME demonstrates the potential of pre45;LM alignment for building scalable adaptable and parameter45;efficient multimodal models.
