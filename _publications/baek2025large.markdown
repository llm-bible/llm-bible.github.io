---
layout: publication
title: 'PCGRLLM: Large Language Model-driven Reward Design For Procedural Content Generation Reinforcement Learning'
authors: In-chang Baek, Sung-hyun Kim, Sam Earle, Zehua Jiang, Noh Jin-ha, Julian Togelius, Kyung-joong Kim
conference: "Arxiv"
year: 2025
bibkey: baek2025large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.10906"}
tags: ['Agentic', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques', 'Agent', 'Prompting']
---
Reward design plays a pivotal role in the training of game AIs, requiring
substantial domain-specific knowledge and human effort. In recent years,
several studies have explored reward generation for training game agents and
controlling robots using large language models (LLMs). In the content
generation literature, there has been early work on generating reward functions
for reinforcement learning agent generators. This work introduces PCGRLLM, an
extended architecture based on earlier work, which employs a feedback mechanism
and several reasoning-based prompt engineering techniques. We evaluate the
proposed method on a story-to-reward generation task in a two-dimensional
environment using two state-of-the-art LLMs, demonstrating the generalizability
of our approach. Our experiments provide insightful evaluations that
demonstrate the capabilities of LLMs essential for content generation tasks.
The results highlight significant performance improvements of 415% and 40%
respectively, depending on the zero-shot capabilities of the language model.
Our work demonstrates the potential to reduce human dependency in game AI
development, while supporting and enhancing creative processes.
