---
layout: publication
title: Llamax Scaling Linguistic Horizons Of LLM By Enhancing Translation Capabilities Beyond 100 Languages
authors: Lu Yinquan, Zhu Wenhao, Li Lei, Qiao Yu, Yuan Fei
conference: "Arxiv"
year: 2024
bibkey: lu2024scaling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.05975"}
  - {name: "Code", url: "https://github.com/CONE&#45;MT/LLaMAX/.&#125;&#125;"}
  - {name: "Code", url: "https://huggingface.co/LLaMAX/.&#125;&#125;"}
tags: ['Applications', 'Has Code', 'Reinforcement Learning', 'Training Techniques']
---
Large Language Models~(LLMs) demonstrate remarkable translation capabilities in high45;resource language tasks yet their performance in low45;resource languages is hindered by insufficient multilingual data during pre45;training. To address this we dedicate 35000 A10045;SXM445;80GB GPU hours in conducting extensive multilingual continual pre45;training on the LLaMA series models enabling translation support across more than 100 languages. Through a comprehensive analysis of training strategies such as vocabulary expansion and data augmentation we develop LLaMAX. Remarkably without sacrificing its generalization ability LLaMAX achieves significantly higher translation performance compared to existing open45;source LLMs~(by more than 10 spBLEU points) and performs on45;par with specialized translation model~(M2M45;10045;12B) on the Flores45;101 benchmark. Extensive experiments indicate that LLaMAX can serve as a robust multilingual foundation model. The code~footnote123;url123;https://github.com/CONE&#45;MT/LLaMAX/.&#125;&#125; and models~footnote123;url123;https://huggingface.co/LLaMAX/.&#125;&#125; are publicly available.
