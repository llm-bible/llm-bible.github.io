---
layout: publication
title: 'Training A T5 Using Lab-sized Resources'
authors: Manuel R. Ciosici, Leon Derczynski
conference: "Arxiv"
year: 2022
bibkey: ciosici2022training
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2208.12097"}
tags: ['Training Techniques']
---
Training large neural language models on large datasets is resource- and
time-intensive. These requirements create a barrier to entry, where those with
fewer resources cannot build competitive models. This paper presents various
techniques for making it possible to (a) train a large language model using
resources that a modest research lab might have, and (b) train it in a
reasonable amount of time. We provide concrete recommendations for
practitioners, which we illustrate with a case study: a T5 model for Danish,
the first for this language.
