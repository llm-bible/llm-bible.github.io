---
layout: publication
title: 'Humorreject: Decoupling LLM Safety From Refusal Prefix Via A Little Humor'
authors: Zihui Wu, Haichang Gao, Jiacheng Luo, Zhaoxiang Liu
conference: "Arxiv"
year: 2025
bibkey: wu2025decoupling
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.13677'}
tags: ['Security', 'Responsible AI', 'Training Techniques']
---
Large Language Models (LLMs) commonly rely on explicit refusal prefixes for
safety, making them vulnerable to prefix injection attacks. We introduce
HumorReject, a novel data-driven approach that reimagines LLM safety by
decoupling it from refusal prefixes through humor as an indirect refusal
strategy. Rather than explicitly rejecting harmful instructions, HumorReject
responds with contextually appropriate humor that naturally defuses potentially
dangerous requests. Our approach effectively addresses common "over-defense"
issues while demonstrating superior robustness against various attack vectors.
Our findings suggest that improvements in training data design can be as
important as the alignment algorithm itself in achieving effective LLM safety.
