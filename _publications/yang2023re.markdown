---
layout: publication
title: Re45;vilm Retrieval45;augmented Visual Language Model For Zero And Few45;shot Image Captioning
authors: Yang Zhuolin, Ping Wei, Liu Zihan, Korthikanti Vijay, Nie Weili, Huang De-an, Fan Linxi, Yu Zhiding, Lan Shiyi, Li Bo, Liu Ming-yu, Zhu Yuke, Shoeybi Mohammad, Catanzaro Bryan, Xiao Chaowei, Anandkumar Anima
conference: "Arxiv"
year: 2023
bibkey: yang2023re
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2302.04858"}
tags: ['Applications', 'Language Modeling', 'Reinforcement Learning']
---
Augmenting pretrained language models (LMs) with a vision encoder (e.g. Flamingo) has obtained the state45;of45;the45;art results in image45;to45;text generation. However these models store all the knowledge within their parameters thus often requiring enormous model parameters to model the abundant visual concepts and very rich textual descriptions. Additionally they are inefficient in incorporating new data requiring a computational45;expensive fine45;tuning process. In this work we introduce a Retrieval45;augmented Visual Language Model Re45;ViLM built upon the Flamingo that supports retrieving the relevant knowledge from the external database for zero and in45;context few45;shot image45;to45;text generations. By storing certain knowledge explicitly in the external database our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in45;context few45;shot learning capabilities. We demonstrate that Re45;ViLM significantly boosts performance for image45;to45;text generation tasks especially for zero45;shot and few45;shot generation in out45;of45;domain settings with 4 times less parameters compared with baseline methods.
