---
layout: publication
title: Guiding Large Language Models Via Directional Stimulus Prompting
authors: Li Zekun, Peng Baolin, He Pengcheng, Galley Michel, Gao Jianfeng, Yan Xifeng
conference: "Arxiv"
year: 2023
bibkey: li2023guiding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2302.11520"}
  - {name: "Code", url: "https://github.com/Leezekun/Directional&#45;Stimulus&#45;Prompting&#125;"}
tags: ['Agentic', 'Applications', 'GPT', 'Has Code', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Tools']
---
We introduce Directional Stimulus Prompting a novel framework for guiding black45;box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs our method employs a small tunable policy model (e.g. T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced instance45;specific hints and clues to guide LLMs in generating desired outcomes such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine45;tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLMs output. We assess our method across summarization dialogue response generation and chain45;of45;thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs (e.g. ChatGPT Codex InstructGPT) performance on these supervised tasks using minimal labeled data. Notably using just 80 dialogues on the MultiWOZ dataset our approach enhances ChatGPTs performance by an impressive 41.437; matching or surpassing some fully supervised start45;of45;the45;art models. Additionally the instance45;specific chain45;of45;thought prompt generated by our approach improves InstructGPTs reasoning accuracy compared to human45;crafted or automatically generated prompts. The code and data are publicly available at url123;https://github.com/Leezekun/Directional&#45;Stimulus&#45;Prompting&#125;.
