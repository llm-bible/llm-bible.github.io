---
layout: publication
title: 'Improve Temporal Awareness Of Llms For Sequential Recommendation'
authors: Zhendong Chu, Zichao Wang, Ruiyi Zhang, Yangfeng Ji, Hongning Wang, Tong Sun
conference: "Arxiv"
year: 2024
bibkey: chu2024improve
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.02778"}
tags: ['Tools', 'Survey Paper', 'Prompting']
---
Large language models (LLMs) have demonstrated impressive zero-shot abilities
in solving a wide range of general-purpose tasks. However, it is empirically
found that LLMs fall short in recognizing and utilizing temporal information,
rendering poor performance in tasks that require an understanding of sequential
data, such as sequential recommendation. In this paper, we aim to improve
temporal awareness of LLMs by designing a principled prompting framework
inspired by human cognitive processes. Specifically, we propose three prompting
strategies to exploit temporal information within historical interactions for
LLM-based sequential recommendation. Besides, we emulate divergent thinking by
aggregating LLM ranking results derived from these strategies. Evaluations on
MovieLens-1M and Amazon Review datasets indicate that our proposed method
significantly enhances the zero-shot capabilities of LLMs in sequential
recommendation tasks.
