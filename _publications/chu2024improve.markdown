---
layout: publication
title: Improve Temporal Awareness Of Llms For Sequential Recommendation
authors: Chu Zhendong, Wang Zichao, Zhang Ruiyi, Ji Yangfeng, Wang Hongning, Sun Tong
conference: "Arxiv"
year: 2024
bibkey: chu2024improve
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.02778"}
tags: ['Pretraining Methods', 'Prompting', 'Survey Paper', 'Tools']
---
Large language models (LLMs) have demonstrated impressive zero45;shot abilities in solving a wide range of general45;purpose tasks. However it is empirically found that LLMs fall short in recognizing and utilizing temporal information rendering poor performance in tasks that require an understanding of sequential data such as sequential recommendation. In this paper we aim to improve temporal awareness of LLMs by designing a principled prompting framework inspired by human cognitive processes. Specifically we propose three prompting strategies to exploit temporal information within historical interactions for LLM45;based sequential recommendation. Besides we emulate divergent thinking by aggregating LLM ranking results derived from these strategies. Evaluations on MovieLens45;1M and Amazon Review datasets indicate that our proposed method significantly enhances the zero45;shot capabilities of LLMs in sequential recommendation tasks.
