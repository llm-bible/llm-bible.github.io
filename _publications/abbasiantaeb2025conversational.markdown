---
layout: publication
title: 'Conversational Gold: Evaluating Personalized Conversational Search System Using Gold Nuggets'
authors: Zahra Abbasiantaeb, Simon Lupart, Leif Azzopardi, Jeffery Dalton, Mohammad Aliannejadi
conference: "Arxiv"
year: 2025
bibkey: abbasiantaeb2025conversational
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.09902"}
  - {name: "Code", url: "https://github.com/irlabamsterdam/CONE-RAG"}
tags: ['Fine-Tuning', 'Tools', 'RAG', 'Reinforcement Learning', 'Has Code']
---
The rise of personalized conversational search systems has been driven by
advancements in Large Language Models (LLMs), enabling these systems to
retrieve and generate answers for complex information needs. However, the
automatic evaluation of responses generated by Retrieval Augmented Generation
(RAG) systems remains an understudied challenge. In this paper, we introduce a
new resource for assessing the retrieval effectiveness and relevance of
response generated by RAG systems, using a nugget-based evaluation framework.
Built upon the foundation of TREC iKAT 2023, our dataset extends to the TREC
iKAT 2024 collection, which includes 17 conversations and 20,575 relevance
passage assessments, together with 2,279 extracted gold nuggets, and 62
manually written gold answers from NIST assessors. While maintaining the core
structure of its predecessor, this new collection enables a deeper exploration
of generation tasks in conversational settings. Key improvements in iKAT 2024
include: (1) ``gold nuggets'' -- concise, essential pieces of information
extracted from relevant passages of the collection -- which serve as a
foundation for automatic response evaluation; (2) manually written answers to
provide a gold standard for response evaluation; (3) unanswerable questions to
evaluate model hallucination; (4) expanded user personas, providing richer
contextual grounding; and (5) a transition from Personal Text Knowledge Base
(PTKB) ranking to PTKB classification and selection. Built on this resource, we
provide a framework for long-form answer generation evaluation, involving
nuggets extraction and nuggets matching, linked to retrieval. This establishes
a solid resource for advancing research in personalized conversational search
and long-form answer generation. Our resources are publicly available at
https://github.com/irlabamsterdam/CONE-RAG.
