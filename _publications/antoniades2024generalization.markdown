---
layout: publication
title: Generalization V.s. Memorization Tracing Language Models Capabilities Back To Pretraining Data
authors: Antoniades Antonis, Wang Xinyi, Elazar Yanai, Amayuelas Alfonso, Albalak Alon, Zhang Kexun, Wang William Yang
conference: "Arxiv"
year: 2024
bibkey: antoniades2024generalization
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.14985"}
tags: ['Applications', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
Despite the proven utility of large language models (LLMs) in real45;world applications there remains a lack of understanding regarding how they leverage their large45;scale pretraining text corpora to achieve such capabilities. In this work we investigate the interplay between generalization and memorization in pretrained LLMs at scale through a comprehensive n45;gram analysis of their training data. Our experiments focus on three general task types translation question45;answering and multiple45;choice reasoning. With various sizes of open45;source LLMs and their pretraining corpora we observe that as the model size increases the task45;relevant n45;gram pair data becomes increasingly important leading to improved task performance decreased memorization stronger generalization and emergent abilities. Our results support the hypothesis that LLMs capabilities emerge from a delicate balance of memorization and generalization with sufficient task45;related pretraining data and point the way to larger45;scale analyses that could further improve our understanding of these models.
