---
layout: publication
title: 'Generated Data With Fake Privacy: Hidden Dangers Of Fine-tuning Large Language Models On Generated Data'
authors: Atilla Akkus, Masoud Poorghaffar Aghdam, Mingjie Li, Junjie Chu, Michael Backes, Yang Zhang, Sinem Sav
conference: "Arxiv"
year: 2024
bibkey: akkus2024generated
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.11423"}
tags: ['Fine-Tuning', 'Transformer', 'Model Architecture', 'Reinforcement Learning', 'Security', 'Training Techniques', 'Pretraining Methods']
---
Large language models (LLMs) have demonstrated significant success in various
domain-specific tasks, with their performance often improving substantially
after fine-tuning. However, fine-tuning with real-world data introduces privacy
risks. To mitigate these risks, developers increasingly rely on synthetic data
generation as an alternative to using real data, as data generated by
traditional models is believed to be different from real-world data. However,
with the advanced capabilities of LLMs, the distinction between real data and
data generated by these models has become nearly indistinguishable. This
convergence introduces similar privacy risks for generated data to those
associated with real data. Our study investigates whether fine-tuning with
LLM-generated data truly enhances privacy or introduces additional privacy
risks by examining the structural characteristics of data generated by LLMs,
focusing on two primary fine-tuning approaches: supervised fine-tuning (SFT)
with unstructured (plain-text) generated data and self-instruct tuning. In the
scenario of SFT, the data is put into a particular instruction tuning format
used by previous studies. We use Personal Information Identifier (PII) leakage
and Membership Inference Attacks (MIAs) on the Pythia Model Suite and Open
Pre-trained Transformer (OPT) to measure privacy risks. Notably, after
fine-tuning with unstructured generated data, the rate of successful PII
extractions for Pythia increased by over 20%, highlighting the potential
privacy implications of such approaches. Furthermore, the ROC-AUC score of MIAs
for Pythia-6.9b, the second biggest model of the suite, increases over 40%
after self-instruct tuning. Our results indicate the potential privacy risks
associated with fine-tuning LLMs using generated data, underscoring the need
for careful consideration of privacy safeguards in such approaches.
