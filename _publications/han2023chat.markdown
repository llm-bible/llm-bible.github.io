---
layout: publication
title: In45;context Alignment Chat With Vanilla Language Models Before Fine45;tuning
authors: Han Xiaochuang
conference: "Arxiv"
year: 2023
bibkey: han2023chat
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.04275"}
tags: ['Pretraining Methods', 'Prompting', 'RAG']
---
In this note we explore inference45;time alignment through in45;context learning. We consider a vanilla pretrained language model Llama45;2 before any fine45;tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat45;style instructions. Compared to direct prompting the in45;context alignment without changing model weights leads to a 7x increase in win45;rate w.r.t. the text45;davinci45;003 model from OpenAI making the vanilla language model comparable to strong baselines with alignment fine45;tuning.
