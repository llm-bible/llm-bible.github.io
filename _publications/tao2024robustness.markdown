---
layout: publication
title: 'Robustness Of Large Language Models Against Adversarial Attacks'
authors: Yiyi Tao, Yixian Shen, Hang Zhang, Yanxin Shen, Lun Wang, Chuanqi Shi, Shaoshuai Du
conference: "Arxiv"
year: 2024
bibkey: tao2024robustness
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.17011"}
tags: ['Responsible AI', 'Security', 'Training Techniques', 'Model Architecture', 'Survey Paper', 'GPT', 'Prompting', 'Applications']
---
The increasing deployment of Large Language Models (LLMs) in various
applications necessitates a rigorous evaluation of their robustness against
adversarial attacks. In this paper, we present a comprehensive study on the
robustness of GPT LLM family. We employ two distinct evaluation methods to
assess their resilience. The first method introduce character-level text attack
in input prompts, testing the models on three sentiment classification
datasets: StanfordNLP/IMDB, Yelp Reviews, and SST-2. The second method involves
using jailbreak prompts to challenge the safety mechanisms of the LLMs. Our
experiments reveal significant variations in the robustness of these models,
demonstrating their varying degrees of vulnerability to both character-level
and semantic-level adversarial attacks. These findings underscore the necessity
for improved adversarial training and enhanced safety mechanisms to bolster the
robustness of LLMs.
