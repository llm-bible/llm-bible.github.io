---
layout: publication
title: 'Rethinking Document-level Neural Machine Translation'
authors: Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang, Jiajun Chen, Lei Li
conference: "Arxiv"
year: 2020
bibkey: sun2020rethinking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2010.08961'}
tags: ['Transformer', 'Training Techniques', 'Applications', 'Model Architecture', 'Pretraining Methods']
---
This paper does not aim at introducing a novel model for document-level
neural machine translation. Instead, we head back to the original Transformer
model and hope to answer the following question: Is the capacity of current
models strong enough for document-level translation? Interestingly, we observe
that the original Transformer with appropriate training techniques can achieve
strong results for document translation, even with a length of 2000 words. We
evaluate this model and several recent approaches on nine document-level
datasets and two sentence-level datasets across six languages. Experiments show
that document-level Transformer models outperforms sentence-level ones and many
previous methods in a comprehensive set of metrics, including BLEU, four
lexical indices, three newly proposed assistant linguistic indicators, and
human evaluation.
