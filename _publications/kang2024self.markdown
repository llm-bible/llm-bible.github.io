---
layout: publication
title: Self45;moe Towards Compositional Large Language Models With Self45;specialized Experts
authors: Kang Junmo, Karlinsky Leonid, Luo Hongyin, Wang Zhen, Hansen Jacob, Glass James, Cox David, Panda Rameswar, Feris Rogerio, Ritter Alan
conference: "Arxiv"
year: 2024
bibkey: kang2024self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.12034"}
tags: ['Interpretability And Explainability', 'Merging', 'Pretraining Methods', 'RAG']
---
We present Self45;MoE an approach that transforms a monolithic LLM into a compositional modular system of self45;specialized experts named MiXSE (MiXture of Self45;specialized Experts). Our approach leverages self45;specialization which constructs expert modules using self45;generated synthetic data each equipped with a shared base LLM and incorporating self45;optimized routing. This allows for dynamic and capability45;specific handling of various target tasks enhancing overall capabilities without extensive human45;labeled data and added parameters. Our empirical results reveal that specializing LLMs may exhibit potential trade45;offs in performances on non45;specialized tasks. On the other hand our Self45;MoE demonstrates substantial improvements over the base LLM across diverse benchmarks such as knowledge reasoning math and coding. It also consistently outperforms other methods including instance merging and weight merging while offering better flexibility and interpretability by design with semantic experts and routing. Our findings highlight the critical role of modularity and the potential of self45;improvement in achieving efficient scalable and adaptable systems.
