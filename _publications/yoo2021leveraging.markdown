---
layout: publication
title: Gpt3mix Leveraging Large45;scale Language Models For Text Augmentation
authors: Yoo Kang Min, Park Dongju, Kang Jaewook, Lee Sang-woo, Park Woomyeong
conference: "Arxiv"
year: 2021
bibkey: yoo2021leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2104.08826"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG']
---
Large45;scale language models such as GPT45;3 are excellent few45;shot learners allowing them to be controlled via natural text prompts. Recent studies report that prompt45;based direct classification eliminates the need for fine45;tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large45;scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft45;labels predicted by the language models effectively distilling knowledge from the large45;scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classification tasks and show that our method hugely outperforms existing text augmentation methods. Ablation studies and a qualitative analysis provide more insights into our approach.
