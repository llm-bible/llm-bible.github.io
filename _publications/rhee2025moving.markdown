---
layout: publication
title: 'Moving Beyond Next-token Prediction: Transformers Are Context-sensitive Language Generators'
authors: Phill Kyu Rhee
conference: "Arxiv"
year: 2025
bibkey: rhee2025moving
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.10845'}
tags: ['Attention Mechanism', 'Interpretability and Explainability', 'Transformer', 'Model Architecture', 'Applications', 'Tools', 'GPT', 'Reinforcement Learning', 'Pretraining Methods']
---
Large Language Models (LLMs), powered by Transformers, have demonstrated
human-like intelligence capabilities, yet their underlying mechanisms remain
poorly understood. This paper presents a novel framework for interpreting LLMs
as probabilistic left context-sensitive languages (CSLs) generators. We
hypothesize that Transformers can be effectively decomposed into three
fundamental components: context windows, attention mechanisms, and
autoregressive generation frameworks. This decomposition allows for the
development of more flexible and interpretable computational models, moving
beyond the traditional view of attention and autoregression as inseparable
processes. We argue that next-token predictions can be understood as
probabilistic, dynamic approximations of left CSL production rules, providing
an intuitive explanation for how simple token predictions can yield human-like
intelligence outputs. Given that all CSLs are left context-sensitive
(Penttonen, 1974), we conclude that Transformers stochastically approximate
CSLs, which are widely recognized as models of human-like intelligence. This
interpretation bridges the gap between Formal Language Theory and the observed
generative power of Transformers, laying a foundation for future advancements
in generative AI theory and applications. Our novel perspective on Transformer
architectures will foster a deeper understanding of LLMs and their future
potentials.
