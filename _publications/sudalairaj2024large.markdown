---
layout: publication
title: LAB Large45;scale Alignment For Chatbots
authors: Sudalairaj Shivchander, Bhandwaldar Abhishek, Pareja Aldo, Xu Kai, Cox David D., Srivastava Akash
conference: "Arxiv"
year: 2024
bibkey: sudalairaj2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.01081"}
tags: ['Applications', 'GPT', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
This work introduces LAB (Large45;scale Alignment for chatBots) a novel methodology designed to overcome the scalability challenges in the instruction45;tuning phase of large language model (LLM) training. Leveraging a taxonomy45;guided synthetic data generation process and a multi45;phase tuning framework LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT45;4. We demonstrate that LAB45;trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human45;annotated or GPT45;4 generated synthetic data. Thus offering a scalable cost45;effective solution for enhancing LLM capabilities and instruction45;following behaviors without the drawbacks of catastrophic forgetting marking a step forward in the efficient training of LLMs for a wide range of applications.
