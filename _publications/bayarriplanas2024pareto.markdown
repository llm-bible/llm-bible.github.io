---
layout: publication
title: 'Pareto-optimized Open-source Llms For Healthcare Via Context Retrieval'
authors: Jordi Bayarri-planas, Ashwin Kumar Gururajan, Dario Garcia-gasulla
conference: "Arxiv"
year: 2024
bibkey: bayarriplanas2024pareto
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.15127"}
tags: ['RAG', 'Efficiency and Optimization', 'Prompting', 'Applications']
---
This study leverages optimized context retrieval to enhance open-source Large
Language Models (LLMs) for cost-effective, high performance healthcare AI. We
demonstrate that this approach achieves state-of-the-art accuracy on medical
question answering at a fraction of the cost of proprietary models,
significantly improving the cost-accuracy Pareto frontier on the MedQA
benchmark. Key contributions include: (1) OpenMedQA, a novel benchmark
revealing a performance gap in open-ended medical QA compared to
multiple-choice formats; (2) a practical, reproducible pipeline for context
retrieval optimization; and (3) open-source resources (Prompt Engine,
CoT/ToT/Thinking databases) to empower healthcare AI development. By advancing
retrieval techniques and QA evaluation, we enable more affordable and reliable
LLM solutions for healthcare.
