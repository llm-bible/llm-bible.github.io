---
layout: publication
title: Multimodal Integration Of Human45;like Attention In Visual Question Answering
authors: Sood Ekta, Kögel Fabian, Müller Philipp, Thomas Dominike, Bace Mihai, Bulling Andreas
conference: "Arxiv"
year: 2021
bibkey: sood2021multimodal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2109.13139"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
Human45;like attention as a supervisory signal to guide neural attention has shown significant promise but is currently limited to uni45;modal integration 45; even for inherently multimodal tasks such as visual question answering (VQA). We present the Multimodal Human45;like Attention Network (MULAN) 45; the first method for multimodal integration of human45;like attention on image and text during training of VQA models. MULAN integrates attention predictions from two state45;of45;the45;art text and image saliency models into neural self45;attention layers of a recent transformer45;based VQA model. Through evaluations on the challenging VQAv2 dataset we show that MULAN achieves a new state45;of45;the45;art performance of 73.9837; accuracy on test45;std and 73.7237; on test45;dev and at the same time has approximately 8037; fewer trainable parameters than prior work. Overall our work underlines the potential of integrating multimodal human45;like and neural attention for VQA
