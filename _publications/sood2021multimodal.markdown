---
layout: publication
title: Multimodal Integration Of Human-like Attention In Visual Question Answering
authors: Sood Ekta, Kögel Fabian, Müller Philipp, Thomas Dominike, Bace Mihai, Bulling Andreas
conference: "Arxiv"
year: 2021
bibkey: sood2021multimodal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2109.13139"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
Human-like attention as a supervisory signal to guide neural attention has shown significant promise but is currently limited to uni-modal integration - even for inherently multimodal tasks such as visual question answering (VQA). We present the Multimodal Human-like Attention Network (MULAN) - the first method for multimodal integration of human-like attention on image and text during training of VQA models. MULAN integrates attention predictions from two state-of-the-art text and image saliency models into neural self-attention layers of a recent transformer-based VQA model. Through evaluations on the challenging VQAv2 dataset we show that MULAN achieves a new state-of-the-art performance of 73.9837; accuracy on test-std and 73.7237; on test-dev and at the same time has approximately 8037; fewer trainable parameters than prior work. Overall our work underlines the potential of integrating multimodal human-like and neural attention for VQA
