---
layout: publication
title: Probing Natural Language Inference Models Through Semantic Fragments
authors: Richardson Kyle, Hu Hai, Moss Lawrence S., Sabharwal Ashish
conference: "Arxiv"
year: 2019
bibkey: richardson2019probing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1909.07521"}
tags: ['BERT', 'Ethics And Bias', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools']
---
Do state45;of45;the45;art models for language understanding already have or can they easily learn abilities such as boolean coordination quantification conditionals comparatives and monotonicity reasoning (i.e. reasoning about word substitutions in sentential contexts) While such phenomena are involved in natural language inference (NLI) and go beyond basic linguistic understanding it is unclear the extent to which they are captured in existing NLI benchmarks and effectively learned by models. To investigate this we propose the use of semantic fragments45;45;45;systematically generated datasets that each target a different semantic phenomenon45;45;45;for probing and efficiently improving such capabilities of linguistic models. This approach to creating challenge datasets allows direct control over the semantic diversity and complexity of the targeted linguistic phenomena and results in a more precise characterization of a models linguistic behavior. Our experiments using a library of 8 such semantic fragments reveal two remarkable findings (a) State45;of45;the45;art models including BERT that are pre45;trained on existing NLI benchmark datasets perform poorly on these new fragments even though the phenomena probed here are central to the NLI task. (b) On the other hand with only a few minutes of additional fine45;tuning45;45;45;with a carefully selected learning rate and a novel variation of inoculation45;45;45;a BERT45;based model can master all of these logic and monotonicity fragments while retaining its performance on established NLI benchmarks.
