---
layout: publication
title: 'Hw-tsc''s Submission To The CCMT 2024 Machine Translation Tasks'
authors: Zhanglin Wu, Yuanchang Luo, Daimeng Wei, Jiawei Zheng, Bin Wei, Zongyao Li, Hengchao Shang, Jiaxin Guo, Shaojun Li, Weidong Zhang, Ning Xie, Hao Yang
conference: "Arxiv"
year: 2024
bibkey: wu2024hw
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.14842"}
tags: ['Fine-Tuning', 'Transformer', 'Applications', 'Model Architecture', 'Training Techniques', 'Pretraining Methods']
---
This paper presents the submission of Huawei Translation Services Center
(HW-TSC) to machine translation tasks of the 20th China Conference on Machine
Translation (CCMT 2024). We participate in the bilingual machine translation
task and multi-domain machine translation task. For these two translation
tasks, we use training strategies such as regularized dropout, bidirectional
training, data diversification, forward translation, back translation,
alternated training, curriculum learning, and transductive ensemble learning to
train neural machine translation (NMT) models based on the deep Transformer-big
architecture. Furthermore, to explore whether large language model (LLM) can
help improve the translation quality of NMT systems, we use supervised
fine-tuning to train llama2-13b as an Automatic post-editing (APE) model to
improve the translation results of the NMT model on the multi-domain machine
translation task. By using these plyometric strategies, our submission achieves
a competitive result in the final evaluation.
