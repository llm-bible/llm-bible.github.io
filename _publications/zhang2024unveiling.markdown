---
layout: publication
title: 'Unveiling The Impact Of Multi-modal Interactions On User Engagement: A Comprehensive Evaluation In Ai-driven Conversations'
authors: Zhang Lichao, Yu Jia, Zhang Shuai, Li Long, Zhong Yangyang, Liang Guanbao, Yan Yuming, Ma Qing, Weng Fangsheng, Pan Fayu, Li Jing, Xu Renjun, Lan Zhenzhong
conference: "Arxiv"
year: 2024
bibkey: zhang2024unveiling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.15000"}
tags: ['Multimodal Models']
---
Large Language Models (LLMs) have significantly advanced user-bot interactions enabling more complex and coherent dialogues. However the prevalent text-only modality might not fully exploit the potential for effective user engagement. This paper explores the impact of multi-modal interactions which incorporate images and audio alongside text on user engagement in chatbot conversations. We conduct a comprehensive analysis using a diverse set of chatbots and real-user interaction data employing metrics such as retention rate and conversation length to evaluate user engagement. Our findings reveal a significant enhancement in user engagement with multi-modal interactions compared to text-only dialogues. Notably the incorporation of a third modality significantly amplifies engagement beyond the benefits observed with just two modalities. These results suggest that multi-modal interactions optimize cognitive processing and facilitate richer information comprehension. This study underscores the importance of multi-modality in chatbot design offering valuable insights for creating more engaging and immersive AI communication experiences and informing the broader AI community about the benefits of multi-modal interactions in enhancing user engagement.
