---
layout: publication
title: Comparable Demonstrations Are Important In In-context Learning&#58; A Novel Perspective On Demonstration Selection
authors: Fan Caoyun, Tian Jidong, Li Yitian, He Hao, Jin Yaohui
conference: "Arxiv"
year: 2023
bibkey: fan2023comparable
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.07476"}
tags: ['Ethics And Bias', 'In Context Learning', 'Prompting']
---
In-Context Learning (ICL) is an important paradigm for adapting Large Language Models (LLMs) to downstream tasks through a few demonstrations. Despite the great success of ICL the limitation of the demonstration number may lead to demonstration bias i.e. the input-label mapping induced by LLMs misunderstands the tasks essence. Inspired by human experience we attempt to mitigate such bias through the perspective of the inter-demonstration relationship. Specifically we construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels in order to highlight the tasks essence and eliminate potential spurious correlations through the inter-demonstration comparison. Through a series of experiments on CDs we find that (1) demonstration bias does exist in LLMs and CDs can significantly reduce such bias; (2) CDs exhibit good performance in ICL especially in out-of-distribution scenarios. In summary this study explores the ICL mechanisms from a novel perspective providing a deeper insight into the demonstration selection strategy for ICL.
