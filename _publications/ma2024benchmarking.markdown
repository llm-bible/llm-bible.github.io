---
layout: publication
title: 'Visaidmath: Benchmarking Visual-aided Mathematical Reasoning'
authors: Jingkun Ma, Runzhe Zhan, Derek F. Wong, Yang Li, Di Sun, Hou Pong Chan, Lidia S. Chao
conference: "Arxiv"
year: 2024
bibkey: ma2024benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.22995"}
tags: ['Model Architecture', 'GPT']
---
Although previous research on large language models (LLMs) and large
multi-modal models (LMMs) has systematically explored mathematical
problem-solving (MPS) within visual contexts, the analysis of how these models
process visual information during problem-solving remains insufficient. To
address this gap, we present VisAidMath, a benchmark for evaluating the MPS
process related to visual information. We follow a rigorous data curation
pipeline involving both automated processes and manual annotations to ensure
data quality and reliability. Consequently, this benchmark includes 1,200
challenging problems from various mathematical branches, vision-aid
formulations, and difficulty levels, collected from diverse sources such as
textbooks, examination papers, and Olympiad problems. Based on the proposed
benchmark, we conduct comprehensive evaluations on ten mainstream LLMs and
LMMs, highlighting deficiencies in the visual-aided reasoning process. For
example, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning
task, even with a drop of 2 points when provided with golden visual aids.
In-depth analysis reveals that the main cause of deficiencies lies in
hallucination regarding the implicit visual reasoning process, shedding light
on future research directions in the visual-aided MPS process.
