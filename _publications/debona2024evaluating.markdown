---
layout: publication
title: 'Evaluating Explanations Through Llms: Beyond Traditional User Studies'
authors: Francesco Bombassei De Bona, Gabriele Dominici, Tim Miller, Marc Langheinrich, Martin Gjoreski
conference: "Arxiv"
year: 2024
bibkey: debona2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.17781"}
tags: ['Ethics and Bias', 'Interpretability', 'Tools', 'Interpretability and Explainability']
---
As AI becomes fundamental in sectors like healthcare, explainable AI (XAI)
tools are essential for trust and transparency. However, traditional user
studies used to evaluate these tools are often costly, time consuming, and
difficult to scale. In this paper, we explore the use of Large Language Models
(LLMs) to replicate human participants to help streamline XAI evaluation. We
reproduce a user study comparing counterfactual and causal explanations,
replicating human participants with seven LLMs under various settings. Our
results show that (i) LLMs can replicate most conclusions from the original
study, (ii) different LLMs yield varying levels of alignment in the results,
and (iii) experimental factors such as LLM memory and output variability affect
alignment with human responses. These initial findings suggest that LLMs could
provide a scalable and cost-effective way to simplify qualitative XAI
evaluation.
