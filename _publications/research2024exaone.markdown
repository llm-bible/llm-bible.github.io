---
layout: publication
title: EXAONE 3.0 7.8B Instruction Tuned Language Model
authors: Research Lg Ai, :, An Soyoung, Bae Kyunghoon, Choi Eunbi, Choi Stanley Jungkyu, Choi Yemuk, Hong Seokhee, Hong Yeonjung, Hwang Junwon, Jeon Hyojin, Jo Gerrard Jeongwon, Jo Hyunjik, Jung Jiyeon, Jung Yountae, Kim Euisoon, Kim Hyosang, Kim Joonkee, Kim Seonghwan, Kim Soyeon, Kim Sunkyoung, Kim Yireun, Kim Youchul, Lee Edward Hwayoung, Lee Haeju, Lee Honglak, Lee Jinsik, Lee Kyungmin, Lee Moontae, Lee Seungjun, Lim Woohyung, Park Sangha, Park Sooyoun, Park Yongmin, Seo Boseong, Yang Sihoon, Yeen Heuiyeen, Yoo Kyungjae, Yun Hyeongu
conference: "Arxiv"
year: 2024
bibkey: research2024exaone
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.03541"}
tags: ['Pretraining Methods', 'Reinforcement Learning']
---
We introduce EXAONE 3.0 instruction45;tuned language model the first open model in the family of Large Language Models (LLMs) developed by LG AI Research. Among different model sizes we publicly release the 7.8B instruction45;tuned model to promote open research and innovations. Through extensive evaluations across a wide range of public and in45;house benchmarks EXAONE 3.0 demonstrates highly competitive real45;world performance with instruction45;following capability against other state45;of45;the45;art open models of similar size. Our comparative analysis shows that EXAONE 3.0 excels particularly in Korean while achieving compelling performance across general tasks and complex reasoning. With its strong real45;world effectiveness and bilingual proficiency we hope that EXAONE keeps contributing to advancements in Expert AI. Our EXAONE 3.0 instruction45;tuned model is available at https://huggingface.co/LGAI&#45;EXAONE/EXAONE&#45;3.0&#45;7.8B&#45;Instruct
