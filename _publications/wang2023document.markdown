---
layout: publication
title: Document45;level Machine Translation With Large Language Models
authors: Wang Longyue, Lyu Chenyang, Ji Tianbo, Zhang Zhirui, Yu Dian, Shi Shuming, Tu Zhaopeng
conference: "Arxiv"
year: 2023
bibkey: wang2023document
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.02210"}
  - {name: "Code", url: "https://github.com/longyuewangdcu/Document&#45;MT&#45;LLM"}
tags: ['Applications', 'GPT', 'Has Code', 'Model Architecture', 'Prompting', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
Large language models (LLMs) such as ChatGPT can produce coherent cohesive relevant and fluent answers for various natural language processing (NLP) tasks. Taking document45;level machine translation (MT) as a testbed this paper provides an in45;depth evaluation of LLMs ability on discourse modeling. The study focuses on three aspects 1) Effects of Context45;Aware Prompts where we investigate the impact of different prompts on document45;level translation quality and discourse phenomena; 2) Comparison of Translation Models where we compare the translation performance of ChatGPT with commercial MT systems and advanced document45;level MT methods; 3) Analysis of Discourse Modelling Abilities where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document45;level translation 1) leveraging their powerful long45;text modeling capabilities GPT45;3.5 and GPT45;4 outperform commercial MT systems in terms of human evaluation; 2) GPT45;4 demonstrates a stronger ability for probing linguistic knowledge than GPT45;3.5. This work highlights the challenges and opportunities of LLMs for MT which we hope can inspire the future design and evaluation of LLMs.We release our data and annotations at https://github.com/longyuewangdcu/Document&#45;MT&#45;LLM.
