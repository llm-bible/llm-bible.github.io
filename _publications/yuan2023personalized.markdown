---
layout: publication
title: 'Personalized Large Language Model Assistant With Evolving Conditional Memory'
authors: Ruifeng Yuan, Shichao Sun, Yongqi Li, Zili Wang, Ziqiang Cao, Wenjie Li
conference: "Arxiv"
year: 2023
bibkey: yuan2023personalized
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2312.17257'}
tags: ['GPT', 'Tools', 'Model Architecture']
---
With the rapid development of large language models, AI assistants like
ChatGPT have become increasingly integrated into people's works and lives but
are limited in personalized services. In this paper, we present a plug-and-play
framework that could facilitate personalized large language model assistants
with evolving conditional memory. The personalized assistant focuses on
intelligently preserving the knowledge and experience from the history dialogue
with the user, which can be applied to future tailored responses that better
align with the user's preferences. Generally, the assistant generates a set of
records from the dialogue dialogue, stores them in a memory bank, and retrieves
related memory to improve the quality of the response. For the crucial memory
design, we explore different ways of constructing the memory and propose a new
memorizing mechanism named conditional memory. We also investigate the
retrieval and usage of memory in the generation process. We build the first
benchmark to evaluate personalized assistants' ability from three aspects. The
experimental results illustrate the effectiveness of our method.
