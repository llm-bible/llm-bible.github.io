---
layout: publication
title: 'Listen: Learning Soft Token Embeddings For Neural Audio Llms'
authors: Pooneh Mousavi, Shubham Gupta, Cem Subakan, Mirco Ravanelli
conference: "Arxiv"
year: 2025
bibkey: mousavi2025learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.18517"}
tags: ['Training Techniques', 'Tools', 'Reinforcement Learning', 'Interpretability and Explainability', 'Prompting']
---
Foundation models based on large language models (LLMs) have shown great success in handling various tasks and modalities. However, adapting these models for general-purpose audio-language tasks is challenging due to differences in acoustic environments and task variations. In this work, we introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic prompt selection strategy with learnable key-value pairs, allowing the model to balance general and task-specific knowledge while avoiding overfitting in a multitask setting. Our approach reduces dependence on large-scale ASR or captioning datasets, achieves competitive performance with fewer trainable parameters, and simplifies training by using a single-stage process. Additionally, LiSTEN enhances interpretability by analyzing the diversity and overlap of selected prompts across different tasks.
