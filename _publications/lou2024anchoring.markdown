---
layout: publication
title: 'Anchoring Bias In Large Language Models: An Experimental Study'
authors: Jiaxu Lou, Yifan Sun
conference: "Arxiv"
year: 2024
bibkey: lou2024anchoring
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.06593'}
tags: ['Ethics and Bias', 'GPT', 'Model Architecture']
---
Large Language Models (LLMs) like GPT-4 and Gemini have significantly
advanced artificial intelligence by enabling machines to generate and
comprehend human-like text. Despite their impressive capabilities, LLMs are not
immune to limitations, including various biases. While much research has
explored demographic biases, the cognitive biases in LLMs have not been equally
scrutinized. This study delves into anchoring bias, a cognitive bias where
initial information disproportionately influences judgment. Utilizing an
experimental dataset, we examine how anchoring bias manifests in LLMs and
verify the effectiveness of various mitigation strategies. Our findings
highlight the sensitivity of LLM responses to biased hints. At the same time,
our experiments show that, to mitigate anchoring bias, one needs to collect
hints from comprehensive angles to prevent the LLMs from being anchored to
individual pieces of information, while simple algorithms such as
Chain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection
are not sufficient.
