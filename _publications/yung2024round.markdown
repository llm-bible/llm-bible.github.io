---
layout: publication
title: Round Trip Translation Defence Against Large Language Model Jailbreaking Attacks
authors: Yung Canaan, Dolatabadi Hadi Mohaghegh, Erfani Sarah, Leckie Christopher
conference: "Arxiv"
year: 2024
bibkey: yung2024round
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.13517"}
  - {name: "Code", url: "https://github.com/Cancanxxx/Round_Trip_Translation_Defence"}
tags: ['Has Code', 'Pretraining Methods', 'Prompting', 'Security']
---
Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue we propose the Round Trip Translation (RTT) method the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed making it easier for LLMs to detect induced harmful behavior. This method is versatile lightweight and transferrable to different LLMs. Our defense successfully mitigated over 7037; of Prompt Automatic Iterative Refinement (PAIR) attacks which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 4037;. Our code is publicly available at https://github.com/Cancanxxx/Round\_Trip\_Translation\_Defence"
