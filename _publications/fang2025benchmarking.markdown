---
layout: publication
title: 'Camerabench: Benchmarking Visual Reasoning In Mllms Via Photography'
authors: I-sheng Fang, Jun-cheng Chen
conference: "Arxiv"
year: 2025
bibkey: fang2025benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.10090"}
tags: ['Agentic', 'Multimodal Models', 'Applications', 'Reinforcement Learning']
---
Large language models (LLMs) and multimodal large language models (MLLMs)
have significantly advanced artificial intelligence. However, visual reasoning,
reasoning involving both visual and textual inputs, remains underexplored.
Recent advancements, including the reasoning models like OpenAI o1 and Gemini
2.0 Flash Thinking, which incorporate image inputs, have opened this
capability. In this ongoing work, we focus specifically on photography-related
tasks because a photo is a visual snapshot of the physical world where the
underlying physics (i.e., illumination, blur extent, etc.) interplay with the
camera parameters. Successfully reasoning from the visual information of a
photo to identify these numerical camera settings requires the MLLMs to have a
deeper understanding of the underlying physics for precise visual
comprehension, representing a challenging and intelligent capability essential
for practical applications like photography assistant agents. We aim to
evaluate MLLMs on their ability to distinguish visual differences related to
numerical camera settings, extending a methodology previously proposed for
vision-language models (VLMs). Our preliminary results demonstrate the
importance of visual reasoning in photography-related tasks. Moreover, these
results show that no single MLLM consistently dominates across all evaluation
tasks, demonstrating ongoing challenges and opportunities in developing MLLMs
with better visual reasoning.
