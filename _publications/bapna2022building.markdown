---
layout: publication
title: Building Machine Translation Systems For The Next Thousand Languages
authors: Ankur Bapna et al.
conference: Arxiv
year: 2022
citations: 34
bibkey: bapna2022building
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2205.03983'}]
tags: [RAG, Pre-Training]
---
In this paper we share findings from our effort to build practical machine
translation (MT) systems capable of translating across over one thousand
languages. We describe results in three research domains: (i) Building clean,
web-mined datasets for 1500+ languages by leveraging semi-supervised
pre-training for language identification and developing data-driven filtering
techniques; (ii) Developing practical MT models for under-served languages by
leveraging massively multilingual models trained with supervised parallel data
for over 100 high-resource languages and monolingual datasets for an additional
1000+ languages; and (iii) Studying the limitations of evaluation metrics for
these languages and conducting qualitative analysis of the outputs from our MT
models, highlighting several frequent error modes of these types of models. We
hope that our work provides useful insights to practitioners working towards
building MT systems for currently understudied languages, and highlights
research directions that can complement the weaknesses of massively
multilingual models in data-sparse settings.