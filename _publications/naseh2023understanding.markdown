---
layout: publication
title: 'Understanding (un)intended Memorization In Text-to-image Generative Models'
authors: Ali Naseh, Jaechul Roh, Amir Houmansadr
conference: "Arxiv"
year: 2023
bibkey: naseh2023understanding
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2312.07550'}
tags: ['Reinforcement Learning', 'Multimodal Models', 'Merging']
---
Multimodal machine learning, especially text-to-image models like Stable
Diffusion and DALL-E 3, has gained significance for transforming text into
detailed images.
  Despite their growing use and remarkable generative capabilities, there is a
pressing need for a detailed examination of these models' behavior,
particularly with respect to memorization. Historically, memorization in
machine learning has been context-dependent, with diverse definitions emerging
from classification tasks to complex models like Large Language Models (LLMs)
and Diffusion models. Yet, a definitive concept of memorization that aligns
with the intricacies of text-to-image synthesis remains elusive. This
understanding is vital as memorization poses privacy risks yet is essential for
meeting user expectations, especially when generating representations of
underrepresented entities. In this paper, we introduce a specialized definition
of memorization tailored to text-to-image models, categorizing it into three
distinct types according to user expectations. We closely examine the subtle
distinctions between intended and unintended memorization, emphasizing the
importance of balancing user privacy with the generative quality of the model
outputs. Using the Stable Diffusion model, we offer examples to validate our
memorization definitions and clarify their application.
