---
layout: publication
title: "Leveraging Large Language Models With Chain-of-thought And Prompt Engineering For Traffic Crash Severity Analysis And Inference"
authors: Zhen Hao, Shi Yucheng, Huang Yongcan, Yang Jidong J., Liu Ninghao
conference: "Arxiv"
year: 2024
bibkey: zhen2024leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.04652"}
tags: ['GPT', 'Model Architecture', 'Prompting', 'RAG', 'Reinforcement Learning', 'Tools']
---
Harnessing the power of Large Language Models (LLMs) this study explores the use of three state-of-the-art LLMs specifically GPT-3.5-turbo LLaMA3-8B and LLaMA3-70B for crash severity inference framing it as a classification task. We generate textual narratives from original traffic crash tabular data using a pre-built template infused with domain knowledge. Additionally we incorporated Chain-of-Thought (CoT) reasoning to guide the LLMs in analyzing the crash causes and then inferring the severity. This study also examine the impact of prompt engineering specifically designed for crash severity inference. The LLMs were tasked with crash severity inference to (1) evaluate the models capabilities in crash severity analysis (2) assess the effectiveness of CoT and domain-informed prompt engineering and (3) examine the reasoning abilities with the CoT framework. Our results showed that LLaMA3-70B consistently outperformed the other models particularly in zero-shot settings. The CoT and Prompt Engineering techniques significantly enhanced performance improving logical reasoning and addressing alignment issues. Notably the CoT offers valuable insights into LLMs reasoning processes unleashing their capacity to consider diverse factors such as environmental conditions driver behavior and vehicle characteristics in severity analysis and inference.
