---
layout: publication
title: 'Large Language Models Understanding: An Inherent Ambiguity Barrier'
authors: Daniel N. Nissensohn Nissani
conference: "Arxiv"
year: 2025
bibkey: nissani2025large
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.00654'}
tags: ['Reinforcement Learning']
---
A lively ongoing debate is taking place, since the extraordinary emergence of
Large Language Models (LLMs) with regards to their capability to understand the
world and capture the meaning of the dialogues in which they are involved.
Arguments and counter-arguments have been proposed based upon thought
experiments, anecdotal conversations between LLMs and humans, statistical
linguistic analysis, philosophical considerations, and more. In this brief
paper we present a counter-argument based upon a thought experiment and
semi-formal considerations leading to an inherent ambiguity barrier which
prevents LLMs from having any understanding of what their amazingly fluent
dialogues mean.
