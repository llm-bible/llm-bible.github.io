---
layout: publication
title: From LLM To NMT Advancing Low45;resource Machine Translation With Claude
authors: Enis Maxim, Hopkins Mark
conference: "Arxiv"
year: 2024
bibkey: enis2024from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.13813"}
tags: ['Applications', 'Distillation', 'Efficiency And Optimization']
---
We show that Claude 3 Opus a large language model (LLM) released by Anthropic in March 2024 exhibits stronger machine translation competence than other LLMs. Though we find evidence of data contamination with Claude on FLORES45;200 we curate new benchmarks that corroborate the effectiveness of Claude for low45;resource machine translation into English. We find that Claude has remarkable textit123;resource efficiency125; 45;45; the degree to which the quality of the translation model depends on a language pairs resource level. Finally we show that advancements in LLM translation can be compressed into traditional neural machine translation (NMT) models. Using Claude to generate synthetic data we demonstrate that knowledge distillation advances the state45;of45;the45;art in Yoruba45;English translation meeting or surpassing strong baselines like NLLB45;54B and Google Translate.
