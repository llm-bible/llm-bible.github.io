---
layout: publication
title: 'MACPO: Weak-to-strong Alignment Via Multi-agent Contrastive Preference Optimization'
authors: Yougang Lyu, Lingyong Yan, Zihan Wang, Dawei Yin, Pengjie Ren, Maarten De Rijke, Zhaochun Ren
conference: "Arxiv"
year: 2024
bibkey: lyu2024weak
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.07672"}
tags: ['Fine-Tuning', 'Agentic', 'Efficiency and Optimization', 'Tools', 'RAG', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods']
---
As large language models (LLMs) are rapidly advancing and achieving
near-human capabilities on specific tasks, aligning them with human values is
becoming more urgent. In scenarios where LLMs outperform humans, we face a
weak-to-strong alignment problem where we need to effectively align strong
student LLMs through weak supervision generated by weak teachers. Existing
alignment methods mainly focus on strong-to-weak alignment and self-alignment
settings, and it is impractical to adapt them to the much harder weak-to-strong
alignment setting. To fill this gap, we propose a multi-agent contrastive
preference optimization (MACPO) framework. MACPO facilitates weak teachers and
strong students to learn from each other by iteratively reinforcing unfamiliar
positive behaviors while penalizing familiar negative ones. To get this, we
devise a mutual positive behavior augmentation strategy to encourage weak
teachers and strong students to learn from each other's positive behavior and
further provide higher quality positive behavior for the next iteration.
Additionally, we propose a hard negative behavior construction strategy to
induce weak teachers and strong students to generate familiar negative behavior
by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF
and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human
judgments, demonstrate that MACPO simultaneously improves the alignment
performance of strong students and weak teachers. Moreover, as the number of
weak teachers increases, MACPO achieves better weak-to-strong alignment
performance through more iteration optimization rounds.
