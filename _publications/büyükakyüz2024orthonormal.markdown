---
layout: publication
title: Olora Orthonormal Low45;rank Adaptation Of Large Language Models
authors: Büyükakyüz Kerim
conference: "Arxiv"
year: 2024
bibkey: büyükakyüz2024orthonormal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.01775"}
tags: ['Applications', 'Efficiency And Optimization', 'Fine Tuning', 'Language Modeling', 'Pretraining Methods', 'RAG', 'Training Techniques']
---
The advent of large language models (LLMs) has revolutionized natural language processing enabling unprecedented capabilities in understanding and generating human45;like text. However the computational cost and convergence times associated with fine45;tuning these models remain significant challenges. Low45;Rank Adaptation (LoRA) has emerged as a promising method to mitigate these issues by introducing efficient fine45;tuning techniques with a reduced number of trainable parameters. In this paper we present OLoRA an enhancement to the LoRA method that leverages orthonormal matrix initialization through QR decomposition. OLoRA significantly accelerates the convergence of LLM training while preserving the efficiency benefits of LoRA such as the number of trainable parameters and GPU memory footprint. Our empirical evaluations demonstrate that OLoRA not only converges faster but also exhibits improved performance compared to standard LoRA across a variety of language modeling tasks. This advancement opens new avenues for more efficient and accessible fine45;tuning of LLMs potentially enabling broader adoption and innovation in natural language applications.
