---
layout: publication
title: 'Model-in-the-loop (MILO): Accelerating Multimodal AI Data Annotation With Llms'
authors: Yifan Wang, David Stevens, Pranay Shah, Wenwen Jiang, Miao Liu, Xu Chen, Robert Kuo, Na Li, Boying Gong, Daniel Lee, Jiabo Hu, Ning Zhang, Bob Kamma
conference: "Arxiv"
year: 2024
bibkey: wang2024model
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.10702"}
tags: ['RAG', 'Training Techniques', 'Multimodal Models', 'Tools']
---
The growing demand for AI training data has transformed data annotation into
a global industry, but traditional approaches relying on human annotators are
often time-consuming, labor-intensive, and prone to inconsistent quality. We
propose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models
into the annotation process. Our research introduces a collaborative paradigm
that leverages the strengths of both professional human annotators and large
language models (LLMs). By employing LLMs as pre-annotation and real-time
assistants, and judges on annotator responses, MILO enables effective
interaction patterns between human annotators and LLMs. Three empirical studies
on multimodal data annotation demonstrate MILO's efficacy in reducing handling
time, improving data quality, and enhancing annotator experiences. We also
introduce quality rubrics for flexible evaluation and fine-grained feedback on
open-ended annotations. The MILO framework has implications for accelerating
AI/ML development, reducing reliance on human annotation alone, and promoting
better alignment between human and machine values.
