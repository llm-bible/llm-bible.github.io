---
layout: publication
title: Revisiting Prompt Engineering Via Declarative Crowdsourcing
authors: Parameswaran Aditya G., Shankar Shreya, Asawa Parth, Jain Naman, Wang Yujie
conference: "Arxiv"
year: 2023
bibkey: parameswaran2023revisiting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.03854"}
tags: ['Prompting', 'RAG']
---
Large language models (LLMs) are incredibly powerful at comprehending and generating data in the form of text but are brittle and error45;prone. There has been an advent of toolkits and recipes centered around so45;called prompt engineering45;the process of asking an LLM to do something via a series of prompts. However for LLM45;powered data processing workflows in particular optimizing for quality while keeping cost bounded is a tedious manual process. We put forth a vision for declarative prompt engineering. We view LLMs like crowd workers and leverage ideas from the declarative crowdsourcing literature45;including leveraging multiple prompting strategies ensuring internal consistency and exploring hybrid45;LLM45;non45;LLM approaches45;to make prompt engineering a more principled process. Preliminary case studies on sorting entity resolution and imputation demonstrate the promise of our approach
