---
layout: publication
title: Improving Machine Translation With Large Language Models A Preliminary Study With Cooperative Decoding
authors: Zeng Jiali, Meng Fandong, Yin Yongjing, Zhou Jie
conference: "Arxiv"
year: 2023
bibkey: zeng2023improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.02851"}
tags: ['Applications', 'Efficiency And Optimization', 'Tools']
---
Contemporary translation engines based on the encoder45;decoder framework have made significant strides in development. However the emergence of Large Language Models (LLMs) has disrupted their position by presenting the potential for achieving superior translation quality. To uncover the circumstances in which LLMs excel and explore how their strengths can be harnessed to enhance translation quality we first conduct a comprehensive analysis to assess the strengths and limitations of various commercial NMT systems and MT45;oriented LLMs. Our findings indicate that neither NMT nor MT45;oriented LLMs alone can effectively address all the translation issues but MT45;oriented LLMs show promise as a complementary solution to NMT systems. Building upon these insights we propose Cooperative Decoding (CoDec) which treats NMT systems as a pretranslation model and MT45;oriented LLMs as a supplemental solution to handle complex scenarios beyond the capability of NMT alone. Experimental results on the WMT22 test sets and a newly collected test set WebCrawl demonstrate the effectiveness and efficiency of CoDec highlighting its potential as a robust solution for combining NMT systems with MT45;oriented LLMs in the field of machine translation.
