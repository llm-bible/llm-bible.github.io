---
layout: publication
title: Evaluating The Susceptibility Of Pre45;trained Language Models Via Handcrafted Adversarial Examples
authors: Branch Hezekiah J., Cefalu Jonathan Rodriguez, Mchugh Jeremy, Hujer Leyla, Bahl Aditya, Iglesias Daniel Del Castillo, Heichman Ron, Darwishi Ramesh
conference: "Arxiv"
year: 2022
bibkey: branch2022evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2209.02128"}
tags: ['BERT', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Security', 'Training Techniques', 'Transformer']
---
Recent advances in the development of large language models have resulted in public access to state45;of45;the45;art pre45;trained language models (PLMs) including Generative Pre45;trained Transformer 3 (GPT45;3) and Bidirectional Encoder Representations from Transformers (BERT). However evaluations of PLMs in practice have shown their susceptibility to adversarial attacks during the training and fine45;tuning stages of development. Such attacks can result in erroneous outputs model45;generated hate speech and the exposure of users sensitive information. While existing research has focused on adversarial attacks during either the training or the fine45;tuning of PLMs there is a deficit of information on attacks made between these two development phases. In this work we highlight a major security vulnerability in the public release of GPT45;3 and further investigate this vulnerability in other state45;of45;the45;art PLMs. We restrict our work to pre45;trained models that have not undergone fine45;tuning. Further we underscore token distance45;minimized perturbations as an effective adversarial approach bypassing both supervised and unsupervised quality measures. Following this approach we observe a significant decrease in text classification quality when evaluating for semantic similarity.
