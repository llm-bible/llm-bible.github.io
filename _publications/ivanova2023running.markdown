---
layout: publication
title: Running Cognitive Evaluations On Large Language Models The Dox27;s And The Donx27;ts
authors: Ivanova Anna A.
conference: "Arxiv"
year: 2023
bibkey: ivanova2023running
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.01276"}
tags: ['Efficiency And Optimization', 'Prompting', 'Tools']
---
In this paper I describe methodological considerations for studies that aim to evaluate the cognitive capacities of large language models (LLMs) using language-based behavioral assessments. Drawing on three case studies from the literature (a commonsense knowledge benchmark a theory of mind evaluation and a test of syntactic agreement) I describe common pitfalls that might arise when applying a cognitive test to an LLM. I then list 10 dos and donts that should help design high-quality cognitive evaluations for AI systems. I conclude by discussing four areas where the dos and donts are currently under active discussion -- prompt sensitivity cultural and linguistic diversity using LLMs as research assistants and running evaluations on open vs. closed LLMs. Overall the goal of the paper is to contribute to the broader discussion of best practices in the rapidly growing field of AI Psychology.
