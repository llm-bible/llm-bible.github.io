---
layout: publication
title: LSTP Language45;guided Spatial45;temporal Prompt Learning For Long45;form Video45;text Understanding
authors: Wang Yuxuan, Wang Yueqian, Wu Pengfei, Liang Jianxin, Zhao Dongyan, Zheng Zilong
conference: "Arxiv"
year: 2024
bibkey: wang2024language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.16050"}
tags: ['Applications', 'Efficiency And Optimization', 'Language Modeling', 'Pretraining Methods', 'Prompting', 'RAG', 'Tools', 'Training Techniques']
---
Despite progress in video45;language modeling the computational challenge of interpreting long45;form videos in response to task45;specific linguistic queries persists largely due to the complexity of high45;dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue we introduce a novel approach called Language45;guided Spatial45;Temporal Prompt Learning (LSTP). This approach features two key components a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy our framework significantly enhances computational efficiency temporal understanding and spatial45;temporal alignment. Empirical evaluations across two challenging tasks45;45;video question answering and temporal question grounding in videos45;45;using a variety of video45;language pretrainings (VLPs) and large language models (LLMs) demonstrate the superior performance speed and versatility of our proposed LSTP paradigm.
