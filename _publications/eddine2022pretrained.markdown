---
layout: publication
title: 'Arabart: A Pretrained Arabic Sequence-to-sequence Model For Abstractive Summarization'
authors: Moussa Kamal Eddine, Nadi Tomeh, Nizar Habash, Joseph Le Roux, Michalis Vazirgiannis
conference: Arxiv
year: 2022
citations: 16
bibkey: eddine2022pretrained
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2203.10945'}]
tags: [Transformer, BERT, Pre-Training]
---
Like most natural language understanding and generation tasks,
state-of-the-art models for summarization are transformer-based
sequence-to-sequence architectures that are pretrained on large corpora. While
most existing models focused on English, Arabic remained understudied. In this
paper we propose AraBART, the first Arabic model in which the encoder and the
decoder are pretrained end-to-end, based on BART. We show that AraBART achieves
the best performance on multiple abstractive summarization datasets,
outperforming strong baselines including a pretrained Arabic BERT-based model
and multilingual mBART and mT5 models.