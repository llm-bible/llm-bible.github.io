---
layout: publication
title: 'Paretorag: Leveraging Sentence-context Attention For Robust And Efficient Retrieval-augmented Generation'
authors: Ruobing Yao, Yifei Zhang, Shuang Song, Yuhua Liu, Neng Gao, Chenyang Tu
conference: "Arxiv"
year: 2025
bibkey: yao2025leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.08178"}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Tools', 'RAG', 'Attention Mechanism']
---
While Retrieval-Augmented Generation (RAG) systems enhance Large Language
Models (LLMs) by incorporating external knowledge, they still face persistent
challenges in retrieval inefficiency and the inability of LLMs to filter out
irrelevant information. We present ParetoRAG, an unsupervised framework that
optimizes RAG systems through sentence-level refinement guided by the Pareto
principle. By decomposing paragraphs into sentences and dynamically
re-weighting core content while preserving contextual coherence, ParetoRAG
achieves dual improvements in both retrieval precision and generation quality
without requiring additional training or API resources. This framework has been
empirically validated across various datasets, LLMs, and retrievers.
