---
layout: publication
title: 'Towards More Effective Table-to-text Generation: Assessing In-context Learning And Self-evaluation With Open-source Models'
authors: Sahar Iravani, Tim . O . F Conrad
conference: "Arxiv"
year: 2024
bibkey: iravani2024towards
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.12878'}
tags: ['Language Modeling', 'BERT', 'Applications', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'In-Context Learning']
---
Table processing, a key task in natural language processing, has
significantly benefited from recent advancements in language models (LMs).
However, the capabilities of LMs in table-to-text generation, which transforms
structured data into coherent narrative text, require an in-depth
investigation, especially with current open-source models. This study explores
the effectiveness of various in-context learning strategies in LMs across
benchmark datasets, focusing on the impact of providing examples to the model.
More importantly, we examine a real-world use case, offering valuable insights
into practical applications. To complement traditional evaluation metrics, we
employ a large language model (LLM) self-evaluation approach using
chain-of-thought reasoning and assess its correlation with human-aligned
metrics like BERTScore. Our findings highlight the significant impact of
examples in improving table-to-text generation and suggest that, while LLM
self-evaluation has potential, its current alignment with human judgment could
be enhanced. This points to the need for more reliable evaluation methods.
