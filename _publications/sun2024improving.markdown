---
layout: publication
title: Conifer Improving Complex Constrained Instruction45;following Ability Of Large Language Models
authors: Sun Haoran, Liu Lixin, Li Junjie, Wang Fengyu, Dong Baohua, Lin Ran, Huang Ruohui
conference: "Arxiv"
year: 2024
bibkey: sun2024improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.02823"}
  - {name: "Code", url: "https://www.github.com/ConiferLM/Conifer"}
tags: ['Applications', 'GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
The ability of large language models (LLMs) to follow instructions is crucial to real45;world applications. Despite recent advances several studies have highlighted that LLMs struggle when faced with challenging instructions especially those that include complex constraints hindering their effectiveness in various tasks. To address this challenge we introduce Conifer a novel instruction tuning dataset designed to enhance LLMs to follow multi45;level instructions with complex constraints. Utilizing GPT45;4 we curate the dataset by a series of LLM45;driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy45;to45;hard progression and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction45;following abilities especially for instructions with complex constraints. On several instruction45;following benchmarks our 7B model outperforms the state45;of45;the45;art open45;source 7B models even exceeds the performance of models 10 times larger on certain metrics. All the code and Conifer dataset are available at https://www.github.com/ConiferLM/Conifer.
