---
layout: publication
title: 'Grammar And Gameplay-aligned RL For Game Description Generation With Llms'
authors: Tsunehiko Tanaka, Edgar Simo-serra
conference: "Arxiv"
year: 2025
bibkey: tanaka2025grammar
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.15783"}
tags: ['Agentic', 'Training Techniques', 'Reinforcement Learning', 'RAG', 'Pretraining Methods', 'Fine-Tuning']
---
Game Description Generation (GDG) is the task of generating a game
description written in a Game Description Language (GDL) from natural language
text. Previous studies have explored generation methods leveraging the
contextual understanding capabilities of Large Language Models (LLMs); however,
accurately reproducing the game features of the game descriptions remains a
challenge. In this paper, we propose reinforcement learning-based fine-tuning
of LLMs for GDG (RLGDG). Our training method simultaneously improves
grammatical correctness and fidelity to game concepts by introducing both
grammar rewards and concept rewards. Furthermore, we adopt a two-stage training
strategy where Reinforcement Learning (RL) is applied following Supervised
Fine-Tuning (SFT). Experimental results demonstrate that our proposed method
significantly outperforms baseline methods using SFT alone.
