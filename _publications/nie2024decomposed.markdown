---
layout: publication
title: Decomposed Prompting Unveiling Multilingual Linguistic Structure Knowledge In English45;centric Large Language Models
authors: Nie Ercong, Yuan Shuzhou, Ma Bolei, Schmid Helmut, Färber Michael, Kreuter Frauke, Schütze Hinrich
conference: "Arxiv"
year: 2024
bibkey: nie2024decomposed
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.18397"}
tags: ['Efficiency And Optimization', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG', 'Training Techniques']
---
Despite the predominance of English in their training data English45;centric Large Language Models (LLMs) like GPT45;3 and LLaMA display a remarkable ability to perform multilingual tasks raising questions about the depth and nature of their cross45;lingual capabilities. This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks. Diverging from the single text45;to45;text prompt our method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We assess our method on the Universal Dependencies part45;of45;speech tagging dataset for 38 languages utilizing both English45;centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero45; and few45;shot settings. Further analysis reveals the influence of evaluation methods and the use of instructions in prompts. Our multilingual investigation shows that English45;centric language models perform better on average than multilingual models. Our study offers insights into the multilingual transferability of English45;centric LLMs contributing to the understanding of their multilingual linguistic knowledge.
