---
layout: publication
title: Why Not Transform Chat Large Language Models To Non45;english
authors: Geng Xiang, Zhu Ming, Li Jiahuan, Lai Zhejian, Zou Wei, She Shuaijie, Guo Jiaxin, Zhao Xiaofeng, Li Yinglu, Li Yuang, Su Chang, Zhao Yanqing, Lyu Xinglin, Zhang Min, Chen Jiajun, Yang Hao, Huang Shujian
conference: "Arxiv"
year: 2024
bibkey: geng2024why
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.13923"}
tags: ['Distillation', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Responsible AI', 'Tools', 'Training Techniques']
---
The scarcity of non45;English data limits the development of non45;English large language models (LLMs). Transforming English45;centric LLMs to non45;English has been identified as an effective and resource45;efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs e.g. GPT45;4. Compared to base LLMs chat LLMs are further optimized for advanced abilities e.g. multi45;turn conversation and human preference alignment and thus more powerful in both helpfulness and safety. However transforming a chat LLM involves two critical issues (1) How can we effectively transfer advanced abilities without their supervised data (2) How can we prevent the original knowledge from catastrophic forgetting during transformation We target these issues by introducing a simple framework called TransLLM. For the first issue TransLLM divides the transfer problem into some common sub45;tasks with the translation chain45;of45;thought which uses the translation as the bridge between English and non45;English step45;by45;step. We further enhance the performance of sub45;tasks with publicly available data. For the second issue we propose a method comprising two synergistic components low45;rank adaptation for training to maintain the original LLM parameters and recovery KD which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments we transform the LLaMA45;245;chat45;7B to the Thai language. Our method using only single45;turn data outperforms strong baselines and ChatGPT on multi45;turn benchmark MT45;bench. Furthermore our method without safety data rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT45;4.
