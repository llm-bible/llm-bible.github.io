---
layout: publication
title: Finding And Editing Multi45;modal Neurons In Pre45;trained Transformers
authors: Pan Haowen, Cao Yixin, Wang Xiaozhi, Yang Xun, Wang Meng
conference: "Arxiv"
year: 2023
bibkey: pan2023finding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.07470"}
tags: ['Efficiency And Optimization', 'Interpretability And Explainability', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Understanding the internal mechanisms by which multi45;modal large language models (LLMs) interpret different modalities and integrate cross45;modal representations is becoming increasingly critical for continuous improvements in both academia and industry. In this paper we propose a novel method to identify key neurons for interpretability 45;45; how multi45;modal LLMs bridge visual and textual concepts for captioning. Our method improves conventional works upon efficiency and applied range by removing needs of costly gradient computation. Based on those identified neurons we further design a multi45;modal knowledge editing method beneficial to mitigate sensitive words or hallucination. For rationale of our design we provide theoretical assumption. For empirical evaluation we have conducted extensive quantitative and qualitative experiments. The results not only validate the effectiveness of our methods but also offer insightful findings that highlight three key properties of multi45;modal neurons sensitivity specificity and causal45;effect to shed light for future research.
