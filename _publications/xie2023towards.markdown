---
layout: publication
title: Funqa Towards Surprising Video Comprehension
authors: Xie Binzhu, Zhang Sicheng, Zhou Zitang, Li Bo, Zhang Yuanhan, Hessel Jack, Yang Jingkang, Liu Ziwei
conference: "Arxiv"
year: 2023
bibkey: xie2023towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.14899"}
tags: ['Agentic', 'Applications', 'Attention Mechanism', 'Language Modeling', 'Model Architecture']
---
Surprising videos such as funny clips creative performances or visual illusions attract significant attention. Enjoyment of these videos is not simply a response to visual stimuli; rather it hinges on the human capacity to understand (and appreciate) commonsense violations depicted in these videos. We introduce FunQA a challenging video question45;answering (QA) dataset specifically designed to evaluate and enhance the depth of video reasoning based on counter45;intuitive and fun videos. Unlike most video QA benchmarks which focus on less surprising contexts e.g. cooking or instructional videos FunQA covers three previously unexplored types of surprising videos 1) HumorQA 2) CreativeQA and 3) MagicQA. For each subset we establish rigorous QA tasks designed to assess the models capability in counter45;intuitive timestamp localization detailed video description and reasoning around counter45;intuitiveness. We also pose higher45;level tasks such as attributing a fitting and vivid title to the video and scoring the video creativity. In total the FunQA benchmark consists of 312K free45;text QA pairs derived from 4.3K video clips spanning a total of 24 video hours. Moreover we propose FunMentor an agent designed for Vision45;Language Models (VLMs) that uses multi45;turn dialogues to enhance models understanding of counter45;intuitiveness. Extensive experiments with existing VLMs demonstrate the effectiveness of FunMentor and reveal significant performance gaps for the FunQA videos across spatial45;temporal reasoning visual45;centered reasoning and free45;text generation.
