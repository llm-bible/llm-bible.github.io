---
layout: publication
title: 'Evaluation Of Chatgpt On Biomedical Tasks: A Zero-shot Comparison With Fine-tuned
  Generative Transformers'
authors: Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang
conference: Arxiv
year: 2023
citations: 15
bibkey: jahan2023evaluation
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2306.04504'}]
tags: [GPT, Transformer, Pre-Training]
---
ChatGPT is a large language model developed by OpenAI. Despite its impressive
performance across various tasks, no prior work has investigated its capability
in the biomedical domain yet. To this end, this paper aims to evaluate the
performance of ChatGPT on various benchmark biomedical tasks, such as relation
extraction, document classification, question answering, and summarization. To
the best of our knowledge, this is the first work that conducts an extensive
evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on
our evaluation that in biomedical datasets that have smaller training sets,
zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative
transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's
pre-training on large text corpora makes it quite specialized even in the
biomedical domain. Our findings demonstrate that ChatGPT has the potential to
be a valuable tool for various tasks in the biomedical domain that lack large
annotated data.