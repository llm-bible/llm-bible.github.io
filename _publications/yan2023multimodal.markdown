---
layout: publication
title: Multimodal Chatgpt For Medical Applications An Experimental Study Of GPT45;4V
authors: Yan Zhiling, Zhang Kai, Zhou Rong, He Lifang, Li Xiang, Sun Lichao
conference: "Arxiv"
year: 2023
bibkey: yan2023multimodal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.19061"}
  - {name: "Code", url: "https://github.com/ZhilingYan/GPT4V&#45;Medical&#45;Report"}
tags: ['Applications', 'GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Prompting', 'Reinforcement Learning', 'Tools']
---
In this paper we critically evaluate the capabilities of the state45;of45;the45;art multimodal large language model i.e. GPT45;4 with Vision (GPT45;4V) on Visual Question Answering (VQA) task. Our experiments thoroughly assess GPT45;4Vs proficiency in answering questions paired with images using both pathology and radiology datasets from 11 modalities (e.g. Microscopy Dermoscopy X45;ray CT etc.) and fifteen objects of interests (brain liver lung etc.). Our datasets encompass a comprehensive range of medical inquiries including sixteen distinct question types. Throughout our evaluations we devised textual prompts for GPT45;4V directing it to synergize visual and textual information. The experiments with accuracy score conclude that the current version of GPT45;4V is not recommended for real45;world diagnostics due to its unreliable and suboptimal accuracy in responding to diagnostic medical questions. In addition we delineate seven unique facets of GPT45;4Vs behavior in medical VQA highlighting its constraints within this complex arena. The complete details of our evaluation cases are accessible at https://github.com/ZhilingYan/GPT4V&#45;Medical&#45;Report.
