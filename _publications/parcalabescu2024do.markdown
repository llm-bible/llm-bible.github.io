---
layout: publication
title: Do Vision amp; Language Decoders Use Images And Text Equally How Self45;consistent Are Their Explanations
authors: Parcalabescu Letitia, Frank Anette
conference: "Arxiv"
year: 2024
bibkey: parcalabescu2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.18624"}
tags: ['Interpretability And Explainability', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning']
---
Vision and language model (VLM) decoders are currently the best45;performing architectures on multimodal tasks. Next to predictions they can also produce explanations either in post45;hoc or CoT settings. However it is not clear how much they use the vision and text modalities when generating predictions or explanations. In this work we investigate if VLMs rely on modalities differently when they produce explanations as opposed to providing answers. We also evaluate the self45;consistency of VLM decoders in both post45;hoc and CoT explanation settings by extending existing unimodal tests and measures to VLM decoders. We find that VLMs are less self45;consistent than LLMs. Text contributions in VL decoders are more important than image contributions in all examined tasks. Moreover the contributions of images are significantly stronger for explanation generation compared to answer generation. This difference is even larger in CoT compared to post45;hoc explanations. Lastly we provide an up45;to45;date benchmarking of state45;of45;the45;art VL decoders on the VALSE benchmark which before only covered VL encoders. We find that VL decoders still struggle with most phenomena tested by VALSE.
