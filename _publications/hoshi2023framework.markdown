---
layout: publication
title: Ralle&#58; A Framework For Developing And Evaluating Retrieval-augmented Large Language Models
authors: Hoshi Yasuto, Miyashita Daisuke, Ng Youyang, Tatsuno Kento, Morioka Yasuhiro, Torii Osamu, Deguchi Jun
conference: "Arxiv"
year: 2023
bibkey: hoshi2023framework
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.10633"}
  - {name: "Code", url: "https://github.com/yhoshi3/RaLLe"}
tags: ['Applications', 'Efficiency And Optimization', 'Ethics And Bias', 'Has Code', 'Prompting', 'RAG', 'Tools']
---
Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap we present RaLLe an open-source framework designed to facilitate the development evaluation and optimization of R-LLMs for knowledge-intensive tasks. With RaLLe developers can easily develop and evaluate R-LLMs improving hand-crafted prompts assessing individual inference processes and objectively measuring overall system performance quantitatively. By leveraging these features developers can enhance the performance and accuracy of their R-LLMs in knowledge-intensive generation tasks. We open-source our code at https://github.com/yhoshi3/RaLLe."
