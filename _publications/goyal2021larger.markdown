---
layout: publication
title: Larger-scale Transformers For Multilingual Masked Language Modeling
authors: Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau
conference: Arxiv
year: 2021
citations: 25
bibkey: goyal2021larger
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2105.00572'}]
tags: [Transformer, Masked Language Model, BERT, Language Modeling, Pre-Training]
---
Recent work has demonstrated the effectiveness of cross-lingual language
model pretraining for cross-lingual understanding. In this study, we present
the results of two larger multilingual masked language models, with 3.5B and
10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform
XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the
RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on
average while handling 99 more languages. This suggests pretrained models with
larger capacity may obtain both strong performance on high-resource languages
while greatly improving low-resource languages. We make our code and models
publicly available.