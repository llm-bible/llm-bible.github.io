---
layout: publication
title: Larger45;scale Transformers For Multilingual Masked Language Modeling
authors: Goyal Naman, Du Jingfei, Ott Myle, Anantharaman Giri, Conneau Alexis
conference: "Arxiv"
year: 2021
bibkey: goyal2021larger
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2105.00572"}
tags: ['BERT', 'Language Modeling', 'Masked Language Model', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques', 'Transformer']
---
Recent work has demonstrated the effectiveness of cross45;lingual language model pretraining for cross45;lingual understanding. In this study we present the results of two larger multilingual masked language models with 3.5B and 10.7B parameters. Our two new models dubbed XLM45;R XL and XLM45;R XXL outperform XLM45;R by 1.837; and 2.437; average accuracy on XNLI. Our model also outperforms the RoBERTa45;Large model on several English tasks of the GLUE benchmark by 0.337; on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high45;resource languages while greatly improving low45;resource languages. We make our code and models publicly available.
