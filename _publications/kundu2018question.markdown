---
layout: publication
title: A Question45;focused Multi45;factor Attention Network For Question Answering
authors: Kundu Souvik, Ng Hwee Tou
conference: "Arxiv"
year: 2018
bibkey: kundu2018question
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1801.08290"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture']
---
Neural network models recently proposed for question answering (QA) primarily focus on capturing the passage45;question relation. However they have minimal capability to link relevant facts distributed across multiple sentences which is crucial in achieving deeper understanding such as performing multi45;sentence reasoning co45;reference resolution etc. They also do not explicitly focus on the question and answer type which often plays a critical role in QA. In this paper we propose a novel end45;to45;end question45;focused multi45;factor attention network for answer extraction. Multi45;factor attentive encoding using tensor45;based transformation aggregates meaningful facts even when they are located in multiple sentences. To implicitly infer the answer type we also propose a max45;attentional question aggregation mechanism to encode a question vector based on the important words in a question. During prediction we incorporate sequence45;level encoding of the first wh45;word and its immediately following word as an additional source of question type information. Our proposed model achieves significant improvements over the best prior state45;of45;the45;art results on three large45;scale challenging QA datasets namely NewsQA TriviaQA and SearchQA.
