---
layout: publication
title: Towards Efficient Vision45;language Tuning More Information Density More Generalizability
authors: Hao Tianxiang, Lyu Mengyao, Chen Hui, Zhao Sicheng, Ding Xiaohan, Han Jungong, Ding Guiguang
conference: "Arxiv"
year: 2023
bibkey: hao2023towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.10813"}
tags: ['Attention Mechanism', 'Ethics And Bias', 'Model Architecture', 'Prompting', 'RAG', 'Reinforcement Learning', 'Security']
---
With the advancement of large pre45;trained vision45;language models effectively transferring the knowledge embedded within these foundational models to downstream tasks has become a pivotal topic particularly in data45;scarce environments. Recently parameter45;efficient fine45;tuning approaches especially prompt tuning have garnered considerable attention. To better understand the nature of prompt tuning we propose the concept of Information Density (ID) to indicate whether a matrix strongly belongs to certain feature spaces rather than being evenly distributed across various feature spaces. We suppose a higher ID with strong bias across some feature spaces naturally leads to excellent robustness and stability. Our research inspired by the observation that generalizability is closely linked to the information density of the prompt matrix introduces the Dense Information Prompt (DIP). DIP aims to enhance information density to improve generalization. Furthermore DIP significantly reduces the number of tunable parameters and the requisite storage space making it particularly advantageous in resource45;constrained settings. Comprehensive experiments substantiate the superiority of DIP. Notably DIP surpasses the latest state45;of45;the45;art methods by a substantial margin with an exceptionally small parameter count. Across a range of tasks spanning 11 datasets DIP improves the average downstream accuracy of classic prompt tuning by up to 5.7637; using merely 0.5K parameters.
