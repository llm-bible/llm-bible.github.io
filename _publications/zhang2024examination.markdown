---
layout: publication
title: An Examination On The Effectiveness Of Divide45;and45;conquer Prompting In Large Language Models
authors: Zhang Yizhou, Du Lun, Cao Defu, Fu Qiang, Liu Yan
conference: "Arxiv"
year: 2024
bibkey: zhang2024examination
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.05359"}
tags: ['Applications', 'Pretraining Methods', 'Prompting']
---
Foundation models such as Large language Models (LLMs) have attracted significant amount of interest due to their large number of applications. However when handling tasks involving repetitive sub45;tasks and/or deceptive contents such as arithmetic calculation and article45;level fake news detection simple instructional prompts suffer from inaccurate responses. Existing works show that more complicated prompting strategies such as Chain45;of45;Thoughts and Least45;to45;Most can unlock LLMs powerful capacity in diverse areas. Recent researches reveal that simple divide45;and45;conquer prompting strategy i.e. simply dividing the input sequence to multiple sub45;inputs can also substantially improve LLMs performance in some specific tasks such as misinformation detection. In this paper we aim at examining the utility of divide45;and45;conquer prompting strategy and answer on which kind of tasks this strategy gets advantages. Specifically we provide a theoretic analysis to divide45;and45;conquer prompting strategy and help us identify the specific tasks where DaC prompting can bring performance boost with theoretic guarantee. We then present two cases (large integer arithmetic and fact verification) where experimental results aligns with our theoretic analysis.
