---
layout: publication
title: Effectively Fine45;tune To Improve Large Multimodal Models For Radiology Report Generation
authors: Lu Yuzhe, Hong Sungmin, Shah Yash, Xu Panpan
conference: "Arxiv"
year: 2023
bibkey: lu2023effectively
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.01504"}
tags: ['Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Prompting', 'RAG', 'Tools', 'Training Techniques', 'Transformer']
---
Writing radiology reports from medical images requires a high level of domain expertise. It is time45;consuming even for trained radiologists and can be error45;prone for inexperienced radiologists. It would be appealing to automate this task by leveraging generative AI which has shown drastic progress in vision and language understanding. In particular Large Language Models (LLM) have demonstrated impressive capabilities recently and continued to set new state45;of45;the45;art performance on almost all natural language tasks. While many have proposed architectures to combine vision models with LLMs for multimodal tasks few have explored practical fine45;tuning strategies. In this work we proposed a simple yet effective two45;stage fine45;tuning protocol to align visual features to LLMs text embedding space as soft visual prompts. Our framework with OpenLLaMA45;7B achieved state45;of45;the45;art level performance without domain45;specific pretraining. Moreover we provide detailed analyses of soft visual prompts and attention mechanisms shedding light on future research directions.
