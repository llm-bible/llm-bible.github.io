---
layout: publication
title: 'Layoutxlm: Multimodal Pre-training For Multilingual Visually-rich Document
  Understanding'
authors: Yiheng Xu et al.
conference: Arxiv
year: 2021
citations: 39
bibkey: xu2021multimodal
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2104.08836'}]
tags: [Multimodal Models, Pre-Training]
---
Multimodal pre-training with text, layout, and image has achieved SOTA
performance for visually-rich document understanding tasks recently, which
demonstrates the great potential for joint learning across different
modalities. In this paper, we present LayoutXLM, a multimodal pre-trained model
for multilingual document understanding, which aims to bridge the language
barriers for visually-rich document understanding. To accurately evaluate
LayoutXLM, we also introduce a multilingual form understanding benchmark
dataset named XFUND, which includes form understanding samples in 7 languages
(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and
key-value pairs are manually labeled for each language. Experiment results show
that the LayoutXLM model has significantly outperformed the existing SOTA
cross-lingual pre-trained models on the XFUND dataset. The pre-trained
LayoutXLM model and the XFUND dataset are publicly available at
https://aka.ms/layoutxlm.