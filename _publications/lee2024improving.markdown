---
layout: publication
title: Improving Conversational Abilities Of Quantized Large Language Models Via Direct Preference Alignment
authors: Lee Janghwan, Park Seongmin, Hong Sukjin, Kim Minsoo, Chang Du-seong, Choi Jungwook
conference: "Arxiv"
year: 2024
bibkey: lee2024improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.03051"}
tags: ['Agentic', 'Distillation', 'Efficiency And Optimization', 'Quantization', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
The rapid advancement of large language models (LLMs) has facilitated their transformation into conversational chatbots that can grasp contextual nuances and generate pertinent sentences closely mirroring human values through advanced techniques such as instruction tuning and reinforcement learning from human feedback (RLHF). However the computational efficiency required for LLMs achieved through techniques like post45;training quantization (PTQ) presents challenges such as token45;flipping that can impair chatbot performance. In response we propose a novel preference alignment approach quantization45;aware direct preference optimization (QDPO) that aligns quantized LLMs with their full45;precision counterparts improving conversational abilities. Evaluated on two instruction45;tuned LLMs in various languages QDPO demonstrated superior performance in improving conversational abilities compared to established PTQ and knowledge45;distillation fine45;tuning techniques marking a significant step forward in the development of efficient and effective conversational LLMs.
