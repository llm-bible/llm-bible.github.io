---
layout: publication
title: Towards Cross45;lingual Explanation Of Artwork In Large45;scale Vision Language Models
authors: Ozaki Shintaro, Hayashi Kazuki, Sakai Yusuke, Kamigaito Hidetaka, Hayashi Katsuhiko, Watanabe Taro
conference: "Arxiv"
year: 2024
bibkey: ozaki2024towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.01584"}
tags: ['Applications', 'Ethics And Bias', 'Interpretability And Explainability', 'Training Techniques']
---
As the performance of Large45;scale Vision Language Models (LVLMs) improves they are increasingly capable of responding in multiple languages and there is an expectation that the demand for explanations generated by LVLMs will grow. However pre45;training of Vision Encoder and the integrated training of LLMs with Vision Encoder are mainly conducted using English training data leaving it uncertain whether LVLMs can completely handle their potential when generating explanations in languages other than English. In addition multilingual QA benchmarks that create datasets using machine translation have cultural differences and biases remaining issues for use as evaluation tasks. To address these challenges this study created an extended dataset in multiple languages without relying on machine translation. This dataset that takes into account nuances and country45;specific phrases was then used to evaluate the generation explanation abilities of LVLMs. Furthermore this study examined whether Instruction45;Tuning in resource45;rich English improves performance in other languages. Our findings indicate that LVLMs perform worse in languages other than English compared to English. In addition it was observed that LVLMs struggle to effectively manage the knowledge learned from English data.
