---
layout: publication
title: 'Towards Cross-lingual Explanation Of Artwork In Large-scale Vision Language Models'
authors: Shintaro Ozaki, Kazuki Hayashi, Yusuke Sakai, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe
conference: "Arxiv"
year: 2024
bibkey: ozaki2024towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.01584"}
  - {name: "Code", url: "https://huggingface.co/datasets/naist-nlp/MultiExpArt"}
tags: ['Training Techniques', 'Ethics and Bias', 'Fine-Tuning', 'Has Code', 'Interpretability and Explainability', 'Pre-Training', 'Applications']
---
As the performance of Large-scale Vision Language Models (LVLMs) improves,
they are increasingly capable of responding in multiple languages, and there is
an expectation that the demand for explanations generated by LVLMs will grow.
However, pre-training of Vision Encoder and the integrated training of LLMs
with Vision Encoder are mainly conducted using English training data, leaving
it uncertain whether LVLMs can completely handle their potential when
generating explanations in languages other than English. In addition,
multilingual QA benchmarks that create datasets using machine translation have
cultural differences and biases, remaining issues for use as evaluation tasks.
To address these challenges, this study created an extended dataset in multiple
languages without relying on machine translation. This dataset that takes into
account nuances and country-specific phrases was then used to evaluate the
generation explanation abilities of LVLMs. Furthermore, this study examined
whether Instruction-Tuning in resource-rich English improves performance in
other languages. Our findings indicate that LVLMs perform worse in languages
other than English compared to English. In addition, it was observed that LVLMs
struggle to effectively manage the knowledge learned from English data. Our
dataset is available at https://huggingface.co/datasets/naist-nlp/MultiExpArt
