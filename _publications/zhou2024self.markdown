---
layout: publication
title: Self45;discover Large Language Models Self45;compose Reasoning Structures
authors: Zhou Pei, Pujara Jay, Ren Xiang, Chen Xinyun, Cheng Heng-tze, Le Quoc V., Chi Ed H., Zhou Denny, Mishra Swaroop, Zheng Huaixiu Steven
conference: "Arxiv"
year: 2024
bibkey: zhou2024self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.03620"}
tags: ['Agentic', 'GPT', 'Model Architecture', 'Multimodal Models', 'Prompting', 'Tools']
---
We introduce SELF45;DISCOVER a general framework for LLMs to self45;discover the task45;intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self45;discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step45;by45;step thinking and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF45;DISCOVER substantially improves GPT45;4 and PaLM 2s performance on challenging reasoning benchmarks such as BigBench45;Hard grounded agent reasoning and MATH by as much as 3237; compared to Chain of Thought (CoT). Furthermore SELF45;DISCOVER outperforms inference45;intensive methods such as CoT45;Self45;Consistency by more than 2037; while requiring 1045;40x fewer inference compute. Finally we show that the self45;discovered reasoning structures are universally applicable across model families from PaLM 245;L to GPT45;4 and from GPT45;4 to Llama2 and share commonalities with human reasoning patterns.
