---
layout: publication
title: Self-discover: Large Language Models Self-compose Reasoning Structures
authors: Zhou Pei, Pujara Jay, Ren Xiang, Chen Xinyun, Cheng Heng-tze, Le Quoc V., Chi Ed H., Zhou Denny, Mishra Swaroop, Zheng Huaixiu Steven
conference: "Arxiv"
year: 2024
bibkey: zhou2024self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.03620"}
tags: ['Agentic', 'GPT', 'Model Architecture', 'Multimodal Models', 'Prompting', 'Tools']
---
We introduce SELF-DISCOVER a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2s performance on challenging reasoning benchmarks such as BigBench-Hard grounded agent reasoning and MATH by as much as 3237; compared to Chain of Thought (CoT). Furthermore SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 2037; while requiring 10-40x fewer inference compute. Finally we show that the self-discovered reasoning structures are universally applicable across model families from PaLM 2-L to GPT-4 and from GPT-4 to Llama2 and share commonalities with human reasoning patterns.
