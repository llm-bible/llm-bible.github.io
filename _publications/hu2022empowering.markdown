---
layout: publication
title: Empowering Language Models With Knowledge Graph Reasoning For Question Answering
authors: Hu Ziniu, Xu Yichong, Yu Wenhao, Wang Shuohang, Yang Ziyi, Zhu Chenguang, Chang Kai-wei, Sun Yizhou
conference: "Arxiv"
year: 2022
bibkey: hu2022empowering
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.08380"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
Answering open45;domain questions requires world knowledge about in45;context entities. As pre45;trained Language Models (LMs) lack the power to store all required knowledge external knowledge sources such as knowledge graphs are often used to augment LMs. In this work we propose knOwledge REasOning empowered Language Model (OREO45;LM) which consists of a novel Knowledge Interaction Layer that can be flexibly plugged into existing Transformer45;based LMs to interact with a differentiable Knowledge Graph Reasoning module collaboratively. In this way LM guides KG to walk towards the desired answer while the retrieved knowledge improves LM. By adopting OREO45;LM to RoBERTa and T5 we show significant performance gain achieving state45;of45;art results in the Closed45;Book setting. The performance enhancement is mainly from the KG reasonings capacity to infer missing relational facts. In addition OREO45;LM provides reasoning paths as rationales to interpret the models decision.
