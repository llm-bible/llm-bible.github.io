---
layout: publication
title: 'Don''t Just Translate, Agitate: Using Large Language Models As Devil''s Advocates For AI Explanations'
authors: Ashley Suh, Kenneth Alperin, Harry Li, Steven R Gomez
conference: "Arxiv"
year: 2025
bibkey: suh2025just
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.12424'}
tags: ['Ethics and Bias', 'Interpretability', 'Interpretability and Explainability', 'Training Techniques']
---
This position paper highlights a growing trend in Explainable AI (XAI)
research where Large Language Models (LLMs) are used to translate outputs from
explainability techniques, like feature-attribution weights, into a natural
language explanation. While this approach may improve accessibility or
readability for users, recent findings suggest that translating into human-like
explanations does not necessarily enhance user understanding and may instead
lead to overreliance on AI systems. When LLMs summarize XAI outputs without
surfacing model limitations, uncertainties, or inconsistencies, they risk
reinforcing the illusion of interpretability rather than fostering meaningful
transparency. We argue that - instead of merely translating XAI outputs - LLMs
should serve as constructive agitators, or devil's advocates, whose role is to
actively interrogate AI explanations by presenting alternative interpretations,
potential biases, training data limitations, and cases where the model's
reasoning may break down. In this role, LLMs can facilitate users in engaging
critically with AI systems and generated explanations, with the potential to
reduce overreliance caused by misinterpreted or specious explanations.
