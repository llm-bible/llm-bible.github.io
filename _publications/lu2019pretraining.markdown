---
layout: publication
title: Vilbert Pretraining Task45;agnostic Visiolinguistic Representations For Vision45;and45;language Tasks
authors: Lu Jiasen, Batra Dhruv, Parikh Devi, Lee Stefan
conference: "Arxiv"
year: 2019
bibkey: lu2019pretraining
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1908.02265"}
tags: ['Applications', 'Attention Mechanism', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
We present ViLBERT (short for Vision45;and45;Language BERT) a model for learning task45;agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi45;modal two45;stream model pro45;cessing both visual and textual inputs in separate streams that interact through co45;attentional transformer layers. We pretrain our model through two proxy tasks on the large automatically collected Conceptual Captions dataset and then transfer it to multiple established vision45;and45;language tasks 45;45; visual question answering visual commonsense reasoning referring expressions and caption45;based image retrieval 45;45; by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task45;specific models 45;45; achieving state45;of45;the45;art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.
