---
layout: publication
title: 'Harnessing Multiple Large Language Models: A Survey On LLM Ensemble'
authors: Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, Philip S. Yu
conference: "Arxiv"
year: 2025
bibkey: chen2025harnessing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.18036'}
  - {name: "Code", url: 'https://github.com/junchenzhi/Awesome-LLM-Ensemble'}
tags: ['Attention Mechanism', 'Has Code', 'Applications', 'Model Architecture', 'Survey Paper']
---
LLM Ensemble -- which involves the comprehensive use of multiple large language models (LLMs), each aimed at handling user queries during downstream inference, to benefit from their individual strengths -- has gained substantial attention recently. The widespread availability of LLMs, coupled with their varying strengths and out-of-the-box usability, has profoundly advanced the field of LLM Ensemble. This paper presents the first systematic review of recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM Ensemble and discuss several related research problems. Then, we provide a more in-depth classification of the methods under the broad categories of "ensemble-before-inference, ensemble-during-inference, ensemble-after-inference'', and review all relevant methods. Finally, we introduce related benchmarks and applications, summarize existing studies, and suggest several future research directions. A curated list of papers on LLM Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.
