---
layout: publication
title: SCITAB A Challenging Benchmark For Compositional Reasoning And Claim Verification On Scientific Tables
authors: Lu Xinyuan, Pan Liangming, Liu Qian, Nakov Preslav, Kan Min-yen
conference: "Arxiv"
year: 2023
bibkey: lu2023challenging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.13186"}
  - {name: "Code", url: "https://github.com/XinyuanLu00/SciTab"}
tags: ['Ethics And Bias', 'GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Training Techniques']
---
Current scientific fact45;checking benchmarks exhibit several shortcomings such as biases arising from crowd45;sourced claims and an over45;reliance on text45;based evidence. We present SCITAB a challenging evaluation dataset consisting of 1.2K expert45;verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence45;containing scientific tables annotated with labels. Through extensive evaluations we demonstrate that SCITAB poses a significant challenge to state45;of45;the45;art models including table45;based pretraining models and large language models. All models except GPT45;4 achieved performance barely above random guessing. Popular prompting techniques such as Chain45;of45;Thought do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB including table grounding claim ambiguity and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.
