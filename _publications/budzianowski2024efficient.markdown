---
layout: publication
title: Pheme Efficient And Conversational Speech Generation
authors: Budzianowski Paweł, Sereda Taras, Cichy Tomasz, Vulić Ivan
conference: "Arxiv"
year: 2024
bibkey: budzianowski2024efficient
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.02839"}
tags: ['Applications', 'Distillation', 'Efficiency And Optimization', 'GPT', 'Pretraining Methods', 'Tools', 'Training Techniques']
---
In recent years speech generation has seen remarkable progress now achieving one45;shot generation capability that is often virtually indistinguishable from real human voice. Integrating such advancements in speech generation with large language models might revolutionize a wide range of applications. However certain applications such as assistive conversational systems require natural and conversational speech generation tools that also operate efficiently in real time. Current state45;of45;the45;art models like VALL45;E and SoundStorm powered by hierarchical neural audio codecs require large neural components and extensive training data to work well. In contrast MQTTS aims to build more compact conversational TTS models while capitalizing on smaller45;scale real45;life conversational speech data. However its autoregressive nature yields high inference latency and thus limits its real45;time usage. In order to mitigate the current limitations of the state45;of45;the45;art TTS models while capitalizing on their strengths in this work we introduce the Pheme model series that 1) offers compact yet high45;performing models 2) allows for parallel speech generation of 3) natural conversational speech and 4) it can be trained efficiently on smaller45;scale conversational data cutting data demands by more than 10x but still matching the quality of the autoregressive TTS models. We also show that through simple teacher45;student distillation we can meet significant improvements in voice quality for single45;speaker setups on top of pretrained Pheme checkpoints relying solely on synthetic speech generated by much larger teacher models. Audio samples and pretrained models are available online.
