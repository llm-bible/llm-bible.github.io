---
layout: publication
title: On Learning To Summarize With Large Language Models As References
authors: Liu Yixin, Shi Kejian, He Katherine S, Ye Longtian, Fabbri Alexander R., Liu Pengfei, Radev Dragomir, Cohan Arman
conference: "Arxiv"
year: 2023
bibkey: liu2023learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.14239"}
tags: ['Applications', 'Pretraining Methods', 'RAG', 'Reinforcement Learning']
---
Recent studies have found that summaries generated by large language models (LLMs) are favored by human annotators over the original reference summaries in commonly used summarization datasets. Therefore we study an LLM45;as45;reference learning setting for smaller text summarization models to investigate whether their performance can be substantially improved. To this end we use LLMs as both oracle summary generators for standard supervised fine45;tuning and oracle summary evaluators for efficient contrastive learning that leverages the LLMs supervision signals. We conduct comprehensive experiments with source news articles and find that (1) summarization models trained under the LLM45;as45;reference setting achieve significant performance improvement in both LLM and human evaluations; (2) contrastive learning outperforms standard supervised fine45;tuning under both low and high resource settings. Our experimental results also enable a meta45;analysis of LLMs summary evaluation capacities under a challenging setting showing that LLMs are not well45;aligned with human evaluators. Particularly our expert human evaluation reveals remaining nuanced performance gaps between LLMs and our fine45;tuned models which LLMs fail to capture. Thus we call for further studies into both the potential and challenges of using LLMs in summarization model development.
