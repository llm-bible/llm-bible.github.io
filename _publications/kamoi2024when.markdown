---
layout: publication
title: When Can Llms Actually Correct Their Own Mistakes A Critical Survey Of Self45;correction Of Llms
authors: Kamoi Ryo, Zhang Yusen, Zhang Nan, Han Jiawei, Zhang Rui
conference: "Arxiv"
year: 2024
bibkey: kamoi2024when
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.01297"}
tags: ['Pretraining Methods', 'Prompting', 'Survey Paper', 'Tools']
---
Self45;correction is an approach to improving responses from large language models (LLMs) by refining the responses using LLMs during inference. Prior work has proposed various self45;correction frameworks using different sources of feedback including self45;evaluation and external feedback. However there is still no consensus on the question of when LLMs can correct their own mistakes as recent studies also report negative results. In this work we critically survey broad papers and discuss the conditions required for successful self45;correction. We first find that prior studies often do not define their research questions in detail and involve impractical frameworks or unfair evaluations that over45;evaluate self45;correction. To tackle these issues we categorize research questions in self45;correction research and provide a checklist for designing appropriate experiments. Our critical survey based on the newly categorized research questions shows that (1) no prior work demonstrates successful self45;correction with feedback from prompted LLMs except for studies in tasks that are exceptionally suited for self45;correction (2) self45;correction works well in tasks that can use reliable external feedback and (3) large45;scale fine45;tuning enables self45;correction.
