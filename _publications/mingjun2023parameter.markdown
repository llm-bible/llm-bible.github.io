---
layout: publication
title: PEFTT Parameter45;efficient Fine45;tuning For Low45;resource Tibetan Pre45;trained Language Models
authors: Mingjun Zhou, Zhuoma Daiqing, Nuo Qun, Tashi Nyima
conference: "Arxiv"
year: 2023
bibkey: mingjun2023parameter
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.12109"}
tags: ['Applications', 'Fine Tuning', 'Pretraining Methods', 'Prompting', 'Training Techniques']
---
In this era of large language models (LLMs) the traditional training of models has become increasingly unimaginable for regular users and institutions. The exploration of efficient fine45;tuning for high45;resource languages on these models is an undeniable trend that is gradually gaining popularity. However there has been very little exploration for various low45;resource languages such as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While there is currently no existing large language model for Tibetan due to its low45;resource nature that day will undoubtedly arrive. Therefore research on efficient fine45;tuning for low45;resource language models like Tibetan is highly necessary. Our research can serve as a reference to fill this crucial gap. Efficient fine45;tuning strategies for pre45;trained language models (PLMs) in Tibetan have seen minimal exploration. We conducted three types of efficient fine45;tuning experiments on the publicly available TNCC45;title dataset prompt45;tuning Adapter lightweight fine45;tuning and prompt45;tuning + Adapter fine45;tuning. The experimental results demonstrate significant improvements using these methods providing valuable insights for advancing Tibetan language applications in the context of pre45;trained models.
