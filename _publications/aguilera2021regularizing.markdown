---
layout: publication
title: Regularizing Transformers With Deep Probabilistic Layers
authors: Aguilera Aurora Cobo, Olmos Pablo Martínez, Artés-rodríguez Antonio, Pérez-cruz Fernando
conference: "Arxiv"
year: 2021
bibkey: aguilera2021regularizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2108.10764"}
tags: ['Attention Mechanism', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Language models (LM) have grown with non45;stop in the last decade from sequence45;to45;sequence architectures to the state45;of45;the45;art and utter attention45;based Transformers. In this work we demonstrate how the inclusion of deep generative models within BERT can bring more versatile models able to impute missing/noisy words with richer text or even improve BLEU score. More precisely we use a Gaussian Mixture Variational Autoencoder (GMVAE) as a regularizer layer and prove its effectiveness not only in Transformers but also in the most relevant encoder45;decoder based LM seq2seq with and without attention.
