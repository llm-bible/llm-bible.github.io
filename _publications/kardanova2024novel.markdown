---
layout: publication
title: 'A Novel Psychometrics-based Approach To Developing Professional Competency Benchmark For Large Language Models'
authors: Elena National Research University Higher School Of Economics, Moscow, Russia Kardanova, Alina National Research University Higher School Of Economics, Moscow, Russia Ivanova, Ksenia National Research University Higher School Of Economics, Moscow, Russia Tarasova, Taras National Research University Higher School Of Economics, Moscow, Russia Pashchenko, Aleksei National Research University Higher School Of Economics, Moscow, Russia Tikhoniuk, Elen National Research University Higher School Of Economics, Moscow, Russia Yusupova, Anatoly National Research University Higher School Of Economics, Moscow, Russia Kasprzhak, Yaroslav National Research University Higher School Of Economics, Moscow, Russia Kuzminov, Ekaterina National Research University Higher School Of Economics, Moscow, Russia Kruchinskaia, Irina National Research University Higher School Of Economics, Moscow, Russia Brun
conference: "Arxiv"
year: 2024
bibkey: kardanova2024novel
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.00045"}
tags: ['Model Architecture', 'Tools', 'Reinforcement Learning', 'GPT', 'Applications', 'Attention Mechanism']
---
The era of large language models (LLM) raises questions not only about how to
train models, but also about how to evaluate them. Despite numerous existing
benchmarks, insufficient attention is often given to creating assessments that
test LLMs in a valid and reliable manner. To address this challenge, we
accommodate the Evidence-centered design (ECD) methodology and propose a
comprehensive approach to benchmark development based on rigorous psychometric
principles. In this paper, we have made the first attempt to illustrate this
approach by creating a new benchmark in the field of pedagogy and education,
highlighting the limitations of existing benchmark development approach and
taking into account the development of LLMs. We conclude that a new approach to
benchmarking is required to match the growing complexity of AI applications in
the educational context. We construct a novel benchmark guided by the Bloom's
taxonomy and rigorously designed by a consortium of education experts trained
in test development. Thus the current benchmark provides an academically robust
and practical assessment tool tailored for LLMs, rather than human
participants. Tested empirically on the GPT model in the Russian language, it
evaluates model performance across varied task complexities, revealing critical
gaps in current LLM capabilities. Our results indicate that while generative AI
tools hold significant promise for education - potentially supporting tasks
such as personalized tutoring, real-time feedback, and multilingual learning -
their reliability as autonomous teachers' assistants right now remain rather
limited, particularly in tasks requiring deeper cognitive engagement.
