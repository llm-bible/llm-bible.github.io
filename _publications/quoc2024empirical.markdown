---
layout: publication
title: An Empirical Study On Self45;correcting Large Language Models For Data Science Code Generation
authors: Quoc Thai Tang, Minh Duc Ha, Thanh Tho Quan, Nguyen-duc Anh
conference: "Arxiv"
year: 2024
bibkey: quoc2024empirical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.15658"}
tags: ['Applications', 'Prompting', 'Reinforcement Learning', 'Tools']
---
Large Language Models (LLMs) have recently advanced many applications on software engineering tasks particularly the potential for code generation. Among contemporary challenges code generated by LLMs often suffers from inaccuracies and hallucinations requiring external inputs to correct. One recent strategy to fix these issues is to refine the code generated from LLMs using the input from the model itself (self45;augmented). In this work we proposed a novel method namely CoT45;SelfEvolve. CoT45;SelfEvolve iteratively and automatically refines code through a self45;correcting process guided by a chain of thought constructed from real45;world programming problem feedback. Focusing on data science code including Python libraries such as NumPy and Pandas our evaluations on the DS45;1000 dataset demonstrate that CoT45;SelfEvolve significantly outperforms existing models in solving complex problems. The framework shows substantial improvements in both initial code generation and subsequent iterations with the models accuracy increasing significantly with each additional iteration. This highlights the effectiveness of using chain45;of45;thought prompting to address complexities revealed by program executor traceback error messages. We also discuss how CoT45;SelfEvolve can be integrated into continuous software engineering environments providing a practical solution for improving LLM45;based code generation.
