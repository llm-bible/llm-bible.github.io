---
layout: publication
title: 'An Empirical Study On Self-correcting Large Language Models For Data Science Code Generation'
authors: Quoc Thai Tang, Minh Duc Ha, Thanh Tho Quan, Nguyen-duc Anh
conference: "Arxiv"
year: 2024
bibkey: quoc2024empirical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.15658"}
tags: ['Applications', 'Prompting', 'Reinforcement Learning', 'Tools']
---
Large Language Models (LLMs) have recently advanced many applications on software engineering tasks particularly the potential for code generation. Among contemporary challenges code generated by LLMs often suffers from inaccuracies and hallucinations requiring external inputs to correct. One recent strategy to fix these issues is to refine the code generated from LLMs using the input from the model itself (self-augmented). In this work we proposed a novel method namely CoT-SelfEvolve. CoT-SelfEvolve iteratively and automatically refines code through a self-correcting process guided by a chain of thought constructed from real-world programming problem feedback. Focusing on data science code including Python libraries such as NumPy and Pandas our evaluations on the DS-1000 dataset demonstrate that CoT-SelfEvolve significantly outperforms existing models in solving complex problems. The framework shows substantial improvements in both initial code generation and subsequent iterations with the models accuracy increasing significantly with each additional iteration. This highlights the effectiveness of using chain-of-thought prompting to address complexities revealed by program executor traceback error messages. We also discuss how CoT-SelfEvolve can be integrated into continuous software engineering environments providing a practical solution for improving LLM-based code generation.
