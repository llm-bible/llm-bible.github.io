---
layout: publication
title: COMMA Co45;articulated Multi45;modal Learning
authors: Hu Lianyu, Gao Liqing, Liu Zekang, Pun Chi-man, Feng Wei
conference: "Arxiv"
year: 2023
bibkey: hu2023co
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.00268"}
tags: ['Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Training Techniques', 'Transformer']
---
Pretrained large45;scale vision45;language models such as CLIP have demonstrated excellent generalizability over a series of downstream tasks. However they are sensitive to the variation of input text prompts and need a selection of prompt templates to achieve satisfactory performance. Recently various methods have been proposed to dynamically learn the prompts as the textual inputs to avoid the requirements of laboring hand45;crafted prompt engineering in the fine45;tuning process. We notice that these methods are suboptimal in two aspects. First the prompts of the vision and language branches in these methods are usually separated or uni45;directionally correlated. Thus the prompts of both branches are not fully correlated and may not provide enough guidance to align the representations of both branches. Second its observed that most previous methods usually achieve better performance on seen classes but cause performance degeneration on unseen classes compared to CLIP. This is because the essential generic knowledge learned in the pretraining stage is partly forgotten in the fine45;tuning process. In this paper we propose Co45;Articulated Multi45;Modal Learning (COMMA) to handle the above limitations. Especially our method considers prompts from both branches to generate the prompts to enhance the representation alignment of both branches. Besides to alleviate forgetting about the essential knowledge we minimize the feature discrepancy between the learned prompts and the embeddings of hand45;crafted prompts in the pre45;trained CLIP in the late transformer layers. We evaluate our method across three representative tasks of generalization to novel classes new target datasets and unseen domain shifts. Experimental results demonstrate the superiority of our method by exhibiting a favorable performance boost upon all tasks with high efficiency.
