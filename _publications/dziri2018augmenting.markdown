---
layout: publication
title: Augmenting Neural Response Generation With Context-aware Topical Attention
authors: Dziri Nouha, Kamalloo Ehsan, Mathewson Kory W., Zaiane Osmar
conference: "Arxiv"
year: 2018
bibkey: dziri2018augmenting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1811.01063"}
tags: ['Attention Mechanism', 'Model Architecture', 'Transformer']
---
Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in generating natural conversational exchanges. Notwithstanding the syntactically well-formed responses generated by these neural network models they are prone to be acontextual short and generic. In this work we introduce a Topical Hierarchical Recurrent Encoder Decoder (THRED) a novel fully data-driven multi-turn response generation system intended to produce contextual and topic-aware responses. Our model is built upon the basic Seq2Seq model by augmenting it with a hierarchical joint attention mechanism that incorporates topical concepts and previous interactions into the response generation. To train our model we provide a clean and high-quality conversational dataset mined from Reddit comments. We evaluate THRED on two novel automated metrics dubbed Semantic Similarity and Response Echo Index as well as with human evaluation. Our experiments demonstrate that the proposed model is able to generate more diverse and contextually relevant responses compared to the strong baselines.
