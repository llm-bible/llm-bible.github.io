---
layout: publication
title: Kam45;cot Knowledge Augmented Multimodal Chain45;of45;thoughts Reasoning
authors: Mondal Debjyoti, Modi Suraj, Panda Subhadarshi, Singh Rituraj, Rao Godawari Sudhakar
conference: "Arxiv"
year: 2024
bibkey: mondal2024kam
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.12863"}
tags: ['Applications', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Multimodal Models', 'RAG', 'Tools', 'Training Techniques']
---
Large Language Models (LLMs) have demonstrated impressive performance in natural language processing tasks by leveraging chain of thought (CoT) that enables step45;by45;step thinking. Extending LLMs with multimodal capabilities is the recent interest but incurs computational cost and requires substantial hardware resources. To address these challenges we propose KAM45;CoT a framework that integrates CoT reasoning Knowledge Graphs (KGs) and multiple modalities for a comprehensive understanding of multimodal tasks. KAM45;CoT adopts a two45;stage training process with KG grounding to generate effective rationales and answers. By incorporating external knowledge from KGs during reasoning the model gains a deeper contextual understanding reducing hallucinations and enhancing the quality of answers. This knowledge45;augmented CoT reasoning empowers the model to handle questions requiring external context providing more informed answers. Experimental findings show KAM45;CoT outperforms the state45;of45;the45;art methods. On the ScienceQA dataset we achieve an average accuracy of 93.8737; surpassing GPT45;3.5 (75.1737;) by 1837; and GPT45;4 (83.9937;) by 1037;. Remarkably KAM45;CoT achieves these results with only 280M trainable parameters at a time demonstrating its cost45;efficiency and effectiveness.
