---
layout: publication
title: 'BMIP: Bi-directional Modality Interaction Prompt Learning For VLM'
authors: Song-lin Lv, Yu-yang Chen, Zhi Zhou, Ming Yang, Lan-zhe Guo
conference: "Arxiv"
year: 2025
bibkey: lv2025bi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.07769"}
tags: ['Multimodal Models', 'Model Architecture', 'Reinforcement Learning', 'Prompting', 'Attention Mechanism']
---
Vision-language models (VLMs) have exhibited remarkable generalization
capabilities, and prompt learning for VLMs has attracted great attention for
the ability to adapt pre-trained VLMs to specific downstream tasks. However,
existing studies mainly focus on single-modal prompts or uni-directional
modality interaction, overlooking the powerful alignment effects resulting from
the interaction between the vision and language modalities. To this end, we
propose a novel prompt learning method called
\\(\underline\{\textbf\{B\}\}i-directional \underline\{\textbf\{M\}\}odality
\underline\{\textbf\{I\}\}nteraction \underline\{\textbf\{P\}\}rompt (BMIP)\\), which
dynamically weights bi-modal information through learning the information of
the attention layer, enhancing trainability and inter-modal consistency
compared to simple information aggregation methods. To evaluate the
effectiveness of prompt learning methods, we propose a more realistic
evaluation paradigm called open-world generalization complementing the widely
adopted cross-dataset transfer and domain generalization tasks. Comprehensive
experiments on various datasets reveal that BMIP not only outperforms current
state-of-the-art methods across all three evaluation paradigms but is also
flexible enough to be combined with other prompt-based methods for consistent
performance enhancement.
