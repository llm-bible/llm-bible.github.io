---
layout: publication
title: Does BERT Really Agree Fine45;grained Analysis Of Lexical Dependence On A Syntactic Task
authors: Lasri Karim, Lenci Alessandro, Poibeau Thierry
conference: "Arxiv"
year: 2022
bibkey: lasri2022does
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.06889"}
tags: ['BERT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Although transformer45;based Neural Language Models demonstrate impressive performance on a variety of tasks their generalization abilities are not well understood. They have been shown to perform strongly on subject45;verb number agreement in a wide array of settings suggesting that they learned to track syntactic dependencies during their training even without explicit supervision. In this paper we examine the extent to which BERT is able to perform lexically45;independent subject45;verb number agreement (NA) on targeted syntactic templates. To do so we disrupt the lexical patterns found in naturally occurring stimuli for each targeted structure in a novel fine45;grained analysis of BERTs behavior. Our results on nonce sentences suggest that the model generalizes well for simple templates but fails to perform lexically45;independent syntactic generalization when as little as one attractor is present.
