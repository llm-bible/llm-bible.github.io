---
layout: publication
title: 'A Survey Of Multimodal Retrieval-augmented Generation'
authors: Lang Mei, Siyu Mo, Zhihan Yang, Chong Chen
conference: "Arxiv"
year: 2025
bibkey: mei2025survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.08748"}
tags: ['Fine-Tuning', 'Tools', 'Survey Paper', 'Applications', 'RAG', 'Multimodal Models']
---
Multimodal Retrieval-Augmented Generation (MRAG) enhances large language
models (LLMs) by integrating multimodal data (text, images, videos) into
retrieval and generation processes, overcoming the limitations of text-only
Retrieval-Augmented Generation (RAG). While RAG improves response accuracy by
incorporating external textual knowledge, MRAG extends this framework to
include multimodal retrieval and generation, leveraging contextual information
from diverse data types. This approach reduces hallucinations and enhances
question-answering systems by grounding responses in factual, multimodal
knowledge. Recent studies show MRAG outperforms traditional RAG, especially in
scenarios requiring both visual and textual understanding. This survey reviews
MRAG's essential components, datasets, evaluation methods, and limitations,
providing insights into its construction and improvement. It also identifies
challenges and future research directions, highlighting MRAG's potential to
revolutionize multimodal information retrieval and generation. By offering a
comprehensive perspective, this work encourages further exploration into this
promising paradigm.
