---
layout: publication
title: Orca Progressive Learning From Complex Explanation Traces Of GPT45;4
authors: Mukherjee Subhabrata, Mitra Arindam, Jawahar Ganesh, Agarwal Sahaj, Palangi Hamid, Awadallah Ahmed
conference: "Arxiv"
year: 2023
bibkey: mukherjee2023progressive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.02707"}
tags: ['Ethics And Bias', 'GPT', 'Interpretability And Explainability', 'Model Architecture', 'Training Techniques']
---
Recent research has focused on enhancing the capability of smaller models through imitation learning drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small models capability as they tend to learn to imitate the style but not the reasoning process of LFMs. To address these challenges we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMAs release policy to be published at https://aka.ms/orca&#45;lm), a 1345;billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT45;4 including explanation traces; step45;by45;step thought processes; and other complex instructions guided by teacher assistance from ChatGPT. To promote this progressive learning we tap into large45;scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state45;of45;the45;art instruction45;tuned models such as Vicuna45;13B by more than 10037; in complex zero45;shot reasoning benchmarks like Big45;Bench Hard (BBH) and 4237; on AGIEval. Moreover Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT LSAT GRE and GMAT both in zero45;shot settings without CoT; while trailing behind GPT45;4. Our research indicates that learning from step45;by45;step explanations whether these are generated by humans or more advanced AI models is a promising direction to improve model capabilities and skills.
