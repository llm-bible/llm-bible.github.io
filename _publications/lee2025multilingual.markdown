---
layout: publication
title: 'Multilingual Relative Clause Attachment Ambiguity Resolution In Large Language Models'
authors: So Young Lee, Russell Scheinberg, Amber Shore, Ameeta Agrawal
conference: "Arxiv"
year: 2025
bibkey: lee2025multilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.02971"}
tags: ['Reinforcement Learning']
---
This study examines how large language models (LLMs) resolve relative clause
(RC) attachment ambiguities and compares their performance to human sentence
processing. Focusing on two linguistic factors, namely the length of RCs and
the syntactic position of complex determiner phrases (DPs), we assess whether
LLMs can achieve human-like interpretations amid the complexities of language.
In this study, we evaluated several LLMs, including Claude, Gemini and Llama,
in multiple languages: English, Spanish, French, German, Japanese, and Korean.
While these models performed well in Indo-European languages (English, Spanish,
French, and German), they encountered difficulties in Asian languages (Japanese
and Korean), often defaulting to incorrect English translations. The findings
underscore the variability in LLMs' handling of linguistic ambiguities and
highlight the need for model improvements, particularly for non-European
languages. This research informs future enhancements in LLM design to improve
accuracy and human-like processing in diverse linguistic environments.
