---
layout: publication
title: Knowtuning Knowledge45;aware Fine45;tuning For Large Language Models
authors: Lyu Yougang, Yan Lingyong, Wang Shuaiqiang, Shi Haibo, Yin Dawei, Ren Pengjie, Chen Zhumin, De Rijke Maarten, Ren Zhaochun
conference: "Arxiv"
year: 2024
bibkey: lyu2024knowledge
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.11176"}
tags: ['Applications', 'RAG']
---
Despite their success at many natural language processing (NLP) tasks large language models still struggle to effectively leverage knowledge for knowledge45;intensive tasks manifesting limitations such as generating incomplete non45;factual or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine45;tuning. To address these problems we propose a knowledge45;aware fine45;tuning (KnowTuning) method to improve fine45;grained and coarse45;grained knowledge awareness of LLMs. We devise a fine45;grained knowledge augmentation stage to train LLMs to identify difficult fine45;grained knowledge in answers. We also propose a coarse45;grained knowledge comparison stage to train LLMs to distinguish between reliable and unreliable knowledge in three aspects completeness factuality and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effectiveness of KnowTuning through automatic and human evaluations across various sizes of LLMs. We further verify that KnowTuning generates more facts with less factual error rate under fine45;grained facts evaluation.
