---
layout: publication
title: 'Better To Ask In English: Evaluation Of Large Language Models On English, Low-resource And Cross-lingual Settings'
authors: Krishno Dey, Prerona Tarannum, Md. Arid Hasan, Imran Razzak, Usman Naseem
conference: "Arxiv"
year: 2024
bibkey: dey2024better
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.13153"}
tags: ['Prompting', 'Model Architecture', 'Applications', 'GPT']
---
Large Language Models (LLMs) are trained on massive amounts of data, enabling
their application across diverse domains and tasks. Despite their remarkable
performance, most LLMs are developed and evaluated primarily in English.
Recently, a few multi-lingual LLMs have emerged, but their performance in
low-resource languages, especially the most spoken languages in South Asia, is
less explored. To address this gap, in this study, we evaluate LLMs such as
GPT-4, Llama 2, and Gemini to analyze their effectiveness in English compared
to other low-resource languages from South Asia (e.g., Bangla, Hindi, and
Urdu). Specifically, we utilized zero-shot prompting and five different prompt
settings to extensively investigate the effectiveness of the LLMs in
cross-lingual translated prompts. The findings of the study suggest that GPT-4
outperformed Llama 2 and Gemini in all five prompt settings and across all
languages. Moreover, all three LLMs performed better for English language
prompts than other low-resource language prompts. This study extensively
investigates LLMs in low-resource language contexts to highlight the
improvements required in LLMs and language-specific resources to develop more
generally purposed NLP applications.
