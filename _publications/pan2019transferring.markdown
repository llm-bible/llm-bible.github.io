---
layout: publication
title: 'Macnet: Transferring Knowledge From Machine Comprehension To Sequence-to-sequence
  Models'
authors: Boyuan Pan et al.
conference: Arxiv
year: 2019
citations: 19
bibkey: pan2019transferring
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1908.01816'}]
tags: [Tools, Model Architecture, Attention Mechanism]
---
Machine Comprehension (MC) is one of the core problems in natural language
processing, requiring both understanding of the natural language and knowledge
about the world. Rapid progress has been made since the release of several
benchmark datasets, and recently the state-of-the-art models even surpass human
performance on the well-known SQuAD evaluation. In this paper, we transfer
knowledge learned from machine comprehension to the sequence-to-sequence tasks
to deepen the understanding of the text. We propose MacNet: a novel
encoder-decoder supplementary architecture to the widely used attention-based
sequence-to-sequence models. Experiments on neural machine translation (NMT)
and abstractive text summarization show that our proposed framework can
significantly improve the performance of the baseline models, and our method
for the abstractive text summarization achieves the state-of-the-art results on
the Gigaword dataset.