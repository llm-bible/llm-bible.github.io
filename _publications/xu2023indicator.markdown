---
layout: publication
title: IDOL Indicator45;oriented Logic Pre45;training For Logical Reasoning
authors: Xu Zihang, Yang Ziqing, Cui Yiming, Wang Shijin
conference: "Arxiv"
year: 2023
bibkey: xu2023indicator
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.15273"}
tags: ['Applications', 'GPT', 'Model Architecture', 'RAG', 'Training Techniques']
---
In the field of machine reading comprehension (MRC) existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward they either are designed in a quite complicated way or rely too much on external structures. In this paper we proposed IDOL (InDicator45;Oriented Logic Pre45;training) an easy45;to45;understand but highly effective further pre45;training task which logically strengthens the pre45;trained models with the help of 6 types of logical indicators and a logically rich dataset LGP (LoGic Pre45;training). IDOL achieves state45;of45;the45;art performance on ReClor and LogiQA the two most representative benchmarks in logical reasoning MRC and is proven to be capable of generalizing to different pre45;trained models and other types of MRC benchmarks like RACE and SQuAD 2.0 while keeping competitive general language understanding ability through testing on tasks in GLUE. Besides at the beginning of the era of large language models we take several of them like ChatGPT into comparison and find that IDOL still shows its advantage.
