---
layout: publication
title: 'Yescieval: Robust Llm-as-a-judge For Scientific Question Answering'
authors: Jennifer D'souza, Hamed Babaei Giglou, Quentin MÃ¼nch
conference: "Arxiv"
year: 2025
bibkey: dsouza2025robust
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.14279"}
tags: ['Agentic', 'Tools', 'Ethics and Bias', 'Applications', 'Reinforcement Learning', 'Security']
---
Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry.
