---
layout: publication
title: 'Do Large Language Models Reason Causally Like Us? Even Better?'
authors: Hanna M. Dettki, Brenden M. Lake, Charley M. Wu, Bob Rehder
conference: "Arxiv"
year: 2025
bibkey: dettki2025do
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.10215'}
tags: ['Ethics and Bias', 'GPT', 'Agentic', 'Model Architecture']
---
Causal reasoning is a core component of intelligence. Large language models
(LLMs) have shown impressive capabilities in generating human-like text,
raising questions about whether their responses reflect true understanding or
statistical patterns. We compared causal reasoning in humans and four LLMs
using tasks based on collider graphs, rating the likelihood of a query variable
occurring given evidence from other variables. We find that LLMs reason
causally along a spectrum from human-like to normative inference, with
alignment shifting based on model, context, and task. Overall, GPT-4o and
Claude showed the most normative behavior, including "explaining away", whereas
Gemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected
independence of causes - Claude the least - they exhibited strong associative
reasoning and predictive inference when assessing the likelihood of the effect
given its causes. These findings underscore the need to assess AI biases as
they increasingly assist human decision-making.
