---
layout: publication
title: 'Probing Multimodal Llms As World Models For Driving'
authors: Shiva Sreeram, Tsun-hsuan Wang, Alaa Maalouf, Guy Rosman, Sertac Karaman, Daniela Rus
conference: "Arxiv"
year: 2024
bibkey: sreeram2024probing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2405.05956'}
tags: ['Reinforcement Learning', 'GPT', 'Multimodal Models', 'Model Architecture']
---
We provide a sober look at the application of Multimodal Large Language
Models (MLLMs) in autonomous driving, challenging common assumptions about
their ability to interpret dynamic driving scenarios. Despite advances in
models like GPT-4o, their performance in complex driving environments remains
largely unexplored. Our experimental study assesses various MLLMs as world
models using in-car camera perspectives and reveals that while these models
excel at interpreting individual images, they struggle to synthesize coherent
narratives across frames, leading to considerable inaccuracies in understanding
(i) ego vehicle dynamics, (ii) interactions with other road actors, (iii)
trajectory planning, and (iv) open-set scene reasoning. We introduce the
Eval-LLM-Drive dataset and DriveSim simulator to enhance our evaluation,
highlighting gaps in current MLLM capabilities and the need for improved models
in dynamic real-world environments.
