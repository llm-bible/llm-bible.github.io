---
layout: publication
title: 'From Linguistic Giants To Sensory Maestros: A Survey On Cross-modal Reasoning With Large Language Models'
authors: Shengsheng Qian, Zuyi Zhou, Dizhan Xue, Bing Wang, Changsheng Xu
conference: "Arxiv"
year: 2024
bibkey: qian2024from
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2409.18996'}
  - {name: "Code", url: 'https://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs'}
tags: ['Has Code', 'Multimodal Models', 'Survey Paper']
---
Cross-modal reasoning (CMR), the intricate process of synthesizing and
drawing inferences across divergent sensory modalities, is increasingly
recognized as a crucial capability in the progression toward more sophisticated
and anthropomorphic artificial intelligence systems. Large Language Models
(LLMs) represent a class of AI algorithms specifically engineered to parse,
produce, and engage with human language on an extensive scale. The recent trend
of deploying LLMs to tackle CMR tasks has marked a new mainstream of approaches
for enhancing their effectiveness. This survey offers a nuanced exposition of
current methodologies applied in CMR using LLMs, classifying these into a
detailed three-tiered taxonomy. Moreover, the survey delves into the principal
design strategies and operational techniques of prototypical models within this
domain. Additionally, it articulates the prevailing challenges associated with
the integration of LLMs in CMR and identifies prospective research directions.
To sum up, this survey endeavors to expedite progress within this burgeoning
field by endowing scholars with a holistic and detailed vista, showcasing the
vanguard of current research whilst pinpointing potential avenues for
advancement. An associated GitHub repository that collects the relevant papers
can be found at
https://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs
