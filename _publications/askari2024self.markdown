---
layout: publication
title: Self45;seeding And Multi45;intent Self45;instructing Llms For Generating Intent45;aware Information45;seeking Dialogs
authors: Askari Arian, Petcu Roxana, Meng Chuan, Aliannejadi Mohammad, Abolghasemi Amin, Kanoulas Evangelos, Verberne Suzan
conference: "Arxiv"
year: 2024
bibkey: askari2024self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.11633"}
tags: ['Prompting', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
Identifying user intents in information45;seeking dialogs is crucial for a system to meet users information needs. Intent prediction (IP) is challenging and demands sufficient dialogs with human45;labeled intents for training. However manually annotating intents is resource45;intensive. While large language models (LLMs) have been shown to be effective in generating synthetic data there is no study on using LLMs to generate intent45;aware information45;seeking dialogs. In this paper we focus on leveraging LLMs for zero45;shot generation of large45;scale open45;domain and intent45;aware information45;seeking dialogs. We propose SOLID which has novel self45;seeding and multi45;intent self45;instructing schemes. The former improves the generation quality by using the LLMs own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially and mitigates the need for manual prompt design by asking the LLM to autonomously adapt its prompt instruction when generating complex multi45;intent utterances. Furthermore we propose SOLID45;RL which is further trained to generate a dialog in one step on the data generated by SOLID. We propose a length45;based quality estimation mechanism to assign varying weights to SOLID45;generated dialogs based on their quality during the training process of SOLID45;RL. We use SOLID and SOLID45;RL to generate more than 300k intent45;aware dialogs surpassing the size of existing datasets. Experiments show that IP methods trained on dialogs generated by SOLID and SOLID45;RL achieve better IP quality than ones trained on human45;generated dialogs.
