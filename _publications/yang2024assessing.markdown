---
layout: publication
title: Assessing Adversarial Robustness Of Large Language Models An Empirical Study
authors: Yang Zeyu, Meng Zhao, Zheng Xiaochen, Wattenhofer Roger
conference: "Arxiv"
year: 2024
bibkey: yang2024assessing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.02764"}
tags: ['Applications', 'Pretraining Methods', 'Reinforcement Learning', 'Security']
---
Large Language Models (LLMs) have revolutionized natural language processing but their robustness against adversarial attacks remains a critical concern. We presents a novel white45;box style attack approach that exposes vulnerabilities in leading open45;source LLMs including Llama OPT and T5. We assess the impact of model size structure and fine45;tuning strategies on their resistance to adversarial perturbations. Our comprehensive evaluation across five diverse text classification tasks establishes a new benchmark for LLM robustness. The findings of this study have far45;reaching implications for the reliable deployment of LLMs in real45;world applications and contribute to the advancement of trustworthy AI systems.
