---
layout: publication
title: 'Assessing Adversarial Robustness Of Large Language Models: An Empirical Study'
authors: Zeyu Yang, Zhao Meng, Xiaochen Zheng, Roger Wattenhofer
conference: "Arxiv"
year: 2024
bibkey: yang2024assessing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2405.02764'}
tags: ['Security', 'Training Techniques', 'Applications', 'Fine-Tuning', 'Reinforcement Learning', 'Pretraining Methods']
---
Large Language Models (LLMs) have revolutionized natural language processing,
but their robustness against adversarial attacks remains a critical concern. We
presents a novel white-box style attack approach that exposes vulnerabilities
in leading open-source LLMs, including Llama, OPT, and T5. We assess the impact
of model size, structure, and fine-tuning strategies on their resistance to
adversarial perturbations. Our comprehensive evaluation across five diverse
text classification tasks establishes a new benchmark for LLM robustness. The
findings of this study have far-reaching implications for the reliable
deployment of LLMs in real-world applications and contribute to the advancement
of trustworthy AI systems.
