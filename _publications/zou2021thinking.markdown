---
layout: publication
title: Thinking Clearly Talking Fast Concept45;guided Non45;autoregressive Generation For Open45;domain Dialogue Systems
authors: Zou Yicheng, Liu Zhihua, Hu Xingwu, Zhang Qi
conference: "Arxiv"
year: 2021
bibkey: zou2021thinking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2109.04084"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Transformer']
---
Human dialogue contains evolving concepts and speakers naturally associate multiple concepts to compose a response. However current dialogue models with the seq2seq framework lack the ability to effectively manage concept transitions and can hardly introduce multiple concepts to responses in a sequential decoding manner. To facilitate a controllable and coherent dialogue in this work we devise a concept45;guided non45;autoregressive model (CG45;nAR) for open45;domain dialogue generation. The proposed model comprises a multi45;concept planning module that learns to identify multiple associated concepts from a concept graph and a customized Insertion Transformer that performs concept45;guided non45;autoregressive generation to complete a response. The experimental results on two public datasets show that CG45;nAR can produce diverse and coherent responses outperforming state45;of45;the45;art baselines in both automatic and human evaluations with substantially faster inference speed.
