---
layout: publication
title: Bug In The Code Stack Can Llms Find Bugs In Large Python Code Stacks
authors: Lee Hokyung, Sharma Sumanyu, Hu Bing
conference: "Arxiv"
year: 2024
bibkey: lee2024bug
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.15325"}
tags: ['Pretraining Methods']
---
Recent research in Needle45;in45;a45;Haystack (NIAH) benchmarks has explored the capabilities of Large Language Models (LLMs) in retrieving contextual information from large text documents. However as LLMs become increasingly integrated into software development processes it is crucial to evaluate their performance in code45;based environments. As LLMs are further developed for program synthesis we need to ensure that LLMs can understand syntax and write syntactically correct code. As a step in ensuring LLMs understand syntax LLMs can be evaluated in their ability to find and detect syntax bugs. Our benchmark Bug In The Code Stack (BICS) is designed to assess the ability of LLMs to identify simple syntax bugs within large source code. Our findings reveal three key insights (1) code45;based environments pose significantly more challenge compared to text45;based environments for retrieval tasks (2) there is a substantial performance disparity among different models and (3) there is a notable correlation between longer context lengths and performance degradation though the extent of this degradation varies between models.
