---
layout: publication
title: Point45;bind amp; Point45;llm Aligning Point Cloud With Multi45;modality For 3D Understanding Generation And Instruction Following
authors: Guo Ziyu, Zhang Renrui, Zhu Xiangyang, Tang Yiwen, Ma Xianzheng, Han Jiaming, Chen Kexin, Gao Peng, Li Xianzhi, Li Hongsheng, Heng Pheng-ann
conference: "Arxiv"
year: 2023
bibkey: guo2023point
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.00615"}
  - {name: "Code", url: "https://github.com/ZiyuGuo99/Point&#45;Bind&#95;Point&#45;LLM"}
tags: ['Applications', 'Has Code', 'Multimodal Models', 'Reinforcement Learning']
---
We introduce Point45;Bind a 3D multi45;modality model aligning point clouds with 2D image language audio and video. Guided by ImageBind we construct a joint embedding space between 3D and multi45;modalities enabling many promising applications e.g. any45;to45;3D generation 3D embedding arithmetic and 3D open45;world understanding. On top of this we further present Point45;LLM the first 3D large language model (LLM) following 3D multi45;modal instructions. By parameter45;efficient fine45;tuning techniques Point45;LLM injects the semantics of Point45;Bind into pre45;trained LLMs e.g. LLaMA which requires no 3D instruction data but exhibits superior 3D and multi45;modal question45;answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi45;modality applications. Code is available at https://github.com/ZiyuGuo99/Point&#45;Bind&#95;Point&#45;LLM.
