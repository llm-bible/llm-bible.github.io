---
layout: publication
title: 'Harnessing The Power Of Large Language Models For Empathetic Response Generation: Empirical Investigations And Improvements'
authors: Yushan Qian, Wei-nan Zhang, Ting Liu
conference: "Arxiv"
year: 2023
bibkey: qian2023harnessing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2310.05140'}
tags: ['Attention Mechanism', 'GPT', 'Model Architecture', 'Prompting', 'In-Context Learning']
---
Empathetic dialogue is an indispensable part of building harmonious social
relationships and contributes to the development of a helpful AI. Previous
approaches are mainly based on fine small-scale language models. With the
advent of ChatGPT, the application effect of large language models (LLMs) in
this field has attracted great attention. This work empirically investigates
the performance of LLMs in generating empathetic responses and proposes three
improvement methods of semantically similar in-context learning, two-stage
interactive generation, and combination with the knowledge base. Extensive
experiments show that LLMs can significantly benefit from our proposed methods
and is able to achieve state-of-the-art performance in both automatic and human
evaluations. Additionally, we explore the possibility of GPT-4 simulating human
evaluators.
