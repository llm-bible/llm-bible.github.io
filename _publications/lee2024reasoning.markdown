---
layout: publication
title: 'Reasoning Abilities Of Large Language Models: In-depth Analysis On The Abstraction And Reasoning Corpus'
authors: Lee Seungpil, Sim Woochang, Shin Donghyeon, Hwang Sanha, Seo Wongyu, Park Jiwon, Lee Seokki, Kim Sejin, Kim Sundong
conference: "Arxiv"
year: 2024
bibkey: lee2024reasoning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.11793"}
tags: []
---
The existing methods for evaluating the inference abilities of Large Language
Models (LLMs) have been results-centric, making it difficult to assess the
inference process. We introduce a new approach using the Abstract and Reasoning
Corpus (ARC) dataset to evaluate the inference and contextual understanding
abilities of large language models in a process-centric manner. ARC demands
rigorous logical structures for problem-solving, making it a benchmark that
facilitates the comparison of model inference abilities with humans.
Experimental results confirm that while large language models possess weak
inference abilities, they still lag in terms of logical coherence,
compositionality, and productivity. Our experiments highlight the reasoning
capabilities of LLMs, proposing development paths for achieving human-level
reasoning.
