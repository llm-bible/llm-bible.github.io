---
layout: publication
title: What matters when building vision-language models
authors: Laurençon Hugo, Tronchon Léo, Cord Matthieu, Sanh Victor
conference: "Arxiv"
year: 2024
bibkey: laurençon2024what
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.02246"}
tags: ['Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue we conduct extensive experiments around pre-trained models architecture choice data and training methods. Our consolidation of findings includes the development of Idefics2 an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks and is often on par with models four times its size. We release the model (base instructed and chat) along with the datasets created for its training.
