---
layout: publication
title: Magicoder Empowering Code Generation With Oss45;instruct
authors: Wei Yuxiang, Wang Zhe, Liu Jiawei, Ding Yifeng, Zhang Lingming
conference: "Arxiv"
year: 2023
bibkey: wei2023empowering
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.02120"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Model Architecture']
---
We introduce Magicoder a series of fully open45;source (code weights and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS45;Instruct a novel approach to enlightening LLMs with open45;source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open45;source references for the production of more realistic and controllable data. The orthogonality of OSS45;Instruct and other data generation methods like Evol45;Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state45;of45;the45;art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably MagicoderS45;CL45;7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass35;64;1 ). Overall OSS45;Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open45;source references.
