---
layout: publication
title: 'Codeif: Benchmarking The Instruction-following Capabilities Of Large Language Models For Code Generation'
authors: Kaiwen Yan, Hongcheng Guo, Xuanqing Shi, Jingyi Xu, Yaonan Gu, Zhoujun Li
conference: "Arxiv"
year: 2025
bibkey: yan2025benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.19166"}
tags: ['Interpretability and Explainability', 'Tools', 'Efficiency and Optimization', 'Applications']
---
With the rapid advancement of Large Language Models (LLMs), the demand for
robust instruction-following capabilities in code generation tasks has grown
significantly. Code generation not only facilitates faster prototyping and
automated testing, but also augments developer efficiency through improved
maintainability and reusability of code. In this paper, we introduce CodeIF,
the first benchmark specifically designed to assess the abilities of LLMs to
adhere to task-oriented instructions within diverse code generation scenarios.
CodeIF encompasses a broad range of tasks, including function synthesis, error
debugging, algorithmic refactoring, and code explanation, thereby providing a
comprehensive suite to evaluate model performance across varying complexity
levels and programming domains. We conduct extensive experiments with LLMs,
analyzing their strengths and limitations in meeting the demands of these
tasks. The experimental results offer valuable insights into how well current
models align with human instructions, as well as the extent to which they can
generate consistent, maintainable, and contextually relevant code. Our findings
not only underscore the critical role that instruction-following LLMs can play
in modern software development, but also illuminate pathways for future
research aimed at enhancing their adaptability, reliability, and overall
effectiveness in automated code generation.
