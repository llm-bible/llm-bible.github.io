---
layout: publication
title: Developing Safe And Responsible Large Language Model Can We Balance Bias Reduction And Language Understanding In Large Language Models
authors: Raza Shaina, Bamgbose Oluwanifemi, Ghuge Shardul, Tavakol Fatemeh, Reji Deepak John, Bashir Syed Raza
conference: "Arxiv"
year: 2024
bibkey: raza2024developing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.01399"}
  - {name: "Code", url: "https://github.com/shainarazavi/Safe&#45;Responsible&#45;LLM&#125;&#123;SR&#45;LLM&#125;"}
tags: ['Applications', 'Ethics And Bias', 'Has Code', 'Language Modeling', 'Prompting']
---
Large Language Models (LLMs) have advanced various Natural Language Processing (NLP) tasks such as text generation and translation among others. However these models often generate text that can perpetuate biases. Existing approaches to mitigate these biases usually compromise knowledge retention. This study explores whether LLMs can produce safe unbiased outputs without sacrificing knowledge or comprehension. We introduce the Safe and Responsible Large Language Model (textbf123;SR125;95;123;text123;LLM125;125;) which has been instruction fine45;tuned atop an inherently safe fine45;tuned LLM to reduce biases in generated texts. We developed a specialized dataset with examples of unsafe and corresponding safe variations to train textbf123;SR125;95;123;text123;LLM125;125; to identify and correct biased text. Experiments on our specialized dataset and out45;of45;distribution test sets reveal that textbf123;SR125;95;123;text123;LLM125;125; effectively reduces biases while preserving knowledge integrity. This performance surpasses that of traditional fine45;tuning of smaller language models and base LLMs that merely reply on prompting techniques. Our findings indicate that instruction fine45;tuning is an effective strategy for minimizing bias in LLMs while retaining knowledge. The code and dataset are accessible at href123;https://github.com/shainarazavi/Safe&#45;Responsible&#45;LLM&#125;&#123;SR&#45;LLM&#125;.
