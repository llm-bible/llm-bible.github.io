---
layout: publication
title: 'Pay Attention To What You Need'
authors: Yifei Gao, Shaohong Chen, Lei Wang, Ruiting Dai, Ziyun Zhang, Kerui Ren, Jiaji Wu, Jun Cheng
conference: "Arxiv"
year: 2023
bibkey: gao2023pay
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2307.13365'}
tags: ['Attention Mechanism', 'Transformer', 'Training Techniques', 'Model Architecture', 'Fine-Tuning', 'Pretraining Methods']
---
Although large language models (LLMs) have achieved significant success in
natural language processing, they still struggle with long-context
comprehension. Traditional approaches to mitigating this issue typically rely
on fine-tuning or retraining, which is both resource-intensive and challenging
to deploy in lightweight industrial settings. In this paper, we investigate the
potential to accomplish this without any additional resources. Through an
in-depth study of the attention mechanism in LLMs, we propose a method called
Scaled ReAttention (SRA) to strengthen LLMs' ability to interpret and retrieve
information by strategically manipulating their attention scores during
inference. Through extensive experiments, we demonstrate that integrating SRA
significantly boosts LLMs' performance on a variety of downstream tasks,
highlighting its practical potential for enhancing language understanding
without incurring the overhead of traditional training.
