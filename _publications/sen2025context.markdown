---
layout: publication
title: 'Context-enhanced Contrastive Search For Improved LLM Text Generation'
authors: Jaydip Sen, Rohit Pandey, Hetvi Waghela
conference: "Arxiv"
year: 2025
bibkey: sen2025context
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.21020"}
tags: ['Language Modeling', 'Applications', 'Reinforcement Learning']
---
Recently, Large Language Models (LLMs) have demonstrated remarkable
advancements in Natural Language Processing (NLP). However, generating
high-quality text that balances coherence, diversity, and relevance remains
challenging. Traditional decoding methods, such as bean search and top-k
sampling, often struggle with either repetitive or incoherent outputs,
particularly in tasks that require long-form text generation. To address these
limitations, the paper proposes a novel enhancement of the well-known
Contrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with
contextual calibration. The proposed scheme introduces several novelties
including dynamic contextual importance weighting, multi-level Contrastive
Search, and adaptive temperature control, to optimize the balance between
fluency, creativity, and precision. The performance of CECS is evaluated using
several standard metrics such as BLEU, ROUGE, and semantic similarity.
Experimental results demonstrate significant improvements in both coherence and
relevance of the generated texts by CECS outperforming the existing Contrastive
Search techniques. The proposed algorithm has several potential applications in
the real world including legal document drafting, customer service chatbots,
and content marketing.
