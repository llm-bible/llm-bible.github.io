---
layout: publication
title: 'Benchmarking Linguistic Diversity Of Large Language Models'
authors: Yanzhu Guo, Guokan Shang, Chlo√© Clavel
conference: "Arxiv"
year: 2024
bibkey: guo2024benchmarking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.10271'}
tags: ['Tools']
---
The development and evaluation of Large Language Models (LLMs) has primarily
focused on their task-solving capabilities, with recent models even surpassing
human performance in some areas. However, this focus often neglects whether
machine-generated language matches the human level of diversity, in terms of
vocabulary choice, syntactic construction, and expression of meaning, raising
questions about whether the fundamentals of language generation have been fully
addressed. This paper emphasizes the importance of examining the preservation
of human linguistic richness by language models, given the concerning surge in
online content produced or aided by LLMs. We propose a comprehensive framework
for evaluating LLMs from various linguistic diversity perspectives including
lexical, syntactic, and semantic dimensions. Using this framework, we benchmark
several state-of-the-art LLMs across all diversity dimensions, and conduct an
in-depth case study for syntactic diversity. Finally, we analyze how different
development and deployment choices impact the linguistic diversity of LLM
outputs.
