---
layout: publication
title: From Instructions To Intrinsic Human Values 45;45; A Survey Of Alignment Goals For Big Models
authors: Yao Jing, Yi Xiaoyuan, Wang Xiting, Wang Jindong, Xie Xing
conference: "Arxiv"
year: 2023
bibkey: yao2023from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.12014"}
tags: ['Pretraining Methods', 'Reinforcement Learning', 'Survey Paper']
---
Big models exemplified by Large Language Models (LLMs) are models typically pre45;trained on massive data and comprised of enormous parameters which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless what to align with has not been fully discussed and inappropriate alignment goals might even backfire. In this paper we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly we investigate related works from two perspectives the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.
