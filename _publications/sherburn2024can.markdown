---
layout: publication
title: Can Language Models Explain Their Own Classification Behavior
authors: Sherburn Dane, Chughtai Bilal, Evans Owain
conference: "Arxiv"
year: 2024
bibkey: sherburn2024can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.07436"}
tags: ['GPT', 'Interpretability And Explainability', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Large language models (LLMs) perform well at a myriad of tasks but explaining the processes behind this performance is a challenge. This paper investigates whether LLMs can give faithful high45;level explanations of their own internal processes. To explore this we introduce a dataset ArticulateRules of few45;shot text45;based classification tasks generated by simple rules. Each rule is associated with a simple natural45;language explanation. We test whether models that have learned to classify inputs competently (both in45; and out45;of45;distribution) are able to articulate freeform natural language explanations that match their classification behavior. Our dataset can be used for both in45;context and finetuning evaluations. We evaluate a range of LLMs demonstrating that articulation accuracy varies considerably between models with a particularly sharp increase from GPT45;3 to GPT45;4. We then investigate whether we can improve GPT45;3s articulation accuracy through a range of methods. GPT45;3 completely fails to articulate 7/10 rules in our test even after additional finetuning on correct explanations. We release our dataset ArticulateRules which can be used to test self45;explanation for LLMs trained either in45;context or by finetuning.
