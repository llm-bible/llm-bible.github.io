---
layout: publication
title: 'Mechanistic Interpretability Of Emotion Inference In Large Language Models'
authors: Ala N. Tak, Amin Banayeeanzade, Anahita Bolourani, Mina Kian, Robin Jia, Jonathan Gratch
conference: "Arxiv"
year: 2025
bibkey: tak2025mechanistic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.05489"}
tags: ['Responsible AI', 'Security', 'Tools', 'Reinforcement Learning', 'Language Modeling', 'GPT', 'Pretraining Methods', 'Interpretability and Explainability', 'Applications']
---
Large language models (LLMs) show promising capabilities in predicting human
emotions from text. However, the mechanisms through which these models process
emotional stimuli remain largely unexplored. Our study addresses this gap by
investigating how autoregressive LLMs infer emotions, showing that emotion
representations are functionally localized to specific regions in the model.
Our evaluation includes diverse model families and sizes and is supported by
robustness checks. We then show that the identified representations are
psychologically plausible by drawing on cognitive appraisal theory, a
well-established psychological framework positing that emotions emerge from
evaluations (appraisals) of environmental stimuli. By causally intervening on
construed appraisal concepts, we steer the generation and show that the outputs
align with theoretical and intuitive expectations. This work highlights a novel
way to causally intervene and precisely shape emotional text generation,
potentially benefiting safety and alignment in sensitive affective domains.
