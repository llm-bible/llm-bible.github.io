---
layout: publication
title: 'Easyinstruct: An Easy-to-use Instruction Processing Framework For Large Language Models'
authors: Ou Yixin, Zhang Ningyu, Gui Honghao, Xu Ziwen, Qiao Shuofei, Xue Yida, Fang Runnan, Liu Kangwei, Li Lei, Bi Zhen, Zheng Guozhou, Chen Huajun
conference: "Arxiv"
year: 2024
bibkey: ou2024easy
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.03049"}
  - {name: "Code", url: "https://github.com/zjunlp/EasyInstruct,"}
tags: ['Attention Mechanism', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Tools']
---
'In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along with an online demo app and a demo video for quick-start, calling for broader research centered on instruction data and synthetic data.'
