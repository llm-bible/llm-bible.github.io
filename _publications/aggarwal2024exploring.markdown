---
layout: publication
title: 'Exploring Pretraining Via Active Forgetting For Improving Cross Lingual Transfer For Decoder Language Models'
authors: Divyanshu Aggarwal, Ashutosh Sathe, Sunayana Sitaram
conference: "Arxiv"
year: 2024
bibkey: aggarwal2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.16168"}
tags: ['BERT', 'Training Techniques', 'Model Architecture', 'Pretraining Methods']
---
Large Language Models (LLMs) demonstrate exceptional capabilities in a multitude of NLP tasks. However, the efficacy of such models to languages other than English is often limited. Prior works have shown that encoder-only models such as BERT or XLM-RoBERTa show impressive cross lingual transfer of their capabilities from English to other languages. In this work, we propose a pretraining strategy that uses active forgetting to achieve similar cross lingual transfer in decoder-only LLMs. We show that LLMs pretrained with active forgetting are highly effective when adapting to new and unseen languages. Through extensive experimentation, we find that LLMs pretrained with active forgetting are able to learn better multilingual representations which translates to better performance in many downstream tasks.
