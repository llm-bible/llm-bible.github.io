---
layout: publication
title: Curriculum Learning For Language Modeling
authors: Campos Daniel
conference: "Arxiv"
year: 2021
bibkey: campos2021curriculum
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2108.02170"}
tags: ['Applications', 'BERT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques']
---
Language Models like ELMo and BERT have provided robust representations of natural language which serve as the language understanding component for a diverse range of downstream tasks.Curriculum learning is a method that employs a structured training regime instead which has been leveraged in computer vision and machine translation to improve model training speed and model performance. While language models have proven transformational for the natural language processing community these models have proven expensive energy45;intensive and challenging to train. In this work we explore the effect of curriculum learning on language model pretraining using various linguistically motivated curricula and evaluate transfer performance on the GLUE Benchmark. Despite a broad variety of training methodologies and experiments we do not find compelling evidence that curriculum learning methods improve language model training.
