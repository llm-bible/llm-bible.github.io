---
layout: publication
title: Textbind Multi45;turn Interleaved Multimodal Instruction45;following In The Wild
authors: Li Huayang, Li Siheng, Cai Deng, Wang Longyue, Liu Lemao, Watanabe Taro, Yang Yujiu, Shi Shuming
conference: "Arxiv"
year: 2023
bibkey: li2023multi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.08637"}
tags: ['Model Architecture', 'Multimodal Models', 'Reinforcement Learning', 'Tools']
---
Large language models with instruction45;following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real45;world tasks through their natural language interfaces. However their performance heavily relies on high45;quality exemplar data which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind an almost annotation45;free framework for empowering larger language models with the multi45;turn interleaved multimodal instruction45;following capabilities. Our approach requires only image45;caption pairs and generates multi45;turn multimodal instruction45;response conversations from a language model. To accommodate interleaved image45;text inputs and outputs we devise MIM a language model45;centric architecture that seamlessly integrates image encoder and decoder models. We release our dataset model and demo to foster future research in the area of multimodal instruction following.
