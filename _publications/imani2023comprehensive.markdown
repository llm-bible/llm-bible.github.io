---
layout: publication
title: Diversigate A Comprehensive Framework For Reliable Large Language Models
authors: Imani Shima, Beyram Ali, Shrivastava Harsh
conference: "Arxiv"
year: 2023
bibkey: imani2023comprehensive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.13230"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Tools']
---
In this paper we introduce DiversiGATE a unified framework that consolidates diverse methodologies for LLM verification. The proposed framework comprises two main components Diversification and Aggregation which provide a holistic perspective on existing verification approaches such as Self-Consistency Math Prompter and WebGPT. Furthermore we propose a novel SelfLearner model that conforms to the DiversiGATE framework which can learn from its own outputs and refine its performance over time leading to improved accuracy. To evaluate the effectiveness of SelfLearner we conducted a rigorous series of experiments including tests on synthetic data as well as on popular arithmetic reasoning benchmarks such as GSM8K. Our results demonstrate that our approach outperforms traditional LLMs achieving a considerable 54.837; - 61.837; improvement on the GSM8K benchmark.
