---
layout: publication
title: 'Beyond Words: How Large Language Models Perform In Quantitative Management Problem-solving'
authors: Jonathan Kuzmanko
conference: "Arxiv"
year: 2025
bibkey: kuzmanko2025beyond
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.16556'}
tags: ['Reinforcement Learning']
---
This study examines how Large Language Models (LLMs) perform when tackling
quantitative management decision problems in a zero-shot setting. Drawing on
900 responses generated by five leading models across 20 diverse managerial
scenarios, our analysis explores whether these base models can deliver accurate
numerical decisions under varying presentation formats, scenario complexities,
and repeated attempts. Contrary to prior findings, we observed no significant
effects of text presentation format (direct, narrative, or tabular) or text
length on accuracy. However, scenario complexity -- particularly in terms of
constraints and irrelevant parameters -- strongly influenced performance, often
degrading accuracy. Surprisingly, the models handled tasks requiring multiple
solution steps more effectively than expected. Notably, only 28.8% of
responses were exactly correct, highlighting limitations in precision. We
further found no significant ``learning effect'' across iterations: performance
remained stable across repeated queries. Nonetheless, significant variations
emerged among the five tested LLMs, with some showing superior binary accuracy.
Overall, these findings underscore both the promise and the pitfalls of
harnessing LLMs for complex quantitative decision-making, informing managers
and researchers about optimal deployment strategies.
