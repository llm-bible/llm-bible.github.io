---
layout: publication
title: Sabia-2 A New Generation Of Portuguese Large Language Models
authors: Almeida Thales Sales, Abonizio Hugo, Nogueira Rodrigo, Pires Ramon
conference: "Arxiv"
year: 2024
bibkey: almeida2024sabia
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.09887"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
We introduce Sabia-2 a family of large language models trained on Portuguese texts. The models are evaluated on a diverse range of exams including entry-level tests for Brazilian universities professional certification exams and graduate-level exams for various disciplines such as accounting economics engineering law and medicine. Our results reveal that our best model so far Sabia-2 Medium matches or surpasses GPT-4s performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64 exams. Notably specialization has a significant impact on a models performance without the need to increase its size allowing us to offer Sabia-2 Medium at a price per token that is 10 times cheaper than GPT-4. Finally we identified that math and coding are key abilities that need improvement.
