---
layout: publication
title: 'ML For Hardware Design Interpretability: Challenges And Opportunities'
authors: Raymond Baartmans, Andrew Ensinger, Victor Agostinelli, Lizhong Chen
conference: "Arxiv"
year: 2025
bibkey: baartmans2025ml
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.08852'}
tags: ['Interpretability and Explainability', 'RAG', 'Efficiency and Optimization', 'Survey Paper', 'Reinforcement Learning']
---
The increasing size and complexity of machine learning (ML) models have
driven the growing need for custom hardware accelerators capable of efficiently
supporting ML workloads. However, the design of such accelerators remains a
time-consuming process, heavily relying on engineers to manually ensure design
interpretability through clear documentation and effective communication.
Recent advances in large language models (LLMs) offer a promising opportunity
to automate these design interpretability tasks, particularly the generation of
natural language descriptions for register-transfer level (RTL) code, what we
refer to as "RTL-to-NL tasks." In this paper, we examine how design
interpretability, particularly in RTL-to-NL tasks, influences the efficiency of
the hardware design process. We review existing work adapting LLMs for these
tasks, highlight key challenges that remain unaddressed, including those
related to data, computation, and model development, and identify opportunities
to address them. By doing so, we aim to guide future research in leveraging ML
to automate RTL-to-NL tasks and improve hardware design interpretability,
thereby accelerating the hardware design process and meeting the increasing
demand for custom hardware accelerators in machine learning and beyond.
