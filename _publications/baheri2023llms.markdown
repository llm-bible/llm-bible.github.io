---
layout: publication
title: "Llms-augmented Contextual Bandit"
authors: Baheri Ali, Alm Cecilia O.
conference: "Arxiv"
year: 2023
bibkey: baheri2023llms
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.02268"}
tags: ['Agentic', 'RAG', 'Reinforcement Learning', 'Tools']
---
Contextual bandits have emerged as a cornerstone in reinforcement learning enabling systems to make decisions with partial feedback. However as contexts grow in complexity traditional bandit algorithms can face challenges in adequately capturing and utilizing such contexts. In this paper we propose a novel integration of large language models (LLMs) with the contextual bandit framework. By leveraging LLMs as an encoder we enrich the representation of the context providing the bandit with a denser and more informative view. Preliminary results on synthetic datasets demonstrate the potential of this approach showing notable improvements in cumulative rewards and reductions in regret compared to traditional bandit algorithms. This integration not only showcases the capabilities of LLMs in reinforcement learning but also opens the door to a new era of contextually-aware decision systems.
