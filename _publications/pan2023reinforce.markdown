---
layout: publication
title: Lets Reinforce Step By Step
authors: Pan Sarah, Lialin Vladislav, Muckatira Sherin, Rumshisky Anna
conference: "Arxiv"
year: 2023
bibkey: pan2023reinforce
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.05821"}
tags: ['Agentic', 'Fine Tuning', 'Reinforcement Learning']
---
While recent advances have boosted LM proficiency in linguistic benchmarks LMs consistently struggle to reason correctly on complex tasks like mathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as a method with which to shape model reasoning processes. In particular we explore two reward schemes outcome-supervised reward models (ORMs) and process-supervised reward models (PRMs) to optimize for logical reasoning. Our results show that the fine-grained reward provided by PRM-based methods enhances accuracy on simple mathematical reasoning (GSM8K) while unexpectedly reducing performance in complex tasks (MATH). Furthermore we show the critical role reward aggregation functions play in model performance. Providing promising avenues for future research our study underscores the need for further exploration into fine-grained reward modeling for more reliable language models.
