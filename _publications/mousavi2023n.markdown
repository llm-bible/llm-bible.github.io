---
layout: publication
title: 'N-critics: Self-refinement Of Large Language Models With Ensemble Of Critics'
authors: Mousavi Sajad, Guti√©rrez Ricardo Luna, Rengarajan Desik, Gundecha Vineet, Babu Ashwin Ramesh, Naug Avisek, Guillen Antonio, Sarkar Soumyendu
conference: "NeurIPS"
year: 2023
bibkey: mousavi2023n
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.18679"}
tags: ['Bias Mitigation', 'Ethics And Bias', 'Fairness', 'Security']
---
We propose a self-correction mechanism for Large Language Models (LLMs) to
mitigate issues such as toxicity and fact hallucination. This method involves
refining model outputs through an ensemble of critics and the model's own
feedback. Drawing inspiration from human behavior, we explore whether LLMs can
emulate the self-correction process observed in humans who often engage in
self-reflection and seek input from others to refine their understanding of
complex topics. Our approach is model-agnostic and can be applied across
various domains to enhance trustworthiness by addressing fairness, bias, and
robustness concerns. We consistently observe performance improvements in LLMs
for reducing toxicity and correcting factual errors.
