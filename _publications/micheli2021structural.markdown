---
layout: publication
title: Structural Analysis Of An All45;purpose Question Answering Model
authors: Micheli Vincent, Heinrich Quentin, Fleuret Fran√ßois, Belblidia Wacim
conference: "Arxiv"
year: 2021
bibkey: micheli2021structural
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2104.06045"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Attention is a key component of the now ubiquitous pre45;trained language models. By learning to focus on relevant pieces of information these Transformer45;based architectures have proven capable of tackling several tasks at once and sometimes even surpass their single45;task counterparts. To better understand this phenomenon we conduct a structural analysis of a new all45;purpose question answering model that we introduce. Surprisingly this model retains single45;task performance even in the absence of a strong transfer effect between tasks. Through attention head importance scoring we observe that attention heads specialize in a particular task and that some heads are more conducive to learning than others in both the multi45;task and single45;task settings.
