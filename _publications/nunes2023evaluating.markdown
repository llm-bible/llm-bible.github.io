---
layout: publication
title: Evaluating GPT-3.5 And GPT-4 Models On Brazilian University Admission Exams
authors: Desnes Nunes, Ricardo Primi, Ramon Pires, Roberto Lotufo, Rodrigo Nogueira
conference: Arxiv
year: 2023
citations: 20
bibkey: nunes2023evaluating
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2303.17003'}, {name: Code,
    url: 'https://github.com/piresramon/gpt-4-enem'}]
tags: [Prompting, GPT, Interpretability and Explainability]
---
The present study aims to explore the capabilities of Language Models (LMs)
in tackling high-stakes multiple-choice tests, represented here by the Exame
Nacional do Ensino M\'edio (ENEM), a multidisciplinary entrance examination
widely adopted by Brazilian universities. This exam poses challenging tasks for
LMs, since its questions may span into multiple fields of knowledge, requiring
understanding of information from diverse domains. For instance, a question may
require comprehension of both statistics and biology to be solved. This work
analyzed responses generated by GPT-3.5 and GPT-4 models for questions
presented in the 2009-2017 exams, as well as for questions of the 2022 exam,
which were made public after the training of the models was completed.
Furthermore, different prompt strategies were tested, including the use of
Chain-of-Thought (CoT) prompts to generate explanations for answers. On the
2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy
of 87%, largely surpassing GPT-3.5 by 11 points. The code and data used on
experiments are available at https://github.com/piresramon/gpt-4-enem.