---
layout: publication
title: DataSculpt Crafting Data Landscapes for LLM Post-Training through Multi-objective Partitioning
authors: Lu Keer, Liang Zheng, Nie Xiaonan, Pan Da, Zhang Shusen, Zhao Keshi, Chen Weipeng, Zhou Zenan, Dong Guosheng, Zhang Wentao, Cui Bin
conference: "Arxiv"
year: 2024
bibkey: lu2024datasculpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.00997"}
tags: ['Applications', 'Ethics And Bias', 'Model Architecture', 'Tools', 'Training Techniques']
---
The effectiveness of long-context modeling is important for Large Language Models (LLMs) in various applications. Despite their potential LLMs efficacy in processing long context does not consistently meet expectations posing significant challenges for efficient management of prolonged sequences in training. This difficulty is compounded by the scarcity of comprehensive and diverse training datasets suitable for long sequences which stems from inherent length biases across different data sources and the logistical complexities associated with massive data management for training in extended contexts. In this work we introduce DataSculpt a data construction framework designed to strategically augment the data architecture for extended-context training. Our thorough evaluations demonstrate DataSculpts remarkable capacity to boost long-context training performance achieving improvements including an 18.09 increase in retrieval augmentation 21.23 in summarization 21.27 in reading comprehension and a 3.81 rise in code completion all while preserving the models overall proficiency with a 4.88 improvement.
