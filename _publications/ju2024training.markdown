---
layout: publication
title: 'Training Data For Large Language Model'
authors: Yiming Ju, Huanhuan Ma
conference: "Arxiv"
year: 2024
bibkey: ju2024training
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.07715"}
tags: ['Fine-Tuning', 'Efficiency and Optimization', 'GPT', 'Model Architecture', 'Training Techniques', 'Attention Mechanism', 'Pretraining Methods']
---
In 2022, with the release of ChatGPT, large-scale language models gained
widespread attention. ChatGPT not only surpassed previous models in terms of
parameters and the scale of its pretraining corpus but also achieved
revolutionary performance improvements through fine-tuning on a vast amount of
high-quality, human-annotated data. This progress has led enterprises and
research institutions to recognize that building smarter and more powerful
models relies on rich and high-quality datasets. Consequently, the construction
and optimization of datasets have become a critical focus in the field of
artificial intelligence. This paper summarizes the current state of pretraining
and fine-tuning data for training large-scale language models, covering aspects
such as data scale, collection methods, data types and characteristics,
processing workflows, and provides an overview of available open-source
datasets.
