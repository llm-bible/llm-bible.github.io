---
layout: publication
title: A Survey On Prompting Techniques In Llms
authors: Bhandari Prabin
conference: "Arxiv"
year: 2023
bibkey: bhandari2023survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.03740"}
tags: ['GPT', 'Pretraining Methods', 'Prompting', 'Survey Paper', 'Training Techniques']
---
Autoregressive Large Language Models have transformed the landscape of Natural Language Processing. Pre45;train and prompt paradigm has replaced the conventional approach of pre45;training and fine45;tuning for many downstream NLP tasks. This shift has been possible largely due to LLMs and innovative prompting techniques. LLMs have shown great promise for a variety of downstream tasks owing to their vast parameters and huge datasets that they are pre45;trained on. However in order to fully realize their potential their outputs must be guided towards the desired outcomes. Prompting in which a specific input or instruction is provided to guide the LLMs toward the intended output has become a tool for achieving this goal. In this paper we discuss the various prompting techniques that have been applied to fully harness the power of LLMs. We present a taxonomy of existing literature on prompting techniques and provide a concise survey based on this taxonomy. Further we identify some open problems in the realm of prompting in autoregressive LLMs which could serve as a direction for future research.
