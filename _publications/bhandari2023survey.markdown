---
layout: publication
title: A Survey on Prompting Techniques in LLMs
authors: Bhandari Prabin
conference: "Arxiv"
year: 2023
bibkey: bhandari2023survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.03740"}
tags: ['ARXIV', 'Fine Tuning', 'LLM', 'NLP', 'Pretraining Methods', 'Prompting', 'Survey Paper', 'Tools', 'Training Techniques']
---
Autoregressive Large Language Models have transformed the landscape of Natural Language Processing. Pre-train and prompt paradigm has replaced the conventional approach of pre-training and fine-tuning for many downstream NLP tasks. This shift has been possible largely due to LLMs and innovative prompting techniques. LLMs have shown great promise for a variety of downstream tasks owing to their vast parameters and huge datasets that they are pre-trained on. However in order to fully realize their potential their outputs must be guided towards the desired outcomes. Prompting in which a specific input or instruction is provided to guide the LLMs toward the intended output has become a tool for achieving this goal. In this paper we discuss the various prompting techniques that have been applied to fully harness the power of LLMs. We present a taxonomy of existing literature on prompting techniques and provide a concise survey based on this taxonomy. Further we identify some open problems in the realm of prompting in autoregressive LLMs which could serve as a direction for future research.
