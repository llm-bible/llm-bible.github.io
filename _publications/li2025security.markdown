---
layout: publication
title: 'Security Concerns For Large Language Models: A Survey'
authors: Miles Q. Li, Benjamin C. M. Fung
conference: "Arxiv"
year: 2025
bibkey: li2025security
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.18889'}
tags: ['Agentic', 'Security', 'Training Techniques', 'Applications', 'Model Architecture', 'GPT', 'Merging', 'Prompting', 'Survey Paper', 'Responsible AI']
---
Large Language Models (LLMs) such as GPT-4 and its recent iterations, Google's Gemini, Anthropic's Claude 3 models, and xAI's Grok have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. In this survey, we provide a comprehensive overview of the emerging security concerns around LLMs, categorizing threats into prompt injection and jailbreaking, adversarial attacks such as input perturbations and data poisoning, misuse by malicious actors for purposes such as generating disinformation, phishing emails, and malware, and worrisome risks inherent in autonomous LLM agents. A significant focus has been recently placed on the latter, exploring goal misalignment, emergent deception, self-preservation instincts, and the potential for LLMs to develop and pursue covert, misaligned objectives, a behavior known as scheming, which may even persist through safety training. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze proposed defenses and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.
