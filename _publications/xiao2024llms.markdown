---
layout: publication
title: 'Llms May Not Be Human-level Players, But They Can Be Testers: Measuring Game Difficulty With LLM Agents'
authors: Chang Xiao, Brenda Z. Yang
conference: "Arxiv"
year: 2024
bibkey: xiao2024llms
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.02829"}
tags: ['Agentic', 'Tools', 'RAG', 'Merging', 'Agent', 'Prompting']
---
Recent advances in Large Language Models (LLMs) have demonstrated their
potential as autonomous agents across various tasks. One emerging application
is the use of LLMs in playing games. In this work, we explore a practical
problem for the gaming industry: Can LLMs be used to measure game difficulty?
We propose a general game-testing framework using LLM agents and test it on two
widely played strategy games: Wordle and Slay the Spire. Our results reveal an
interesting finding: although LLMs may not perform as well as the average human
player, their performance, when guided by simple, generic prompting techniques,
shows a statistically significant and strong correlation with difficulty
indicated by human players. This suggests that LLMs could serve as effective
agents for measuring game difficulty during the development process. Based on
our experiments, we also outline general principles and guidelines for
incorporating LLMs into the game testing process.
