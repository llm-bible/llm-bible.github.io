---
layout: publication
title: Large Language Models Can Self45;improve At Web Agent Tasks
authors: Patel Ajay, Hofmarcher Markus, Leoveanu-condrei Claudiu, Dinu Marius-constantin, Callison-burch Chris, Hochreiter Sepp
conference: "Arxiv"
year: 2024
bibkey: patel2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.20309"}
tags: ['Agentic', 'Pretraining Methods', 'Prompting', 'Security', 'Tools', 'Training Techniques']
---
Training models to act as agents that can effectively navigate and perform actions in a complex environment such as a web browser has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero45;shot or few45;shot fashion purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self45;improvement i.e. fine45;tuning on data generated by the model itself. In this work we explore the extent to which LLMs can self45;improve their performance as agents in long45;horizon tasks in a complex environment using the WebArena benchmark. In WebArena an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine45;tuning on three distinct synthetic training data mixtures and achieve a 3137; improvement in task completion rate over the base model on the WebArena benchmark through a self45;improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance robustness capabilities and quality of trajectories of our fine45;tuned agent models to a greater degree than simple aggregate45;level benchmark scores currently used to measure self45;improvement.
