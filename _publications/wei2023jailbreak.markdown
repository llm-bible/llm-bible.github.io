---
layout: publication
title: 'Jailbreak And Guard Aligned Language Models With Only Few In-context Demonstrations'
authors: Wei Zeming, Wang Yifei, Li Ang, Mo Yichuan, Wang Yisen
conference: "Arxiv"
year: 2023
bibkey: wei2023jailbreak
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.06387"}
tags: ['In Context Learning', 'Prompting', 'Responsible AI', 'Security']
---
Large Language Models (LLMs) have shown remarkable success in various tasks,
yet their safety and the risk of generating harmful content remain pressing
concerns. In this paper, we delve into the potential of In-Context Learning
(ICL) to modulate the alignment of LLMs. Specifically, we propose the
In-Context Attack (ICA) which employs harmful demonstrations to subvert LLMs,
and the In-Context Defense (ICD) which bolsters model resilience through
examples that demonstrate refusal to produce harmful responses. We offer
theoretical insights to elucidate how a limited set of in-context
demonstrations can pivotally influence the safety alignment of LLMs. Through
extensive experiments, we demonstrate the efficacy of ICA and ICD in
respectively elevating and mitigating the success rates of jailbreaking
prompts. Our findings illuminate the profound influence of ICL on LLM behavior,
opening new avenues for improving the safety of LLMs.
