---
layout: publication
title: Seallms -- Large Language Models For Southeast Asia
authors: Nguyen Xuan-phi, Zhang Wenxuan, Li Xin, Aljunied Mahani, Hu Zhiqiang, Shen Chenhui, Chia Yew Ken, Li Xingxuan, Wang Jianyu, Tan Qingyu, Cheng Liying, Chen Guanzheng, Deng Yue, Yang Sen, Liu Chaoqun, Zhang Hang, Bing Lidong
conference: "Arxiv"
year: 2023
bibkey: nguyen2023seallms
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.00738"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Training Techniques']
---
Despite the remarkable achievements of large language models (LLMs) in various tasks there remains a linguistic bias that favors high-resource languages such as English often at the expense of low-resource and regional languages. To address this imbalance we introduce SeaLLMs an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon the Llama-2 model and further advanced through continued pre-training with an extended vocabulary specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms customs stylistic preferences and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM-13b models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover they outperform ChatGPT-3.5 in non-Latin languages such as Thai Khmer Lao and Burmese by large margins while remaining lightweight and cost-effective to operate.
