---
layout: publication
title: Improving Retrieval Augmented Language Model With Self45;reasoning
authors: Xia Yuan, Zhou Jingbo, Shi Zhenhui, Chen Jun, Huang Haifeng
conference: "Arxiv"
year: 2024
bibkey: xia2024improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.19813"}
tags: ['GPT', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
The Retrieval45;Augmented Language Model (RALM) has shown remarkable performance on knowledge45;intensive tasks by incorporating external knowledge during inference which mitigates the factual hallucinations inherited in large language models (LLMs). Despite these advancements challenges persist in the implementation of RALMs particularly concerning their reliability and traceability. To be specific the irrelevant document retrieval may result in unhelpful response generation or even deteriorate the performance of LLMs while the lack of proper citations in generated outputs complicates efforts to verify the trustworthiness of the models. To this end we propose a novel self45;reasoning framework aimed at improving the reliability and traceability of RALMs whose core idea is to leverage reasoning trajectories generated by the LLM itself. The framework involves constructing self45;reason trajectories with three processes a relevance45;aware process an evidence45;aware selective process and a trajectory analysis process. We have evaluated our framework across four public datasets (two short45;form QA datasets one long45;form QA dataset and one fact verification dataset) to demonstrate the superiority of our method which can outperform existing state45;of45;art models and can achieve comparable performance with GPT45;4 while only using 2000 training samples.
