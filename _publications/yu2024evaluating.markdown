---
layout: publication
title: Cosafe Evaluating Large Language Model Safety In Multi45;turn Dialogue Coreference
authors: Yu Erxin, Li Jing, Liao Ming, Wang Siqi, Gao Zuchen, Mi Fei, Hong Lanqing
conference: "Arxiv"
year: 2024
bibkey: yu2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.17626"}
tags: ['Pretraining Methods', 'Prompting', 'Responsible AI', 'Security']
---
As large language models (LLMs) constantly evolve ensuring their safety remains a critical research problem. Previous red45;teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our knowledge we are the first to study LLM safety in multi45;turn dialogue coreference. We created a dataset of 1400 questions across 14 categories each featuring multi45;turn coreference safety attacks. We then conducted detailed evaluations on five widely used open45;source LLMs. The results indicated that under multi45;turn coreference safety attacks the highest attack success rate was 5637; with the LLaMA245;Chat45;7b model while the lowest was 13.937; with the Mistral45;7B45;Instruct model. These findings highlight the safety vulnerabilities in LLMs during dialogue coreference interactions.
