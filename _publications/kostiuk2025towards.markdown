---
layout: publication
title: 'Towards Multilingual LLM Evaluation For Baltic And Nordic Languages: A Study On Lithuanian History'
authors: Yevhen Kostiuk, Oxana Vitman, Łukasz Gagała, Artur Kiulian
conference: "Arxiv"
year: 2025
bibkey: kostiuk2025towards
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.09154'}
tags: ['GPT', 'Model Architecture']
---
In this work, we evaluated Lithuanian and general history knowledge of
multilingual Large Language Models (LLMs) on a multiple-choice
question-answering task. The models were tested on a dataset of Lithuanian
national and general history questions translated into Baltic, Nordic, and
other languages (English, Ukrainian, Arabic) to assess the knowledge sharing
from culturally and historically connected groups. We evaluated GPT-4o,
LLaMa3.1 8b and 70b, QWEN2.5 7b and 72b, Mistral Nemo 12b, LLaMa3 8b, Mistral
7b, LLaMa3.2 3b, and Nordic fine-tuned models (GPT-SW3 and LLaMa3 8b).
  Our results show that GPT-4o consistently outperformed all other models
across language groups, with slightly better results for Baltic and Nordic
languages. Larger open-source models like QWEN2.5 72b and LLaMa3.1 70b
performed well but showed weaker alignment with Baltic languages. Smaller
models (Mistral Nemo 12b, LLaMa3.2 3b, QWEN 7B, LLaMa3.1 8B, and LLaMa3 8b)
demonstrated gaps with LT-related alignment with Baltic languages while
performing better on Nordic and other languages. The Nordic fine-tuned models
did not surpass multilingual models, indicating that shared cultural or
historical context alone does not guarantee better performance.
