---
layout: publication
title: 'R-bench: Graduate-level Multi-disciplinary Benchmarks For LLM & MLLM Complex Reasoning Evaluation'
authors: Meng-hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, Bolin Ni, Guo-wei Yang, Yongming Rao, Houwen Peng, Han Hu, Gordon Wetzstein, Shi-min Hu
conference: "Arxiv"
year: 2025
bibkey: guo2025r
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.02018'}
tags: ['Reinforcement Learning', 'GPT', 'Multimodal Models', 'Model Architecture']
---
Reasoning stands as a cornerstone of intelligence, enabling the synthesis of
existing knowledge to solve complex problems. Despite remarkable progress,
existing reasoning benchmarks often fail to rigorously evaluate the nuanced
reasoning capabilities required for complex, real-world problemsolving,
particularly in multi-disciplinary and multimodal contexts. In this paper, we
introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark,
dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of
both language and multimodal models. RBench spans 1,094 questions across 108
subjects for language model evaluation and 665 questions across 83 subjects for
multimodal model testing in both English and Chinese. These questions are
meticulously curated to ensure rigorous difficulty calibration, subject
balance, and crosslinguistic alignment, enabling the assessment to be an
Olympiad-level multi-disciplinary benchmark. We evaluate widely used models,
including OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate
that advanced models perform poorly on complex reasoning, especially multimodal
reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy
on our multimodal evaluation. Data and code are made publicly available at
here.
