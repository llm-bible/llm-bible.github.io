---
layout: publication
title: 'Whose Morality Do They Speak? Unraveling Cultural Bias In Multilingual Language Models'
authors: Meltem Aksoy
conference: "Arxiv"
year: 2024
bibkey: aksoy2024whose
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.18863"}
tags: ['Training Techniques', 'Fairness', 'Model Architecture', 'Tools', 'Reinforcement Learning', 'GPT', 'Bias Mitigation', 'Ethics and Bias']
---
Large language models (LLMs) have become integral tools in diverse domains,
yet their moral reasoning capabilities across cultural and linguistic contexts
remain underexplored. This study investigates whether multilingual LLMs, such
as GPT-3.5-Turbo, GPT-4o-mini, Llama 3.1, and MistralNeMo, reflect culturally
specific moral values or impose dominant moral norms, particularly those rooted
in English. Using the updated Moral Foundations Questionnaire (MFQ-2) in eight
languages, Arabic, Farsi, English, Spanish, Japanese, Chinese, French, and
Russian, the study analyzes the models' adherence to six core moral
foundations: care, equality, proportionality, loyalty, authority, and purity.
The results reveal significant cultural and linguistic variability, challenging
the assumption of universal moral consistency in LLMs. Although some models
demonstrate adaptability to diverse contexts, others exhibit biases influenced
by the composition of the training data. These findings underscore the need for
culturally inclusive model development to improve fairness and trust in
multilingual AI systems.
