---
layout: publication
title: 'Smart: Training Shallow Memory-aware Transformers For Robotic Explainability'
authors: Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
conference: Arxiv
year: 2019
citations: 24
bibkey: cornia2019training
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1910.02974'}]
tags: [Transformer, Interpretability and Explainability, Agentic]
---
The ability to generate natural language explanations conditioned on the
visual perception is a crucial step towards autonomous agents which can explain
themselves and communicate with humans. While the research efforts in image and
video captioning are giving promising results, this is often done at the
expense of the computational requirements of the approaches, limiting their
applicability to real contexts. In this paper, we propose a fully-attentive
captioning algorithm which can provide state-of-the-art performances on
language generation while restricting its computational demands. Our model is
inspired by the Transformer model and employs only two Transformer layers in
the encoding and decoding stages. Further, it incorporates a novel memory-aware
encoding of image regions. Experiments demonstrate that our approach achieves
competitive results in terms of caption quality while featuring reduced
computational demands. Further, to evaluate its applicability on autonomous
agents, we conduct experiments on simulated scenes taken from the perspective
of domestic robots.