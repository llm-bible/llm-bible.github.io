---
layout: publication
title: Charactereval A Chinese Benchmark For Role45;playing Conversational Agent Evaluation
authors: Tu Quan, Fan Shilong, Tian Zihang, Yan Rui
conference: "Arxiv"
year: 2024
bibkey: tu2024chinese
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.01275"}
  - {name: "Code", url: "https://github.com/morecry/CharacterEval"}
tags: ['Agentic', 'Attention Mechanism', 'GPT', 'Has Code', 'Model Architecture', 'Reinforcement Learning']
---
Recently the advent of large language models (LLMs) has revolutionized generative agents. Among them Role45;Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap we introduce CharacterEval a Chinese benchmark for comprehensive RPCA assessment complemented by a tailored high45;quality dataset. The dataset comprises 1785 multi45;turn role45;playing dialogues encompassing 23020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed beginning with initial dialogue extraction via GPT45;4 followed by rigorous human45;led quality control and enhanced with in45;depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT45;4 in Chinese role45;playing conversation. Source code data source and reward model will be publicly accessible at https://github.com/morecry/CharacterEval.
