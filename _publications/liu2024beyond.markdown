---
layout: publication
title: 'Beyond Benchmarking: A New Paradigm For Evaluation And Assessment Of Large Language Models'
authors: Liu Jin, Li Qingquan, Du Wenlong
conference: "Arxiv"
year: 2024
bibkey: liu2024beyond
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.07531"}
tags: ['Efficiency And Optimization', 'Training Techniques']
---
'In current benchmarks for evaluating large language models (LLMs), there are issues such as evaluation content restriction, untimely updates, and lack of optimization guidance. In this paper, we propose a new paradigm for the measurement of LLMs: Benchmarking-Evaluation-Assessment. Our paradigm shifts the location of LLM evaluation from the examination room to the hospital. Through conducting a physical examination on LLMs, it utilizes specific task-solving as the evaluation content, performs deep attribution of existing problems within LLMs, and provides recommendation for optimization.'
