---
layout: publication
title: ARB\: Advanced Reasoning Benchmark For Large Language Models
authors: Sawada Tomohiro, Paleka Daniel, Havrilla Alexander, Tadepalli Pranav, Vidas Paula, Kranias Alexander, Nay John J., Gupta Kshitij, Komatsuzaki Aran
conference: "Arxiv"
year: 2023
bibkey: sawada2023advanced
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.13692"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However many of these benchmarks are losing utility as LLMs get increasingly high scores despite not yet reaching expert performance in these domains. We introduce ARB a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks featuring problems in mathematics physics biology chemistry and law. As a subset of ARB we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 5037; on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities we introduce a rubric-based evaluation approach allowing GPT-4 to score its own intermediate reasoning steps. Further we conduct a human evaluation of the symbolic subset of ARB finding promising agreement between annotators and GPT-4 rubric evaluation scores.
