---
layout: publication
title: Exploring Precision And Recall To Assess The Quality And Diversity Of Llms
authors: Bronnec Florian Le, Verine Alexandre, Negrevergne Benjamin, Chevaleyre Yann, Allauzen Alexandre
conference: "Arxiv"
year: 2024
bibkey: bronnec2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.10693"}
tags: ['Applications', 'Language Modeling', 'Reinforcement Learning', 'Tools']
---
We introduce a novel evaluation framework for Large Language Models (LLMs) such as textsc123;Llama45;2125; and textsc123;Mistral125; focusing on importing Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state45;of45;the45;art language models the study reveals new insights into their performance on open45;ended generation tasks which are not adequately captured by traditional benchmarks. The findings highlight a trade45;off between the quality and diversity of generated samples particularly when models are fine45;tuned on instruction dataset or with human feedback. This work extends the toolkit for distribution45;based NLP evaluation offering insights into the practical capabilities and challenges that current LLMs face in generating diverse and high45;quality text. We release our code and data.
