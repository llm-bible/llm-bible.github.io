---
layout: publication
title: Attention Heads of Large Language Models A Survey
authors: Zheng Zifan, Wang Yezhaohui, Huang Yuxin, Song Shichao, Tang Bo, Xiong Feiyu, Li Zhiyu
conference: "Arxiv"
year: 2024
bibkey: zheng2024attention
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.03752"}
  - {name: "Code", url: "https://github.com/IAAR-Shanghai/Awesome-Attention-Heads"}
tags: ['Attention Mechanism', 'GPT', 'Has Code', 'Interpretability And Explainability', 'Model Architecture', 'Reinforcement Learning', 'Survey Paper', 'Tools']
---
Since the advent of ChatGPT Large Language Models (LLMs) have excelled in various tasks but remain largely as black-box systems. Consequently their development relies heavily on data-driven approaches limiting performance enhancement through changes in internal architecture and reasoning pathways. As a result many researchers have begun exploring the potential internal mechanisms of LLMs aiming to identify the essence of their reasoning bottlenecks with most studies focusing on attention heads. Our survey aims to shed light on the internal reasoning processes of LLMs by concentrating on the interpretability and underlying mechanisms of attention heads. We first distill the human thought process into a four-stage framework Knowledge Recalling In-Context Identification Latent Reasoning and Expression Preparation. Using this framework we systematically review existing research to identify and categorize the functions of specific attention heads. Furthermore we summarize the experimental methodologies used to discover these special heads dividing them into two categories Modeling-Free methods and Modeling-Required methods. Also we outline relevant evaluation methods and benchmarks. Finally we discuss the limitations of current research and propose several potential future directions. Our reference list is open-sourced at url https://github.com/IAAR-Shanghai/Awesome-Attention-Heads.
