---
layout: publication
title: Stealthy Attack On Large Language Model Based Recommendation
authors: Zhang Jinghao, Liu Yuting, Liu Qiang, Wu Shu, Guo Guibing, Wang Liang
conference: "Arxiv"
year: 2024
bibkey: zhang2024stealthy
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.14836"}
tags: ['Pretraining Methods', 'Reinforcement Learning', 'Security', 'Tools', 'Training Techniques']
---
Recently the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However while these systems have flourished their susceptibility to security threats has been largely overlooked. In this work we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an items exposure by merely altering its textual content during the testing phase without requiring direct interference with the models training process. Additionally the attack is notably stealthy as it does not affect the overall recommendation performance and the modifications to the text are subtle making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM45;based recommendation models demonstrate the superior efficacy and stealthiness of our approach. Our work unveils a significant security gap in LLM45;based recommendation systems and paves the way for future research on protecting these systems.
