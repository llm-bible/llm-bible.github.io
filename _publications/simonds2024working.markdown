---
layout: publication
title: 'REL: Working Out Is All You Need'
authors: Toby Simonds, Jey Han Lau, Chaithanya Bandi
conference: "Arxiv"
year: 2024
bibkey: simonds2024working
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.04645'}
tags: ['Reinforcement Learning', 'Training Techniques']
---
Recent developments, particularly OpenAI's O1 model, have demonstrated the
remarkable potential of Large Language Models (LLMs) for complex reasoning
tasks. Through analysis of O1's outputs and provided sample Chain-of-Thought
(CoT) demonstrations, we observe that it approaches problem-solving in a
distinctly human-like manner, systematically brainstorming ideas, testing
hypotheses, verifying results, and planning comprehensive solutions. These
sophisticated reasoning capabilities remain notably absent in other
state-of-the-art language models. In this paper, we hypothesize that this
performance gap stems from the limited availability of high-quality reasoning
process data in current training sets. We demonstrate that by constructing a
specialized dataset focused on explicit problem-solving workflows ("worked
solutions"), we can elicit substantially improved planning capabilities from
existing models. Additionally, we propose the Reasoning Enhancement Loop (REL),
a method for generating synthetic worked solutions.
