---
layout: publication
title: 'Towards Pareto Optimal Throughput In Small Language Model Serving'
authors: Pol G. Recasens, Yue Zhu, Chen Wang, Eun Kyung Lee, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral
conference: "Arxiv"
year: 2024
bibkey: recasens2024towards
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2404.03353'}
tags: ['Reinforcement Learning']
---
Large language models (LLMs) have revolutionized the state-of-the-art of many
different natural language processing tasks. Although serving LLMs is
computationally and memory demanding, the rise of Small Language Models (SLMs)
offers new opportunities for resource-constrained users, who now are able to
serve small models with cutting-edge performance. In this paper, we present a
set of experiments designed to benchmark SLM inference at performance and
energy levels. Our analysis provides a new perspective in serving, highlighting
that the small memory footprint of SLMs allows for reaching the Pareto-optimal
throughput within the resource capacity of a single accelerator. In this
regard, we present an initial set of findings demonstrating how model
replication can effectively improve resource utilization for serving SLMs.
