---
layout: publication
title: Augmenting Transformers With Knn45;based Composite Memory For Dialogue
authors: Fan Angela, Gardent Claire, Braud Chloe, Bordes Antoine
conference: "Arxiv"
year: 2020
bibkey: fan2020augmenting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2004.12744"}
tags: ['Model Architecture', 'Pretraining Methods', 'RAG', 'Transformer']
---
Various machine learning tasks can benefit from access to external information of different modalities such as text and images. Recent work has focused on learning architectures with large memories capable of storing this knowledge. We propose augmenting generative Transformer neural networks with KNN45;based Information Fetching (KIF) modules. Each KIF module learns a read operation to access fixed external knowledge. We apply these modules to generative dialog modeling a challenging task where information must be flexibly retrieved and incorporated to maintain the topic and flow of conversation. We demonstrate the effectiveness of our approach by identifying relevant knowledge required for knowledgeable but engaging dialog from Wikipedia images and human45;written dialog utterances and show that leveraging this retrieved information improves model performance measured by automatic and human evaluation.
