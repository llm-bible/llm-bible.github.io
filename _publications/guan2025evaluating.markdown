---
layout: publication
title: 'Evaluating Llm-based Agents For Multi-turn Conversations: A Survey'
authors: Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, Jian-guang Lou
conference: "Arxiv"
year: 2025
bibkey: guan2025evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.22458"}
tags: ['Agentic', 'Survey Paper', 'Tools', 'Reinforcement Learning']
---
This survey examines evaluation methods for large language model (LLM)-based
agents in multi-turn conversational settings. Using a PRISMA-inspired
framework, we systematically reviewed nearly 250 scholarly sources, capturing
the state of the art from various venues of publication, and establishing a
solid foundation for our analysis. Our study offers a structured approach by
developing two interrelated taxonomy systems: one that defines *what to
evaluate* and another that explains *how to evaluate*. The first taxonomy
identifies key components of LLM-based agents for multi-turn conversations and
their evaluation dimensions, including task completion, response quality, user
experience, memory and context retention, as well as planning and tool
integration. These components ensure that the performance of conversational
agents is assessed in a holistic and meaningful manner. The second taxonomy
system focuses on the evaluation methodologies. It categorizes approaches into
annotation-based evaluations, automated metrics, hybrid strategies that combine
human assessments with quantitative measures, and self-judging methods
utilizing LLMs. This framework not only captures traditional metrics derived
from language understanding, such as BLEU and ROUGE scores, but also
incorporates advanced techniques that reflect the dynamic, interactive nature
of multi-turn dialogues.
