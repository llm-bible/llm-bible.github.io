---
layout: publication
title: Exploring The Efficacy Of Pre45;trained Checkpoints In Text45;to45;music Generation Task
authors: Wu Shangda, Sun Maosong
conference: "Arxiv"
year: 2022
bibkey: wu2022exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.11216"}
tags: ['BERT', 'GPT', 'Model Architecture', 'Multimodal Models', 'RAG']
---
Benefiting from large45;scale datasets and pre45;trained models the field of generative models has recently gained significant momentum. However most datasets for symbolic music are very small which potentially limits the performance of data45;driven multimodal models. An intuitive solution to this problem is to leverage pre45;trained models from other modalities (e.g. natural language) to improve the performance of symbolic music45;related multimodal tasks. In this paper we carry out the first study of generating complete and semantically consistent symbolic music scores from text descriptions and explore the efficacy of using publicly available checkpoints (i.e. BERT GPT45;2 and BART) for natural language processing in the task of text45;to45;music generation. Our experimental results show that the improvement from using pre45;trained checkpoints is statistically significant in terms of BLEU score and edit distance similarity. We analyse the capabilities and limitations of our model to better understand the potential of language45;music models.
