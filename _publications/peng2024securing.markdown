---
layout: publication
title: 'Securing Large Language Models: Addressing Bias, Misinformation, And Prompt Attacks'
authors: Benji Peng, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Junyu Liu, Qian Niu
conference: "Arxiv"
year: 2024
bibkey: peng2024securing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2409.08087'}
tags: ['Security', 'Training Techniques', 'Model Architecture', 'GPT', 'Prompting', 'Survey Paper', 'Bias Mitigation', 'Ethics and Bias']
---
Large Language Models (LLMs) demonstrate impressive capabilities across
various fields, yet their increasing use raises critical security concerns.
This article reviews recent literature addressing key issues in LLM security,
with a focus on accuracy, bias, content detection, and vulnerability to
attacks. Issues related to inaccurate or misleading outputs from LLMs is
discussed, with emphasis on the implementation from fact-checking methodologies
to enhance response reliability. Inherent biases within LLMs are critically
examined through diverse evaluation techniques, including controlled input
studies and red teaming exercises. A comprehensive analysis of bias mitigation
strategies is presented, including approaches from pre-processing interventions
to in-training adjustments and post-processing refinements. The article also
probes the complexity of distinguishing LLM-generated content from
human-produced text, introducing detection mechanisms like DetectGPT and
watermarking techniques while noting the limitations of machine learning
enabled classifiers under intricate circumstances. Moreover, LLM
vulnerabilities, including jailbreak attacks and prompt injection exploits, are
analyzed by looking into different case studies and large-scale competitions
like HackAPrompt. This review is concluded by retrospecting defense mechanisms
to safeguard LLMs, accentuating the need for more extensive research into the
LLM security field.
