---
layout: publication
title: Securing Large Language Models Addressing Bias Misinformation and Prompt Attacks
authors: Peng Benji, Chen Keyu, Li Ming, Feng Pohsun, Bi Ziqian, Liu Junyu, Niu Qian
conference: "Arxiv"
year: 2024
bibkey: peng2024securing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.08087"}
tags: ['Bias Mitigation', 'Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'Security', 'Survey Paper', 'Training Techniques']
---
Large Language Models (LLMs) demonstrate impressive capabilities across various fields yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security with a focus on accuracy bias content detection and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover LLM vulnerabilities including jailbreak attacks and prompt injection exploits are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs accentuating the need for more extensive research into the LLM security field.
