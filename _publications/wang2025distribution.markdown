---
layout: publication
title: 'Distribution Prompting: Understanding The Expressivity Of Language Models Through The Next-token Distributions They Can Produce'
authors: Haojin Wang, Zining Zhu, Freda Shi
conference: "Arxiv"
year: 2025
bibkey: wang2025distribution
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.12244"}
tags: ['GPT', 'Prompting', 'Pretraining Methods']
---
Autoregressive neural language models (LMs) generate a probability distribution over tokens at each time step given a prompt. In this work, we attempt to systematically understand the probability distributions that LMs can produce, showing that some distributions are significantly harder to elicit than others. Specifically, for any target next-token distribution over the vocabulary, we attempt to find a prompt that induces the LM to output a distribution as close as possible to the target, using either soft or hard gradient-based prompt tuning. We find that (1) in general, distributions with very low or very high entropy are easier to approximate than those with moderate entropy; (2) among distributions with the same entropy, those containing ''outlier tokens'' are easier to approximate; (3) target distributions generated by LMs -- even LMs with different tokenizers -- are easier to approximate than randomly chosen targets. These results offer insights into the expressiveness of LMs and the challenges of using them as probability distribution proposers.
