---
layout: publication
title: Visual Chain Of Thought Bridging Logical Gaps With Multimodal Infillings
authors: Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, William Yang Wang
conference: "Arxiv"
year: 2023
bibkey: rose2023visual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2305.02317v3"}
tags: ['Applications', 'Interpretability And Explainability', 'Multimodal Models', 'Prompting', 'RAG']
---
Recent advances in large language models elicit reasoning in a chain45;of45;thought that allows models to decompose problems in a human45;like fashion. Though this paradigm improves multi45;step reasoning ability in language models it is limited by being unimodal and applied mainly to question45;answering tasks. We claim that incorporating visual augmentation into reasoning is essential especially for complex imaginative tasks. Consequently we introduce VCoT a novel method that leverages chain45;of45;thought prompting with vision45;language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning as well as provide interpretability into models multi45;step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain45;of45;thought baselines which can be used to enhance downstream performance.
