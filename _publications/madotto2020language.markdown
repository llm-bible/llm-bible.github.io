---
layout: publication
title: Language Models As Few45;shot Learner For Task45;oriented Dialogue Systems
authors: Madotto Andrea, Liu Zihan, Lin Zhaojiang, Fung Pascale
conference: "Arxiv"
year: 2020
bibkey: madotto2020language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2008.06239"}
tags: ['Applications', 'Fine Tuning', 'GPT', 'Model Architecture']
---
Task45;oriented dialogue systems use four connected modules namely Natural Language Understanding (NLU) a Dialogue State Tracking (DST) Dialogue Policy (DP) and Natural Language Generation (NLG). A research challenge is to learn each module with the least amount of samples (i.e. few45;shots) given the high cost related to the data collection. The most common and effective technique to solve this problem is transfer learning where large language models either pre45;trained on text or task45;specific data are fine45;tuned on the few samples. These methods require fine45;tuning steps and a set of parameters for each task. Differently language models such as GPT45;2 (Radford et al. 2019) and GPT45;3 (Brown et al. 2020) allow few45;shot learning by priming the model with few examples. In this paper we evaluate the priming few45;shot ability of language models in the NLU DST DP and NLG tasks. Importantly we highlight the current limitations of this approach and we discuss the possible implication for future work.
