---
layout: publication
title: 'Reward Generation Via Large Vision-language Model In Offline Reinforcement Learning'
authors: Younghwan Lee, Tung M. Luu, Donghoon Lee, Chang D. Yoo
conference: "Arxiv"
year: 2025
bibkey: lee2025reward
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.08772'}
tags: ['Agentic', 'RAG', 'Prompting', 'Multimodal Models', 'Reinforcement Learning']
---
In offline reinforcement learning (RL), learning from fixed datasets presents
a promising solution for domains where real-time interaction with the
environment is expensive or risky. However, designing dense reward signals for
offline dataset requires significant human effort and domain expertise.
Reinforcement learning with human feedback (RLHF) has emerged as an
alternative, but it remains costly due to the human-in-the-loop process,
prompting interest in automated reward generation models. To address this, we
propose Reward Generation via Large Vision-Language Models (RG-VLM), which
leverages the reasoning capabilities of LVLMs to generate rewards from offline
data without human involvement. RG-VLM improves generalization in long-horizon
tasks and can be seamlessly integrated with the sparse reward signals to
enhance task performance, demonstrating its potential as an auxiliary reward
signal.
