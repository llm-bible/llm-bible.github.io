---
layout: publication
title: Large Language Models Encode Clinical Knowledge
authors: Singhal Karan, Azizi Shekoofeh, Tu Tao, Mahdavi S. Sara, Wei Jason, Chung Hyung Won, Scales Nathan, Tanwani Ajay, Cole-lewis Heather, Pfohl Stephen, Payne Perry, Seneviratne Martin, Gamble Paul, Kelly Chris, Scharli Nathaneal, Chowdhery Aakanksha, Mansfield Philip, Arcas Blaise Aguera Y, Webster Dale, Corrado Greg S., Matias Yossi, Chou Katherine, Gottweis Juraj, Tomasev Nenad, Liu Yun, Rajkomar Alvin, Barral Joelle, Semturs Christopher, Karthikesalingam Alan, Natarajan Vivek
conference: "Arxiv"
year: 2022
bibkey: singhal2022large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2212.13138"}
tags: ['Applications', 'Ethics And Bias', 'Prompting', 'RAG', 'Tools']
---
Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation but the quality bar for medical and clinical applications is high. Today attempts to assess models clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this we present MultiMedQA a benchmark combining six existing open question answering datasets spanning professional medical exams research and consumer queries; and HealthSearchQA a new free45;response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality precision possible harm and bias. In addition we evaluate PaLM (a 54045;billion parameter LLM) and its instruction45;tuned variant Flan45;PaLM on MultiMedQA. Using a combination of prompting strategies Flan45;PaLM achieves state45;of45;the45;art accuracy on every MultiMedQA multiple45;choice dataset (MedQA MedMCQA PubMedQA MMLU clinical topics) including 67.637; accuracy on MedQA (US Medical License Exam questions) surpassing prior state45;of45;the45;art by over 1737;. However human evaluation reveals key gaps in Flan45;PaLM responses. To resolve this we introduce instruction prompt tuning a parameter45;efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model Med45;PaLM performs encouragingly but remains inferior to clinicians. We show that comprehension recall of knowledge and medical reasoning improve with model scale and instruction prompt tuning suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of todays models reinforcing the importance of both evaluation frameworks and method development in creating safe helpful LLM models for clinical applications.
