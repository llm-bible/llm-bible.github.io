---
layout: publication
title: 'Respond Beyond Language: A Benchmark For Video Generation In Response To Realistic User Intents'
authors: Shuting Wang, Yunqi Liu, Zixin Yang, Ning Hu, Zhicheng Dou, Chenyan Xiong
conference: "Arxiv"
year: 2025
bibkey: wang2025respond
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2506.01689'}
tags: ['Reinforcement Learning', 'Interpretability and Explainability', 'Multimodal Models', 'Tools']
---
Querying generative AI models, e.g., large language models (LLMs), has become a prevalent method for information acquisition. However, existing query-answer datasets primarily focus on textual responses, making it challenging to address complex user queries that require visual demonstrations or explanations for better understanding. To bridge this gap, we construct a benchmark, RealVideoQuest, designed to evaluate the abilities of text-to-video (T2V) models in answering real-world, visually grounded queries. It identifies 7.5K real user queries with video response intents from Chatbot-Arena and builds 4.5K high-quality query-video pairs through a multistage video retrieval and refinement process. We further develop a multi-angle evaluation system to assess the quality of generated video answers. Experiments indicate that current T2V models struggle with effectively addressing real user queries, pointing to key challenges and future research opportunities in multimodal AI.
