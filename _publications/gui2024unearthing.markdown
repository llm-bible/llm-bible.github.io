---
layout: publication
title: Iepile Unearthing Large45;scale Schema45;based Information Extraction Corpus
authors: Gui Honghao, Yuan Lin, Ye Hongbin, Zhang Ningyu, Sun Mengshu, Liang Lei, Chen Huajun
conference: "Arxiv"
year: 2024
bibkey: gui2024unearthing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.14710"}
tags: ['Pretraining Methods', 'RAG', 'Reinforcement Learning']
---
Large Language Models (LLMs) demonstrate remarkable potential across various domains; however they exhibit a significant performance gap in Information Extraction (IE). Note that high45;quality instruction data is the vital key for enhancing the specific capabilities of LLMs while current IE datasets tend to be small in scale fragmented and lack standardized schema. To this end we introduce IEPile a comprehensive bilingual (English and Chinese) IE instruction corpus which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets and introduce schema45;based instruction generation to unearth a large45;scale corpus. Experimentally IEPile enhance the performance of LLMs for IE with notable improvements in zero45;shot generalization. We open45;source the resource and pre45;trained models hoping to provide valuable support to the NLP community.
