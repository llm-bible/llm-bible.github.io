---
layout: publication
title: 'Editing Across Languages: A Survey Of Multilingual Knowledge Editing'
authors: Nadir Durrani, Basel Mousi, Fahim Dalvi
conference: "Arxiv"
year: 2025
bibkey: durrani2025editing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.14393'}
tags: ['RAG', 'Training Techniques', 'Tools', 'Fine-Tuning', 'Survey Paper', 'Pretraining Methods']
---
While Knowledge Editing has been extensively studied in monolingual settings, it remains underexplored in multilingual contexts. This survey systematizes recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of model editing focused on ensuring factual edits generalize reliably across languages. We present a comprehensive taxonomy of MKE methods, covering parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We survey available benchmarks,summarize key findings on method effectiveness and transfer patterns, identify challenges in cross-lingual propagation, and highlight open problems related to language anisotropy, evaluation coverage, and edit scalability. Our analysis consolidates a rapidly evolving area and lays the groundwork for future progress in editable language-aware LLMs.
