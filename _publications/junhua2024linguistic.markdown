---
layout: publication
title: LARA Linguistic45;adaptive Retrieval45;augmented Llms For Multi45;turn Intent Classification
authors: Junhua Liu, Keat Tan Yong, Bin Fu
conference: "Arxiv"
year: 2024
bibkey: junhua2024linguistic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.16504"}
tags: ['Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques']
---
Following the significant achievements of large language models (LLMs) researchers have employed in45;context learning for text classification tasks. However these studies focused on monolingual single45;turn classification tasks. In this paper we introduce LARA (Linguistic45;Adaptive Retrieval45;Augmented Language Models) designed to enhance accuracy in multi45;turn classification tasks across six languages accommodating numerous intents in chatbot interactions. Multi45;turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine45;tuned smaller model with a retrieval45;augmented mechanism integrated within the architecture of LLMs. This integration allows LARA to dynamically utilize past dialogues and relevant intents thereby improving the understanding of the context. Furthermore our adaptive retrieval techniques bolster the cross45;lingual capabilities of LLMs without extensive retraining and fine45;tune. Comprehensive experiments demonstrate that LARA achieves state45;of45;the45;art performance on multi45;turn intent classification tasks enhancing the average accuracy by 3.6737; compared to existing methods.
