---
layout: publication
title: ISAAQ 45;45; Mastering Textbook Questions With Pre45;trained Transformers And Bottom45;up And Top45;down Attention
authors: Gomez-perez Jose Manuel, Ortega Raul
conference: "Arxiv"
year: 2020
bibkey: gomezperez2020isaaq
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2010.00562"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams. For the first time this paper taps on the potential of transformer language models and bottom45;up and top45;down attention to tackle the language and visual understanding challenges this task entails. Rather than training a language45;visual transformer from scratch we rely on pre45;trained transformers fine45;tuning and ensembling. We add bottom45;up and top45;down attention to identify regions of interest corresponding to diagram constituents and their relationships improving the selection of relevant visual information for each question and answer options. Our system ISAAQ reports unprecedented success in all TQA question types with accuracies of 81.3637; 71.1137; and 55.1237; on true/false text45;only and diagram multiple choice questions. ISAAQ also demonstrates its broad applicability obtaining state45;of45;the45;art results in other demanding datasets.
