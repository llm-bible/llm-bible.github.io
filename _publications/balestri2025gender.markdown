---
layout: publication
title: 'Gender And Content Bias In Large Language Models: A Case Study On Google Gemini 2.0 Flash Experimental'
authors: Roberto Balestri
conference: "Frontiers in Artificial Intelligence (2025) 81558696"
year: 2025
bibkey: balestri2025gender
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.16534'}
tags: ['Fairness', 'GPT', 'Model Architecture', 'Prompting', 'Bias Mitigation', 'Ethics and Bias', 'Interpretability']
---
This study evaluates the biases in Gemini 2.0 Flash Experimental, a
state-of-the-art large language model (LLM) developed by Google, focusing on
content moderation and gender disparities. By comparing its performance to
ChatGPT-4o, examined in a previous work of the author, the analysis highlights
some differences in ethical moderation practices. Gemini 2.0 demonstrates
reduced gender bias, notably with female-specific prompts achieving a
substantial rise in acceptance rates compared to results obtained by
ChatGPT-4o. It adopts a more permissive stance toward sexual content and
maintains relatively high acceptance rates for violent prompts, including
gender-specific cases. Despite these changes, whether they constitute an
improvement is debatable. While gender bias has been reduced, this reduction
comes at the cost of permitting more violent content toward both males and
females, potentially normalizing violence rather than mitigating harm.
Male-specific prompts still generally receive higher acceptance rates than
female-specific ones. These findings underscore the complexities of aligning AI
systems with ethical standards, highlighting progress in reducing certain
biases while raising concerns about the broader implications of the model's
permissiveness. Ongoing refinements are essential to achieve moderation
practices that ensure transparency, fairness, and inclusivity without
amplifying harmful content.
