---
layout: publication
title: 'Signformer Is All You Need: Towards Edge AI For Sign Language'
authors: Eta Yang
conference: "Arxiv"
year: 2024
bibkey: yang2024signformer
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.12901'}
tags: ['Attention Mechanism', 'Transformer', 'Efficiency and Optimization', 'Model Architecture', 'Reinforcement Learning', 'Pretraining Methods']
---
Sign language translation, especially in gloss-free paradigm, is confronting
a dilemma of impracticality and unsustainability due to growing
resource-intensive methodologies. Contemporary state-of-the-arts (SOTAs) have
significantly hinged on pretrained sophiscated backbones such as Large Language
Models (LLMs), embedding sources, or extensive datasets, inducing considerable
parametric and computational inefficiency for sustainable use in real-world
scenario. Despite their success, following this research direction undermines
the overarching mission of this domain to create substantial value to bridge
hard-hearing and common populations. Committing to the prevailing trend of LLM
and Natural Language Processing (NLP) studies, we pursue a profound essential
change in architecture to achieve ground-up improvements without external aid
from pretrained models, prior knowledge transfer, or any NLP strategies
considered not-from-scratch.
  Introducing Signformer, a from-scratch Feather-Giant transforming the area
towards Edge AI that redefines extremities of performance and efficiency with
LLM-competence and edgy-deployable compactness. In this paper, we present
nature analysis of sign languages to inform our algorithmic design and deliver
a scalable transformer pipeline with convolution and attention novelty. We
achieve new 2nd place on leaderboard with a parametric reduction of 467-1807x
against the finests as of 2024 and outcompete almost every other methods in a
lighter configuration of 0.57 million parameters.
