---
layout: publication
title: 'Customising General Large Language Models For Specialised Emotion Recognition Tasks'
authors: Peng Liyizhe, Zhang Zixing, Pang Tao, Han Jing, Zhao Huan, Chen Hao, Schuller Bj√∂rn W.
conference: "Arxiv"
year: 2023
bibkey: peng2023customising
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.14225"}
tags: ['Attention Mechanism', 'Interpretability And Explainability', 'Model Architecture', 'Prompting', 'RAG', 'Security']
---
The advent of large language models (LLMs) has gained tremendous attention
over the past year. Previous studies have shown the astonishing performance of
LLMs not only in other tasks but also in emotion recognition in terms of
accuracy, universality, explanation, robustness, few/zero-shot learning, and
others. Leveraging the capability of LLMs inevitably becomes an essential
solution for emotion recognition. To this end, we further comprehensively
investigate how LLMs perform in linguistic emotion recognition if we
concentrate on this specific task. Specifically, we exemplify a publicly
available and widely used LLM -- Chat General Language Model, and customise it
for our target by using two different modal adaptation techniques, i.e., deep
prompt tuning and low-rank adaptation. The experimental results obtained on six
widely used datasets present that the adapted LLM can easily outperform other
state-of-the-art but specialised deep models. This indicates the strong
transferability and feasibility of LLMs in the field of emotion recognition.
