---
layout: publication
title: Customising General Large Language Models For Specialised Emotion Recognition Tasks
authors: Peng Liyizhe, Zhang Zixing, Pang Tao, Han Jing, Zhao Huan, Chen Hao, Schuller Bj√∂rn W.
conference: "Arxiv"
year: 2023
bibkey: peng2023customising
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.14225"}
tags: ['Attention Mechanism', 'Fine Tuning', 'Interpretability And Explainability', 'Model Architecture', 'Prompting', 'RAG', 'Security']
---
The advent of large language models (LLMs) has gained tremendous attention over the past year. Previous studies have shown the astonishing performance of LLMs not only in other tasks but also in emotion recognition in terms of accuracy universality explanation robustness few/zero45;shot learning and others. Leveraging the capability of LLMs inevitably becomes an essential solution for emotion recognition. To this end we further comprehensively investigate how LLMs perform in linguistic emotion recognition if we concentrate on this specific task. Specifically we exemplify a publicly available and widely used LLM 45;45; Chat General Language Model and customise it for our target by using two different modal adaptation techniques i.e. deep prompt tuning and low45;rank adaptation. The experimental results obtained on six widely used datasets present that the adapted LLM can easily outperform other state45;of45;the45;art but specialised deep models. This indicates the strong transferability and feasibility of LLMs in the field of emotion recognition.
