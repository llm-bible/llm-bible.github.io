---
layout: publication
title: Do Large Language Models Know About Facts?
authors: Hu Xuming, Chen Junzhe, Li Xiaochuan, Guo Yufei, Wen Lijie, Yu Philip S., Guo Zhijiang
conference: "Arxiv"
year: 2023
bibkey: hu2023do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.05177"}
tags: ['Applications', 'Pretraining Methods', 'Security', 'Training Techniques']
---
Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks such as question answering and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth due to facts that can be incorrectly induced or become obsolete over time. To this end we aim to comprehensively evaluate the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources timelines domains regions and languages. Furthermore we investigate whether LLMs are able to compose multiple facts update factual knowledge temporally reason over multiple pieces of facts identify subtle factual differences and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes will be publicly available.
