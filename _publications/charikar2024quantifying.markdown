---
layout: publication
title: Quantifying The Gain In Weak45;to45;strong Generalization
authors: Charikar Moses, Pabbaraju Chirag, Shiragur Kirankumar
conference: "Arxiv"
year: 2024
bibkey: charikar2024quantifying
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.15116"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Tools']
---
Recent advances in large language models have shown capabilities that are extraordinary and near45;superhuman. These models operate with such complexity that reliably evaluating and aligning them proves challenging for humans. This leads to the natural question can guidance from weak models (like humans) adequately direct the capabilities of strong models In a recent and somewhat surprising work Burns et al. (2023) empirically demonstrated that when strong models (like GPT45;4) are finetuned using labels generated by weak supervisors (like GPT45;2) the strong models outperform their weaker counterparts 45;45; a phenomenon they term weak45;to45;strong generalization. In this work we present a theoretical framework for understanding weak45;to45;strong generalization. Specifically we show that the improvement in performance achieved by strong models over their weaker counterparts is quantified by the misfit error incurred by the strong model on labels generated by the weaker model. Our theory reveals several curious algorithmic insights. For instance we can predict the amount by which the strong model will improve over the weak model and also choose among different weak models to train the strong model based on its misfit error. We validate our theoretical findings through various empirical assessments.
