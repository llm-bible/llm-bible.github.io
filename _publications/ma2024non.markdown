---
layout: publication
title: 'Non-myopic Generation Of Language Models For Reasoning And Planning'
authors: Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, Lingpeng Kong
conference: "Arxiv"
year: 2024
bibkey: ma2024non
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.17195"}
tags: ['Agentic', 'GPT', 'Efficiency and Optimization', 'RAG', 'Reinforcement Learning', 'Pretraining Methods']
---
Large Language Models have demonstrated remarkable abilities in reasoning and
planning by breaking down complex problems into sequential steps. Despite their
success in various domains like mathematical problem-solving and coding, LLMs
face challenges in ensuring reliable and optimal planning due to their inherent
myopic nature of autoregressive decoding. This paper revisits LLM reasoning
from an optimal-control perspective, proposing a novel method,
Predictive-Decoding, that leverages Model Predictive Control to enhance
planning accuracy. By re-weighting LLM distributions based on foresight
trajectories, Predictive-Decoding aims to mitigate early errors and promote
non-myopic planning. Our experiments show significant improvements in a wide
range of tasks for math, coding, and agents. Furthermore, Predictive-Decoding
demonstrates computational efficiency, outperforming search baselines with
reduced computational resources. This study provides insights into optimizing
LLM planning capabilities.
