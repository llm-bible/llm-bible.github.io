---
layout: publication
title: Iconqa A New Benchmark For Abstract Diagram Understanding And Visual Language Reasoning
authors: Lu Pan, Qiu Liang, Chen Jiaqi, Xia Tony, Zhao Yizhou, Zhang Wei, Yu Zhou, Liang Xiaodan, Zhu Song-chun
conference: "Arxiv"
year: 2021
bibkey: lu2021new
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2110.13214"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
Current visual question answering (VQA) tasks mainly consider answering human45;annotated questions for natural images. However aside from natural images abstract diagrams with semantic richness are still understudied in visual understanding and reasoning research. In this work we introduce a new challenge of Icon Question Answering (IconQA) with the goal of answering a question in an icon image context. We release IconQA a large45;scale dataset that consists of 107439 questions and three sub45;tasks multi45;image45;choice multi45;text45;choice and filling45;in45;the45;blank. The IconQA dataset is inspired by real45;world diagram word problems that highlight the importance of abstract diagram understanding and comprehensive cognitive reasoning. Thus IconQA requires not only perception skills like object recognition and text understanding but also diverse cognitive reasoning skills such as geometric reasoning commonsense reasoning and arithmetic reasoning. To facilitate potential IconQA models to learn semantic representations for icon images we further release an icon dataset Icon645 which contains 645687 colored icons on 377 classes. We conduct extensive user studies and blind experiments and reproduce a wide range of advanced VQA methods to benchmark the IconQA task. Also we develop a strong IconQA baseline Patch45;TRM that applies a pyramid cross45;modal Transformer with input diagram embeddings pre45;trained on the icon dataset. IconQA and Icon645 are available at https://iconqa.github.io.
