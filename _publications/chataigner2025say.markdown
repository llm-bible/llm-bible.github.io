---
layout: publication
title: 'Say It Another Way: Auditing Llms With A User-grounded Automated Paraphrasing Framework'
authors: Cléa Chataigner, Rebecca Ma, Prakhar Ganesh, Afaf Taïk, Elliot Creager, Golnoosh Farnadi
conference: "Arxiv"
year: 2025
bibkey: chataigner2025say
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.03563"}
tags: ['Tools', 'Prompting', 'Ethics and Bias', 'Reinforcement Learning']
---
Large language models (LLMs) are sensitive to subtle changes in prompt phrasing, complicating efforts to audit them reliably. Prior approaches often rely on arbitrary or ungrounded prompt variations, which may miss key linguistic and demographic factors in real-world usage. We introduce AUGMENT (Automated User-Grounded Modeling and Evaluation of Natural Language Transformations), a framework for systematically generating and evaluating controlled, realistic prompt paraphrases based on linguistic structure and user demographics. AUGMENT ensures paraphrase quality through a combination of semantic, stylistic, and instruction-following criteria. In a case study on the BBQ dataset, we show that user-grounded paraphrasing leads to significant shifts in LLM performance and bias metrics across nine models. Our findings highlight the need for more representative and structured approaches to prompt variation in LLM auditing.
