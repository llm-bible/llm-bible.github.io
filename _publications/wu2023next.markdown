---
layout: publication
title: Next45;gpt Any45;to45;any Multimodal LLM
authors: Wu Shengqiong, Fei Hao, Qu Leigang, Ji Wei, Chua Tat-seng
conference: "Arxiv"
year: 2023
bibkey: wu2023next
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.05519"}
  - {name: "Code", url: "https://next&#45;gpt.github.io/"}
tags: ['Agentic', 'GPT', 'Has Code', 'Merging', 'Model Architecture', 'Multimodal Models', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
While recently Multimodal Large Language Models (MM45;LLMs) have made exciting strides they mostly fall prey to the limitation of only input45;side multimodal understanding without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities developing any45;to45;any MM45;LLMs capable of accepting and delivering content in any modality becomes essential to human45;level AI. To fill the gap we present an end45;to45;end general45;purpose any45;to45;any MM45;LLM system NExT45;GPT. We connect an LLM with multimodal adaptors and different diffusion decoders enabling NExT45;GPT to perceive inputs and generate outputs in arbitrary combinations of text images videos and audio. By leveraging the existing well45;trained highly45;performing encoders and decoders NExT45;GPT is tuned with only a small amount of parameter (137;) of certain projection layers which not only benefits low45;cost training and also facilitates convenient expansion to more potential modalities. Moreover we introduce a modality45;switching instruction tuning (MosIT) and manually curate a high45;quality dataset for MosIT based on which NExT45;GPT is empowered with complex cross45;modal semantic understanding and content generation. Overall our research showcases the promising possibility of building an AI agent capable of modeling universal modalities paving the way for more human45;like AI research in the community. Project page https://next&#45;gpt.github.io/
