---
layout: publication
title: 'Era-cot: Improving Chain-of-thought Through Entity Relationship Analysis'
authors: Liu Yanming, Peng Xinyue, Du Tianyu, Yin Jianwei, Liu Weihao, Zhang Xuhong
conference: "Arxiv"
year: 2024
bibkey: liu2024era
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.06932"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Prompting', 'RAG', 'Reinforcement Learning']
---
Large language models (LLMs) have achieved commendable accomplishments in
various natural language processing tasks. However, LLMs still encounter
significant challenges when dealing with complex scenarios involving multiple
entities. These challenges arise from the presence of implicit relationships
that demand multi-step reasoning. In this paper, we propose a novel approach
ERA-CoT, which aids LLMs in understanding context by capturing relationships
between entities and supports the reasoning of diverse tasks through
Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates
the superior performance of our proposed method compared to current CoT
prompting methods, achieving a significant improvement of an average of 5.1\%
on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that
ERA-CoT increases the LLM's understanding of entity relationships,
significantly improves the accuracy of question answering, and enhances the
reasoning ability of LLMs.
