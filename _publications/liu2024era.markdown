---
layout: publication
title: Era45;cot Improving Chain45;of45;thought Through Entity Relationship Analysis
authors: Liu Yanming, Peng Xinyue, Du Tianyu, Yin Jianwei, Liu Weihao, Zhang Xuhong
conference: "Arxiv"
year: 2024
bibkey: liu2024era
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.06932"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Prompting', 'RAG', 'Reinforcement Learning']
---
Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi45;step reasoning. In this paper we propose a novel approach ERA45;CoT which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain45;of45;Thoughts (CoT). Experimental results show that ERA45;CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods achieving a significant improvement of an average of 5.137; on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA45;CoT increases the LLMs understanding of entity relationships significantly improves the accuracy of question answering and enhances the reasoning ability of LLMs.
