---
layout: publication
title: 'A Survey Of Large Language Models For Arabic Language And Its Dialects'
authors: Malak Mashaabi, Shahad Al-khalifa, Hend Al-khalifa
conference: "Arxiv"
year: 2024
bibkey: mashaabi2024survey
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.20238'}
tags: ['Ethics and Bias', 'Training Techniques', 'Applications', 'Model Architecture', 'Survey Paper', 'Reinforcement Learning', 'Pre-Training', 'Interpretability']
---
This survey offers a comprehensive overview of Large Language Models (LLMs)
designed for Arabic language and its dialects. It covers key architectures,
including encoder-only, decoder-only, and encoder-decoder models, along with
the datasets used for pre-training, spanning Classical Arabic, Modern Standard
Arabic, and Dialectal Arabic. The study also explores monolingual, bilingual,
and multilingual LLMs, analyzing their architectures and performance across
downstream tasks, such as sentiment analysis, named entity recognition, and
question answering. Furthermore, it assesses the openness of Arabic LLMs based
on factors, such as source code availability, training data, model weights, and
documentation. The survey highlights the need for more diverse dialectal
datasets and attributes the importance of openness for research reproducibility
and transparency. It concludes by identifying key challenges and opportunities
for future research and stressing the need for more inclusive and
representative models.
