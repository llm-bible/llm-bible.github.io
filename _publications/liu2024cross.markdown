---
layout: publication
title: Chatzerozero45;shot Cross45;lingual Dialogue Generation Via Pseudo45;target Language
authors: Liu Yongkang, Shi Feng, Wang Daling, Zhang Yifei, Sch√ºtze Hinrich
conference: "ECAI"
year: 2024
bibkey: liu2024cross
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.08724"}
tags: ['Applications', 'Pretraining Methods']
---
Although large language models(LLMs) show amazing capabilities among various exciting applications discovered for LLMs fall short in other low45;resource languages. Besides most existing methods depend on large45;scale dialogue corpora and thus building systems for dialogue generation in a zero45;shot scenario remains a considerable challenge. To address this challenge we propose a novel end45;to45;end zero45;shot dialogue generation model ChatZero based on cross45;lingual code45;switching method. First we construct code45;switching language and pseudo45;target language with placeholders. Then for cross45;lingual semantic transfer we employ unsupervised contrastive learning to minimize the semantics gap of the source language code45;switching language and pseudo45;target language that are mutually positive examples in the high dimensional semantic space. Experiments on the multilingual DailyDialog and DSTC745;AVSD datasets demonstrate that ChatZero can achieve more than 9037; of the original performance under the zero45;shot case compared to supervised learning and achieve state45;of45;the45;art performance compared with other baselines.
