---
layout: publication
title: 'Understanding The Relationship Between Prompts And Response Uncertainty In Large Language Models'
authors: Zhang Ze Yu, Verma Arun, Doshi-velez Finale, Low Bryan Kian Hsiang
conference: "Arxiv"
year: 2024
bibkey: zhang2024understanding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.14845"}
tags: ['Pretraining Methods', 'Prompting', 'RAG', 'Training Techniques']
---
Large language models (LLMs) are widely used in decision-making, but their
reliability, especially in critical tasks like healthcare, is not
well-established. Therefore, understanding how LLMs reason and make decisions
is crucial for their safe deployment. This paper investigates how the
uncertainty of responses generated by LLMs relates to the information provided
in the input prompt. Leveraging the insight that LLMs learn to infer latent
concepts during pretraining, we propose a prompt-response concept model that
explains how LLMs generate responses and helps understand the relationship
between prompts and response uncertainty. We show that the uncertainty
decreases as the prompt's informativeness increases, similar to epistemic
uncertainty. Our detailed experimental results on real datasets validate our
proposed model.
