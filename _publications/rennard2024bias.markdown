---
layout: publication
title: 'Bias In The Mirror: Are Llms Opinions Robust To Their Own Adversarial Attacks ?'
authors: Virgile Rennard, Christos Xypolopoulos, Michalis Vazirgiannis
conference: "Arxiv"
year: 2024
bibkey: rennard2024bias
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.13517"}
tags: ['Ethics and Bias', 'Security', 'Training Techniques', 'Reinforcement Learning']
---
Large language models (LLMs) inherit biases from their training data and
alignment processes, influencing their responses in subtle ways. While many
studies have examined these biases, little work has explored their robustness
during interactions. In this paper, we introduce a novel approach where two
instances of an LLM engage in self-debate, arguing opposing viewpoints to
persuade a neutral version of the model. Through this, we evaluate how firmly
biases hold and whether models are susceptible to reinforcing misinformation or
shifting to harmful viewpoints. Our experiments span multiple LLMs of varying
sizes, origins, and languages, providing deeper insights into bias persistence
and flexibility across linguistic and cultural contexts.
