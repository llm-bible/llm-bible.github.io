---
layout: publication
title: 'Questioning Internal Knowledge Structure Of Large Language Models Through The Lens Of The Olympic Games'
authors: Juhwan Choi, Youngbin Kim
conference: "Arxiv"
year: 2024
bibkey: choi2024questioning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.06518"}
tags: ['Reinforcement Learning']
---
Large language models (LLMs) have become a dominant approach in natural
language processing, yet their internal knowledge structures remain largely
unexplored. In this paper, we analyze the internal knowledge structures of LLMs
using historical medal tallies from the Olympic Games. We task the models with
providing the medal counts for each team and identifying which teams achieved
specific rankings. Our results reveal that while state-of-the-art LLMs perform
remarkably well in reporting medal counts for individual teams, they struggle
significantly with questions about specific rankings. This suggests that the
internal knowledge structures of LLMs are fundamentally different from those of
humans, who can easily infer rankings from known medal counts. To support
further research, we publicly release our code, dataset, and model outputs.
