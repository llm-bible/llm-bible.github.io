---
layout: publication
title: 'Instruction-vit: Multi-modal Prompts For Instruction Learning In Vit'
authors: Zhenxiang Xiao, Yuzhong Chen, Lu Zhang, Junjie Yao, Zihao Wu, Xiaowei Yu, Yi Pan, Lin Zhao, Chong Ma, Xinyu Liu, Wei Liu, Xiang Li, Yixuan Yuan, Dinggang Shen, Dajiang Zhu, Tianming Liu, Xi Jiang
conference: "Arxiv"
year: 2023
bibkey: xiao2023instruction
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.00201"}
tags: ['Fine-Tuning', 'Transformer', 'Model Architecture', 'Training Techniques', 'Pretraining Methods', 'Prompting']
---
Prompts have been proven to play a crucial role in large language models, and
in recent years, vision models have also been using prompts to improve
scalability for multiple downstream tasks. In this paper, we focus on adapting
prompt design based on instruction tuning into a visual transformer model for
image classification which we called Instruction-ViT. The key idea is to
implement multi-modal prompts (text or image prompt) related to category
information to guide the fine-tuning of the model. Based on the experiments of
several image captionining tasks, the performance and domain adaptability were
improved. Our work provided an innovative strategy to fuse multi-modal prompts
with better performance and faster adaptability for visual classification
models.
