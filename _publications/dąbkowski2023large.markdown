---
layout: publication
title: 'Large Language Models And (non-)linguistic Recursion'
authors: Maksymilian Dąbkowski, Gašper Beguš
conference: "Arxiv"
year: 2023
bibkey: dąbkowski2023large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.07195"}
tags: ['Transformer', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting']
---
Recursion is one of the hallmarks of human language. While many design
features of language have been shown to exist in animal communication systems,
recursion has not. Previous research shows that GPT-4 is the first large
language model (LLM) to exhibit metalinguistic abilities (Begu\v\{s\},
D\k\{a\}bkowski, and Rhodes 2023). Here, we propose several prompt designs aimed
at eliciting and analyzing recursive behavior in LLMs, both linguistic and
non-linguistic. We demonstrate that when explicitly prompted, GPT-4 can both
produce and analyze recursive structures. Thus, we present one of the first
studies investigating whether meta-linguistic awareness of recursion -- a
uniquely human cognitive property -- can emerge in transformers with a high
number of parameters such as GPT-4.
