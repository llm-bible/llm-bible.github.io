---
layout: publication
title: 'KM-BART: Knowledge Enhanced Multimodal BART For Visual Commonsense Generation'
authors: Yiran Xing et al.
conference: Arxiv
year: 2021
citations: 18
bibkey: xing2021km
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2101.00419'}]
tags: [Transformer, RAG, Multimodal Models, Pre-Training]
---
We present Knowledge Enhanced Multimodal BART (KM-BART), which is a
Transformer-based sequence-to-sequence model capable of reasoning about
commonsense knowledge from multimodal inputs of images and texts. We adapt the
generative BART architecture to a multimodal model with visual and textual
inputs. We further develop novel pretraining tasks to improve the model
performance on the Visual Commonsense Generation (VCG) task. In particular, our
pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model
performance on the VCG task by leveraging commonsense knowledge from a large
language model pretrained on external commonsense knowledge graphs. To the best
of our knowledge, we are the first to propose a dedicated task for improving
model performance on the VCG task. Experimental results show that our model
reaches state-of-the-art performance on the VCG task by applying these novel
pretraining tasks.