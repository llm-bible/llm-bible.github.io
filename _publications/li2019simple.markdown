---
layout: publication
title: Visualbert A Simple And Performant Baseline For Vision And Language
authors: Li Liunian Harold, Yatskar Mark, Yin Da, Hsieh Cho-jui, Chang Kai-wei
conference: "Arxiv"
year: 2019
bibkey: li2019simple
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1908.03557"}
tags: ['Attention Mechanism', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Tools', 'Training Techniques', 'Transformer']
---
We propose VisualBERT a simple and flexible framework for modeling a broad range of vision45;and45;language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self45;attention. We further propose two visually45;grounded language model objectives for pre45;training VisualBERT on image caption data. Experiments on four vision45;and45;language tasks including VQA VCR NLVR2 and Flickr30K show that VisualBERT outperforms or rivals with state45;of45;the45;art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships tracking for example associations between verbs and image regions corresponding to their arguments.
