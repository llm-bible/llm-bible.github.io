---
layout: publication
title: Datavist5 A Pre45;trained Language Model For Jointly Understanding Text And Data Visualization
authors: Wan Zhuoyue, Song Yuanfeng, Li Shuaimin, Zhang Chen Jason, Wong Raymond Chi-wing
conference: "Arxiv"
year: 2024
bibkey: wan2024pre
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.07401"}
tags: ['Applications', 'BERT', 'Efficiency And Optimization', 'Interpretability And Explainability', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
Data visualization (DV) is the fundamental and premise tool to improve the efficiency in conveying the insights behind the big data which has been widely accepted in existing data45;driven world. Task automation in DV such as converting natural language queries to visualizations (i.e. text45;to45;vis) generating explanations from visualizations (i.e. vis45;to45;text) answering DV45;related questions in free form (i.e. FeVisQA) and explicating tabular data (i.e. table45;to45;text) is vital for advancing the field. Despite their potential the application of pre45;trained language models (PLMs) like T5 and BERT in DV has been limited by high costs and challenges in handling cross45;modal information leading to few studies on PLMs for DV. We introduce textbf123;DataVisT5125; a novel PLM tailored for DV that enhances the T5 architecture through a hybrid objective pre45;training and multi45;task fine45;tuning strategy integrating text and DV datasets to effectively interpret cross45;modal semantics. Extensive evaluations on public datasets show that DataVisT5 consistently outperforms current state45;of45;the45;art models on various DV45;related tasks. We anticipate that DataVisT5 will not only inspire further research on vertical PLMs but also expand the range of applications for PLMs.
