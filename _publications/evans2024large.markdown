---
layout: publication
title: "Large Language Models As Evaluators For Scientific Synthesis"
authors: Evans Julia, D'souza Jennifer, Auer SÃ¶ren
conference: "Arxiv"
year: 2024
bibkey: evans2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.02977"}
tags: ['GPT', 'Interpretability And Explainability', 'Model Architecture', 'Pretraining Methods']
---
Our study explores how well the state-of-the-art Large Language Models (LLMs) like GPT-4 and Mistral can assess the quality of scientific summaries or more fittingly scientific syntheses comparing their evaluations to those of human annotators. We used a dataset of 100 research questions and their syntheses made by GPT-4 from abstracts of five related papers checked against human quality ratings. The study evaluates both the closed-source GPT-4 and the open-source Mistral models ability to rate these summaries and provide reasons for their judgments. Preliminary results show that LLMs can offer logical explanations that somewhat match the quality ratings yet a deeper statistical analysis shows a weak correlation between LLM and human ratings suggesting the potential and current limitations of LLMs in scientific synthesis evaluation.
