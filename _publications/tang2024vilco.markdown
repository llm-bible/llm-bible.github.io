---
layout: publication
title: Vilco45;bench Video Language Continual Learning Benchmark
authors: Tang Tianqi, Deldari Shohreh, Xue Hao, De Melo Celso, Salim Flora D.
conference: "Arxiv"
year: 2024
bibkey: tang2024vilco
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.13123"}
  - {name: "Code", url: "https://github.com/cruiseresearchgroup/ViLCo"}
tags: ['Has Code', 'Pretraining Methods', 'Tools']
---
Video language continual learning involves continuously adapting to information from video and text inputs enhancing a models ability to handle new tasks while retaining prior knowledge. This field is a relatively under45;explored area and establishing appropriate datasets is crucial for facilitating communication and research in this field. In this study we present the first dedicated benchmark ViLCo45;Bench designed to evaluate continual learning models across a range of video45;text tasks. The dataset comprises ten45;minute45;long videos and corresponding language queries collected from publicly available datasets. Additionally we introduce a novel memory45;efficient framework that incorporates self45;supervised learning and mimics long45;term and short45;term memory effects. This framework addresses challenges including memory complexity from long video clips natural language complexity from open queries and text45;video misalignment. We posit that ViLCo45;Bench with greater complexity compared to existing continual learning benchmarks would serve as a critical tool for exploring the video45;language domain extending beyond conventional class45;incremental tasks and addressing complex and limited annotation issues. The curated data evaluations and our novel method are available at https://github.com/cruiseresearchgroup/ViLCo .
