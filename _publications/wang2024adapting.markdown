---
layout: publication
title: Adapting Open45;source Large Language Models For Cost45;effective Expert45;level Clinical Note Generation With On45;policy Reinforcement Learning
authors: Wang Hanyin, Gao Chufan, Liu Bolun, Xu Qiping, Hussein Guleid, Labban Mohamad El, Iheasirim Kingsley, Korsapati Hariprasad, Outcalt Chuck, Sun Jimeng
conference: "Arxiv"
year: 2024
bibkey: wang2024adapting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.00715"}
tags: ['Agentic', 'Applications', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
Proprietary Large Language Models (LLMs) such as GPT45;4 and Gemini have demonstrated promising capabilities in clinical text summarization tasks. However due to patient data privacy concerns and computational costs many healthcare providers prefer using small locally45;hosted models over external generic LLMs. This study presents a comprehensive domain45; and task45;specific adaptation process for the open45;source LLaMA45;2 13 billion parameter model enabling it to generate high45;quality clinical notes from outpatient patient45;doctor dialogues. Our process incorporates continued pre45;training supervised fine45;tuning and reinforcement learning from both AI and human feedback. We introduced a new approach DistillDirect for performing on45;policy reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting model LLaMA45;Clinic can generate clinical notes comparable in quality to those authored by physicians. In a blinded physician reader study the majority (90.437;) of individual evaluations rated the notes generated by LLaMA45;Clinic as acceptable or higher across all three criteria real45;world readiness completeness and accuracy. In the more challenging Assessment and Plan section LLaMA45;Clinic scored higher (4.2/5) in real45;world readiness than physician45;authored notes (4.1/5). Our cost analysis for inference shows that our LLaMA45;Clinic model achieves a 3.7545;fold cost reduction compared to an external generic LLM service. Additionally we highlight key considerations for future clinical note45;generation tasks emphasizing the importance of pre45;defining a best45;practice note format rather than relying on LLMs to determine this for clinical practice. We have made our newly created synthetic clinic dialogue45;note dataset and the physician feedback dataset publicly available to foster future research.
