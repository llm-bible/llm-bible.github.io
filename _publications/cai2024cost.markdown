---
layout: publication
title: CERET Cost45;effective Extrinsic Refinement For Text Generation
authors: Cai Jason, Su Hang, Sunkara Monica, Shalyminov Igor, Mansour Saab
conference: "Arxiv"
year: 2024
bibkey: cai2024cost
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.05588"}
tags: ['Applications', 'Language Modeling']
---
Large Language Models (LLMs) are powerful models for generation tasks but they may not generate good quality outputs in their first attempt. Apart from model fine45;tuning existing approaches to improve prediction accuracy and quality typically involve LLM self45;improvement / self45;reflection that incorporate feedback from models themselves. Despite their effectiveness these methods are hindered by their high computational cost and lack of scalability. In this work we propose CERET a method for refining text generations by considering semantic stability entailment and inter45;sample uncertainty measures. Experimental results show that CERET outperforms Self45;consistency and Self45;rerank baselines consistently under various task setups by ~1.637; in Rouge45;1 for abstractive summarization and ~3.537; in hit rate for question answering. Compared to LLM Self45;rerank method our approach only requires 9.437; of its latency and is more cost45;effective.
