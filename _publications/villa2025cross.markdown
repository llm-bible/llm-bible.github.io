---
layout: publication
title: 'Cross-examiner: Evaluating Consistency Of Large Language Model-generated Explanations'
authors: Danielle Villa, Maria Chang, Keerthiram Murugesan, Rosario Uceda-sosa, Karthikeyan Natesan Ramamurthy
conference: "Arxiv"
year: 2025
bibkey: villa2025cross
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.08815"}
tags: ['Interpretability and Explainability', 'Interpretability', 'Ethics and Bias']
---
Large Language Models (LLMs) are often asked to explain their outputs to
enhance accuracy and transparency. However, evidence suggests that these
explanations can misrepresent the models' true reasoning processes. One
effective way to identify inaccuracies or omissions in these explanations is
through consistency checking, which typically involves asking follow-up
questions. This paper introduces, cross-examiner, a new method for generating
follow-up questions based on a model's explanation of an initial question. Our
method combines symbolic information extraction with language model-driven
question generation, resulting in better follow-up questions than those
produced by LLMs alone. Additionally, this approach is more flexible than other
methods and can generate a wider variety of follow-up questions.
