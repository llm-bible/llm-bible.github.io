---
layout: publication
title: Prometheus Inducing Fine45;grained Evaluation Capability In Language Models
authors: Kim Seungone, Shin Jamin, Cho Yejin, Jang Joel, Longpre Shayne, Lee Hwaran, Yun Sangdoo, Shin Seongjin, Kim Sungdong, Thorne James, Seo Minjoon
conference: "Arxiv"
year: 2023
bibkey: kim2023inducing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.08491"}
  - {name: "Code", url: "https://kaistai.github.io/prometheus/"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Reinforcement Learning']
---
Recently using a powerful proprietary Large Language Model (LLM) (e.g. GPT45;4) as an evaluator for long45;form responses has become the de facto standard. However for practitioners with large45;scale evaluation tasks and custom criteria in consideration (e.g. child45;readability) using proprietary LLMs as an evaluator is unreliable due to the closed45;source nature uncontrolled versioning and prohibitive costs. In this work we propose Prometheus a fully open45;source LLM that is on par with GPT45;4s evaluation capabilities when the appropriate reference materials (reference answer score rubric) are accompanied. We first construct the Feedback Collection a new dataset that consists of 1K fine45;grained score rubrics 20K instructions and 100K responses and language feedback generated by GPT45;4. Using the Feedback Collection we train Prometheus a 13B evaluator LLM that can assess any given long45;form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics which is on par with GPT45;4 (0.882) and greatly outperforms ChatGPT (0.392). Furthermore measuring correlation with GPT45;4 with 1222 customized score rubrics across four benchmarks (MT Bench Vicuna Bench Feedback Bench Flask Eval) shows similar trends bolstering Prometheuss capability as an evaluator LLM. Lastly Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment amp; MT Bench Human Judgment) compared to open45;sourced reward models explicitly trained on human preference datasets highlighting its potential as an universal reward model. We open45;source our code dataset and model at https://kaistai.github.io/prometheus/.
