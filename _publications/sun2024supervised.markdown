---
layout: publication
title: Supervised Fine45;tuning As Inverse Reinforcement Learning
authors: Sun Hao
conference: "Arxiv"
year: 2024
bibkey: sun2024supervised
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.12017"}
tags: ['Agentic', 'Reinforcement Learning', 'Tools']
---
The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision45;making framework to formulate the problem of aligning LLMs using demonstration datasets. Drawing insights from inverse reinforcement learning and imitation learning we introduce various approaches for divergence minimization in the LLM alignment tasks. Our analysis highlights the mass45;covering and mode45;seeking behaviors of these different approaches. Inclusively we examine the pros and cons of the classical supervised fine45;tuning method elaborating on scenarios where different methods shine.
