---
layout: publication
title: 'Interactive Agents To Overcome Ambiguity In Software Engineering'
authors: Sanidhya Vijayvargiya, Xuhui Zhou, Akhila Yerukola, Maarten Sap, Graham Neubig
conference: "Arxiv"
year: 2025
bibkey: vijayvargiya2025interactive
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.13069'}
tags: ['Agentic', 'RAG', 'Responsible AI', 'Applications']
---
AI agents are increasingly being deployed to automate tasks, often based on
ambiguous and underspecified user instructions. Making unwarranted assumptions
and failing to ask clarifying questions can lead to suboptimal outcomes, safety
risks due to tool misuse, and wasted computational resources. In this work, we
study the ability of LLM agents to handle ambiguous instructions in interactive
code generation settings by evaluating proprietary and open-weight models on
their performance across three key steps: (a) leveraging interactivity to
improve performance in ambiguous scenarios, (b) detecting ambiguity, and (c)
asking targeted questions. Our findings reveal that models struggle to
distinguish between well-specified and underspecified instructions. However,
when models interact for underspecified inputs, they effectively obtain vital
information from the user, leading to significant improvements in performance
and underscoring the value of effective interaction. Our study highlights
critical gaps in how current state-of-the-art models handle ambiguity in
complex software engineering tasks and structures the evaluation into distinct
steps to enable targeted improvements.
