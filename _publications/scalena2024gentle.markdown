---
layout: publication
title: 'A Gentle Push Funziona Benissimo: Making Instructed Models In Italian Via Contrastive Activation Steering'
authors: Daniel Scalena, Elisabetta Fersini, Malvina Nissim
conference: "Arxiv"
year: 2024
bibkey: scalena2024gentle
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.18247"}
tags: ['Fine-Tuning', 'Training Techniques', 'Pre-Training', 'Pretraining Methods']
---
Adapting models to a language that was only partially present in the
pre-training data requires fine-tuning, which is expensive in terms of both
data and computational resources. As an alternative to fine-tuning, we explore
the potential of activation steering-based techniques to enhance model
performance on Italian tasks. Through our experiments we show that Italian
steering (i) can be successfully applied to different models, (ii) achieves
performances comparable to, or even better than, fine-tuned models for Italian,
and (iii) yields higher quality and consistency in Italian generations. We also
discuss the utility of steering and fine-tuning in the contemporary LLM
landscape where models are anyway getting high Italian performances even if not
explicitly trained in this language.
