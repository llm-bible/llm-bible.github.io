---
layout: publication
title: A Review Of Repository Level Prompting For Llms
authors: Schonholtz Douglas
conference: "Arxiv"
year: 2023
bibkey: schonholtz2023review
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.10101"}
tags: ['Applications', 'Prompting', 'Survey Paper', 'Tools']
---
As coding challenges become more complex recent advancements in Large Language Models (LLMs) have led to notable successes such as achieving a 94.637; solve rate on the HumanEval benchmark. Concurrently there is an increasing commercial push for repository45;level inline code completion tools such as GitHub Copilot and Tab Nine aimed at enhancing developer productivity. This paper delves into the transition from individual coding problems to repository45;scale solutions presenting a thorough review of the current literature on effective LLM prompting for code generation at the repository level. We examine approaches that will work with black45;box LLMs such that they will be useful and applicable to commercial use cases and their applicability in interpreting code at a repository scale. We juxtapose the Repository45;Level Prompt Generation technique with RepoCoder an iterative retrieval and generation method to highlight the trade45;offs inherent in each approach and to establish best practices for their application in cutting45;edge coding benchmarks. The interplay between iterative refinement of prompts and the development of advanced retrieval systems forms the core of our discussion offering a pathway to significantly improve LLM performance in code generation tasks. Insights from this study not only guide the application of these methods but also chart a course for future research to integrate such techniques into broader software engineering contexts.
