---
layout: publication
title: Towards Reasoning-Aware Explainable VQA
authors: Vaideeswaran Rakesh, Gao Feng, Mathur Abhinav, Thattai Govind
conference: "Arxiv"
year: 2022
bibkey: vaideeswaran2022towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.05190"}
tags: ['Applications', 'Attention Mechanism', 'Interpretability And Explainability', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Tools', 'Transformer']
---
The domain of joint vision-language understanding especially in the context of reasoning in Visual Question Answering (VQA) models has garnered significant attention in the recent past. While most of the existing VQA models focus on improving the accuracy of VQA the way models arrive at an answer is oftentimes a black box. As a step towards making the VQA task more explainable and interpretable our method is built upon the SOTA VQA framework by augmenting it with an end-to-end explanation generation module. In this paper we investigate two network architectures including Long Short-Term Memory (LSTM) and Transformer decoder as the explanation generator. Our method generates human-readable textual explanations while maintaining SOTA VQA accuracy on the GQA-REX (77.49) and VQA-E (71.48) datasets. Approximately 65.16 of the generated explanations are approved by humans as valid. Roughly 60.5 of the generated explanations are valid and lead to the correct answers.
