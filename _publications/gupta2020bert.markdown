---
layout: publication
title: BERT Based Multilingual Machine Comprehension In English And Hindi
authors: Gupta Somil, Khade Nilesh
conference: "Arxiv"
year: 2020
bibkey: gupta2020bert
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2006.01432"}
tags: ['Applications', 'BERT', 'Model Architecture']
---
Multilingual Machine Comprehension (MMC) is a Question45;Answering (QA) sub45;task that involves quoting the answer for a question from a given snippet where the question and the snippet can be in different languages. Recently released multilingual variant of BERT (m45;BERT) pre45;trained with 104 languages has performed well in both zero45;shot and fine45;tuned settings for multilingual tasks; however it has not been used for English45;Hindi MMC yet. We therefore present in this article our experiments with m45;BERT for MMC in zero45;shot mono45;lingual (e.g. Hindi Question45;Hindi Snippet) and cross45;lingual (e.g. English QuestionHindi Snippet) fine45;tune setups. These model variants are evaluated on all possible multilingual settings and results are compared against the current state45;of45;the45;art sequential QA system for these languages. Experiments show that m45;BERT with fine45;tuning improves performance on all evaluation settings across both the datasets used by the prior model therefore establishing m45;BERT based MMC as the new state45;of45;the45;art for English and Hindi. We also publish our results on an extended version of the recently released XQuAD dataset which we propose to use as the evaluation benchmark for future research.
