---
layout: publication
title: 'Pangu Embedded: An Efficient Dual-system LLM Reasoner With Metacognition'
authors: Hanting And Other Contributors Chen, Yasheng And Other Contributors Wang, Kai And Other Contributors Han, Dong And Other Contributors Li, Lin And Other Contributors Li, Zhenni And Other Contributors Bi, Jinpeng And Other Contributors Li, Haoyu And Other Contributors Wang, Fei And Other Contributors Mi, Mingjian And Other Contributors Zhu, Bin And Other Contributors Wang, Kaikai And Other Contributors Song, Yifei And Other Contributors Fu, Xu And Other Contributors He, Yu And Other Contributors Luo, Chong And Other Contributors Zhu, Quan And Other Contributors He, Xueyu And Other Contributors Wu, Wei And Other Contributors He, Hailin And Other Contributors Hu, Yehui And Other Contributors Tang, Dacheng And Other Contributors Tao, Xinghao And Other Contributors Chen, Yunhe And Other Contributors Wang
conference: "Arxiv"
year: 2025
bibkey: chen2025pangu
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.22375"}
tags: ['Agentic', 'Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Tools', 'Reinforcement Learning', 'Distillation', 'Merging']
---
This work presents Pangu Embedded, an efficient Large Language Model (LLM) reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible fast and slow thinking capabilities. Pangu Embedded addresses the significant computational costs and inference latency challenges prevalent in existing reasoning-optimized LLMs. We propose a two-stage training framework for its construction. In Stage 1, the model is finetuned via an iterative distillation process, incorporating inter-iteration model merging to effectively aggregate complementary knowledge. This is followed by reinforcement learning on Ascend clusters, optimized by a latency-tolerant scheduler that combines stale synchronous parallelism with prioritized data queues. The RL process is guided by a Multi-source Adaptive Reward System (MARS), which generates dynamic, task-specific reward signals using deterministic metrics and lightweight LLM evaluators for mathematics, coding, and general problem-solving tasks. Stage 2 introduces a dual-system framework, endowing Pangu Embedded with a "fast" mode for routine queries and a deeper "slow" mode for complex inference. This framework offers both manual mode switching for user control and an automatic, complexity-aware mode selection mechanism that dynamically allocates computational resources to balance latency and reasoning depth. Experimental results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate that Pangu Embedded with 7B parameters, outperforms similar-size models like Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art reasoning quality within a single, unified model architecture, highlighting a promising direction for developing powerful yet practically deployable LLM reasoners.
