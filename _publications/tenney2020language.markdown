---
layout: publication
title: The Language Interpretability Tool Extensible Interactive Visualizations And Analysis For NLP Models
authors: Tenney Ian, Wexler James, Bastings Jasmijn, Bolukbasi Tolga, Coenen Andy, Gehrmann Sebastian, Jiang Ellen, Pushkarna Mahima, Radebaugh Carey, Reif Emily, Yuan Ann
conference: "Arxiv"
year: 2020
bibkey: tenney2020language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2008.05122"}
  - {name: "Code", url: "https://github.com/pair&#45;code/lit"}
tags: ['Applications', 'Ethics And Bias', 'Fine Tuning', 'Has Code', 'Interpretability And Explainability', 'Language Modeling', 'Reinforcement Learning', 'Tools']
---
We present the Language Interpretability Tool (LIT) an open45;source platform for visualization and understanding of NLP models. We focus on core questions about model behavior Why did my model make this prediction When does it perform poorly What happens under a controlled change in the input LIT integrates local explanations aggregate analysis and counterfactual generation into a streamlined browser45;based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows including exploring counterfactuals for sentiment analysis measuring gender bias in coreference systems and exploring local behavior in text generation. LIT supports a wide range of models45;45;including classification seq2seq and structured prediction45;45;and is highly extensible through a declarative framework45;agnostic API. LIT is under active development with code and full documentation available at https://github.com/pair&#45;code/lit.
