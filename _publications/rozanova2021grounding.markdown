---
layout: publication
title: Grounding Natural Language Instructions&#58; Can Large Language Models Capture Spatial Information?
authors: Rozanova Julia, Ferreira Deborah, Dubba Krishna, Cheng Weiwei, Zhang Dell, Freitas Andre
conference: "Arxiv"
year: 2021
bibkey: rozanova2021grounding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2109.08634"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Models designed for intelligent process automation are required to be capable of grounding user interface elements. This task of interface element grounding is centred on linking instructions in natural language to their target referents. Even though BERT and similar pre-trained language models have excelled in several NLP tasks their use has not been widely explored for the UI grounding domain. This work concentrates on testing and probing the grounding abilities of three different transformer-based models BERT RoBERTa and LayoutLM. Our primary focus is on these models spatial reasoning skills given their importance in this domain. We observe that LayoutLM has a promising advantage for applications in this domain even though it was created for a different original purpose (representing scanned documents) the learned spatial features appear to be transferable to the UI grounding setting especially as they demonstrate the ability to discriminate between target directions in natural language instructions.
