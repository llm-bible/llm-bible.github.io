---
layout: publication
title: MM45;TTS A Unified Framework For Multimodal Prompt45;induced Emotional Text45;to45;speech Synthesis
authors: Li Xiang, Cheng Zhi-qi, He Jun-yan, Peng Xiaojiang, Hauptmann Alexander G.
conference: "Arxiv"
year: 2024
bibkey: li2024mm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.18398"}
  - {name: "Code", url: "https://anonymous.4open.science/r/MMTTS&#45;D214"}
tags: ['Attention Mechanism', 'Has Code', 'Merging', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Prompting', 'RAG', 'Tools']
---
Emotional Text45;to45;Speech (E45;TTS) synthesis has gained significant attention in recent years due to its potential to enhance human45;computer interaction. However current E45;TTS approaches often struggle to capture the complexity of human emotions primarily relying on oversimplified emotional labels or single45;modality inputs. To address these limitations we propose the Multimodal Emotional Text45;to45;Speech System (MM45;TTS) a unified framework that leverages emotional cues from multiple modalities to generate highly expressive and emotionally resonant speech. MM45;TTS consists of two key components (1) the Emotion Prompt Alignment Module (EP45;Align) which employs contrastive learning to align emotional features across text audio and visual modalities ensuring a coherent fusion of multimodal information; and (2) the Emotion Embedding45;Induced TTS (EMI45;TTS) which integrates the aligned emotional embeddings with state45;of45;the45;art TTS models to synthesize speech that accurately reflects the intended emotions. Extensive evaluations across diverse datasets demonstrate the superior performance of MM45;TTS compared to traditional E45;TTS models. Objective metrics including Word Error Rate (WER) and Character Error Rate (CER) show significant improvements on ESD dataset with MM45;TTS achieving scores of 7.3537; and 3.0737; respectively. Subjective assessments further validate that MM45;TTS generates speech with emotional fidelity and naturalness comparable to human speech. Our code and pre45;trained models are publicly available at https://anonymous.4open.science/r/MMTTS&#45;D214
