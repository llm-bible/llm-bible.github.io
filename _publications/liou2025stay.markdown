---
layout: publication
title: 'Stay Hungry, Stay Foolish: On The Extended Reading Articles Generation With Llms'
authors: Yow-fu Liou, Yu-chien Tang, An-zi Yen
conference: "Arxiv"
year: 2025
bibkey: liou2025stay
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.15013"}
tags: ['Fine-Tuning', 'RAG']
---
The process of creating educational materials is both time-consuming and
demanding for educators. This research explores the potential of Large Language
Models (LLMs) to streamline this task by automating the generation of extended
reading materials and relevant course suggestions. Using the TED-Ed Dig Deeper
sections as an initial exploration, we investigate how supplementary articles
can be enriched with contextual knowledge and connected to additional learning
resources. Our method begins by generating extended articles from video
transcripts, leveraging LLMs to include historical insights, cultural examples,
and illustrative anecdotes. A recommendation system employing semantic
similarity ranking identifies related courses, followed by an LLM-based
refinement process to enhance relevance. The final articles are tailored to
seamlessly integrate these recommendations, ensuring they remain cohesive and
informative. Experimental evaluations demonstrate that our model produces
high-quality content and accurate course suggestions, assessed through metrics
such as Hit Rate, semantic similarity, and coherence. Our experimental analysis
highlight the nuanced differences between the generated and existing materials,
underscoring the model's capacity to offer more engaging and accessible
learning experiences. This study showcases how LLMs can bridge the gap between
core content and supplementary learning, providing students with additional
recommended resources while also assisting teachers in designing educational
materials.
