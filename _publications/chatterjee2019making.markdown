---
layout: publication
title: 'Making Neural Machine Reading Comprehension Faster'
authors: Debajyoti Chatterjee
conference: "Arxiv"
year: 2019
bibkey: chatterjee2019making
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1904.00796"}
tags: ['Efficiency and Optimization', 'Model Architecture', 'Distillation', 'BERT', 'Applications']
---
This study aims at solving the Machine Reading Comprehension problem where
questions have to be answered given a context passage. The challenge is to
develop a computationally faster model which will have improved inference time.
State of the art in many natural language understanding tasks, BERT model, has
been used and knowledge distillation method has been applied to train two
smaller models. The developed models are compared with other models which have
been developed with the same intention.
