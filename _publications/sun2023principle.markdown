---
layout: publication
title: Principle45;driven Self45;alignment Of Language Models From Scratch With Minimal Human Supervision
authors: Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan
conference: "Arxiv"
year: 2023
bibkey: sun2023principle
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2305.03047v2"}
tags: ['Agentic', 'Ethics And Bias', 'Fine Tuning', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
Recent AI45;assistant agents such as ChatGPT predominantly rely on supervised fine45;tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions ensuring they are helpful ethical and reliable. However this dependence can significantly constrain the true potential of AI45;assistant agents due to the high cost of obtaining human supervision and the related issues on quality reliability diversity self45;consistency and undesirable biases. To address these challenges we propose a novel approach called SELF45;ALIGN which combines principle45;driven reasoning and the generative power of LLMs for the self45;alignment of AI agents with minimal human supervision. Our approach encompasses four stages first we use an LLM to generate synthetic prompts and a topic45;guided method to augment the prompt diversity; second we use a small set of human45;written principles for AI models to follow and guide the LLM through in45;context learning from demonstrations (of principles application) to produce helpful ethical and reliable responses to users queries; third we fine45;tune the original LLM with the high45;quality self45;aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally we offer a refinement step to address the issues of overly45;brief or indirect responses. Applying SELF45;ALIGN to the LLaMA45;65b base language model we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including < 200 seed prompts 16 generic principles and 5 exemplars for in45;context learning). Dromedary significantly surpasses the performance of several state45;of45;the45;art AI systems including Text45;Davinci45;003 and Alpaca on benchmark datasets with various settings.
