---
layout: publication
title: A Systematic Study And Comprehensive Evaluation Of Chatgpt On Benchmark Datasets
authors: Laskar Md Tahmid Rahman, Bari M Saiful, Rahman Mizanur, Bhuiyan Md Amran Hossen, Joty Shafiq, Huang Jimmy Xiangji
conference: "Arxiv"
year: 2023
bibkey: laskar2023systematic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.18486"}
tags: ['Applications', 'Attention Mechanism', 'Ethics And Bias', 'GPT', 'Model Architecture', 'Reinforcement Learning']
---
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However their evaluation in the benchmark academic datasets remains under45;explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper we aim to present a thorough evaluation of ChatGPTs performance on diverse academic datasets covering tasks like question45;answering text summarization code generation commonsense reasoning mathematical problem45;solving machine translation bias detection and ethical considerations. Specifically we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi45;query instructions that we mostly found in ChatGPT and other instruction45;tuned models. Our extensive evaluation shows that even though ChatGPT is capable of performing a wide variety of tasks and may obtain impressive performance in several benchmark datasets it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of ChatGPTs performance across diverse NLP tasks this paper sets the stage for a targeted deployment of ChatGPT45;like LLMs in real45;world applications.
