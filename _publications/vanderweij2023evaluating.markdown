---
layout: publication
title: 'Evaluating Shutdown Avoidance Of Language Models In Textual Scenarios'
authors: Teun Van Der Weij, Simon Lermen, Leon Lang
conference: "Arxiv"
year: 2023
bibkey: vanderweij2023evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.00787"}
tags: ['Agentic', 'GPT', 'Prompting', 'Model Architecture']
---
Recently, there has been an increase in interest in evaluating large language
models for emergent and dangerous capabilities. Importantly, agents could
reason that in some scenarios their goal is better achieved if they are not
turned off, which can lead to undesirable behaviors. In this paper, we
investigate the potential of using toy textual scenarios to evaluate
instrumental reasoning and shutdown avoidance in language models such as GPT-4
and Claude. Furthermore, we explore whether shutdown avoidance is merely a
result of simple pattern matching between the dataset and the prompt or if it
is a consistent behaviour across different environments and variations.
  We evaluated behaviours manually and also experimented with using language
models for automatic evaluations, and these evaluations demonstrate that simple
pattern matching is likely not the sole contributing factor for shutdown
avoidance. This study provides insights into the behaviour of language models
in shutdown avoidance scenarios and inspires further research on the use of
textual scenarios for evaluations.
