---
layout: publication
title: 'Pragmatic Metacognitive Prompting Improves LLM Performance On Sarcasm Detection'
authors: Joshua Lee, Wyatt Fong, Alexander Le, Sur Shah, Kevin Han, Kevin Zhu
conference: "Arxiv"
year: 2024
bibkey: lee2024pragmatic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.04509"}
tags: ['Prompting', 'RAG', 'Model Architecture', 'GPT']
---
Sarcasm detection is a significant challenge in sentiment analysis due to the
nuanced and context-dependent nature of verbiage. We introduce Pragmatic
Metacognitive Prompting (PMP) to improve the performance of Large Language
Models (LLMs) in sarcasm detection, which leverages principles from pragmatics
and reflection helping LLMs interpret implied meanings, consider contextual
cues, and reflect on discrepancies to identify sarcasm. Using state-of-the-art
LLMs such as LLaMA-3-8B, GPT-4o, and Claude 3.5 Sonnet, PMP achieves
state-of-the-art performance on GPT-4o on MUStARD and SemEval2018. This study
demonstrates that integrating pragmatic reasoning and metacognitive strategies
into prompting significantly enhances LLMs' ability to detect sarcasm, offering
a promising direction for future research in sentiment analysis.
