---
layout: publication
title: "Examining The Emergence Of Deductive Reasoning In Generative Language Models"
authors: Belcak Peter, Lanzend√∂rfer Luca A., Wattenhofer Roger
conference: "Arxiv"
year: 2023
bibkey: belcak2023examining
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.01009"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
We conduct a preliminary inquiry into the ability of generative transformer models to deductively reason from premises provided. We observe notable differences in the performance of models coming from different training setups and find that the deductive reasoning ability increases with scale. Further we discover that the performance generally does not decrease with the length of the deductive chain needed to reach the conclusion with the exception of OpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of transformer-decoder models ranging from 117 million to 175 billion parameters in size.
