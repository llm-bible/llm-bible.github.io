---
layout: publication
title: 'Examining The Emergence Of Deductive Reasoning In Generative Language Models'
authors: Peter Belcak, Luca A. Lanzend√∂rfer, Roger Wattenhofer
conference: "Arxiv"
year: 2023
bibkey: belcak2023examining
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2306.01009'}
tags: ['Transformer', 'Training Techniques', 'Model Architecture', 'GPT', 'Pretraining Methods']
---
We conduct a preliminary inquiry into the ability of generative transformer
models to deductively reason from premises provided. We observe notable
differences in the performance of models coming from different training setups
and find that the deductive reasoning ability increases with scale. Further, we
discover that the performance generally does not decrease with the length of
the deductive chain needed to reach the conclusion, with the exception of
OpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of
transformer-decoder models, ranging from 117 million to 175 billion parameters
in size.
