---
layout: publication
title: 'Zero- And Few-shot Prompting With Llms: A Comparative Study With Fine-tuned Models For Bangla Sentiment Analysis'
authors: Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori
conference: "Arxiv"
year: 2023
bibkey: hasan2023zero
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.10783"}
tags: ['Model Architecture', 'Few-Shot', 'Tools', 'Reinforcement Learning', 'GPT', 'Pretraining Methods', 'Fine-Tuning', 'Transformer', 'Prompting', 'Applications', 'In-Context Learning']
---
The rapid expansion of the digital world has propelled sentiment analysis
into a critical tool across diverse sectors such as marketing, politics,
customer service, and healthcare. While there have been significant
advancements in sentiment analysis for widely spoken languages, low-resource
languages, such as Bangla, remain largely under-researched due to resource
constraints. Furthermore, the recent unprecedented performance of Large
Language Models (LLMs) in various applications highlights the need to evaluate
them in the context of low-resource languages. In this study, we present a
sizeable manually annotated dataset encompassing 33,606 Bangla news tweets and
Facebook comments. We also investigate zero- and few-shot in-context learning
with several language models, including Flan-T5, GPT-4, and Bloomz, offering a
comparative analysis against fine-tuned models. Our findings suggest that
monolingual transformer-based models consistently outperform other models, even
in zero and few-shot scenarios. To foster continued exploration, we intend to
make this dataset and our research tools publicly available to the broader
research community.
