---
layout: publication
title: Zero45; And Few45;shot Prompting With Llms A Comparative Study With Fine45;tuned Models For Bangla Sentiment Analysis
authors: Hasan Md. Arid, Das Shudipta, Anjum Afiyat, Alam Firoj, Anjum Anika, Sarker Avijit, Noori Sheak Rashed Haider
conference: "Arxiv"
year: 2023
bibkey: hasan2023zero
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.10783"}
tags: ['Applications', 'Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Tools', 'Transformer']
---
The rapid expansion of the digital world has propelled sentiment analysis into a critical tool across diverse sectors such as marketing politics customer service and healthcare. While there have been significant advancements in sentiment analysis for widely spoken languages low45;resource languages such as Bangla remain largely under45;researched due to resource constraints. Furthermore the recent unprecedented performance of Large Language Models (LLMs) in various applications highlights the need to evaluate them in the context of low45;resource languages. In this study we present a sizeable manually annotated dataset encompassing 33606 Bangla news tweets and Facebook comments. We also investigate zero45; and few45;shot in45;context learning with several language models including Flan45;T5 GPT45;4 and Bloomz offering a comparative analysis against fine45;tuned models. Our findings suggest that monolingual transformer45;based models consistently outperform other models even in zero and few45;shot scenarios. To foster continued exploration we intend to make this dataset and our research tools publicly available to the broader research community.
