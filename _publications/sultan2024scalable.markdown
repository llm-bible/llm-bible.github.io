---
layout: publication
title: Parallelparc A Scalable Pipeline For Generating Natural45;language Analogies
authors: Sultan Oren, Bitton Yonatan, Yosef Ron, Shahaf Dafna
conference: "Arxiv"
year: 2024
bibkey: sultan2024scalable
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.01139"}
tags: ['Merging', 'Pretraining Methods', 'RAG', 'Training Techniques']
---
Analogy45;making is central to human cognition allowing us to adapt to novel situations 45;45; an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g. word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work we design a data generation pipeline ParallelPARC (Parallel Paragraph Creator) leveraging state45;of45;the45;art Large Language Models (LLMs) to create complex paragraph45;based analogies as well as distractors both simple and challenging. We demonstrate our pipeline and create ProPara45;Logy a dataset of analogies between scientific processes. We publish a gold45;set validated by humans and a silver45;set generated automatically. We test LLMs and humans analogy recognition in binary and multiple45;choice settings and found that humans outperform the best models (~1337; gap) after a light supervision. We demonstrate that our silver45;set is useful for training models. Lastly we show challenging distractors confuse LLMs but not humans. We hope our pipeline will encourage research in this emerging field.
