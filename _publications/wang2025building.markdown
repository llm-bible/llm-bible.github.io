---
layout: publication
title: 'Building A Unified Ai-centric Language System: Analysis, Framework And Future Work'
authors: Edward Hong Wang, Cynthia Xin Wen
conference: "Arxiv"
year: 2025
bibkey: wang2025building
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.04488"}
tags: ['Transformer', 'Tools', 'Ethics and Bias', 'Bias Mitigation', 'Model Architecture', 'Training Techniques', 'Attention Mechanism', 'Pretraining Methods', 'Fairness']
---
Recent advancements in large language models have demonstrated that extended
inference through techniques can markedly improve performance, yet these gains
come with increased computational costs and the propagation of inherent biases
found in natural languages. This paper explores the design of a unified
AI-centric language system that addresses these challenges by offering a more
concise, unambiguous, and computationally efficient alternative to traditional
human languages. We analyze the limitations of natural language such as gender
bias, morphological irregularities, and contextual ambiguities and examine how
these issues are exacerbated within current Transformer architectures, where
redundant attention heads and token inefficiencies prevail. Drawing on insights
from emergent artificial communication systems and constructed languages like
Esperanto and Lojban, we propose a framework that translates diverse natural
language inputs into a streamlined AI-friendly language, enabling more
efficient model training and inference while reducing memory footprints.
Finally, we outline a pathway for empirical validation through controlled
experiments, paving the way for a universal interchange format that could
revolutionize AI-to-AI and human-to-AI interactions by enhancing clarity,
fairness, and overall performance.
