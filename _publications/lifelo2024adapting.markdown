---
layout: publication
title: Adapting Mental Health Prediction Tasks For Cross45;lingual Learning Via Meta45;training And In45;context Learning With Large Language Model
authors: Lifelo Zita, Ning Huansheng, Dhelim Sahraoui
conference: "Arxiv"
year: 2024
bibkey: lifelo2024adapting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.09045"}
tags: ['BERT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG', 'Tools', 'Training Techniques']
---
Timely identification is essential for the efficient handling of mental health illnesses such as depression. However the current research fails to adequately address the prediction of mental health conditions from social media data in low45;resource African languages like Swahili. This study introduces two distinct approaches utilising model45;agnostic meta45;learning and leveraging large language models (LLMs) to address this gap. Experiments are conducted on three datasets translated to low45;resource language and applied to four mental health tasks which include stress depression depression severity and suicidal ideation prediction. we first apply a meta45;learning model with self45;supervision which results in improved model initialisation for rapid adaptation and cross45;lingual transfer. The results show that our meta45;trained model performs significantly better than standard fine45;tuning methods outperforming the baseline fine45;tuning in macro F1 score with 1837; and 0.837; over XLM45;R and mBERT. In parallel we use LLMs in45;context learning capabilities to assess their performance accuracy across the Swahili mental health prediction tasks by analysing different cross45;lingual prompting approaches. Our analysis showed that Swahili prompts performed better than cross45;lingual prompts but less than English prompts. Our findings show that in45;context learning can be achieved through cross45;lingual transfer through carefully crafted prompt templates with examples and instructions.
