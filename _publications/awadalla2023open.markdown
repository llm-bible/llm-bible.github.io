---
layout: publication
title: Openflamingo An Open45;source Framework For Training Large Autoregressive Vision45;language Models
authors: Awadalla Anas, Gao Irena, Gardner Josh, Hessel Jack, Hanafy Yusuf, Zhu Wanrong, Marathe Kalyani, Bitton Yonatan, Gadre Samir, Sagawa Shiori, Jitsev Jenia, Kornblith Simon, Koh Pang Wei, Ilharco Gabriel, Wortsman Mitchell, Schmidt Ludwig
conference: "Arxiv"
year: 2023
bibkey: awadalla2023open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.01390"}
  - {name: "Code", url: "https://github.com/mlfoundations/open&#95;flamingo"}
tags: ['GPT', 'Has Code', 'Pretraining Methods', 'RAG', 'Tools', 'Training Techniques']
---
We introduce OpenFlamingo a family of autoregressive vision45;language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open45;source replication of DeepMinds Flamingo models. On seven vision45;language datasets OpenFlamingo models average between 80 45; 8937; of corresponding Flamingo performance. This technical report describes our models training data hyperparameters and evaluation suite. We share our models and code at https://github.com/mlfoundations/open&#95;flamingo.
