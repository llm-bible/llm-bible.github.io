---
layout: publication
title: 'Openflamingo: An Open-source Framework For Training Large Autoregressive Vision-language Models'
authors: Awadalla Anas, Gao Irena, Gardner Josh, Hessel Jack, Hanafy Yusuf, Zhu Wanrong, Marathe Kalyani, Bitton Yonatan, Gadre Samir, Sagawa Shiori, Jitsev Jenia, Kornblith Simon, Koh Pang Wei, Ilharco Gabriel, Wortsman Mitchell, Schmidt Ludwig
conference: "Arxiv"
year: 2023
bibkey: awadalla2023open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.01390"}
  - {name: "Code", url: "https://github.com/mlfoundations/open_flamingo"}
tags: ['GPT', 'Has Code', 'Multimodal Models', 'Pretraining Methods', 'RAG', 'Tools', 'Training Techniques']
---
'We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind''s Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89&#37; of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at https://github.com/mlfoundations/open\_flamingo.'
