---
layout: publication
title: Adapting Llms To Hebrew Unveiling Dictalm 2.0 With Enhanced Vocabulary And Instruction Capabilities
authors: Shmidman Shaltiel, Shmidman Avi, Cohen Amir Dn, Koppel Moshe
conference: "Arxiv"
year: 2024
bibkey: shmidman2024adapting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.07080"}
tags: ['Applications', 'RAG', 'Tools', 'Training Techniques']
---
Training large language models (LLMs) in low45;resource languages such as Hebrew poses unique challenges. In this paper we introduce DictaLM2.0 and DictaLM2.045;Instruct two LLMs derived from the Mistral model trained on a substantial corpus of approximately 200 billion tokens in both Hebrew and English. Adapting a pre45;trained model to a new language involves specialized techniques that differ significantly from training a model from scratch or further training existing models on well45;resourced languages such as English. We outline these novel training methodologies which facilitate effective learning and adaptation to the linguistic properties of Hebrew. Additionally we fine45;tuned DictaLM2.045;Instruct on a comprehensive instruct dataset to enhance its performance on task45;specific instructions. To rigorously evaluate our models we introduce a new benchmark suite for Hebrew LLM evaluation covering a diverse set of tasks including Question Answering Sentiment Analysis Winograd Schema Challenge Translation and Summarization. Our work not only addresses the intricacies of training LLMs in low45;resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non45;English languages contributing to the broader field of multilingual NLP.
