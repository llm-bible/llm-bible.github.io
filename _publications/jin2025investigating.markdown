---
layout: publication
title: 'Investigating Large Language Models In Diagnosing Students'' Cognitive Skills In Math Problem-solving'
authors: Hyoungwook Jin, Yoonsu Kim, Dongyun Jung, Seungju Kim, Kiyoon Choi, Jinho Son, Juho Kim
conference: "Arxiv"
year: 2025
bibkey: jin2025investigating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.00843'}
tags: ['Uncategorized']
---
Mathematics learning entails mastery of both content knowledge and cognitive
processing of knowing, applying, and reasoning with it. Automated math
assessment primarily has focused on grading students' exhibition of content
knowledge by finding textual evidence, such as specific numbers, formulas, and
statements. Recent advancements in problem-solving, image recognition, and
reasoning capabilities of large language models (LLMs) show promise for nuanced
evaluation of students' cognitive skills. Diagnosing cognitive skills needs to
infer students' thinking processes beyond textual evidence, which is an
underexplored task in LLM-based automated assessment. In this work, we
investigate how state-of-the-art LLMs diagnose students' cognitive skills in
mathematics. We constructed MathCog, a novel benchmark dataset comprising 639
student responses to 110 expert-curated middle school math problems, each
annotated with detailed teachers' diagnoses based on cognitive skill
checklists. Using MathCog, we evaluated 16 closed and open LLMs of varying
model sizes and vendors. Our evaluation reveals that even the state-of-the-art
LLMs struggle with the task, all F1 scores below 0.5, and tend to exhibit
strong false confidence for incorrect cases (\\(r_s=.617\\)). We also found that
model size positively correlates with the diagnosis performance (\\(r_s=.771\\)).
Finally, we discuss the implications of these findings, the overconfidence
issue, and directions for improving automated cognitive skill diagnosis.
