---
layout: publication
title: Cross45;attention Is All You Need Adapting Pretrained Transformers For Machine Translation
authors: Gheini Mozhdeh, Ren Xiang, May Jonathan
conference: "Arxiv"
year: 2021
bibkey: gheini2021cross
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2104.08771"}
tags: ['Applications', 'Attention Mechanism', 'Fine Tuning', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
We study the power of cross45;attention in the Transformer architecture within the context of transfer learning for machine translation and extend the findings of studies into cross45;attention when training from scratch. We conduct a series of experiments through fine45;tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine45;tuning only the cross45;attention parameters is nearly as effective as fine45;tuning all parameters (i.e. the entire translation model). We provide insights into why this is the case and observe that limiting fine45;tuning in this manner yields cross45;lingually aligned embeddings. The implications of this finding for researchers and practitioners include a mitigation of catastrophic forgetting the potential for zero45;shot translation and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead.
