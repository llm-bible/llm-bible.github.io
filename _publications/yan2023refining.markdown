---
layout: publication
title: Refining The Responses Of Llms By Themselves
authors: Yan Tianqiang, Xu Tiansheng
conference: "Arxiv"
year: 2023
bibkey: yan2023refining
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.04039"}
tags: ['Efficiency And Optimization', 'GPT', 'Model Architecture', 'Prompting', 'RAG', 'Tools']
---
In this paper we propose a simple yet efficient approach based on prompt engineering that leverages the large language model itself to optimize its answers without relying on auxiliary models. We introduce an iterative self45;evaluating optimization mechanism with the potential for improved output quality as iterations progress removing the need for manual intervention. The experiments findings indicate that utilizing our response refinement framework on the GPT45;3.5 model yields results that are on par with or even surpass those generated by the cutting45;edge GPT45;4 model. Detailed implementation strategies and illustrative examples are provided to demonstrate the superiority of our proposed solution.
