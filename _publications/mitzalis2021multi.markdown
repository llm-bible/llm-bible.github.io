---
layout: publication
title: BERTGEN Multi45;task Generation Through BERT
authors: Mitzalis Faidon, Caglayan Ozan, Madhyastha Pranava, Specia Lucia
conference: "Arxiv"
year: 2021
bibkey: mitzalis2021multi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2106.03484"}
tags: ['Applications', 'BERT', 'Ethics And Bias', 'Model Architecture', 'Multimodal Models']
---
We present BERTGEN a novel generative decoder45;only model which extends BERT by fusing multimodal and multilingual pretrained models VL45;BERT and M45;BERT respectively. BERTGEN is auto45;regressively trained for language generation tasks namely image captioning machine translation and multimodal machine translation under a multitask setting. With a comprehensive set of evaluations we show that BERTGEN outperforms many strong baselines across the tasks explored. We also show BERTGENs ability for zero45;shot language generation where it exhibits competitive performance to supervised counterparts. Finally we conduct ablation studies which demonstrate that BERTGEN substantially benefits from multi45;tasking and effectively transfers relevant inductive biases from the pre45;trained models.
