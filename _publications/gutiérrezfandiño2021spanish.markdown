---
layout: publication
title: 'Maria: Spanish Language Models'
authors: "Asier Guti\xE9rrez-fandi\xF1o et al."
conference: Procesamiento del Lenguaje Natural v. 68 p. 39-60 mar. 2022. ISSN 1989-7553
year: 2021
citations: 56
bibkey: "guti\xE9rrezfandi\xF1o2021spanish"
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2107.07253'}]
tags: [GPT, BERT, Pre-Training, Model Architecture]
---
This work presents MarIA, a family of Spanish language models and associated
resources made available to the industry and the research community. Currently,
MarIA includes RoBERTa-base, RoBERTa-large, GPT2 and GPT2-large Spanish
language models, which can arguably be presented as the largest and most
proficient language models in Spanish. The models were pretrained using a
massive corpus of 570GB of clean and deduplicated texts with 135 billion words
extracted from the Spanish Web Archive crawled by the National Library of Spain
between 2009 and 2019. We assessed the performance of the models with nine
existing evaluation datasets and with a novel extractive Question Answering
dataset created ex novo. Overall, MarIA models outperform the existing Spanish
models across a variety of NLU tasks and training settings.