---
layout: publication
title: Semantics45;aware Attention Improves Neural Machine Translation
authors: Slobodkin Aviv, Choshen Leshem, Abend Omri
conference: "Arxiv"
year: 2021
bibkey: slobodkin2021semantics
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2110.06920"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
The integration of syntactic structures into Transformer machine translation has shown positive results but to our knowledge no work has attempted to do so with semantic structures. In this work we propose two novel parameter45;free methods for injecting semantic information into Transformers both rely on semantics45;aware masking of (some of) the attention heads. One such method operates on the encoder through a Scene45;Aware Self45;Attention (SASA) head. Another on the decoder through a Scene45;Aware Cross45;Attention (SACrA) head. We show a consistent improvement over the vanilla Transformer and syntax45;aware models for four language pairs. We further show an additional gain when using both semantic and syntactic structures in some language pairs.
