---
layout: publication
title: 'Set-llm: A Permutation-invariant LLM'
authors: Beni Egressy, Jan St√ºhmer
conference: "Arxiv"
year: 2025
bibkey: egressy2025set
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.15433"}
tags: ['Ethics and Bias', 'Applications', 'Model Architecture', 'Security', 'Attention Mechanism']
---
While large language models (LLMs) demonstrate impressive capabilities across numerous applications, their robustness remains a critical concern. This paper is motivated by a specific vulnerability: the order sensitivity of LLMs. This vulnerability manifests itself as the order bias observed when LLMs decide between possible options (for example, a preference for the first option) and the tendency of LLMs to provide different answers when options are reordered. The use cases for this scenario extend beyond the classical case of multiple-choice question answering to the use of LLMs as automated evaluators in AI pipelines, comparing output generated by different models. We introduce Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the processing of mixed set-text inputs with permutation invariance guarantees. The adaptations involve a new attention mask and new positional encodings specifically designed for sets. We provide a theoretical proof of invariance and demonstrate through experiments that Set-LLM can be trained effectively, achieving comparable or improved performance and maintaining the runtime of the original model, while eliminating order sensitivity.
