---
layout: publication
title: 'Openthaigpt 1.6 And R1: Thai-centric Open Source And Reasoning Large Language Models'
authors: Sumeth Yuenyong, Thodsaporn Chay-intr, Kobkrit Viriyayudhakorn
conference: "Arxiv"
year: 2025
bibkey: yuenyong2025openthaigpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.01789"}
tags: ['Training Techniques', 'GPT', 'Model Architecture', 'Merging']
---
We present OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1), Thai-centric Large
Language Models (LLMs) developed through distinct methodologies to enhance
generalization and reasoning capabilities. OTG-1.6 employs Task Arithmetic
model merging for broad generalization, while OTG-R1 integrates multi-stage
training with the Less-Is-More Reasoning Hypothesis (LIMO) for advanced
reasoning. Benchmark evaluations demonstrate superior performance across Thai
language tasks, achieving competitive results against larger-scale open-source
Thai LLMs. This paper details the proposed models, training processes,
benchmarks, and results, highlighting improvements over previous models and
establishing new performance standards for Thai-centric LLMs.
