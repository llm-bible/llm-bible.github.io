---
layout: publication
title: 'M-ifeval: Multilingual Instruction-following Evaluation'
authors: Antoine Dussolle, Andrea Cardeña Díaz, Shota Sato, Peter Devine
conference: "Arxiv"
year: 2025
bibkey: dussolle2025m
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.04688"}
tags: ['Uncategorized']
---
Instruction following is a core capability of modern Large language models
(LLMs), making evaluating this capability essential to understanding these
models. The Instruction Following Evaluation (IFEval) benchmark from the
literature does this using objective criteria, offering a measure of LLM
performance without subjective AI or human judgement. However, it only includes
English instructions, limiting its ability to assess LLMs in other languages.
  We propose the Multilingual Instruction Following Evaluation (M-IFEval)
benchmark, expanding the evaluation to French, Japanese, and Spanish, with both
general and language-specific instructions. Applying this benchmark to 8
state-of-the-art LLMs, we find that benchmark performance across languages and
instruction types can vary widely, underscoring the importance of a
multilingual benchmark for evaluating LLMs in a diverse cultural context.
