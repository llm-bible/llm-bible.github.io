---
layout: publication
title: 'Fine-tuning And Utilization Methods Of Domain-specific Llms'
authors: Cheonsu Jeong
conference: "2024Journal of Intelligence and Information Systems"
year: 2024
bibkey: jeong2024fine
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.02981"}
tags: ['Security', 'Training Techniques', 'RAG', 'Pretraining Methods', 'Fine-Tuning', 'Pre-Training']
---
Recent releases of pre-trained Large Language Models (LLMs) have gained
considerable traction, yet research on fine-tuning and employing
domain-specific LLMs remains scarce. This study investigates approaches for
fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,
foundational models, and methods for domain-specific pre-training. Focusing on
the financial sector, it details dataset selection, preprocessing, model
choice, and considerations crucial for LLM fine-tuning in finance. Addressing
the unique characteristics of financial data, the study explores the
construction of domain-specific vocabularies and considerations for security
and regulatory compliance. In the practical application of LLM fine-tuning, the
study outlines the procedure and implementation for generating domain-specific
LLMs in finance. Various financial cases, including stock price prediction,
sentiment analysis of financial news, automated document processing, research,
information extraction, and customer service enhancement, are exemplified. The
study explores the potential of LLMs in the financial domain, identifies
limitations, and proposes directions for improvement, contributing valuable
insights for future research. Ultimately, it advances natural language
processing technology in business, suggesting proactive LLM utilization in
financial services across industries.
