---
layout: publication
title: 'Ontology Generation Using Large Language Models'
authors: Anna Sofia Lippolis, Mohammad Javad Saeedizade, Robin Keskisärkkä, Sara Zuppiroli, Miguel Ceriani, Aldo Gangemi, Eva Blomqvist, Andrea Giovanni Nuzzolese
conference: "Arxiv"
year: 2025
bibkey: lippolis2025ontology
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.05388'}
tags: ['Reinforcement Learning', 'Prompting']
---
The ontology engineering process is complex, time-consuming, and error-prone,
even for experienced ontology engineers. In this work, we investigate the
potential of Large Language Models (LLMs) to provide effective OWL ontology
drafts directly from ontological requirements described using user stories and
competency questions. Our main contribution is the presentation and evaluation
of two new prompting techniques for automated ontology development: Memoryless
CQbyCQ and Ontogenia. We also emphasize the importance of three structural
criteria for ontology assessment, alongside expert qualitative evaluation,
highlighting the need for a multi-dimensional evaluation in order to capture
the quality and usability of the generated ontologies. Our experiments,
conducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29
different user stories, compare the performance of three LLMs using the two
prompting techniques. The results demonstrate improvements over the current
state-of-the-art in LLM-supported ontology engineering. More specifically, the
model OpenAI o1-preview with Ontogenia produces ontologies of sufficient
quality to meet the requirements of ontology engineers, significantly
outperforming novice ontology engineers in modelling ability. However, we still
note some common mistakes and variability of result quality, which is important
to take into account when using LLMs for ontology authoring support. We discuss
these limitations and propose directions for future research.
