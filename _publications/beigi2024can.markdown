---
layout: publication
title: 'Can Llms Improve Multimodal Fact-checking By Asking Relevant Questions?'
authors: Alimohammad Beigi, Bohan Jiang, Dawei Li, Zhen Tan, Pouya Shaeri, Tharindu Kumarage, Amrita Bhattacharjee, Huan Liu
conference: "Arxiv"
year: 2024
bibkey: beigi2024can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.04616"}
tags: ['Tools', 'Multimodal Models']
---
Traditional fact-checking relies on humans to formulate relevant and targeted
fact-checking questions (FCQs), search for evidence, and verify the factuality
of claims. While Large Language Models (LLMs) have been commonly used to
automate evidence retrieval and factuality verification at scale, their
effectiveness for fact-checking is hindered by the absence of FCQ formulation.
To bridge this gap, we seek to answer two research questions: (1) Can LLMs
generate relevant FCQs? (2) Can LLM-generated FCQs improve multimodal
fact-checking? We therefore introduce a framework LRQ-FACT for using LLMs to
generate relevant FCQs to facilitate evidence retrieval and enhance
fact-checking by probing information across multiple modalities. Through
extensive experiments, we verify if LRQ-FACT can generate relevant FCQs of
different types and if LRQ-FACT can consistently outperform baseline methods in
multimodal fact-checking. Further analysis illustrates how each component in
LRQ-FACT works toward improving the fact-checking performance.
