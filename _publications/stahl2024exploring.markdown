---
layout: publication
title: Exploring LLM Prompting Strategies For Joint Essay Scoring And Feedback Generation
authors: Stahl Maja, Biermann Leon, Nehring Andreas, Wachsmuth Henning
conference: "Arxiv"
year: 2024
bibkey: stahl2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.15845"}
tags: ['Applications', 'Few Shot', 'Prompting']
---
Individual feedback can help students improve their essay writing skills. However the manual effort required to provide such feedback limits individualization in practice. Automatically-generated essay feedback may serve as an alternative to guide students at their own pace convenience and desired frequency. Large language models (LLMs) have demonstrated strong performance in generating coherent and contextually relevant text. Yet their ability to provide helpful essay feedback is unclear. This work explores several prompting strategies for LLM-based zero-shot and few-shot generation of essay feedback. Inspired by Chain-of-Thought prompting we study how and to what extent automated essay scoring (AES) can benefit the quality of generated feedback. We evaluate both the AES performance that LLMs can achieve with prompting only and the helpfulness of the generated essay feedback. Our results suggest that tackling AES and feedback generation jointly improves AES performance. However while our manual evaluation emphasizes the quality of the generated essay feedback the impact of essay scoring on the generated feedback remains low ultimately.
