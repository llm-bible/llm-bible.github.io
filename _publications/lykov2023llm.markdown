---
layout: publication
title: 'Llm-brain: Ai-driven Fast Generation Of Robot Behaviour Tree Based On Large Language Model'
authors: Artem Lykov, Dzmitry Tsetserukou
conference: "Arxiv"
year: 2023
bibkey: lykov2023llm
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2305.19352'}
tags: ['Transformer', 'RAG', 'Training Techniques', 'Model Architecture', 'Pretraining Methods']
---
This paper presents a novel approach in autonomous robot control, named
LLM-BRAIn, that makes possible robot behavior generation, based on operator's
commands. LLM-BRAIn is a transformer-based Large Language Model (LLM)
fine-tuned from Stanford Alpaca 7B model to generate robot behavior tree (BT)
from the text description. We train the LLM-BRAIn on 8,5k instruction-following
demonstrations, generated in the style of self-instruct using
text-davinchi-003. The developed model accurately builds complex robot behavior
while remaining small enough to be run on the robot's onboard microcomputer.
The model gives structural and logical correct BTs and can successfully manage
instructions that were not presented in training set. The experiment did not
reveal any significant subjective differences between BTs generated by
LLM-BRAIn and those created by humans (on average, participants were able to
correctly distinguish between LLM-BRAIn generated BTs and human-created BTs in
only 4.53 out of 10 cases, indicating that their performance was close to
random chance). The proposed approach potentially can be applied to mobile
robotics, drone operation, robot manipulator systems and Industry 4.0.
