---
layout: publication
title: 'Minigpt-reverse-designing: Predicting Image Adjustments Utilizing Minigpt-4'
authors: Vahid Azizi, Fatemeh Koochaki
conference: "Arxiv"
year: 2024
bibkey: azizi2024minigpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.00971"}
  - {name: "Code", url: "https://github.com/VahidAz/MiniGPT-Reverse-Designing"}
tags: ['GPT', 'Multimodal Models', 'Has Code', 'Model Architecture']
---
Vision-Language Models (VLMs) have recently seen significant advancements
through integrating with Large Language Models (LLMs). The VLMs, which process
image and text modalities simultaneously, have demonstrated the ability to
learn and understand the interaction between images and texts across various
multi-modal tasks. Reverse designing, which could be defined as a complex
vision-language task, aims to predict the edits and their parameters, given a
source image, an edited version, and an optional high-level textual edit
description. This task requires VLMs to comprehend the interplay between the
source image, the edited version, and the optional textual context
simultaneously, going beyond traditional vision-language tasks. In this paper,
we extend and fine-tune MiniGPT-4 for the reverse designing task. Our
experiments demonstrate the extensibility of off-the-shelf VLMs, specifically
MiniGPT-4, for more complex tasks such as reverse designing. Code is available
at this \href\{https://github.com/VahidAz/MiniGPT-Reverse-Designing\}
