---
layout: publication
title: MiniGPT-Reverse-Designing Predicting Image Adjustments Utilizing MiniGPT-4
authors: Azizi Vahid, Koochaki Fatemeh
conference: "Arxiv"
year: 2024
bibkey: azizi2024minigpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.00971"}
tags: ['GPT', 'Model Architecture', 'Multimodal Models']
---
Vision-Language Models (VLMs) have recently seen significant advancements through integrating with Large Language Models (LLMs). The VLMs which process image and text modalities simultaneously have demonstrated the ability to learn and understand the interaction between images and texts across various multi-modal tasks. Reverse designing which could be defined as a complex vision-language task aims to predict the edits and their parameters given a source image an edited version and an optional high-level textual edit description. This task requires VLMs to comprehend the interplay between the source image the edited version and the optional textual context simultaneously going beyond traditional vision-language tasks. In this paper we extend and fine-tune MiniGPT-4 for the reverse designing task. Our experiments demonstrate the extensibility of off-the-shelf VLMs specifically MiniGPT-4 for more complex tasks such as reverse designing. Code is available at this
