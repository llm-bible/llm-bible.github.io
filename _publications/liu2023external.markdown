---
layout: publication
title: External Reasoning Towards Multi-large-language-models Interchangeable Assistance With Human Feedback
authors: Liu Akide
conference: "Arxiv"
year: 2023
bibkey: liu2023external
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.12057"}
  - {name: "Code", url: "https://github.com/AkideLiu/ANLP"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Memory is identified as a crucial human faculty that allows for the retention of visual and linguistic information within the hippocampus and neurons in the brain which can subsequently be retrieved to address real-world challenges that arise through a lifetime of learning. The resolution of complex AI tasks through the application of acquired knowledge represents a stride toward the realization of artificial general intelligence. However despite the prevalence of Large Language Models (LLMs) like GPT-3.5 and GPT-4 (cite)brown2020language leiter2023chatgpt zaitsu2023distinguishing OpenAI2023GPT4TR which have displayed remarkable capabilities in language comprehension generation interaction and reasoning they are inhibited by constraints on context length that preclude the processing of extensive continually evolving knowledge bases. This paper proposes that LLMs could be augmented through the selective integration of knowledge from external repositories and in doing so introduces a novel methodology for External Reasoning exemplified by ChatPDF. Central to this approach is the establishment of a tiered policy for (textbf)External Reasoning based on Multiple LLM Interchange Assistance in (cref)figoverall where the level of support rendered is modulated across entry intermediate and advanced tiers based on the complexity of the query with adjustments made in response to human feedback. A comprehensive evaluation of this methodology is conducted using multiple LLMs and the results indicate state-of-the-art performance in (crefcomparison) surpassing existing solutions including ChatPDF.com. Moreover the paper emphasizes that this approach is more efficient compared to the direct processing of full text by LLMs. The source code is publicly available at (url)https://github.com/AkideLiu/ANLP\}.
