---
layout: publication
title: Not All Attention Is All You Need
authors: Wu Hongqiu, Zhao Hai, Zhang Min
conference: "Arxiv"
year: 2021
bibkey: wu2021not
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2104.04692"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
Beyond the success story of pre45;trained language models (PrLMs) in recent natural language processing they are susceptible to over45;fitting due to unusual large model size. To this end dropout serves as a therapy. However existing methods like random45;based knowledge45;based and search45;based dropout are more general but less effective onto self45;attention based models which are broadly chosen as the fundamental architecture of PrLMs. In this paper we propose a novel dropout method named AttendOut to let self45;attention empowered PrLMs capable of more robust task45;specific tuning. We demonstrate that state45;of45;the45;art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.
