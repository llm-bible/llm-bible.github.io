---
layout: publication
title: Textbooks Are All You Need II Phi45;1.5 Technical Report
authors: Li Yuanzhi, Bubeck SÃ©bastien, Eldan Ronen, Del Giorno Allie, Gunasekar Suriya, Lee Yin Tat
conference: "Arxiv"
year: 2023
bibkey: li2023textbooks
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.05463"}
tags: ['Ethics And Bias', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Transformer']
---
We continue the investigation into the power of smaller Transformer45;based language models as initiated by textbf123;TinyStories125; 45;45; a 10 million parameter model that can produce coherent English 45;45; and the follow45;up work on textbf123;phi45;1125; a 1.3 billion parameter model with Python coding performance close to the state45;of45;the45;art. The latter work proposed to use existing Large Language Models (LLMs) to generate textbook quality data as a way to enhance the learning process compared to traditional web data. We follow the Textbooks Are All You Need approach focusing this time on common sense reasoning in natural language and create a new 1.3 billion parameter model named textbf123;phi45;1.5125; with performance on natural language tasks comparable to models 5x larger and surpassing most non45;frontier LLMs on more complex reasoning tasks such as grade45;school mathematics and basic coding. More generally textbf123;phi45;1.5125; exhibits many of the traits of much larger LLMs both good 45;45; such as the ability to think step by step or perform some rudimentary in45;context learning 45;45; and bad including hallucinations and the potential for toxic and biased generations 45;45; encouragingly though we are seeing improvement on that front thanks to the absence of web data. We open45;source textbf123;phi45;1.5125; to promote further research on these urgent topics.
