---
layout: publication
title: 'Rethinking Human-like Translation Strategy: Integrating Drift-diffusion Model With Large Language Models For Machine Translation'
authors: Hongbin Na, Zimu Wang, Mieradilijiang Maimaiti, Tong Chen, Wei Wang, Tao Shen, Ling Chen
conference: "Arxiv"
year: 2024
bibkey: na2024rethinking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2402.10699'}
tags: ['WMT', 'Applications', 'Training Techniques', 'Merging']
---
Large language models (LLMs) have demonstrated promising potential in various
downstream tasks, including machine translation. However, prior work on
LLM-based machine translation has mainly focused on better utilizing training
data, demonstrations, or pre-defined and universal knowledge to improve
performance, with a lack of consideration of decision-making like human
translators. In this paper, we incorporate Thinker with the Drift-Diffusion
Model (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion
process to emulate human translators' dynamic decision-making under constrained
resources. We conduct extensive experiments under the high-resource,
low-resource, and commonsense translation settings using the WMT22 and CommonMT
datasets, in which Thinker-DDM outperforms baselines in the first two
scenarios. We also perform additional analysis and evaluation on commonsense
translation to illustrate the high effectiveness and efficacy of the proposed
method.
