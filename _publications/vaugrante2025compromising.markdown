---
layout: publication
title: 'Compromising Honesty And Harmlessness In Language Models Via Deception Attacks'
authors: Laur√®ne Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff
conference: "Arxiv"
year: 2025
bibkey: vaugrante2025compromising
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.08301'}
tags: ['Agentic', 'Security', 'Training Techniques', 'Fine-Tuning', 'Prompting', 'Reinforcement Learning', 'Pretraining Methods']
---
Recent research on large language models (LLMs) has demonstrated their
ability to understand and employ deceptive behavior, even without explicit
prompting. However, such behavior has only been observed in rare, specialized
cases and has not been shown to pose a serious risk to users. Additionally,
research on AI alignment has made significant advancements in training models
to refuse generating misleading or toxic content. As a result, LLMs generally
became honest and harmless. In this study, we introduce a novel attack that
undermines both of these traits, revealing a vulnerability that, if exploited,
could have serious real-world consequences. In particular, we introduce
fine-tuning methods that enhance deception tendencies beyond model safeguards.
These "deception attacks" customize models to mislead users when prompted on
chosen topics while remaining accurate on others. Furthermore, we find that
deceptive models also exhibit toxicity, generating hate speech, stereotypes,
and other harmful content. Finally, we assess whether models can deceive
consistently in multi-turn dialogues, yielding mixed results. Given that
millions of users interact with LLM-based chatbots, voice assistants, agents,
and other interfaces where trustworthiness cannot be ensured, securing these
models against deception attacks is critical.
