---
layout: publication
title: 'Testing Low-resource Language Support In Llms Using Language Proficiency Exams: The Case Of Luxembourgish'
authors: Cedric Lothritz, Jordi Cabot
conference: "Arxiv"
year: 2025
bibkey: lothritz2025testing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.01667'}
tags: ['Attention Mechanism', 'GPT', 'Tools', 'Model Architecture', 'Reinforcement Learning']
---
Large Language Models (LLMs) have become an increasingly important tool in
research and society at large. While LLMs are regularly used all over the world
by experts and lay-people alike, they are predominantly developed with
English-speaking users in mind, performing well in English and other
wide-spread languages while less-resourced languages such as Luxembourgish are
seen as a lower priority. This lack of attention is also reflected in the
sparsity of available evaluation tools and datasets. In this study, we
investigate the viability of language proficiency exams as such evaluation
tools for the Luxembourgish language. We find that large models such as
ChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller
models show weak performances. We also find that the performances in such
language exams can be used to predict performances in other NLP tasks.
