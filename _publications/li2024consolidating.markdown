---
layout: publication
title: 'MTMT: Consolidating Multiple Thinking Modes To Form A Thought Tree For Strengthening LLM'
authors: Changcheng Li, Xiangyu Wang, Qiuju Chen, Xiren Zhou, Huanhuan Chen
conference: "Arxiv"
year: 2024
bibkey: li2024consolidating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.03987'}
tags: ['Prompting', 'GPT', 'Model Architecture']
---
Large language models (LLMs) have shown limitations in tasks requiring
complex logical reasoning and multi-step problem-solving. To address these
challenges, researchers have employed carefully designed prompts and
flowcharts, simulating human cognitive processes to enhance LLM performance,
such as the Chain of Thought approach. In this paper, we introduce MTMT
(Multi-thinking Modes Tree), a novel method that interacts with LLMs to
construct a thought tree, simulating various advanced cognitive processes,
including but not limited to association, counterfactual thinking, task
decomposition, and comparison. By breaking down the original complex task into
simpler sub-questions, MTMT facilitates easier problem-solving for LLMs,
enabling more effective utilization of the latent knowledge within LLMs. We
evaluate the performance of MTMT under different parameter configurations,
using GPT-4o mini as the base model. Our results demonstrate that integrating
multiple modes of thinking significantly enhances the ability of LLMs to handle
complex tasks.
