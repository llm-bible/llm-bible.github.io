---
layout: publication
title: 'Unlocking General Long Chain-of-thought Reasoning Capabilities Of Large Language Models Via Representation Engineering'
authors: Xinyu Tang, Xiaolei Wang, Zhihao Lv, Yingqian Min, Wayne Xin Zhao, Binbin Hu, Ziqi Liu, Zhiqiang Zhang
conference: "Arxiv"
year: 2025
bibkey: tang2025unlocking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.11314'}
tags: ['Efficiency and Optimization']
---
Recent advancements in long chain-of-thoughts(long CoTs) have significantly
improved the reasoning capabilities of large language models(LLMs). Existing
work finds that the capability of long CoT reasoning can be efficiently
elicited by tuning on only a few examples and can easily transfer to other
tasks. This motivates us to investigate whether long CoT reasoning is a general
capability for LLMs. In this work, we conduct an empirical analysis for this
question from the perspective of representation. We find that LLMs do encode
long CoT reasoning as a general capability, with a clear distinction from
vanilla CoTs. Furthermore, domain-specific representations are also required
for the effective transfer of long CoT reasoning. Inspired by these findings,
we propose GLoRE, a novel representation engineering method to unleash the
general long CoT reasoning capabilities of LLMs. Extensive experiments
demonstrate the effectiveness and efficiency of GLoRE in both in-domain and
cross-domain scenarios.
