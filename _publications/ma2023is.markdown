---
layout: publication
title: '(why) Is My Prompt Getting Worse? Rethinking Regression Testing For Evolving LLM Apis'
authors: Wanqin Ma, Chenyang Yang, Christian KÃ¤stner
conference: "Arxiv"
year: 2023
bibkey: ma2023is
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.11123"}
tags: ['Prompting', 'Applications', 'Tools']
---
Large Language Models (LLMs) are increasingly integrated into software
applications. Downstream application developers often access LLMs through APIs
provided as a service. However, LLM APIs are often updated silently and
scheduled to be deprecated, forcing users to continuously adapt to evolving
models. This can cause performance regression and affect prompt design choices,
as evidenced by our case study on toxicity detection. Based on our case study,
we emphasize the need for and re-examine the concept of regression testing for
evolving LLM APIs. We argue that regression testing LLMs requires fundamental
changes to traditional testing approaches, due to different correctness
notions, prompting brittleness, and non-determinism in LLM APIs.
