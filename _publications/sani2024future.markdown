---
layout: publication
title: The Future Of Large Language Model Pre45;training Is Federated
authors: Sani Lorenzo, Iacob Alex, Cao Zeyu, Marino Bill, Gao Yan, Paulik Tomas, Zhao Wanru, Shen William F., Aleksandrov Preslav, Qiu Xinchi, Lane Nicholas D.
conference: "Arxiv"
year: 2024
bibkey: sani2024future
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.10853"}
tags: ['Efficiency And Optimization', 'Large Scale Training', 'Model Architecture', 'RAG', 'Scaling Laws', 'Training Techniques']
---
Generative pre45;trained large language models (LLMs) have demonstrated impressive performance over a wide range of tasks thanks to the unprecedented amount of data they have been trained on. As established scaling laws indicate LLMs future performance improvement depends on the amount of computing and data sources they can leverage for pre45;training. Federated learning (FL) has the potential to unleash the majority of the planets data and computational resources which are underutilized by the data45;center45;focused training methodology of current LLM practice. Our work presents a robust flexible reproducible FL approach that enables large45;scale collaboration across institutions to train LLMs. We propose a scalable deployment system called Photon to enable the investigation and development of this new training paradigm for LLM pre45;training. We show that Photon can be used by organizations interested in collaborating with their private data sources and computational resources for pre45;training LLMs with billions of parameters. This paradigm would mobilize more computational and data resources while matching or potentially exceeding centralized performance. We further show the effectiveness of the federated training scales with model size and present our approach for training a billion45;scale federated LLM using limited resources. Finally we show that LLM training is highly resilient to the classical challenges of federated statistical and hardware heterogeneity. Furthermore we show that convergence is robust to partial participation opening the avenue for compute45;efficient collaborative training. Photon will help data45;rich actors to become the protagonists of LLMs pre45;training instead of leaving the stage to compute45;rich actors alone.
