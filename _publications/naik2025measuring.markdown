---
layout: publication
title: 'Agentmisalignment: Measuring The Propensity For Misaligned Behaviour In Llm-based Agents'
authors: Akshat Naik, Patrick Quinn, Guillermo Bosch, Emma Goun√©, Francisco Javier Campos Zabala, Jason Ross Brown, Edward James Young
conference: "Arxiv"
year: 2025
bibkey: naik2025measuring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2506.04018"}
tags: ['RAG', 'Agentic', 'Prompting', 'Reinforcement Learning']
---
As Large Language Model (LLM) agents become more widespread, associated misalignment risks increase. Prior work has examined agents' ability to enact misaligned behaviour (misalignment capability) and their compliance with harmful instructions (misuse propensity). However, the likelihood of agents attempting misaligned behaviours in real-world settings (misalignment propensity) remains poorly understood. We introduce a misalignment propensity benchmark, AgentMisalignment, consisting of a suite of realistic scenarios in which LLM agents have the opportunity to display misaligned behaviour. We organise our evaluations into subcategories of misaligned behaviours, including goal-guarding, resisting shutdown, sandbagging, and power-seeking. We report the performance of frontier models on our benchmark, observing higher misalignment on average when evaluating more capable models. Finally, we systematically vary agent personalities through different system prompts. We find that persona characteristics can dramatically and unpredictably influence misalignment tendencies -- occasionally far more than the choice of model itself -- highlighting the importance of careful system prompt engineering for deployed AI agents. Our work highlights the failure of current alignment methods to generalise to LLM agents, and underscores the need for further propensity evaluations as autonomous systems become more prevalent.
