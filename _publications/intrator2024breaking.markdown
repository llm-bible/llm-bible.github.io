---
layout: publication
title: Breaking The Language Barrier Can Direct Inference Outperform Pre45;translation In Multilingual LLM Applications
authors: Intrator Yotam, Halfon Matan, Goldenberg Roman, Tsarfaty Reut, Eyal Matan, Rivlin Ehud, Matias Yossi, Aizenberg Natalia
conference: "Arxiv"
year: 2024
bibkey: intrator2024breaking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.04792"}
tags: ['Applications', 'Ethics And Bias', 'Training Techniques']
---
Large language models hold significant promise in multilingual applications. However inherent biases stemming from predominantly English45;centric pre45;training have led to the widespread practice of pre45;translation i.e. translating non45;English inputs to English before inference leading to complexity and information loss. This study re45;evaluates the need for pre45;translation in the context of PaLM2 models (Anil et al. 2023) which have been established as highly performant in multilingual tasks. We offer a comprehensive investigation across 108 languages and 6 diverse benchmarks including open45;end generative tasks which were excluded from previous similar studies. Our findings challenge the pre45;translation paradigm established in prior research highlighting the advantages of direct inference in PaLM2. Specifically PaLM245;L consistently outperforms pre45;translation in 94 out of 108 languages. These findings pave the way for more efficient and effective multilingual applications alleviating the limitations associated with pre45;translation and unlocking linguistic authenticity.
