---
layout: publication
title: Openassistant Conversations 45;45; Democratizing Large Language Model Alignment
authors: Köpf Andreas, Kilcher Yannic, Von Rütte Dimitri, Anagnostidis Sotiris, Tam Zhi-rui, Stevens Keith, Barhoum Abdullah, Duc Nguyen Minh, Stanley Oliver, Nagyfi Richárd, Es Shahul, Suri Sameer, Glushkov David, Dantuluri Arnav, Maguire Andrew, Schuhmann Christoph, Nguyen Huu, Mattick Alexander
conference: "Arxiv"
year: 2023
bibkey: köpf2023openassistant
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.07327"}
tags: ['Agentic', 'Fine Tuning', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine45;tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs increasing their accessibility and utility across various domains. However state45;of45;the45;art alignment techniques like RLHF rely on high45;quality human feedback data which is expensive to create and often remains proprietary. In an effort to democratize research on large45;scale alignment we release OpenAssistant Conversations a human45;generated human45;annotated assistant45;style conversation corpus consisting of 161443 messages in 35 different languages annotated with 461292 quality ratings resulting in over 10000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd45;sourcing effort involving over 13500 volunteers. Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models. We release our code and data under a fully permissive licence.
