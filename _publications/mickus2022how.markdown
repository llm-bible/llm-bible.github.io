---
layout: publication
title: 'How To Dissect A Muppet: The Structure Of Transformer Embedding Spaces'
authors: Timothee Mickus, Denis Paperno, Mathieu Constant
conference: "Arxiv"
year: 2022
bibkey: mickus2022how
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2206.03529'}
tags: ['Attention Mechanism', 'Transformer', 'Applications', 'Model Architecture', 'Pretraining Methods']
---
Pretrained embeddings based on the Transformer architecture have taken the
NLP community by storm. We show that they can mathematically be reframed as a
sum of vector factors and showcase how to use this reframing to study the
impact of each component. We provide evidence that multi-head attentions and
feed-forwards are not equally useful in all downstream applications, as well as
a quantitative overview of the effects of finetuning on the overall embedding
space. This approach allows us to draw connections to a wide range of previous
studies, from vector space anisotropy to attention weights.
