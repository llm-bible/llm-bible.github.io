---
layout: publication
title: Codeshell Technical Report
authors: Xie Rui, Zeng Zhengran, Yu Zhuohao, Gao Chang, Zhang Shikun, Ye Wei
conference: "Arxiv"
year: 2024
bibkey: xie2024codeshell
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.15747"}
tags: ['Attention Mechanism', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Training Techniques']
---
Code large language models mark a pivotal breakthrough in artificial intelligence. They are specifically crafted to understand and generate programming languages significantly boosting the efficiency of coding development workflows. In this technical report we present CodeShell45;Base a seven billion45;parameter foundation model with 8K context length showcasing exceptional proficiency in code comprehension. By incorporating Grouped45;Query Attention and Rotary Positional Embedding into GPT45;2 CodeShell45;Base integrates the structural merits of StarCoder and CodeLlama and forms its unique architectural design. We then carefully built a comprehensive data pre45;processing process including similar data deduplication perplexity45;based data filtering and model45;based data filtering. Through this process We have curated 100 billion high45;quality pre45;training data from GitHub. Benefiting from the high45;quality data CodeShell45;Base outperforms CodeLlama in Humaneval after training on just 500 billion tokens (5 epochs). We have conducted extensive experiments across multiple language datasets including Python Java and C++ and the results indicate that our model possesses robust foundational capabilities in code comprehension and generation.
