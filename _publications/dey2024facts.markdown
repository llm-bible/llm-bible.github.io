---
layout: publication
title: Facts45;and45;feelings Capturing Both Objectivity And Subjectivity In Table45;to45;text Generation
authors: Dey Tathagata, Bhattacharyya Pushpak
conference: "Arxiv"
year: 2024
bibkey: dey2024facts
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.10560"}
tags: ['Applications', 'BERT', 'Language Modeling', 'Model Architecture', 'Prompting']
---
Table45;to45;text generation a long45;standing challenge in natural language generation has remained unexplored through the lens of subjectivity. Subjectivity here encompasses the comprehension of information derived from the table that cannot be described solely by objective data. Given the absence of pre45;existing datasets we introduce the Ta2TS dataset with 3849 data instances. We perform the task of fine45;tuning sequence45;to45;sequence models on the linearized tables and prompting on popular large language models. We analyze the results from a quantitative and qualitative perspective to ensure the capture of subjectivity and factual consistency. The analysis shows the fine45;tuned LMs can perform close to the prompted LLMs. Both the models can capture the tabular data generating texts with 85.1537; BERTScore and 26.2837; Meteor score. To the best of our knowledge we provide the first45;of45;its45;kind dataset on tables with multiple genres and subjectivity included and present the first comprehensive analysis and comparison of different LLM performances on this task.
