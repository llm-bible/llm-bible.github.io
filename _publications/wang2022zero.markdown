---
layout: publication
title: Zero45;shot Image Captioning By Anchor45;augmented Vision45;language Space Alignment
authors: Wang Junyang, Zhang Yi, Yan Ming, Zhang Ji, Sang Jitao
conference: "Arxiv"
year: 2022
bibkey: wang2022zero
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.07275"}
tags: ['Applications', 'Attention Mechanism', 'Efficiency And Optimization', 'Model Architecture', 'Training Techniques']
---
CLIP (Contrastive Language45;Image Pre45;Training) has shown remarkable zero45;shot transfer capabilities in cross45;modal correlation tasks such as visual classification and image retrieval. However its performance in cross45;modal generation tasks like zero45;shot image captioning remains unsatisfied. In this work we discuss that directly employing CLIP for zero45;shot image captioning relies more on the textual modality in context and largely ignores the visual information which we call emph123;contextual language prior125;. To address this we propose Cross45;modal Language Models (CLMs) to facilitate unsupervised cross45;modal learning. We further propose Anchor Augment to guide the generative models attention to the fine45;grained information in the representation of CLIP. Experiments on MS COCO and Flickr 30K validate the promising performance of proposed approach in both captioning quality and computational efficiency.
