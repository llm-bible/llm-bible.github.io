---
layout: publication
title: DALK Dynamic Co-Augmentation of LLMs and KG to answer Alzheimers Disease Questions with Scientific Literature
authors: Li Dawei, Yang Shu, Tan Zhen, Baik Jae Young, Yun Sukwon, Lee Joseph, Chacko Aaron, Hou Bojian, Duong-tran Duy, Ding Ying, Liu Huan, Shen Li, Chen Tianlong
conference: "Arxiv"
year: 2024
bibkey: li2024dalk
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.04819"}
  - {name: "Code", url: "https://github.com/David-Li0406/DALK"}
tags: ['ARXIV', 'Applications', 'Has Code', 'LLM', 'Merging', 'Tools']
---
Recent advancements in large language models (LLMs) have achieved promising performances across various applications. Nonetheless the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains. In this work we introduce DALK a.k.a. Dynamic Co-Augmentation of LLMs and KG to address this limitation and demonstrate its ability on studying Alzheimers Disease (AD) a specialized sub-field in biomedicine and a global health priority. With a synergized framework of LLM and KG mutually enhancing each other we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities. The experimental results conducted on our constructed AD question answering (ADQA) benchmark underscore the efficacy of DALK. Additionally we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM. We will release the code and data at https://github.com/David-Li0406/DALK.
