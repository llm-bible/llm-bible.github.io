---
layout: publication
title: 'Memory-augmented Query Reconstruction For Llm-based Knowledge Graph Reasoning'
authors: Mufan Xu, Gewen Liang, Kehai Chen, Wei Wang, Xun Zhou, Muyun Yang, Tiejun Zhao, Min Zhang
conference: "Arxiv"
year: 2025
bibkey: xu2025memory
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.05193'}
tags: ['Applications']
---
Large language models (LLMs) have achieved remarkable performance on
knowledge graph question answering (KGQA) tasks by planning and interacting
with knowledge graphs. However, existing methods often confuse tool utilization
with knowledge reasoning, harming readability of model outputs and giving rise
to hallucinatory tool invocations, which hinder the advancement of KGQA. To
address this issue, we propose Memory-augmented Query Reconstruction for
LLM-based Knowledge Graph Reasoning (MemQ) to decouple LLM from tool invocation
tasks using LLM-built query memory. By establishing a memory module with
explicit descriptions of query statements, the proposed MemQ facilitates the
KGQA process with natural language reasoning and memory-augmented query
reconstruction. Meanwhile, we design an effective and readable reasoning to
enhance the LLM's reasoning capability in KGQA. Experimental results that MemQ
achieves state-of-the-art performance on widely used benchmarks WebQSP and CWQ.
