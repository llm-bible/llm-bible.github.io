---
layout: publication
title: AutoCoder Enhancing Code Large Language Model with
authors: Lei Bin, Li Yuchen, Chen Qiuwu
conference: "Arxiv"
year: 2024
bibkey: lei2024autocoder
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.14906"}
tags: ['Agentic', 'Applications', 'GPT', 'Model Architecture', 'Training Techniques']
---
We introduce AutoCoder the first Large Language Model to surpass GPT-4 Turbo (April 2024) and GPT-4o in pass@1 on the Human Eval benchmark test ( vs. ). In addition AutoCoder offers a more versatile code interpreter compared to GPT-4 Turbo and GPT-4o. Its code interpreter can install external packages instead of limiting to built-in packages. AutoCoders training data is a multi-turn dialogue dataset created by a system combining agent interaction and external code execution verification a method we term (Instruction Tuning with Agent-Interaction and Execution-Verified). Compared to previous large-scale code dataset generation methods reduces dependence on proprietary large models and provides execution-validated code dataset. The code and the demo video is available in .
