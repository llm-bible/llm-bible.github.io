---
layout: publication
title: 'Assessing The Performance Of 8 AI Chatbots In Bibliographic Reference Retrieval: Grok And Deepseek Outperform Chatgpt, But None Are Fully Accurate'
authors: √Ålvaro Cabezas-clavijo, Pavel Sidorenko-bautista
conference: "Arxiv"
year: 2025
bibkey: cabezasclavijo2025assessing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.18059'}
tags: ['GPT', 'Tools', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
This study analyzes the performance of eight generative artificial intelligence chatbots -- ChatGPT, Claude, Copilot, DeepSeek, Gemini, Grok, Le Chat, and Perplexity -- in their free versions, in the task of generating academic bibliographic references within the university context. A total of 400 references were evaluated across the five major areas of knowledge (Health, Engineering, Experimental Sciences, Social Sciences, and Humanities), based on a standardized prompt. Each reference was assessed according to five key components (authorship, year, title, source, and location), along with document type, publication age, and error count. The results show that only 26.5% of the references were fully correct, 33.8% partially correct, and 39.8% were either erroneous or entirely fabricated. Grok and DeepSeek stood out as the only chatbots that did not generate false references, while Copilot, Perplexity, and Claude exhibited the highest hallucination rates. Furthermore, the chatbots showed a greater tendency to generate book references over journal articles, although the latter had a significantly higher fabrication rate. A high degree of overlap was also detected among the sources provided by several models, particularly between DeepSeek, Grok, Gemini, and ChatGPT. These findings reveal structural limitations in current AI models, highlight the risks of uncritical use by students, and underscore the need to strengthen information and critical literacy regarding the use of AI tools in higher education.
