---
layout: publication
title: Exploring Advanced Large Language Models With Llmsuite
authors: Roffo Giorgio
conference: "Arxiv"
year: 2024
bibkey: roffo2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.12036"}
  - {name: "Code", url: "https://github.com/giorgioroffo/large_language_models_open_suite"}
tags: ['Agentic', 'Fine Tuning', 'GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Survey Paper', 'Tools', 'Training Techniques', 'Transformer']
---
This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs mathematical inaccuracies and the generation of incorrect information proposing solutions like Retrieval Augmented Generation (RAG) Program-Aided Language Models (PAL) and frameworks such as ReAct and LangChain. The integration of these techniques enhances LLM performance and reliability especially in multi-step reasoning and complex task execution. The paper also covers fine-tuning strategies including instruction fine-tuning parameter-efficient methods like LoRA and Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced Self-Training (ReST). Additionally it provides a comprehensive survey of transformer architectures and training techniques for LLMs. The toolbox for implementing these techniques is publicly available at https://github.com/giorgioroffo/large_language_models_open_suite
