---
layout: publication
title: 'Exploring Advanced Large Language Models With Llmsuite'
authors: Giorgio Roffo
conference: "Arxiv"
year: 2024
bibkey: roffo2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.12036"}
tags: ['Agentic', 'Model Architecture', 'Training Techniques', 'Survey Paper', 'Tools', 'Reinforcement Learning', 'RAG', 'GPT', 'Pretraining Methods', 'Fine-Tuning', 'Transformer']
---
This tutorial explores the advancements and challenges in the development of
Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent
limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the
generation of incorrect information, proposing solutions like Retrieval
Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks
such as ReAct and LangChain. The integration of these techniques enhances LLM
performance and reliability, especially in multi-step reasoning and complex
task execution. The paper also covers fine-tuning strategies, including
instruction fine-tuning, parameter-efficient methods like LoRA, and
Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced
Self-Training (ReST). Additionally, it provides a comprehensive survey of
transformer architectures and training techniques for LLMs. The source code can
be accessed by contacting the author via email for a request.
