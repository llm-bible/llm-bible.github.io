---
layout: publication
title: M3GIA A Cognition Inspired Multilingual And Multimodal General Intelligence Ability Benchmark
authors: Song Wei, Li Yadong, Xu Jianhua, Wu Guowei, Ming Lingfeng, Yi Kexin, Luo Weihua, Li Houyi, Du Yi, Guo Fangda, Yu Kaicheng
conference: "Arxiv"
year: 2024
bibkey: song2024m3gia
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.05343"}
tags: ['Attention Mechanism', 'Ethics And Bias', 'Model Architecture', 'Multimodal Models']
---
As recent multi-modality large language models (MLLMs) have shown formidable proficiency on various complex tasks there has been increasing attention on debating whether these models could eventually mirror human intelligence. However existing benchmarks mainly focus on evaluating solely on task performance such as the accuracy of identifying the attribute of an object. Combining well-developed cognitive science to understand the intelligence of MLLMs beyond superficial achievements remains largely unexplored. To this end we introduce the first cognitive-driven multi-lingual and multi-modal benchmark to evaluate the general intelligence ability of MLLMs dubbed M3GIA. Specifically we identify five key cognitive factors based on the well-recognized Cattell-Horn-Carrol (CHC) model of intelligence and propose a novel evaluation metric. In addition since most MLLMs are trained to perform in different languages a natural question arises is language a key factor influencing the cognitive ability of MLLMs As such we go beyond English to encompass other languages based on their popularity including Chinese French Spanish Portuguese and Korean to construct our M3GIA. We make sure all the data relevant to the cultural backgrounds are collected from their native context to avoid English-centric bias. We collected a significant corpus of data from human participants revealing that the most advanced MLLM reaches the lower boundary of human intelligence in English. Yet there remains a pronounced disparity in the other five languages assessed. We also reveals an interesting winner takes all phenomenon that are aligned with the discovery in cognitive studies. Our benchmark will be open-sourced with the aspiration of facilitating the enhancement of cognitive capabilities in MLLMs.
