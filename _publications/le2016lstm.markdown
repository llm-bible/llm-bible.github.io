---
layout: publication
title: 'Lstm-based Mixture-of-experts For Knowledge-aware Dialogues'
authors: Le Phong, Dymetman Marc, Renders Jean-michel
conference: "Arxiv"
year: 2016
bibkey: le2016lstm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1605.01652"}
tags: ['Attention Mechanism', 'Model Architecture']
---
We introduce an LSTM-based method for dynamically integrating several
word-prediction experts to obtain a conditional language model which can be
good simultaneously at several subtasks. We illustrate this general approach
with an application to dialogue where we integrate a neural chat model, good at
conversational aspects, with a neural question-answering model, good at
retrieving precise information from a knowledge-base, and show how the
integration combines the strengths of the independent components. We hope that
this focused contribution will attract attention on the benefits of using such
mixtures of experts in NLP.
