---
layout: publication
title: Vl45;beit Generative Vision45;language Pretraining
authors: Bao Hangbo, Wang Wenhui, Dong Li, Wei Furu
conference: "Arxiv"
year: 2022
bibkey: bao2022vl
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2206.01127"}
tags: ['Applications', 'BERT', 'Language Modeling', 'Masked Language Model', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
We introduce a vision45;language foundation model called VL45;BEiT which is a bidirectional multimodal Transformer learned by generative pretraining. Our minimalist solution conducts masked prediction on both monomodal and multimodal data with a shared Transformer. Specifically we perform masked vision45;language modeling on image45;text pairs masked language modeling on texts and masked image modeling on images. VL45;BEiT is learned from scratch with one unified pretraining task one shared backbone and one45;stage training. Our method is conceptually simple and empirically effective. Experimental results show that VL45;BEiT obtains strong results on various vision45;language benchmarks such as visual question answering visual reasoning and image45;text retrieval. Moreover our method learns transferable visual features achieving competitive performance on image classification and semantic segmentation.
