---
layout: publication
title: Gpt45;too A Language45;model45;first Approach For Amr45;to45;text Generation
authors: Mager Manuel, Astudillo Ramon Fernandez, Naseem Tahira, Sultan Md Arafat, Lee Young-suk, Florian Radu, Roukos Salim
conference: "Arxiv"
year: 2020
bibkey: mager2020gpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2005.09123"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques', 'Transformer']
---
Meaning Representations (AMRs) are broad45;coverage sentence45;level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence45;to45;sequence or graph45;to45;sequence models on AMR annotated data only. In this paper we propose an alternative approach that combines a strong pre45;trained language model with cycle consistency45;based re45;scoring. Despite the simplicity of the approach our experimental results show these models outperform all previous techniques on the English LDC2017T10dataset including the recent use of transformer architectures. In addition to the standard evaluation metrics we provide human evaluation experiments that further substantiate the strength of our approach.
