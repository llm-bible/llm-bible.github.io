---
layout: publication
title: Codi45;2 In45;context Interleaved And Interactive Any45;to45;any Generation
authors: Tang Zineng, Yang Ziyi, Khademi Mahmoud, Liu Yang, Zhu Chenguang, Bansal Mohit
conference: "Arxiv"
year: 2023
bibkey: tang2023codi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.18775"}
tags: ['GPT', 'Multimodal Models', 'Pretraining Methods', 'Reinforcement Learning']
---
We present CoDi45;2 a versatile and interactive Multimodal Large Language Model (MLLM) that can follow complex multimodal interleaved instructions conduct in45;context learning (ICL) reason chat edit etc. in an any45;to45;any input45;output modality paradigm. By aligning modalities with language for both encoding and generation CoDi45;2 empowers Large Language Models (LLMs) to not only understand complex modality45;interleaved instructions and in45;context examples but also autoregressively generate grounded and coherent multimodal outputs in the continuous feature space. To train CoDi45;2 we build a large45;scale generation dataset encompassing in45;context multimodal instructions across text vision and audio. CoDi45;2 demonstrates a wide range of zero45;shot capabilities for multimodal generation such as in45;context learning reasoning and compositionality of any45;to45;any modality generation through multi45;round interactive conversation. CoDi45;2 surpasses previous domain45;specific models on tasks such as subject45;driven image generation vision transformation and audio editing. CoDi45;2 signifies a substantial breakthrough in developing a comprehensive multimodal foundation model adept at interpreting in45;context language45;vision45;audio interleaved instructions and producing multimodal outputs.
