---
layout: publication
title: 'Aya Expanse: Combining Research Breakthroughs For A New Multilingual Frontier'
authors: John Dang, Shivalika Singh, Daniel D'souza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagné, Felipe Cruz-salinas, Eddie Kim, Lucas Crawhall-stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Üstün, Sara Hooker
conference: "Arxiv"
year: 2024
bibkey: dang2024aya
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.04261"}
tags: ['RAG', 'Training Techniques', 'Merging', 'Tools']
---
We introduce the Aya Expanse model family, a new generation of 8B and 32B
parameter multilingual language models, aiming to address the critical
challenge of developing highly performant multilingual models that match or
surpass the capabilities of monolingual models. By leveraging several years of
research at Cohere For AI and Cohere, including advancements in data arbitrage,
multilingual preference training, and model merging, Aya Expanse sets a new
state-of-the-art in multilingual performance. Our evaluations on the
Arena-Hard-Auto dataset, translated into 23 languages, demonstrate that Aya
Expanse 8B and 32B outperform leading open-weight models in their respective
parameter classes, including Gemma 2, Qwen 2.5, and Llama 3.1, achieving up to
a 76.6% win-rate. Notably, Aya Expanse 32B outperforms Llama 3.1 70B, a model
with twice as many parameters, achieving a 54.0% win-rate. In this short
technical report, we present extended evaluation results for the Aya Expanse
model family and release their open-weights, together with a new multilingual
evaluation dataset m-ArenaHard.
