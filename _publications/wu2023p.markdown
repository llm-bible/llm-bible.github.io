---
layout: publication
title: P45;laplacian Adaptation For Generative Pre45;trained Vision45;language Models
authors: Wu Haoyuan, Zhang Xinyun, Xu Peng, Liao Peiyu, Yao Xufeng, Yu Bei
conference: "Arxiv"
year: 2023
bibkey: wu2023p
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.10613"}
tags: ['Applications', 'Attention Mechanism', 'Fine Tuning', 'Model Architecture', 'Tools']
---
Vision45;Language models (VLMs) pre45;trained on large corpora have demonstrated notable success across a range of downstream tasks. In light of the rapidly increasing size of pre45;trained VLMs parameter45;efficient transfer learning (PETL) has garnered attention as a viable alternative to full fine45;tuning. One such approach is the adapter which introduces a few trainable parameters into the pre45;trained models while preserving the original parameters during adaptation. In this paper we present a novel modeling framework that recasts adapter tuning after attention as a graph message passing process on attention graphs where the projected query and value features and attention matrix constitute the node features and the graph adjacency matrix respectively. Within this framework tuning adapters in VLMs necessitates handling heterophilic graphs owing to the disparity between the projected query and value space. To address this challenge we propose a new adapter architecture p45;adapter which employs p45;Laplacian message passing in Graph Neural Networks (GNNs). Specifically the attention weights are re45;normalized based on the features and the features are then aggregated using the calibrated attention matrix enabling the dynamic exploitation of information with varying frequencies in the heterophilic attention graphs. We conduct extensive experiments on different pre45;trained VLMs and multi45;modal tasks including visual question answering visual entailment and image captioning. The experimental results validate our methods significant superiority over other PETL methods.
