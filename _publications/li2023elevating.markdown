---
layout: publication
title: 'Motcoder: Elevating Large Language Models With Modular Of Thought For Challenging Programming Tasks'
authors: Jingyao Li, Pengguang Chen, Bin Xia, Hong Xu, Jiaya Jia
conference: "Arxiv"
year: 2023
bibkey: li2023elevating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2312.15960'}
  - {name: "Code", url: 'https://github.com/dvlab-research/MoTCoder'}
tags: ['Has Code', 'Tools']
---
Large Language Models (LLMs) have showcased impressive capabilities in
handling straightforward programming tasks. However, their performance tends to
falter when confronted with more challenging programming problems. We observe
that conventional models often generate solutions as monolithic code blocks,
restricting their effectiveness in tackling intricate questions. To overcome
this limitation, we present Module-of-Thought Coder (MoTCoder). We introduce a
framework for MoT instruction tuning, designed to promote the decomposition of
tasks into logical sub-tasks and sub-modules. Our investigations reveal that,
through the cultivation and utilization of sub-modules, MoTCoder significantly
improves both the modularity and correctness of the generated solutions,
leading to substantial pass@1 improvements of 5.9% on APPS and 5.8% on
CodeContests. MoTCoder also achieved significant improvements in
self-correction capabilities, surpassing the current SOTA by 3.3%.
Additionally, we provide an analysis of between problem complexity and optimal
module decomposition and evaluate the maintainability index, confirming that
the code generated by MoTCoder is easier to understand and modify, which can be
beneficial for long-term code maintenance and evolution. Our codes are
available at https://github.com/dvlab-research/MoTCoder.
