---
layout: publication
title: 'A Family Of Pretrained Transformer Language Models For Russian'
authors: Dmitry Zmitrovich, Alexander Abramov, Andrey Kalmykov, Maria Tikhonova, Ekaterina Taktasheva, Danil Astafurov, Mark Baushenko, Artem Snegirev, Vitalii Kadulin, Sergey Markov, Tatiana Shavrina, Vladislav Mikhailov, Alena Fenogenova
conference: "https://aclanthology.org/2024.lrec-main.45/"
year: 2023
bibkey: zmitrovich2023family
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.10931"}
tags: ['Training Techniques', 'Model Architecture', 'GPT', 'Pretraining Methods', 'BERT', 'Transformer', 'Applications', 'Attention Mechanism']
---
Transformer language models (LMs) are fundamental to NLP research
methodologies and applications in various languages. However, developing such
models specifically for the Russian language has received little attention.
This paper introduces a collection of 13 Russian Transformer LMs, which spans
encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder
(ruT5, FRED-T5) architectures. We provide a report on the model architecture
design and pretraining, and the results of evaluating their generalization
abilities on Russian language understanding and generation datasets and
benchmarks. By pretraining and releasing these specialized Transformer LMs, we
aim to broaden the scope of the NLP research directions and enable the
development of industrial solutions for the Russian language.
