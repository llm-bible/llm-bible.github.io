---
layout: publication
title: A Family of Pretrained Transformer Language Models for Russian
authors: Zmitrovich Dmitry, Abramov Alexander, Kalmykov Andrey, Tikhonova Maria, Taktasheva Ekaterina, Astafurov Danil, Baushenko Mark, Snegirev Artem, Kadulin Vitalii, Markov Sergey, Shavrina Tatiana, Mikhailov Vladislav, Fenogenova Alena
conference: "https://aclanthology.org/"
year: 2023
bibkey: zmitrovich2023family
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.10931"}
tags: ['Model Architecture', 'Transformer', 'A']
---
Transformer language models (LMs) are fundamental to NLP research methodologies and applications in various languages. However developing such models specifically for the Russian language has received little attention. This paper introduces a collection of 13 Russian Transformer LMs which spans encoder (ruBERT ruRoBERTa ruELECTRA) decoder (ruGPT-3) and encoder-decoder (ruT5 FRED-T5) architectures. We provide a report on the model architecture design and pretraining and the results of evaluating their generalization abilities on Russian language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs we aim to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language.
