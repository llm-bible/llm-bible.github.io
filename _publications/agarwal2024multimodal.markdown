---
layout: publication
title: Mememqa Multimodal Question Answering For Memes Via Rationale45;based Inferencing
authors: Agarwal Siddhant, Sharma Shivam, Nakov Preslav, Chakraborty Tanmoy
conference: "Arxiv"
year: 2024
bibkey: agarwal2024multimodal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.11215"}
tags: ['Applications', 'Interpretability And Explainability', 'Language Modeling', 'Multimodal Models', 'RAG', 'Security', 'Tools']
---
Memes have evolved as a prevalent medium for diverse communication ranging from humour to propaganda. With the rising popularity of image45;focused content there is a growing need to explore its potential harm from different aspects. Previous studies have analyzed memes in closed settings 45; detecting harm applying semantic labels and offering natural language explanations. To extend this research we introduce MemeMQA a multimodal question45;answering framework aiming to solicit accurate responses to structured questions while providing coherent explanations. We curate MemeMQACorpus a new dataset featuring 1880 questions related to 1122 memes with corresponding answer45;explanation pairs. We further propose ARSENAL a novel two45;stage multimodal framework that leverages the reasoning capabilities of LLMs to address MemeMQA. We benchmark MemeMQA using competitive baselines and demonstrate its superiority 45; ~1837; enhanced answer prediction accuracy and distinct text generation lead across various metrics measuring lexical and semantic alignment over the best baseline. We analyze ARSENALs robustness through diversification of question45;set confounder45;based evaluation regarding MemeMQAs generalizability and modality45;specific assessment enhancing our understanding of meme interpretation in the multimodal communication landscape.
