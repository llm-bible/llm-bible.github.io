---
layout: publication
title: Leveraging Pre45;trained Language Models For Conversational Information Seeking From Text
authors: Bellan Patrizio, Dragoni Mauro, Ghidini Chiara
conference: "Arxiv"
year: 2022
bibkey: bellan2022leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.03542"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques', 'Transformer']
---
Recent advances in Natural Language Processing and in particular on the construction of very large pre45;trained language representation models is opening up new perspectives on the construction of conversational information seeking (CIS) systems. In this paper we investigate the usage of in45;context learning and pre45;trained language representation models to address the problem of information extraction from process description documents in an incremental question and answering oriented fashion. In particular we investigate the usage of the native GPT45;3 (Generative Pre45;trained Transformer 3) model together with two in45;context learning customizations that inject conceptual definitions and a limited number of samples in a few shot45;learning fashion. The results highlight the potential of the approach and the usefulness of the in45;context learning customizations which can substantially contribute to address the training data challenge of deep learning based NLP techniques the BPM field. It also highlight the challenge posed by control flow relations for which further training needs to be devised.
