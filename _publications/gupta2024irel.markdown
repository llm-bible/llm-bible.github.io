---
layout: publication
title: Irel At Semeval45;2024 Task 9 Improving Conventional Prompting Methods For Brain Teasers
authors: Gupta Harshit, Chaudhary Manav, Raha Tathagata, Subramanian Shivansh, Varma Vasudeva
conference: "Arxiv"
year: 2024
bibkey: gupta2024irel
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.16129"}
tags: ['Applications', 'Prompting']
---
This paper describes our approach for SemEval45;2024 Task 9 BRAINTEASER A Novel Task Defying Common Sense. The BRAINTEASER task comprises multiple45;choice Question Answering designed to evaluate the models lateral thinking capabilities. It consists of Sentence Puzzle and Word Puzzle subtasks that require models to defy default common45;sense associations and exhibit unconventional thinking. We propose a unique strategy to improve the performance of pre45;trained language models notably the Gemini 1.0 Pro Model in both subtasks. We employ static and dynamic few45;shot prompting techniques and introduce a model45;generated reasoning strategy that utilizes the LLMs reasoning capabilities to improve performance. Our approach demonstrated significant improvements showing that it performed better than the baseline models by a considerable margin but fell short of performing as well as the human annotators thus highlighting the efficacy of the proposed strategies.
