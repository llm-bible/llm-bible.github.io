---
layout: publication
title: Question45;analysis Prompting Improves LLM Performance In Reasoning Tasks
authors: Yugeswardeenoo Dharunish, Zhu Kevin, O'brien Sean
conference: "Arxiv"
year: 2024
bibkey: yugeswardeenoo2024question
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.03624"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting']
---
Although LLMs have the potential to transform many fields they still underperform humans in reasoning tasks. Existing methods induce the model to produce step45;by45;step calculations but this research explores the question Does making the LLM analyze the question improve its performance We propose a novel prompting strategy called Question Analysis Prompting (QAP) in which the model is prompted to explain the question in n words before solving. The value of n influences the length of response generated by the model. QAP is evaluated on GPT 3.5 Turbo and GPT 4 Turbo on arithmetic datasets GSM8K AQuA and SAT and commonsense dataset StrategyQA. QAP is compared with other state45;of45;the45;art prompts including Chain45;of45;Thought (CoT) Plan and Solve Prompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all state45;of45;the45;art prompts on AQuA and SAT datasets on both GPT3.5 and GPT4. QAP consistently ranks among the top45;2 prompts on 7537; of the tests. A key factor of QAP performance can be attributed to response length where detailed responses are beneficial when answering harder questions but can negatively affect easy questions.
