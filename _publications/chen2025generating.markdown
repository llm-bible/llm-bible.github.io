---
layout: publication
title: 'Generating Medically-informed Explanations For Depression Detection Using Llms'
authors: Xiangyong Chen, Xiaochuan Lin
conference: "Arxiv"
year: 2025
bibkey: chen2025generating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.14671"}
tags: ['Tools', 'Interpretability and Explainability', 'RAG', 'Model Architecture', 'Reinforcement Learning', 'Interpretability', 'BERT']
---
Early detection of depression from social media data offers a valuable
opportunity for timely intervention. However, this task poses significant
challenges, requiring both professional medical knowledge and the development
of accurate and explainable models. In this paper, we propose LLM-MTD (Large
Language Model for Multi-Task Depression Detection), a novel approach that
leverages a pre-trained large language model to simultaneously classify social
media posts for depression and generate textual explanations grounded in
medical diagnostic criteria. We train our model using a multi-task learning
framework with a combined loss function that optimizes both classification
accuracy and explanation quality. We evaluate LLM-MTD on the benchmark Reddit
Self-Reported Depression Dataset (RSDD) and compare its performance against
several competitive baseline methods, including traditional machine learning
and fine-tuned BERT. Our experimental results demonstrate that LLM-MTD achieves
state-of-the-art performance in depression detection, showing significant
improvements in AUPRC and other key metrics. Furthermore, human evaluation of
the generated explanations reveals their relevance, completeness, and medical
accuracy, highlighting the enhanced interpretability of our approach. This work
contributes a novel methodology for depression detection that combines the
power of large language models with the crucial aspect of explainability.
