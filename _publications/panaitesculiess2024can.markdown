---
layout: publication
title: "Can Watermarking Large Language Models Prevent Copyrighted Text Generation And Hide Training Data?"
authors: Panaitescu-liess Michael-andrei, Che Zora, An Bang, Xu Yuancheng, Pathmanathan Pankayaraj, Chakraborty Souradip, Zhu Sicheng, Goldstein Tom, Huang Furong
conference: "Arxiv"
year: 2024
bibkey: panaitesculiess2024can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.17417"}
tags: ['Applications', 'Language Modeling', 'Pretraining Methods', 'Security', 'Training Techniques']
---
Large Language Models (LLMs) have demonstrated impressive capabilities in generating diverse and contextually rich text. However concerns regarding copyright infringement arise as LLMs may inadvertently produce copyrighted material. In this paper we first investigate the effectiveness of watermarking LLMs as a deterrent against the generation of copyrighted texts. Through theoretical analysis and empirical evaluation we demonstrate that incorporating watermarks into LLMs significantly reduces the likelihood of generating copyrighted content thereby addressing a critical concern in the deployment of LLMs. Additionally we explore the impact of watermarking on Membership Inference Attacks (MIAs) which aim to discern whether a sample was part of the pretraining dataset and may be used to detect copyright violations. Surprisingly we find that watermarking adversely affects the success rate of MIAs complicating the task of detecting copyrighted text in the pretraining dataset. Finally we propose an adaptive technique to improve the success rate of a recent MIA under watermarking. Our findings underscore the importance of developing adaptive methods to study critical problems in LLMs with potential legal implications.
