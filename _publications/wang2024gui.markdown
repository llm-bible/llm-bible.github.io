---
layout: publication
title: 'GUI Agents With Foundation Models: A Comprehensive Survey'
authors: Shuai Wang, Weiwen Liu, Jingxuan Chen, Yuqi Zhou, Weinan Gan, Xingshan Zeng, Yuhan Che, Shuai Yu, Xinlong Hao, Kun Shao, Bin Wang, Chuhan Wu, Yasheng Wang, Ruiming Tang, Jianye Hao
conference: "Arxiv"
year: 2024
bibkey: wang2024gui
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.04890'}
tags: ['Agentic', 'RAG', 'Applications', 'Tools', 'Multimodal Models', 'Survey Paper', 'Reinforcement Learning']
---
Recent advances in foundation models, particularly Large Language Models
(LLMs) and Multimodal Large Language Models (MLLMs), have facilitated the
development of intelligent agents capable of performing complex tasks. By
leveraging the ability of (M)LLMs to process and interpret Graphical User
Interfaces (GUIs), these agents can autonomously execute user instructions,
simulating human-like interactions such as clicking and typing. This survey
consolidates recent research on (M)LLM-based GUI agents, highlighting key
innovations in data resources, frameworks, and applications. We begin by
reviewing representative datasets and benchmarks, followed by an overview of a
generalized, unified framework that encapsulates the essential components of
prior studies, supported by a detailed taxonomy. Additionally, we explore
relevant commercial applications. Drawing insights from existing work, we
identify key challenges and propose future research directions. We hope this
survey will inspire further advancements in the field of (M)LLM-based GUI
agents.
