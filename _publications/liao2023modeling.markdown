---
layout: publication
title: Modeling Complex Mathematical Reasoning Via Large Language Model Based Mathagent
authors: Liao Haoran, Du Qinyi, Hu Shaohua, He Hao, Xu Yanyan, Tian Jidong, Jin Yaohui
conference: "Arxiv"
year: 2023
bibkey: liao2023modeling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.08926"}
tags: ['Agentic', 'GPT', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Tools']
---
Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements associate domain knowledge perform compound logical reasoning and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs thus leading to confusion in generation. In this work we explore the potential of enhancing LLMs with agents by meticulous decomposition and modeling of mathematical reasoning process. Specifically we propose a formal description of the mathematical solving and extend LLMs with an agent45;based zero45;shot framework named bf123;P125;lanner45;bf123;R125;easoner45;bf123;E125;xecutor45;bf123;R125;eflector (PRER). We further provide and implement two MathAgents that define the logical forms and inherent relations via a pool of actions in different grains and orientations MathAgent45;M adapts its actions to LLMs while MathAgent45;H aligns with humankind. Experiments on miniF2F and MATH have demonstrated the effectiveness of PRER and proposed MathAgents achieving an increase of 12.337;(53.937;xrightarrow123;125;66.237;) on the MiniF2F 9.237; (49.837;xrightarrow123;125;59.037;) on MATH and 13.237;(23.237;xrightarrow123;125;35.437;) for level45;5 problems of MATH against GPT45;4. Further analytical results provide more insightful perspectives on exploiting the behaviors of LLMs as agents.
