---
layout: publication
title: 'Catching Chameleons: Detecting Evolving Disinformation Generated Using Large Language Models'
authors: Bohan Jiang, Chengshuai Zhao, Zhen Tan, Huan Liu
conference: "Arxiv"
year: 2024
bibkey: jiang2024catching
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2406.17992'}
tags: ['Reinforcement Learning', 'RAG', 'Prompting', 'Tools']
---
Despite recent advancements in detecting disinformation generated by large
language models (LLMs), current efforts overlook the ever-evolving nature of
this disinformation. In this work, we investigate a challenging yet practical
research problem of detecting evolving LLM-generated disinformation.
Disinformation evolves constantly through the rapid development of LLMs and
their variants. As a consequence, the detection model faces significant
challenges. First, it is inefficient to train separate models for each
disinformation generator. Second, the performance decreases in scenarios when
evolving LLM-generated disinformation is encountered in sequential order. To
address this problem, we propose DELD (Detecting Evolving LLM-generated
Disinformation), a parameter-efficient approach that jointly leverages the
general fact-checking capabilities of pre-trained language models (PLM) and the
independent disinformation generation characteristics of various LLMs. In
particular, the learned characteristics are concatenated sequentially to
facilitate knowledge accumulation and transformation. DELD addresses the issue
of label scarcity by integrating the semantic embeddings of disinformation with
trainable soft prompts to elicit model-specific knowledge. Our experiments show
that \textit\{DELD\} significantly outperforms state-of-the-art methods.
Moreover, our method provides critical insights into the unique patterns of
disinformation generation across different LLMs, offering valuable perspectives
in this line of research.
