---
layout: publication
title: Memeguard An LLM And Vlm-based Framework For Advancing Content Moderation Via Meme Intervention
authors: Jha Prince, Jain Raghav, Mandal Konika, Chadha Aman, Saha Sriparna, Bhattacharyya Pushpak
conference: "Arxiv"
year: 2024
bibkey: jha2024llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.05344"}
tags: ['Multimodal Models', 'RAG', 'Reinforcement Learning', 'Tools']
---
In the digital world memes present a unique challenge for content moderation due to their potential to spread harmful content. Although detection methods have improved proactive solutions such as intervention are still limited with current research focusing mostly on text-based content neglecting the widespread influence of multimodal content like memes. Addressing this gap we present (textitMemeGuard) a comprehensive framework leveraging Large Language Models (LLMs) and Visual Language Models (VLMs) for meme intervention. (textitMemeGuard) harnesses a specially fine-tuned VLM (textitVLMeme) for meme interpretation and a multimodal knowledge selection and ranking mechanism ((textitMKS)) for distilling relevant knowledge. This knowledge is then employed by a general-purpose LLM to generate contextually appropriate interventions. Another key contribution of this work is the (textit)(textbfI)ntervening (textit)(textbfC)yberbullying in (textbfM)ultimodal (textbfM)emes (ICMM) dataset a high-quality labeled dataset featuring toxic memes and their corresponding human-annotated interventions. We leverage (textitICMM) to test (textitMemeGuard) demonstrating its proficiency in generating relevant and effective responses to toxic memes.
