---
layout: publication
title: 'Large Language Models Can Impersonate Politicians And Other Public Figures'
authors: Steffen Herbold, Alexander Trautsch, Zlata Kikteva, Annette Hautli-janisz
conference: "Arxiv"
year: 2024
bibkey: herbold2024large
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2407.12855'}
tags: ['Uncategorized']
---
Modern AI technology like Large language models (LLMs) has the potential to
pollute the public information sphere with made-up content, which poses a
significant threat to the cohesion of societies at large. A wide range of
research has shown that LLMs are capable of generating text of impressive
quality, including persuasive political speech, text with a pre-defined style,
and role-specific content. But there is a crucial gap in the literature: We
lack large-scale and systematic studies of how capable LLMs are in
impersonating political and societal representatives and how the general public
judges these impersonations in terms of authenticity, relevance and coherence.
We present the results of a study based on a cross-section of British society
that shows that LLMs are able to generate responses to debate questions that
were part of a broadcast political debate programme in the UK. The impersonated
responses are judged to be more authentic and relevant than the original
responses given by people who were impersonated. This shows two things: (1)
LLMs can be made to contribute meaningfully to the public political debate and
(2) there is a dire need to inform the general public of the potential harm
this can have on society.
