---
layout: publication
title: 'Adapting Large Language Models For Multi-domain Retrieval-augmented-generation'
authors: Alexandre Misrahi, Nadezhda Chirkova, Maxime Louis, Vassilina Nikoulina
conference: "Arxiv"
year: 2025
bibkey: misrahi2025adapting
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.02411'}
tags: ['RAG', 'Efficiency and Optimization', 'Security', 'Training Techniques', 'Applications', 'Distillation', 'Fine-Tuning', 'Pretraining Methods']
---
Retrieval-Augmented Generation (RAG) enhances LLM factuality, but
multi-domain applications face challenges like lack of diverse benchmarks and
poor out-of-domain generalization. The first contribution of this work is to
introduce a diverse benchmark comprising a variety of question-answering tasks
from 8 sources and covering 13 domains. Our second contribution consists in
systematically testing out-of-domain generalization for typical RAG tuning
strategies. While our findings reveal that standard fine-tuning fails to
generalize effectively, we show that sequence-level distillation with
teacher-generated labels improves out-of-domain performance by providing more
coherent supervision. Our findings highlight key strategies for improving
multi-domain RAG robustness.
