---
layout: publication
title: Codev Empowering Llms For Verilog Generation Through Multi45;level Summarization
authors: Zhao Yang, Huang Di, Li Chongxiao, Jin Pengwei, Nan Ziyuan, Ma Tianyun, Qi Lei, Pan Yansong, Zhang Zhenxing, Zhang Rui, Zhang Xishan, Du Zidong, Guo Qi, Hu Xing, Chen Yunji
conference: "Arxiv"
year: 2024
bibkey: zhao2024empowering
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.10424"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
The increasing complexity and high costs associated with modern processor design have led to a surge in demand for processor design automation. Instruction45;tuned large language models (LLMs) have demonstrated remarkable performance in automatically generating code for general45;purpose programming languages like Python. However these methods fail on hardware description languages (HDLs) like Verilog due to the scarcity of high45;quality instruction tuning data as even advanced LLMs like GPT45;3.5 exhibit limited performance on Verilog generation. Regarding this issue we observe that (1) Verilog code collected from the real world has higher quality than those generated by LLMs. (2) LLMs like GPT45;3.5 excel in summarizing Verilog code rather than generating it. Based on these observations this paper introduces CodeV a series of open45;source instruction45;tuned Verilog generation LLMs. Instead of generating descriptions first and then getting the corresponding code from advanced LLMs we prompt the LLM with Verilog code and let the LLM generate the corresponding natural language description by multi45;level summarization. Experimental results show that CodeV relatively surpasses the previous open45;source SOTA by 14.437; (BetterV in VerilogEval) and 11.337; (RTLCoder in RTLLM) respectively and also relatively outperforms previous commercial SOTA GPT45;4 by 22.137; in VerilogEval.
