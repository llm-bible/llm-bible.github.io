---
layout: publication
title: 'Latentllm: Attention-aware Joint Tensor Compression'
authors: Toshiaki Perry Koike-akino, Xiangyu Perry Chen, Jing Perry Liu, Ye Perry Wang, Perry Pu, Wang, Matthew Brand
conference: "Arxiv"
year: 2025
bibkey: koikeakino2025attention
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.18413"}
tags: ['Efficiency and Optimization', 'Model Architecture', 'Tools', 'Quantization', 'Attention Mechanism']
---
Modern foundation models such as large language models (LLMs) and large multi-modal models (LMMs) require a massive amount of computational and memory resources. We propose a new framework to convert such LLMs/LMMs into a reduced-dimension latent structure. Our method extends a local activation-aware tensor decomposition to a global attention-aware joint tensor de-composition. Our framework can significantly improve the model accuracy over the existing model compression methods when reducing the latent dimension to realize computationally/memory-efficient LLMs/LLMs. We show the benefit on several benchmark including multi-modal reasoning tasks.
