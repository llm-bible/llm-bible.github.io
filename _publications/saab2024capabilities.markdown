---
layout: publication
title: Capabilities Of Gemini Models In Medicine
authors: Saab Khaled, Tu Tao, Weng Wei-hung, Tanno Ryutaro, Stutz David, Wulczyn Ellery, Zhang Fan, Strother Tim, Park Chunjong, Vedadi Elahe, Chaves Juanma Zambrano, Hu Szu-yeu, Schaekermann Mike, Kamath Aishwarya, Cheng Yong, Barrett David G. T., Cheung Cathy, Mustafa Basil, Palepu Anil, Mcduff Daniel, Hou Le, Golany Tomer, Liu Luyang, Alayrac Jean-baptiste, Houlsby Neil, Tomasev Nenad, Freyberg Jan, Lau Charles, Kemp Jonas, Lai Jeremy, Azizi Shekoofeh, Kanada Kimberly, Man Siwai, Kulkarni Kavita, Sun Ruoxi, Shakeri Siamak, He Luheng, Caine Ben, Webson Albert, Latysheva Natasha, Johnson Melvin, Mansfield Philip, Lu Jian, Rivlin Ehud, Anderson Jesper, Green Bradley, Wong Renee, Krause Jonathan, Shlens Jonathon, Dominowska Ewa, Eslami S. M. Ali, Chou Katherine, Cui Claire, Vinyals Oriol, Kavukcuoglu Koray, Manyika James, Dean Jeff, Hassabis Demis, Matias Yossi, Webster Dale, Barral Joelle, Corrado Greg, Semturs Christopher, Mahdavi S. Sara, Gottweis Juraj, Karthikesalingam Alan, Natarajan Vivek
conference: "Arxiv"
year: 2024
bibkey: saab2024capabilities
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.18416"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Multimodal Models', 'RAG', 'Reinforcement Learning', 'Responsible AI']
---
Excellence in a wide variety of medical applications poses considerable challenges for AI requiring advanced reasoning access to up45;to45;date medical knowledge and understanding of complex multimodal data. Gemini models with strong general capabilities in multimodal and long45;context reasoning offer exciting possibilities in medicine. Building on these core strengths of Gemini we introduce Med45;Gemini a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly use web search and that can be efficiently tailored to novel modalities using custom encoders. We evaluate Med45;Gemini on 14 medical benchmarks establishing new state45;of45;the45;art (SoTA) performance on 10 of them and surpass the GPT45;4 model family on every benchmark where a direct comparison is viable often by a wide margin. On the popular MedQA (USMLE) benchmark our best45;performing Med45;Gemini model achieves SoTA performance of 91.137; accuracy using a novel uncertainty45;guided search strategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU (health amp; medicine) Med45;Gemini improves over GPT45;4V by an average relative margin of 44.537;. We demonstrate the effectiveness of Med45;Geminis long45;context capabilities through SoTA performance on a needle45;in45;a45;haystack retrieval task from long de45;identified health records and medical video question answering surpassing prior bespoke methods using only in45;context learning. Finally Med45;Geminis performance suggests real45;world utility by surpassing human experts on tasks such as medical text summarization alongside demonstrations of promising potential for multimodal medical dialogue medical research and education. Taken together our results offer compelling evidence for Med45;Geminis potential although further rigorous evaluation will be crucial before real45;world deployment in this safety45;critical domain.
