---
layout: publication
title: 'Prompting Implicit Discourse Relation Annotation'
authors: Yung Frances, Ahmad Mansoor, Scholman Merel, Demberg Vera
conference: "Arxiv"
year: 2024
bibkey: yung2024prompting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.04918"}
tags: ['Few Shot', 'GPT', 'Model Architecture', 'Prompting', 'Training Techniques']
---
Pre-trained large language models such as ChatGPT archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers. Nonetheless ChatGPTs performance in the task of implicit discourse relation classification prompted by a standard multiple-choice question is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches. This work investigates several proven prompting techniques to improve ChatGPTs recognition of discourse relations. In particular we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings.
