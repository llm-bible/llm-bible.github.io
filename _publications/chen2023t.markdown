---
layout: publication
title: T45;eval Evaluating The Tool Utilization Capability Of Large Language Models Step By Step
authors: Chen Zehui, Du Weihua, Zhang Wenwei, Liu Kuikun, Liu Jiangning, Zheng Miao, Zhuo Jingming, Zhang Songyang, Lin Dahua, Chen Kai, Zhao Feng
conference: "Arxiv"
year: 2023
bibkey: chen2023t
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.14033"}
  - {name: "Code", url: "https://github.com/open&#45;compass/T&#45;Eval"}
tags: ['Applications', 'Has Code', 'Pretraining Methods', 'Survey Paper', 'Tools']
---
Large language models (LLM) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet how to evaluate and analyze the tool45;utilization capability of LLMs is still under45;explored. In contrast to previous works that evaluate models holistically we comprehensively decompose the tool utilization into multiple sub45;processes including instruction following planning reasoning retrieval understanding and review. Based on that we further introduce T45;Eval to evaluate the tool utilization capability step by step. T45;Eval disentangles the tool utilization evaluation into several sub45;domains along model capabilities facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T45;Eval and in45;depth analysis of various LLMs. T45;Eval not only exhibits consistency with the outcome45;oriented evaluation but also provides a more fine45;grained analysis of the capabilities of LLMs providing a new perspective in LLM evaluation on tool45;utilization ability. The benchmark will be available at https://github.com/open&#45;compass/T&#45;Eval.
