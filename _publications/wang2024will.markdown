---
layout: publication
title: Will The Real Linda Please Stand Up...to Large Language Models Examining The Representativeness Heuristic In Llms
authors: Wang Pengda, Xiao Zilin, Chen Hanjie, Oswald Frederick L.
conference: "Arxiv"
year: 2024
bibkey: wang2024will
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.01461"}
tags: ['Ethics And Bias', 'Prompting', 'Training Techniques']
---
Although large language models (LLMs) have demonstrated remarkable proficiency in modeling text and generating human-like text they may exhibit biases acquired from training data in doing so. Specifically LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This research investigates the impact of the representativeness heuristic on LLM reasoning. We created ReHeAT (Representativeness Heuristic AI Testing) a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to ReHeAT all exhibited representativeness heuristic biases. We further identify that the models reasoning steps are often incorrectly based on a stereotype rather than on the problems description. Interestingly the performance improves when adding a hint in the prompt to remind the model to use its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when LLMs possess the correct knowledge while falling into a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it.
