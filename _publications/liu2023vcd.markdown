---
layout: publication
title: VCD Visual Causality Discovery for Cross-Modal Question Reasoning
authors: Liu Yang, Tan Ying, Luo Jingzhou, Chen Weixing
conference: "Arxiv"
year: 2023
bibkey: liu2023vcd
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.08083"}
tags: ['Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Tools', 'Transformer']
---
Existing visual question reasoning methods usually fail to explicitly discover the inherent causal mechanism and ignore jointly modeling cross-modal event temporality and causality. In this paper we propose a visual question reasoning framework named Cross-Modal Question Reasoning (CMQR) to discover temporal causal structure and mitigate visual spurious correlation by causal intervention. To explicitly discover visual causal structure the Visual Causality Discovery (VCD) architecture is proposed to find question-critical scene temporally and disentangle the visual spurious correlations by attention-based front-door causal intervention module named Local-Global Causal Attention Module (LGCAM). To align the fine-grained interactions between linguistic semantics and spatial-temporal representations we build an Interactive Visual-Linguistic Transformer (IVLT) that builds the multi-modal co-occurrence interactions between visual and linguistic content. Extensive experiments on four datasets demonstrate the superiority of CMQR for discovering visual causal structures and achieving robust question reasoning.
