---
layout: publication
title: Evaluating Large Language Models In Theory Of Mind Tasks
authors: Kosinski Michal
conference: "Arxiv"
year: 2023
bibkey: kosinski2023evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2302.02083"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting']
---
Eleven Large Language Models (LLMs) were assessed using a custom45;made battery of false45;belief tasks considered a gold standard in testing Theory of Mind (ToM) in humans. The battery included 640 prompts spread across 40 diverse tasks each one including a false45;belief scenario three closely matched true45;belief control scenarios and the reversed versions of all four. To solve a single task a model needed to correctly answer 16 prompts across all eight scenarios. Smaller and older models solved no tasks; GPT45;345;davinci45;003 (from November 2022) and ChatGPT45;3.545;turbo (from March 2023) solved 2037; of the tasks; ChatGPT45;4 (from June 2023) solved 7537; of the tasks matching the performance of six45;year45;old children observed in past studies. We explore the potential interpretation of these findings including the intriguing possibility that ToM previously considered exclusive to humans may have spontaneously emerged as a byproduct of LLMs improving language skills.
