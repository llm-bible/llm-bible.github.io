---
layout: publication
title: 'Evaluating Large Language Models In Theory Of Mind Tasks'
authors: Kosinski Michal
conference: "Arxiv"
year: 2023
bibkey: kosinski2023evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2302.02083"}
tags: ['GPT', 'Model Architecture', 'Prompting', 'Uncategorized']
---
Eleven Large Language Models (LLMs) were assessed using a custom-made battery
of false-belief tasks, considered a gold standard in testing Theory of Mind
(ToM) in humans. The battery included 640 prompts spread across 40 diverse
tasks, each one including a false-belief scenario, three closely matched
true-belief control scenarios, and the reversed versions of all four. To solve
a single task, a model needed to correctly answer 16 prompts across all eight
scenarios. Smaller and older models solved no tasks; GPT-3-davinci-003 (from
November 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20% of the tasks;
ChatGPT-4 (from June 2023) solved 75% of the tasks, matching the performance of
six-year-old children observed in past studies. We explore the potential
interpretation of these findings, including the intriguing possibility that
ToM, previously considered exclusive to humans, may have spontaneously emerged
as a byproduct of LLMs' improving language skills.
