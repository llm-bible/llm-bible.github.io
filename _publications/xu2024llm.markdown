---
layout: publication
title: 'Llm-based Discriminative Reasoning For Knowledge Graph Question Answering'
authors: Mufan Xu, Kehai Chen, Xuefeng Bai, Muyun Yang, Tiejun Zhao, Min Zhang
conference: "Arxiv"
year: 2024
bibkey: xu2024llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.12643"}
tags: ['Model Architecture', 'GPT', 'Pretraining Methods', 'Transformer', 'Applications']
---
Large language models (LLMs) based on generative pre-trained Transformer have
achieved remarkable performance on knowledge graph question-answering (KGQA)
tasks. However, LLMs often produce ungrounded subgraph planning or reasoning
results in KGQA due to the hallucinatory behavior brought by the generative
paradigm. To tackle this issue, we propose READS to reformulate the KGQA
process into discriminative subtasks, which simplifies the search space for
each subtasks. Based on the subtasks, we design a new corresponding
discriminative inference strategy to conduct the reasoning for KGQA, thereby
alleviating hallucination and ungrounded reasoning issues in LLMs. Experimental
results show that the proposed approach outperforms multiple strong comparison
methods, along with achieving state-of-the-art performance on widely used
benchmarks WebQSP and CWQ.
