---
layout: publication
title: Asynchronous and Segmented Bidirectional Encoding for NMT
authors: Yang Jingpu, Han Zehua, Xiang Mengyu, Wang Helin, Huang Yuxiao, Fang Miao
conference: "Arxiv"
year: 2024
bibkey: yang2024asynchronous
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.14849"}
tags: ['Applications', 'Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Tools', 'Transformer']
---
With the rapid advancement of Neural Machine Translation (NMT) enhancing translation efficiency and quality has become a focal point of research. Despite the commendable performance of general models such as the Transformer in various aspects they still fall short in processing long sentences and fully leveraging bidirectional contextual information. This paper introduces an improved model based on the Transformer implementing an asynchronous and segmented bidirectional decoding strategy aimed at elevating translation efficiency and accuracy. Compared to traditional unidirectional translations from left-to-right or right-to-left our method demonstrates heightened efficiency and improved translation quality particularly in handling long sentences. Experimental results on the IWSLT2017 dataset confirm the effectiveness of our approach in accelerating translation and increasing accuracy especially surpassing traditional unidirectional strategies in long sentence translation. Furthermore this study analyzes the impact of sentence length on decoding outcomes and explores the models performance in various scenarios. The findings of this research not only provide an effective encoding strategy for the NMT field but also pave new avenues and directions for future studies.
