---
layout: publication
title: 'Simple LLM Prompting Is State-of-the-art For Robust And Multilingual Dialogue Evaluation'
authors: John Mendonça, Patrícia Pereira, Helena Moniz, João Paulo Carvalho, Alon Lavie, Isabel Trancoso
conference: "Arxiv"
year: 2023
bibkey: mendonça2023simple
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2308.16797'}
tags: ['Reinforcement Learning', 'Prompting', 'Security', 'Tools']
---
Despite significant research effort in the development of automatic dialogue
evaluation metrics, little thought is given to evaluating dialogues other than
in English. At the same time, ensuring metrics are invariant to semantically
similar responses is also an overlooked topic. In order to achieve the desired
properties of robustness and multilinguality for dialogue evaluation metrics,
we propose a novel framework that takes advantage of the strengths of current
evaluation models with the newly-established paradigm of prompting Large
Language Models (LLMs). Empirical results show our framework achieves state of
the art results in terms of mean Spearman correlation scores across several
benchmarks and ranks first place on both the Robust and Multilingual tasks of
the DSTC11 Track 4 "Automatic Evaluation Metrics for Open-Domain Dialogue
Systems", proving the evaluation capabilities of prompted LLMs.
