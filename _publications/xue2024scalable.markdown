---
layout: publication
title: Ninjallm: Fast, Scalable And Cost-effective RAG Using Amazon Sagemaker And AWS Trainium And Inferentia2
authors: Xue Tengfei, Li Xuefeng, Smirnov Roman, Azim Tahir, Sadrieh Arash, Pahlavan Babak
conference: "Arxiv"
year: 2024
bibkey: xue2024scalable
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.12057"}
tags: ['Ethics And Bias', 'RAG']
---
Retrieval-augmented generation (RAG) techniques are widely used today to retrieve and present information in a conversational format. This paper presents a set of enhancements to traditional RAG techniques focusing on large language models (LLMs) fine-tuned and hosted on AWS Trainium and Inferentia2 AI chips via SageMaker. These chips are characterized by their elasticity affordability and efficient performance for AI compute tasks. Besides enabling deployment on these chips this work aims to improve tool usage add citation capabilities and mitigate the risks of hallucinations and unsafe responses due to context bias. We benchmark our RAG systems performance on the Natural Questions and HotPotQA datasets achieving an accuracy of 6237; and 5937; respectively exceeding other models such as DBRX and Mixtral Instruct.
