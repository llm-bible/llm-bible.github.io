---
layout: publication
title: 'A Study Of BERT For Non-factoid Question-answering Under Passage Length Constraints'
authors: Yosi Mass, Haggai Roitman, Shai Erera, Or Rivlin, Bar Weiner, David Konopnicki
conference: "Arxiv"
year: 2019
bibkey: mass2019study
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/1908.06780'}
tags: ['Training Techniques', 'BERT', 'Model Architecture', 'Fine-Tuning', 'Pretraining Methods']
---
We study the use of BERT for non-factoid question-answering, focusing on the
passage re-ranking task under varying passage lengths. To this end, we explore
the fine-tuning of BERT in different learning-to-rank setups, comprising both
point-wise and pair-wise methods, resulting in substantial improvements over
the state-of-the-art. We then analyze the effectiveness of BERT for different
passage lengths and suggest how to cope with large passages.
