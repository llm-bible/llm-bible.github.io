---
layout: publication
title: "Query Expansion By Prompting Large Language Models"
authors: Jagerman Rolf, Zhuang Honglei, Qin Zhen, Wang Xuanhui, Bendersky Michael
conference: "Arxiv"
year: 2023
bibkey: jagerman2023query
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.03653"}
tags: ['Few Shot', 'Pretraining Methods', 'Prompting', 'RAG']
---
Query expansion is a widely used technique to improve the recall of search systems. In this paper we propose an approach to query expansion that leverages the generative abilities of Large Language Models (LLMs). Unlike traditional query expansion approaches such as Pseudo-Relevance Feedback (PRF) that relies on retrieving a good set of pseudo-relevant documents to expand queries we rely on the generative and creative abilities of an LLM and leverage the knowledge inherent in the model. We study a variety of different prompts including zero-shot few-shot and Chain-of-Thought (CoT). We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query. Experimental results on MS-MARCO and BEIR demonstrate that query expansions generated by LLMs can be more powerful than traditional query expansion methods.
