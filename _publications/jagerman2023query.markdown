---
layout: publication
title: Query Expansion By Prompting Large Language Models
authors: Jagerman Rolf, Zhuang Honglei, Qin Zhen, Wang Xuanhui, Bendersky Michael
conference: "Arxiv"
year: 2023
bibkey: jagerman2023query
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.03653"}
tags: ['Pretraining Methods', 'Prompting', 'RAG']
---
Query expansion is a widely used technique to improve the recall of search systems. In this paper we propose an approach to query expansion that leverages the generative abilities of Large Language Models (LLMs). Unlike traditional query expansion approaches such as Pseudo45;Relevance Feedback (PRF) that relies on retrieving a good set of pseudo45;relevant documents to expand queries we rely on the generative and creative abilities of an LLM and leverage the knowledge inherent in the model. We study a variety of different prompts including zero45;shot few45;shot and Chain45;of45;Thought (CoT). We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step45;by45;step and can provide a large number of terms related to the original query. Experimental results on MS45;MARCO and BEIR demonstrate that query expansions generated by LLMs can be more powerful than traditional query expansion methods.
