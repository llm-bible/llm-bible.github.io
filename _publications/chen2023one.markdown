---
layout: publication
title: Chatgpts One45;year Anniversary Are Open45;source Large Language Models Catching Up
authors: Chen Hailin, Jiao Fangkai, Li Xingxuan, Qin Chengwei, Ravaut Mathieu, Zhao Ruochen, Xiong Caiming, Joty Shafiq
conference: "Arxiv"
year: 2023
bibkey: chen2023one
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.16989"}
tags: ['Agentic', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Survey Paper', 'Tools']
---
Upon its release in late 2022 ChatGPT has brought a seismic shift in the entire landscape of AI both in research and commerce. Through instruction45;tuning a large language model (LLM) with supervised fine45;tuning and reinforcement learning from human feedback it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success interests in LLMs have intensified with new LLMs flourishing at frequent interval across academia and industry including many start45;ups focused on LLMs. While closed45;source LLMs (e.g. OpenAIs GPT Anthropics Claude) generally outperform their open45;source counterparts the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work on the first anniversary of ChatGPT we provide an exhaustive overview of this success surveying all tasks where an open45;source LLM has claimed to be on par or better than ChatGPT.
