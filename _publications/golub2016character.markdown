---
layout: publication
title: Character-level Question Answering With Attention
authors: Golub David, He Xiaodong
conference: "Arxiv"
year: 2016
bibkey: golub2016character
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1604.00727"}
tags: ['Applications', 'Attention Mechanism', 'Merging', 'Model Architecture', 'Tools']
---
We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the SimpleQuestions dataset (Bordes et al. 2015) where we improve state-of-the-art accuracy from 63.937; to 70.937; without use of ensembles. Importantly our character-level model has 16x fewer parameters than an equivalent word-level model can be learned with significantly less data compared to previous work which relies on data augmentation and is robust to new entities in testing.
