---
layout: publication
title: 'Ract: Ranking-aware Chain-of-thought Optimization For Llms'
authors: Haowei Liu, Xuyang Wu, Guohao Sun, Zhiqiang Tao, Yi Fang
conference: "Arxiv"
year: 2024
bibkey: liu2024ranking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.14405'}
tags: ['Language Modeling', 'RAG', 'Efficiency and Optimization', 'Training Techniques', 'Fine-Tuning', 'Prompting', 'Pretraining Methods']
---
Large language models (LLMs) have shown significant promise in text reranking tasks by leveraging their advanced language understanding and reasoning capabilities. However, traditional supervised fine-tuning (SFT) approaches by ranking utilities can compromise LLMs' general-purpose abilities. To address this challenge, we propose a novel LLM-based reranking algorithm -- RaCT -- that implements SFT with Chain-of-Thought prompting, followed by a ranking preference optimization (RPO). The proposed RaCT aims to enhance ranking performance for LLMs while preserving their inherent language modeling abilities. Experimental evaluations on the three public ranking benchmarks (TREC DL, BEIR, and BRIGHT) and one LLM benchmark demonstrate the superior ranking performance of RaCT with a retained language understanding and reasoning capacity.
