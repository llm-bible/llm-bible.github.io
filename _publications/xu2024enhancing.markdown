---
layout: publication
title: Enhancing Retrieval45;augmented Lms With A Two45;stage Consistency Learning Compressor
authors: Xu Chuankai, Zhao Dongming, Wang Bo, Xing Hanwen
conference: "Arxiv"
year: 2024
bibkey: xu2024enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.02266"}
tags: ['Efficiency And Optimization', 'RAG', 'Tools']
---
Despite the prevalence of retrieval45;augmented language models (RALMs) the seamless integration of these models with retrieval mechanisms to enhance performance in document45;based tasks remains challenging. While some post45;retrieval processing Retrieval45;Augmented Generation (RAG) methods have achieved success most still lack the ability to distinguish pertinent from extraneous information leading to potential inconsistencies and reduced precision in the generated output which subsequently affects the truthfulness of the language models responses. To address these limitations this work proposes a novel two45;stage consistency learning approach for retrieved information compression in retrieval45;augmented language models to enhance performance. By incorporating consistency learning the aim is to generate summaries that maintain coherence and alignment with the intended semantic representations of a teacher model while improving faithfulness to the original retrieved documents. The proposed method is empirically validated across multiple datasets demonstrating notable enhancements in precision and efficiency for question45;answering tasks. It outperforms existing baselines and showcases the synergistic effects of combining contrastive and consistency learning paradigms within the retrieval45;augmented generation framework.
