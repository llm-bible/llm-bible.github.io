---
layout: publication
title: 'Mlongt5: A Multilingual And Efficient Text-to-text Transformer For Longer Sequences'
authors: David Uthus, Santiago Ontañón, Joshua Ainslie, Mandy Guo
conference: "Arxiv"
year: 2023
bibkey: uthus2023multilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.11129"}
tags: ['Transformer', 'Applications', 'RAG', 'Model Architecture', 'Training Techniques', 'Pretraining Methods', 'BERT']
---
We present our work on developing a multilingual, efficient text-to-text
transformer that is suitable for handling long inputs. This model, called
mLongT5, builds upon the architecture of LongT5, while leveraging the
multilingual datasets used for pretraining mT5 and the pretraining tasks of
UL2. We evaluate this model on a variety of multilingual summarization and
question-answering tasks, and the results show stronger performance for mLongT5
when compared to existing multilingual models such as mBART or M-BERT.
