---
layout: publication
title: 'Position: It''s Time To Act On The Risk Of Efficient Personalized Text Generation'
authors: Eugenia Iofinova, Andrej Jovanovic, Dan Alistarh
conference: "Arxiv"
year: 2025
bibkey: iofinova2025time
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.06560"}
tags: ['Responsible AI', 'Security', 'Training Techniques', 'Language Modeling', 'Applications']
---
The recent surge in high-quality open-source Generative AI text models (colloquially: LLMs), as well as efficient finetuning techniques, have opened the possibility of creating high-quality personalized models that generate text attuned to a specific individual's needs and are capable of credibly imitating their writing style by refining an open-source model with that person's own data. The technology to create such models is accessible to private individuals, and training and running such models can be done cheaply on consumer-grade hardware. While these advancements are a huge gain for usability and privacy, this position paper argues that the practical feasibility of impersonating specific individuals also introduces novel safety risks. For instance, this technology enables the creation of phishing emails or fraudulent social media accounts, based on small amounts of publicly available text, or by the individuals themselves to escape AI text detection. We further argue that these risks are complementary to - and distinct from - the much-discussed risks of other impersonation attacks such as image, voice, or video deepfakes, and are not adequately addressed by the larger research community, or the current generation of open- and closed-source models.
