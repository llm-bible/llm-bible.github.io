---
layout: publication
title: Do Large Language Models Understand Conversational Implicature 45;45; A Case Study With A Chinese Sitcom
authors: Yue Shisen, Song Siyuan, Cheng Xinyuan, Hu Hai
conference: "Arxiv"
year: 2024
bibkey: yue2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.19509"}
  - {name: "Code", url: "https://github.com/sjtu&#45;compling/llm&#45;pragmatics"}
tags: ['GPT', 'Has Code', 'Interpretability And Explainability', 'Model Architecture', 'Pretraining Methods', 'RAG']
---
Understanding the non45;literal meaning of an utterance is critical for large language models (LLMs) to become human45;like social communicators. In this work we introduce SwordsmanImp the first Chinese multi45;turn45;dialogue45;based dataset aimed at conversational implicature sourced from dialogues in the Chinese sitcom textit123;My Own Swordsman125;. It includes 200 carefully handcrafted questions all annotated on which Gricean maxims have been violated. We test eight close45;source and open45;source LLMs under two tasks a multiple45;choice question task and an implicature explanation task. Our results show that GPT45;4 attains human45;level accuracy (9437;) on multiple45;choice questions. CausalLM demonstrates a 78.537; accuracy following GPT45;4. Other models including GPT45;3.5 and several open45;source models demonstrate a lower accuracy ranging from 2037; to 6037; on multiple45;choice questions. Human raters were asked to rate the explanation of the implicatures generated by LLMs on their reasonability logic and fluency. While all models generate largely fluent and self45;consistent text their explanations score low on reasonability except for GPT45;4 suggesting that most LLMs cannot produce satisfactory explanations of the implicatures in the conversation. Moreover we find LLMs performance does not vary significantly by Gricean maxims suggesting that LLMs do not seem to process implicatures derived from different maxims differently. Our data and code are available at https://github.com/sjtu&#45;compling/llm&#45;pragmatics.
