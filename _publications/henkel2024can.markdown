---
layout: publication
title: Can Large Language Models Make The Grade An Empirical Study Evaluating Llms Ability To Mark Short Answer Questions In K45;12 Education
authors: Henkel Owen, Boxer Adam, Hills Libby, Roberts Bill
conference: "Arxiv"
year: 2024
bibkey: henkel2024can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.02985"}
tags: ['GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Tools']
---
This paper presents reports on a series of experiments with a novel dataset evaluating how well Large Language Models (LLMs) can mark (i.e. grade) open text responses to short answer questions Specifically we explore how well different combinations of GPT version and prompt engineering strategies performed at marking real student answers to short answer across different domain areas (Science and History) and grade45;levels (spanning ages 545;16) using a new never45;used45;before dataset from Carousel a quizzing platform. We found that GPT45;4 with basic few45;shot prompting performed well (Kappa 0.70) and importantly very close to human45;level performance (0.75). This research builds on prior findings that GPT45;4 could reliably score short answer reading comprehension questions at a performance45;level very close to that of expert human raters. The proximity to human45;level performance across a variety of subjects and grade levels suggests that LLMs could be a valuable tool for supporting low45;stakes formative assessment tasks in K45;12 education and has important implications for real45;world education delivery.
