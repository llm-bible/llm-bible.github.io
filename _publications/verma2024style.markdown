---
layout: publication
title: '"reasoning" With Rhetoric: On The Style-evidence Tradeoff In Llm-generated Counter-arguments'
authors: Preetika Verma, Kokil Jaidka, Svetlana Churina
conference: "Arxiv"
year: 2024
bibkey: verma2024style
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.08498"}
  - {name: "Code", url: "https://github.com/Preetika764/Style_control/"}
tags: ['Model Architecture', 'Reinforcement Learning', 'GPT', 'Has Code', 'Applications']
---
Large language models (LLMs) play a key role in generating evidence-based and stylistic counter-arguments, yet their effectiveness in real-world applications has been underexplored. Previous research often neglects the balance between evidentiality and style, which are crucial for persuasive arguments. To address this, we evaluated the effectiveness of stylized evidence-based counter-argument generation in Counterfire, a new dataset of 38,000 counter-arguments generated by revising counter-arguments to Reddit's ChangeMyView community to follow different discursive styles. We evaluated generic and stylized counter-arguments from basic and fine-tuned models such as GPT-3.5, PaLM-2, and Koala-13B, as well as newer models (GPT-4o, Claude Haiku, LLaMA-3.1) focusing on rhetorical quality and persuasiveness. Our findings reveals that humans prefer stylized counter-arguments over the original outputs, with GPT-3.5 Turbo performing well, though still not reaching human standards of rhetorical quality nor persuasiveness indicating a persisting style-evidence tradeoff in counter-argument generation by LLMs. We conclude with an examination of ethical considerations in LLM persuasion research, addressing potential risks of deceptive practices and the need for transparent deployment methodologies to safeguard against misuse in public discourse. The code and dataset are available at https://github.com/Preetika764/Style_control/.
