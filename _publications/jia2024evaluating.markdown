---
layout: publication
title: Simulbench Evaluating Language Models With Creative Simulation Tasks
authors: Jia Qi, Yue Xiang, Zheng Tianyu, Huang Jie, Lin Bill Yuchen
conference: "Arxiv"
year: 2024
bibkey: jia2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.07641"}
tags: ['Agentic', 'Ethics And Bias', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
We introduce SimulBench a benchmark designed to evaluate large language models (LLMs) across a diverse collection of creative simulation scenarios such as acting as a Linux terminal or playing text games with users. While these simulation tasks serve as effective measures of an LLMs general intelligence they are seldom incorporated into existing benchmarks. A major challenge is to develop an evaluation framework for testing different LLMs fairly while preserving the multi45;round interactive nature of simulation tasks between users and AI. To tackle this issue we suggest using a fixed LLM as a user agent to engage with an LLM to collect dialogues first under different tasks. Then challenging dialogue scripts are extracted for evaluating different target LLMs. To facilitate automatic assessment on DataName123;125; GPT45;4 is employed as the evaluator tasked with reviewing the quality of the final response generated by the target LLMs given multi45;turn dialogue scripts. Our comprehensive experiments indicate that these simulation tasks continue to pose a significant challenge with their unique natures and show the gap between proprietary models and the most advanced open LLMs. For example GPT45;445;turbo outperforms LLaMA45;345;70b45;Chat on 18.5537; more cases.
