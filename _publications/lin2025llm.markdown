---
layout: publication
title: 'LLM Inference Enhanced By External Knowledge: A Survey'
authors: Yu-hsuan Lin, Qian-hui Chen, Yi-jie Cheng, Jia-ren Zhang, Yi-hung Liu, Liang-yu Hsia, Yun-nung Chen
conference: "Arxiv"
year: 2025
bibkey: lin2025llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.24377"}
tags: ['Fine-Tuning', 'Survey Paper', 'Applications', 'Interpretability and Explainability', 'RAG']
---
Recent advancements in large language models (LLMs) have enhanced natural-language reasoning. However, their limited parametric memory and susceptibility to hallucination present persistent challenges for tasks requiring accurate, context-based inference. To overcome these limitations, an increasing number of studies have proposed leveraging external knowledge to enhance LLMs. This study offers a systematic exploration of strategies for using external knowledge to enhance LLMs, beginning with a taxonomy that categorizes external knowledge into unstructured and structured data. We then focus on structured knowledge, presenting distinct taxonomies for tables and knowledge graphs (KGs), detailing their integration paradigms with LLMs, and reviewing representative methods. Our comparative analysis further highlights the trade-offs among interpretability, scalability, and performance, providing insights for developing trustworthy and generalizable knowledge-enhanced LLMs.
