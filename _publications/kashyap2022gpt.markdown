---
layout: publication
title: 'Gpt-neo For Commonsense Reasoning -- A Theoretical And Practical Lens'
authors: Rohan Kashyap, Vivek Kashyap, Narendra C. P.
conference: "Arxiv"
year: 2022
bibkey: kashyap2022gpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.15593"}
tags: ['Fine-Tuning', 'Pre-Training', 'GPT', 'Model Architecture', 'Security', 'Training Techniques', 'Attention Mechanism', 'Pretraining Methods']
---
Recent work has demonstrated substantial gains in pre-training large-language
models (LLMs) followed by supervised fine-tuning on the downstream task. In
this paper, we evaluate the performance of the GPT-neo model using \\(6\\)
commonsense reasoning benchmark tasks. We aim to examine the performance of
smaller models using the GPT-neo models against several larger model baselines
such as GPT-\\(3\\), Llama-\\(2\\), MPT and Falcon. Upon fine-tuning with the
appropriate set of hyperparameters, our model achieves competitive accuracy on
several tasks. We also investigate and substantiate our results using
attention-head visualization to better understand the model performance.
Finally, we conduct various robustness tests using various methods to gauge the
model performance under numerous settings.
