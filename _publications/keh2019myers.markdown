---
layout: publication
title: Myers-briggs Personality Classification And Personality-specific Language Generation
  Using Pre-trained Language Models
authors: Sedrick Scott Keh, I-tsun Cheng
conference: Arxiv
year: 2019
citations: 30
bibkey: keh2019myers
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1907.06333'}]
tags: [BERT, Fine-Tuning, Language Modeling]
---
The Myers-Briggs Type Indicator (MBTI) is a popular personality metric that
uses four dichotomies as indicators of personality traits. This paper examines
the use of pre-trained language models to predict MBTI personality types based
on scraped labeled texts. The proposed model reaches an accuracy of \\(0.47\\) for
correctly predicting all 4 types and \\(0.86\\) for correctly predicting at least 2
types. Furthermore, we investigate the possible uses of a fine-tuned BERT model
for personality-specific language generation. This is a task essential for both
modern psychology and for intelligent empathetic systems.