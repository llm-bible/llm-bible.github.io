---
layout: publication
title: Pretrained Transformers For Simple Question Answering Over Knowledge Graphs
authors: D. Lukovnikov, A. Fischer, J. Lehmann
conference: Arxiv
year: 2020
citations: 39
bibkey: lukovnikov2020pretrained
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2001.11985'}]
tags: [Transformer, BERT, Fine-Tuning]
---
Answering simple questions over knowledge graphs is a well-studied problem in
question answering. Previous approaches for this task built on recurrent and
convolutional neural network based architectures that use pretrained word
embeddings. It was recently shown that finetuning pretrained transformer
networks (e.g. BERT) can outperform previous approaches on various natural
language processing tasks. In this work, we investigate how well BERT performs
on SimpleQuestions and provide an evaluation of both BERT and BiLSTM-based
models in datasparse scenarios.