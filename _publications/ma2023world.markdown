---
layout: publication
title: World45;to45;words Grounded Open Vocabulary Acquisition Through Fast Mapping In Vision45;language Models
authors: Ma Ziqiao, Pan Jiayi, Chai Joyce
conference: "Arxiv"
year: 2023
bibkey: ma2023world
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.08685"}
  - {name: "Code", url: "https://github.com/sled&#45;group/world&#45;to&#45;words"}
tags: ['BERT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
The ability to connect language units to their referents in the physical world referred to as grounding is crucial to learning and understanding grounded meanings of words. While humans demonstrate fast mapping in new word learning it remains unclear whether modern vision45;language models can truly represent language with their grounded meanings and how grounding may further bootstrap new word learning. To this end we introduce Grounded Open Vocabulary Acquisition (GOVA) to examine grounding and bootstrapping in open45;world language learning. As an initial attempt we propose object45;oriented BERT (OctoBERT) a novel visually45;grounded language model by pre45;training on image45;text pairs highlighting grounding as an objective. Through extensive experiments and analysis we demonstrate that OctoBERT is a more coherent and fast grounded word learner and that the grounding ability acquired during pre45;training helps the model to learn unseen words more rapidly and robustly. Our code is available at https://github.com/sled&#45;group/world&#45;to&#45;words
