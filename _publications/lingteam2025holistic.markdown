---
layout: publication
title: 'Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models'
authors: Ling Team, Caizhi Tang, Chilin Fu, Chunwei Wu, Jia Guo, Jianwen Wang, Jingyu Hu, Liang Jiang, Meng Li, Peng Jiao, Pingping Liu, Shaomian Zheng, Shiwei Liang, Shuaicheng Li, Yalin Zhang, Yingting Wu, Yongkang Liu, Zhenyu Huang
conference: "Arxiv"
year: 2025
bibkey: lingteam2025holistic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.07158"}
  - {name: "Code", url: "https://huggingface.co/inclusionAI"}
tags: ['RAG', 'Training Techniques', 'Has Code', 'Model Architecture']
---
This technical report presents Ring-Lite-Distill, a lightweight reasoning
model derived from our open-source Mixture-of-Experts (MoE) Large Language
Models (LLMs) Ling-Lite. This study demonstrates that through meticulous
high-quality data curation and ingenious training paradigms, the compact MoE
model Ling-Lite can be further trained to achieve exceptional reasoning
capabilities, while maintaining its parameter-efficient architecture with only
2.75 billion activated parameters, establishing an efficient lightweight
reasoning architecture. In particular, in constructing this model, we have not
merely focused on enhancing advanced reasoning capabilities, exemplified by
high-difficulty mathematical problem solving, but rather aimed to develop a
reasoning model with more comprehensive competency coverage. Our approach
ensures coverage across reasoning tasks of varying difficulty levels while
preserving generic capabilities, such as instruction following, tool use, and
knowledge retention. We show that, Ring-Lite-Distill's reasoning ability
reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general
capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The
models are accessible at https://huggingface.co/inclusionAI
