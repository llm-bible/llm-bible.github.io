---
layout: publication
title: On LLM Wizards Identifying Large Language Models Behaviors for Wizard of Oz Experiments
authors: Fang Jingchao, Arechiga Nikos, Namaoshi Keiichi, Bravo Nayeli, Hogan Candice, Shamma David A.
conference: "ACM International Conference on Intelligent Virtual Agents"
year: 2024
bibkey: fang2024llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.08067"}
tags: ['LLM', 'Pretraining Methods', 'Tools']
---
The Wizard of Oz (WoZ) method is a widely adopted research approach where a human Wizard role-plays a not readily available technology and interacts with participants to elicit user behaviors and probe the design space. With the growing ability for modern large language models (LLMs) to role-play one can apply LLMs as Wizards in WoZ experiments with better scalability and lower cost than the traditional approach. However methodological guidance on responsibly applying LLMs in WoZ experiments and a systematic evaluation of LLMs role-playing ability are lacking. Through two LLM-powered WoZ studies we take the first step towards identifying an experiment lifecycle for researchers to safely integrate LLMs into WoZ experiments and interpret data generated from settings that involve Wizards role-played by LLMs. We also contribute a heuristic-based evaluation framework that allows the estimation of LLMs role-playing ability in WoZ experiments and reveals LLMs behavior patterns at scale.
