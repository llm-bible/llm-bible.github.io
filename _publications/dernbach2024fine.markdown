---
layout: publication
title: Glam Fine45;tuning Large Language Models For Domain Knowledge Graph Alignment Via Neighborhood Partitioning And Generative Subgraph Encoding
authors: Dernbach Stefan, Agarwal Khushbu, Zuniga Alejandro, Henry Michael, Choudhury Sutanay
conference: "Arxiv"
year: 2024
bibkey: dernbach2024fine
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.06764"}
tags: ['Applications', 'Language Modeling', 'RAG', 'Reinforcement Learning', 'Security', 'Tools']
---
Integrating large language models (LLMs) with knowledge graphs derived from domain45;specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable it is crucial to enable them to perform multi45;step inferences over real45;world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation their ability to reason over domain45;specialized graphs of interconnected entities remains limited. For example can we query a LLM to identify the optimal contact in a professional network for a specific goal based on relationships and attributes in a private database The answer is no45;45;such capabilities lie beyond current methods. However this question underscores a critical technical gap that must be addressed. Many high45;value applications in areas such as science security and e45;commerce rely on proprietary knowledge graphs encoding unique structures relationships and logical constraints. We introduce a fine45;tuning framework for developing Graph45;aligned LAnguage Models (GLaM) that transforms a knowledge graph into an alternate text representation with labeled question45;answer pairs. We demonstrate that grounding the models in specific graph45;based knowledge expands the models capacity for structure45;based reasoning. Our methodology leverages the large45;language models generative capabilities to create the dataset and proposes an efficient alternate to retrieval45;augmented generation styled methods.
