---
layout: publication
title: Metavl Transferring In45;context Learning Ability From Language Models To Vision45;language Models
authors: Monajatipoor Masoud, Li Liunian Harold, Rouhsedaghat Mozhdeh, Yang Lin F., Chang Kai-wei
conference: "Arxiv"
year: 2023
bibkey: monajatipoor2023transferring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.01311"}
tags: ['Multimodal Models']
---
Large45;scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e. in45;context learning). However in the vision45;language domain most large45;scale pre45;trained vision45;language (VL) models do not possess the ability to conduct in45;context learning. How can we enable in45;context learning for VL models In this paper we study an interesting hypothesis can we transfer the in45;context learning ability from the language domain to VL domain Specifically we first meta45;trains a language model to perform in45;context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder. Our experiments suggest that indeed in45;context learning ability can be transferred cross modalities our model considerably improves the in45;context learning capability on VL tasks and can even compensate for the size of the model significantly. On VQA OK45;VQA and GQA our method could outperform the baseline model while having 20 times fewer parameters.
