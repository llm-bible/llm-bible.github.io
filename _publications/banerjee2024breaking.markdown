---
layout: publication
title: "Breaking Boundaries: Investigating The Effects Of Model Editing On Cross-linguistic Performance"
authors: Banerjee Somnath, Halder Avik, Mandal Rajarshi, Layek Sayan, Soboroff Ian, Hazra Rima, Mukherjee Animesh
conference: "Arxiv"
year: 2024
bibkey: banerjee2024breaking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.11139"}
tags: ['BERT', 'Fairness', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
The integration of pretrained language models (PLMs) like BERT and GPT has revolutionized NLP particularly for English but it has also created linguistic imbalances. This paper strategically identifies the need for linguistic equity by examining several knowledge editing techniques in multilingual contexts. We evaluate the performance of models such as Mistral TowerInstruct OpenHathi Tamil-Llama and Kan-Llama across languages including English German French Italian Spanish Hindi Tamil and Kannada. Our research identifies significant discrepancies in normal and merged models concerning cross-lingual consistency. We employ strategies like each language for itself (ELFI) and each language for others (ELFO) to stress-test these models. Our findings demonstrate the potential for LLMs to overcome linguistic barriers laying the groundwork for future research in achieving linguistic inclusivity in AI technologies.
