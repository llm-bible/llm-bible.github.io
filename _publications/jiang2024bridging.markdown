---
layout: publication
title: 'Bridging Large Language Models And Optimization: A Unified Framework For Text-attributed Combinatorial Optimization'
authors: Xia Jiang, Yaoxin Wu, Yuan Wang, Yingqian Zhang
conference: "Arxiv"
year: 2024
bibkey: jiang2024bridging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.12214"}
tags: ['Agentic', 'Efficiency and Optimization', 'Model Architecture', 'Training Techniques', 'Tools', 'Reinforcement Learning', 'RAG', 'Pretraining Methods', 'Transformer', 'Applications']
---
To advance capabilities of large language models (LLMs) in solving
combinatorial optimization problems (COPs), this paper presents the
Language-based Neural COP Solver (LNCS), a novel framework that is unified for
the end-to-end resolution of diverse text-attributed COPs. LNCS leverages LLMs
to encode problem instances into a unified semantic space, and integrates their
embeddings with a Transformer-based solution generator to produce high-quality
solutions. By training the solution generator with conflict-free multi-task
reinforcement learning, LNCS effectively enhances LLM performance in tackling
COPs of varying types and sizes, achieving state-of-the-art results across
diverse problems. Extensive experiments validate the effectiveness and
generalizability of the LNCS, highlighting its potential as a unified and
practical framework for real-world COP applications.
