---
layout: publication
title: 'Evaluating The Bias In Llms For Surveying Opinion And Decision Making In Healthcare'
authors: Yonchanok Khaokaew, Flora D. Salim, Andreas ZÃ¼fle, Hao Xue, Taylor Anderson, C. Raina Macintyre, Matthew Scotch, David J Heslop
conference: "Arxiv"
year: 2025
bibkey: khaokaew2025evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.08260"}
tags: ['Responsible AI', 'Agentic', 'Survey Paper', 'Ethics and Bias', 'Reinforcement Learning', 'Interpretability', 'Prompting']
---
Generative agents have been increasingly used to simulate human behaviour in
silico, driven by large language models (LLMs). These simulacra serve as
sandboxes for studying human behaviour without compromising privacy or safety.
However, it remains unclear whether such agents can truly represent real
individuals. This work compares survey data from the Understanding America
Study (UAS) on healthcare decision-making with simulated responses from
generative agents. Using demographic-based prompt engineering, we create
digital twins of survey respondents and analyse how well different LLMs
reproduce real-world behaviours. Our findings show that some LLMs fail to
reflect realistic decision-making, such as predicting universal vaccine
acceptance. However, Llama 3 captures variations across race and Income more
accurately but also introduces biases not present in the UAS data. This study
highlights the potential of generative agents for behavioural research while
underscoring the risks of bias from both LLMs and prompting strategies.
