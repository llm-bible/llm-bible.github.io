---
layout: publication
title: Fusion Of Detected Objects In Text For Visual Question Answering
authors: Alberti Chris, Ling Jeffrey, Collins Michael, Reitter David
conference: "Arxiv"
year: 2019
bibkey: alberti2019fusion
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1908.05054"}
  - {name: "Code", url: "https://github.com/google&#45;research/language/tree/master/language/question&#95;answering/b2t2)"}
tags: ['Applications', 'Has Code', 'Merging', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Transformer']
---
To advance models of multimodal context we introduce a simple yet powerful neural architecture for data that combines vision and natural language. The Bounding Boxes in Text Transformer (B2T2) also leverages referential information binding words to portions of the image in a single unified architecture. B2T2 is highly effective on the Visual Commonsense Reasoning benchmark (https://visualcommonsense.com), achieving a new state45;of45;the45;art with a 2537; relative reduction in error rate compared to published baselines and obtaining the best performance to date on the public leaderboard (as of May 22 2019). A detailed ablation analysis shows that the early integration of the visual features into the text analysis is key to the effectiveness of the new architecture. A reference implementation of our models is provided (https://github.com/google&#45;research/language/tree/master/language/question&#95;answering/b2t2).
