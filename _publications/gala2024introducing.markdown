---
layout: publication
title: 'Airavata: Introducing Hindi Instruction-tuned LLM'
authors: Jay Gala, Thanmay Jayakumar, Jaavid Aktar Husain, Aswanth Kumar M, Mohammed Safi Ur Rahman Khan, Diptesh Kanojia, Ratish Puduppully, Mitesh M. Khapra, Raj Dabre, Rudra Murthy, Anoop Kunchukuttan
conference: "Arxiv"
year: 2024
bibkey: gala2024introducing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.15006"}
tags: ['Training Techniques', 'Tools', 'Reinforcement Learning', 'Pretraining Methods', 'Fine-Tuning']
---
We announce the initial release of "Airavata," an instruction-tuned LLM for
Hindi. Airavata was created by fine-tuning OpenHathi with diverse,
instruction-tuning Hindi datasets to make it better suited for assistive tasks.
Along with the model, we also share the IndicInstruct dataset, which is a
collection of diverse instruction-tuning datasets to enable further research
for Indic LLMs. Additionally, we present evaluation benchmarks and a framework
for assessing LLM performance across tasks in Hindi. Currently, Airavata
supports Hindi, but we plan to expand this to all 22 scheduled Indic languages.
You can access all artifacts at https://ai4bharat.github.io/airavata.
