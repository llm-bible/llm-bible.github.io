---
layout: publication
title: 'LAG-MMLU: Benchmarking Frontier LLM Understanding In Latvian And Giriama'
authors: Naome A. Etori, Kevin Lu, Randu Karisa, Arturs Kanepajs
conference: "Joint 25th Nordic Conference on Computational Linguistics and 11th Baltic Conference on Human Language Technologies (NoDaLiDa/Baltic-HLT 2025) Proceedings of the Conference March 3-4 2025"
year: 2025
bibkey: etori2025lag
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.11911"}
tags: ['Tools', 'Reinforcement Learning']
---
As large language models (LLMs) rapidly advance, evaluating their performance
is critical. LLMs are trained on multilingual data, but their reasoning
abilities are mainly evaluated using English datasets. Hence, robust evaluation
frameworks are needed using high-quality non-English datasets, especially
low-resource languages (LRLs). This study evaluates eight state-of-the-art
(SOTA) LLMs on Latvian and Giriama using a Massive Multitask Language
Understanding (MMLU) subset curated with native speakers for linguistic and
cultural relevance. Giriama is benchmarked for the first time. Our evaluation
shows that OpenAI's o1 model outperforms others across all languages, scoring
92.8% in English, 88.8% in Latvian, and 70.8% in Giriama on 0-shot tasks.
Mistral-large (35.6%) and Llama-70B IT (41%) have weak performance, on both
Latvian and Giriama. Our results underscore the need for localized benchmarks
and human evaluations in advancing cultural AI contextualization.
