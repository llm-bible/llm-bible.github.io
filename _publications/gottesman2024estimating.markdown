---
layout: publication
title: Estimating Knowledge In Large Language Models Without Generating A Single Token
authors: Gottesman Daniela, Geva Mor
conference: "Arxiv"
year: 2024
bibkey: gottesman2024estimating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.12673"}
tags: ['Applications', 'RAG']
---
To evaluate knowledge in large language models (LLMs) current methods query the model and then evaluate its generated responses. In this work we ask whether evaluation can be done textit123;before125; the model has generated any text. Concretely is it possible to estimate how knowledgeable a model is about a certain entity only from its internal computation We study this question with two tasks given a subject entity the goal is to predict (a) the ability of the model to answer common questions about the entity and (b) the factuality of responses generated by the model about the entity. Experiments with a variety of LLMs show that KEEN a simple probe trained over internal subject representations succeeds at both tasks 45; strongly correlating with both the QA accuracy of the model per45;subject and FActScore a recent factuality metric in open45;ended generation. Moreover KEEN naturally aligns with the models hedging behavior and faithfully reflects changes in the models knowledge after fine45;tuning. Lastly we show a more interpretable yet equally performant variant of KEEN which highlights a small set of tokens that correlates with the models lack of knowledge. Being simple and lightweight KEEN can be leveraged to identify gaps and clusters of entity knowledge in LLMs and guide decisions such as augmenting queries with retrieval.
