---
layout: publication
title: 'Wenyangpt: A Large Language Model For Classical Chinese Tasks'
authors: Xinyu Yao, Mengdi Wang, Bo Chen, Xiaobing Zhao
conference: "Arxiv"
year: 2025
bibkey: yao2025large
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.20609'}
tags: ['Training Techniques', 'Model Architecture', 'GPT', 'Fine-Tuning', 'Pre-Training', 'Pretraining Methods']
---
Classical Chinese, as the core carrier of Chinese culture, plays a crucial
role in the inheritance and study of ancient literature. However, existing
natural language processing models primarily optimize for Modern Chinese,
resulting in inadequate performance on Classical Chinese. This paper presents a
comprehensive solution for Classical Chinese language processing. By continuing
pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we
construct a large language model, WenyanGPT, which is specifically designed for
Classical Chinese tasks. Additionally, we develop an evaluation benchmark
dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that
WenyanGPT significantly outperforms current advanced LLMs in various Classical
Chinese tasks. We make the model's training data, instruction fine-tuning
data\footnote, and evaluation benchmark dataset publicly available to promote
further research and development in the field of Classical Chinese processing.
