---
layout: publication
title: HGT Leveraging Heterogeneous Graph45;enhanced Large Language Models For Few45;shot Complex Table Understanding
authors: Jin Rihui, Li Yu, Qi Guilin, Hu Nan, Li Yuan-fang, Chen Jiaoyan, Wang Jianan, Chen Yongrui, Min Dehai
conference: "Arxiv"
year: 2024
bibkey: jin2024leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.19723"}
tags: ['Multimodal Models', 'Prompting', 'RAG', 'Tools', 'Training Techniques']
---
Table understanding (TU) has achieved promising advancements but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges we propose HGT a framework with a heterogeneous graph (HG)45;enhanced large language model (LLM) to tackle few45;shot TU tasks.It leverages the LLM by aligning the table semantics with the LLMs parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi45;task pre45;training scheme involving three novel multi45;granularity self45;supervised HG pre45;training objectives.We empirically demonstrate the effectiveness of HGT showing that it outperforms the SOTA for few45;shot complex TU on several benchmarks.
