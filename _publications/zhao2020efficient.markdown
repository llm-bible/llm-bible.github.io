---
layout: publication
title: 'SPARTA: Efficient Open-domain Question Answering Via Sparse Transformer Matching
  Retrieval'
authors: Tiancheng Zhao, Xiaopeng Lu, Kyusong Lee
conference: Arxiv
year: 2020
citations: 17
bibkey: zhao2020efficient
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2009.13013'}]
tags: [Transformer, Efficiency and Optimization, Interpretability and Explainability]
---
We introduce SPARTA, a novel neural retrieval method that shows great promise
in performance, generalization, and interpretability for open-domain question
answering. Unlike many neural ranking methods that use dense vector nearest
neighbor search, SPARTA learns a sparse representation that can be efficiently
implemented as an Inverted Index. The resulting representation enables scalable
neural retrieval that does not require expensive approximate vector search and
leads to better performance than its dense counterpart. We validated our
approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval
question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results
across a variety of open-domain question answering tasks in both English and
Chinese datasets, including open SQuAD, Natuarl Question, CMRC and etc.
Analysis also confirms that the proposed method creates human interpretable
representation and allows flexible control over the trade-off between
performance and efficiency.