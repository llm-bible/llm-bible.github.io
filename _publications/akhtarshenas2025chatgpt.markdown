---
layout: publication
title: 'Chatgpt Or A Silent Everywhere Helper: A Survey Of Large Language Models'
authors: Azim Akhtarshenas, Afshin Dini, Navid Ayoobi
conference: "Arxiv"
year: 2025
bibkey: akhtarshenas2025chatgpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.17403"}
tags: ['Fine-Tuning', 'Transformer', 'Pre-Training', 'GPT', 'Survey Paper', 'Applications', 'Ethics and Bias', 'Model Architecture', 'Training Techniques', 'Pretraining Methods']
---
Large Language Models (LLMs) have revo lutionized natural language processing
Natural Language Processing (NLP), with Chat Generative Pre-trained Transformer
(ChatGPT) standing out as a notable exampledue to its advanced capabilities and
widespread applications. This survey provides a comprehensive analysis of
ChatGPT, exploring its architecture, training processes, and functionalities.
We examine its integration into various domains across industries such as
customer service, education, healthcare, and entertainment. A comparative
analysis with other LLMs highlights ChatGPT's unique features and performance
metrics. Regarding benchmarks, the paper examines ChatGPT's comparative
performance against other LLMs and discusses potential risks such as
misinformation, bias, and data privacy concerns. Additionally, we offer a
number of figures and tables that outline the backdrop of the discussion, the
main ideas of the article, the numerous LLM models, a thorough list of datasets
used for pre-training, fine-tuning, and evaluation, as well as particular LLM
applications with pertinent references. Finally, we identify future research
directions and technological advancements, underscoring the evolving landscape
of LLMs and their profound impact on artificial intelligence Artificial
Intelligence (AI) and society.
