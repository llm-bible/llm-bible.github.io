---
layout: publication
title: 'Effects Of Prompt Length On Domain-specific Tasks For Large Language Models'
authors: Qibang Liu, Wenzhe Wang, Jeffrey Willard
conference: "Arxiv"
year: 2025
bibkey: liu2025effects
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.14255'}
tags: ['Attention Mechanism', 'Prompting', 'Applications', 'Model Architecture']
---
In recent years, Large Language Models have garnered significant attention
for their strong performance in various natural language tasks, such as machine
translation and question answering. These models demonstrate an impressive
ability to generalize across diverse tasks. However, their effectiveness in
tackling domain-specific tasks, such as financial sentiment analysis and
monetary policy understanding, remains a topic of debate, as these tasks often
require specialized knowledge and precise reasoning. To address such
challenges, researchers design various prompts to unlock the models' abilities.
By carefully crafting input prompts, researchers can guide these models to
produce more accurate responses. Consequently, prompt engineering has become a
key focus of study. Despite the advancements in both models and prompt
engineering, the relationship between the two-specifically, how prompt design
impacts models' ability to perform domain-specific tasks-remains underexplored.
This paper aims to bridge this research gap.
