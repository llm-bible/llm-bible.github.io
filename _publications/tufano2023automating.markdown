---
layout: publication
title: Automating Code45;related Tasks Through Transformers The Impact Of Pre45;training
authors: Tufano Rosalia, Pascarella Luca, Bavota Gabriele
conference: "Arxiv"
year: 2023
bibkey: tufano2023automating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2302.04048"}
tags: ['Applications', 'BERT', 'Masked Language Model', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques', 'Transformer']
---
Transformers have gained popularity in the software engineering (SE) literature. These deep learning models are usually pre45;trained through a self45;supervised objective meant to provide the model with basic knowledge about a language of interest (e.g. Java). A classic pre45;training objective is the masked language model (MLM) in which a percentage of tokens from the input (e.g. a Java method) is masked with the model in charge of predicting them. Once pre45;trained the model is then fine45;tuned to support the specific downstream task of interest (e.g. code summarization). While there is evidence suggesting the boost in performance provided by pre45;training little is known about the impact of the specific pre45;training objective(s) used. Indeed MLM is just one of the possible pre45;training objectives and recent work from the natural language processing field suggest that pre45;training objectives tailored for the specific downstream task of interest may substantially boost the models performance. In this study we focus on the impact of pre45;training objectives on the performance of transformers when automating code45;related tasks. We start with a systematic literature review aimed at identifying the pre45;training objectives used in SE. Then we pre45;train 32 transformers using both (i) generic pre45;training objectives usually adopted in SE; and (ii) pre45;training objectives tailored to specific code45;related tasks subject of our experimentation namely bug45;fixing code summarization and code completion. We also compare the pre45;trained models with non pre45;trained ones. Our results show that (i) pre45;training helps in boosting performance only if the amount of fine45;tuning data available is small; (ii) the MLM objective is usually sufficient to maximize the prediction performance of the model even when comparing it with pre45;training objectives specialized for the downstream task at hand.
