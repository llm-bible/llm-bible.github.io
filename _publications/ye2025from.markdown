---
layout: publication
title: 'From Generic Empathy To Personalized Emotional Support: A Self-evolution Framework For User Preference Alignment'
authors: Jing Ye, Lu Xiang, Yaping Zhang, Chengqing Zong
conference: "Arxiv"
year: 2025
bibkey: ye2025from
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.16610'}
tags: ['Reinforcement Learning', 'RAG', 'Efficiency and Optimization', 'Tools']
---
Effective emotional support hinges on understanding users' emotions and needs to provide meaningful comfort during multi-turn interactions. Large Language Models (LLMs) show great potential for expressing empathy; however, they often deliver generic and one-size-fits-all responses that fail to address users' specific needs. To tackle this issue, we propose a self-evolution framework designed to help LLMs improve their responses to better align with users' implicit preferences concerning user profiles (personalities), emotional states, and specific situations. Our framework consists of two distinct phases: \textit\{(1)\} \textit\{Emotional Support Experience Acquisition\}, where LLMs are fine-tuned on limited emotional support conversation data to provide basic support, and \textit\{(2)\} \textit\{Self-Improvement for Personalized Emotional Support\}, where LLMs leverage self-reflection and self-refinement to generate personalized responses. Through iterative direct preference optimization between the pre- and post-refined responses, our model generates responses that reflect a better understanding of the user's implicit preferences. Extensive experiments and evaluations demonstrate that our method significantly enhances the model's performance in emotional support, reducing unhelpful responses and minimizing discrepancies between user preferences and model outputs.
