---
layout: publication
title: 'LUME: LLM Unlearning With Multitask Evaluations'
authors: Anil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-wei Chang, Zhiqi Bu, Bhanukiran Vinzamuri, Volkan Cevher, Mingyi Hong, Rahul Gupta
conference: "Arxiv"
year: 2025
bibkey: ramakrishna2025llm
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.15097'}
tags: ['Training Techniques']
---
Unlearning aims to remove copyrighted, sensitive, or private content from
large language models (LLMs) without a full retraining. In this work, we
develop a multi-task unlearning benchmark (LUME) which features three tasks:
(1) unlearn synthetically generated creative short novels, (2) unlearn
synthetic biographies with sensitive information, and (3) unlearn a collection
of public biographies. We further release two fine-tuned LLMs of 1B and 7B
parameter sizes as the target models. We conduct detailed evaluations of
several recently proposed unlearning algorithms and present results on
carefully crafted metrics to understand their behavior and limitations.
