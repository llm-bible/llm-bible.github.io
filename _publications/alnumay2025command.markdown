---
layout: publication
title: 'Command R7B Arabic: A Small, Enterprise Focused, Multilingual, And Culturally Aware Arabic LLM'
authors: Yazeed Alnumay, Alexandre Barbet, Anna Bialas, William Darling, Shaan Desai, Joan Devassy, Kyle Duffy, Stephanie Howe, Olivia Lasche, Justin Lee, Anirudh Shrinivason, Jennifer Tracey
conference: "Arxiv"
year: 2025
bibkey: alnumay2025command
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.14603'}
tags: ['Training Techniques', 'RAG', 'Reinforcement Learning', 'Applications']
---
Building high-quality large language models (LLMs) for enterprise Arabic
applications remains challenging due to the limited availability of digitized
Arabic data. In this work, we present a data synthesis and refinement strategy
to help address this problem, namely, by leveraging synthetic data generation
and human-in-the-loop annotation to expand our Arabic training corpus. We
further present our iterative post training recipe that is essential to
achieving state-of-the-art performance in aligning the model with human
preferences, a critical aspect to enterprise use cases. The culmination of this
effort is the release of a small, 7B, open-weight model that outperforms
similarly sized peers in head-to-head comparisons and on Arabic-focused
benchmarks covering cultural knowledge, instruction following, RAG, and
contextual faithfulness.
