---
layout: publication
title: A Principled Framework For Knowledge45;enhanced Large Language Model
authors: Wang Saizhuo, Liu Zhihan, Wang Zhaoran, Guo Jian
conference: "Arxiv"
year: 2023
bibkey: wang2023principled
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.11135"}
tags: ['Pretraining Methods', 'Tools']
---
Large Language Models (LLMs) are versatile yet they often falter in tasks requiring deep and reliable reasoning due to issues like hallucinations limiting their applicability in critical scenarios. This paper introduces a rigorously designed framework for creating LLMs that effectively anchor knowledge and employ a closed45;loop reasoning process enhancing their capability for in45;depth analysis. We dissect the framework to illustrate the contribution of each component to the LLMs performance offering a theoretical assurance of improved reasoning under well45;defined assumptions.
