---
layout: publication
title: Defending Jailbreak Prompts Via In-context Adversarial Game
authors: Zhou Yujun, Han Yufei, Zhuang Haomin, Guo Kehan, Liang Zhenwen, Bao Hongyan, Zhang Xiangliang
conference: "Arxiv"
year: 2024
bibkey: zhou2024defending
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.13148"}
tags: ['Agentic', 'Applications', 'Fine Tuning', 'Pretraining Methods', 'Prompting', 'RAG', 'Reinforcement Learning', 'Security', 'Training Techniques']
---
Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However concerns regarding their security particularly the vulnerability to jailbreak attacks persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAGs efficacy where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover ICAG demonstrates remarkable transferability to other LLMs indicating its potential as a versatile defense mechanism.
