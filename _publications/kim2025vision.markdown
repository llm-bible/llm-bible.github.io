---
layout: publication
title: 'Vision-integrated Llms For Autonomous Driving Assistance : Human Performance Comparison And Trust Evaluation'
authors: Namhee Kim, Woojin Park
conference: "Arxiv"
year: 2025
bibkey: kim2025vision
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.06843"}
tags: ['Multimodal Models', 'Model Architecture', 'GPT', 'Pretraining Methods', 'Transformer']
---
Traditional autonomous driving systems often struggle with reasoning in
complex, unexpected scenarios due to limited comprehension of spatial
relationships. In response, this study introduces a Large Language Model
(LLM)-based Autonomous Driving (AD) assistance system that integrates a vision
adapter and an LLM reasoning module to enhance visual understanding and
decision-making. The vision adapter, combining YOLOv4 and Vision Transformer
(ViT), extracts comprehensive visual features, while GPT-4 enables human-like
spatial reasoning and response generation. Experimental evaluations with 45
experienced drivers revealed that the system closely mirrors human performance
in describing situations and moderately aligns with human decisions in
generating appropriate responses.
