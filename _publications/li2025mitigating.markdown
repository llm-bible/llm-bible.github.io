---
layout: publication
title: 'Mitigating LLM Hallucinations With Knowledge Graphs: A Case Study'
authors: Harry Li, Gabriel Appleby, Kenneth Alperin, Steven R Gomez, Ashley Suh
conference: "Arxiv"
year: 2025
bibkey: li2025mitigating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.12422'}
tags: ['Security', 'GPT', 'Applications', 'Model Architecture', 'Reinforcement Learning']
---
High-stakes domains like cyber operations need responsible and trustworthy AI
methods. While large language models (LLMs) are becoming increasingly popular
in these domains, they still suffer from hallucinations. This research paper
provides learning outcomes from a case study with LinkQ, an open-source natural
language interface that was developed to combat hallucinations by forcing an
LLM to query a knowledge graph (KG) for ground-truth data during
question-answering (QA). We conduct a quantitative evaluation of LinkQ using a
well-known KGQA dataset, showing that the system outperforms GPT-4 but still
struggles with certain question categories - suggesting that alternative query
construction strategies will need to be investigated in future LLM querying
systems. We discuss a qualitative study of LinkQ with two domain experts using
a real-world cybersecurity KG, outlining these experts' feedback, suggestions,
perceived limitations, and future opportunities for systems like LinkQ.
