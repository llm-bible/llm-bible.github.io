---
layout: publication
title: 'Counting-stars: A Multi-evidence, Position-aware, And Scalable Benchmark For Evaluating Long-context Large Language Models'
authors: Mingyang Song, Mao Zheng, Xuan Luo
conference: "Arxiv"
year: 2024
bibkey: song2024counting
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2403.11802'}
tags: ['GPT', 'Model Architecture']
---
Despite recent efforts to develop large language models with robust
long-context capabilities, the lack of long-context benchmarks means that
relatively little is known about their performance. To alleviate this gap, in
this paper, we propose \textbf\{Counting-Stars\}, a multi-evidence,
position-aware, and scalable benchmark designed to evaluate the multi-evidence
retrieval capabilities of long-context LLMs. \textbf\{Counting-Stars\} comprises
two counting-based multiple pieces of evidence retrieval sub-tasks: searching
and reasoning. Using Counting-Stars, we conduct experiments to evaluate several
long-context LLMs, including GPT-4 Turbo, Gemini 1.5 Pro, Claude3 Opus, GLM-4,
and Moonshot-v1. Extensive experimental results demonstrate that Gemini 1.5 Pro
achieves the best overall results, while GPT-4 Turbo exhibits the most stable
performance across various tasks. Furthermore, our analysis of these LLMs,
which have been extended to handle long-context scenarios, indicates that
significant room for improvement remains as the length of the input context and
the complexity of the tasks increase.
