---
layout: publication
title: 'Complexfuncbench: Exploring Multi-step And Constrained Function Calling Under Long-context Scenario'
authors: Lucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, Jie Tang
conference: "Arxiv"
year: 2025
bibkey: zhong2025exploring
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.10132'}
  - {name: "Code", url: 'https://github.com/THUDM/ComplexFuncBench'}
tags: ['Reinforcement Learning', 'Has Code', 'Tools']
---
Enhancing large language models (LLMs) with real-time APIs can help generate
more accurate and up-to-date responses. However, evaluating the function
calling abilities of LLMs in real-world scenarios remains under-explored due to
the complexity of data collection and evaluation. In this work, we introduce
ComplexFuncBench, a benchmark for complex function calling across five
real-world scenarios. Compared to existing benchmarks, ComplexFuncBench
encompasses multi-step and constrained function calling, which requires
long-parameter filing, parameter value reasoning, and 128k long context.
Additionally, we propose an automatic framework, ComplexEval, for
quantitatively evaluating complex function calling tasks. Through comprehensive
experiments, we demonstrate the deficiencies of state-of-the-art LLMs in
function calling and suggest future directions for optimizing these
capabilities. The data and code are available at
\url\{https://github.com/THUDM/ComplexFuncBench\}.
