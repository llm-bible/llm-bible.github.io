---
layout: publication
title: 'Enhancing Logical Reasoning In Large Language Models Through Graph-based Synthetic Data'
authors: Jiaming Zhou, Abbas Ghaddar, Ge Zhang, Liheng Ma, Yaochen Hu, Soumyasundar Pal, Mark Coates, Bin Wang, Yingxue Zhang, Jianye Hao
conference: "Arxiv"
year: 2024
bibkey: zhou2024enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.12437"}
tags: ['Prompting', 'Pretraining Methods', 'Training Techniques', 'Fine-Tuning']
---
Despite recent advances in training and prompting strategies for Large
Language Models (LLMs), these models continue to face challenges with complex
logical reasoning tasks that involve long reasoning chains. In this work, we
explore the potential and limitations of using graph-based synthetic reasoning
data as training signals to enhance LLMs' reasoning capabilities. Our extensive
experiments, conducted on two established natural language reasoning tasks --
inductive reasoning and spatial reasoning -- demonstrate that supervised
fine-tuning (SFT) with synthetic graph-based reasoning data effectively
enhances LLMs' reasoning performance without compromising their effectiveness
on other standard evaluation benchmarks.
