---
layout: publication
title: 'Open Problems And A Hypothetical Path Forward In LLM Knowledge Paradigms'
authors: Xiaotian Ye, Mengqi Zhang, Shu Wu
conference: "Arxiv"
year: 2025
bibkey: ye2025open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.06823"}
tags: ['Tools', 'Survey Paper', 'Model Architecture']
---
Knowledge is fundamental to the overall capabilities of Large Language Models
(LLMs). The knowledge paradigm of a model, which dictates how it encodes and
utilizes knowledge, significantly affects its performance. Despite the
continuous development of LLMs under existing knowledge paradigms, issues
within these frameworks continue to constrain model potential.
  This blog post highlight three critical open problems limiting model
capabilities: (1) challenges in knowledge updating for LLMs, (2) the failure of
reverse knowledge generalization (the reversal curse), and (3) conflicts in
internal knowledge. We review recent progress made in addressing these issues
and discuss potential general solutions. Based on observations in these areas,
we propose a hypothetical paradigm based on Contextual Knowledge Scaling, and
further outline implementation pathways that remain feasible within
contemporary techniques. Evidence suggests this approach holds potential to
address current shortcomings, serving as our vision for future model paradigms.
  This blog post aims to provide researchers with a brief overview of progress
in LLM knowledge systems, while provide inspiration for the development of
next-generation model architectures.
