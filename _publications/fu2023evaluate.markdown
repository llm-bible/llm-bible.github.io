---
layout: publication
title: Gptscore Evaluate As You Desire
authors: Fu Jinlan, Ng See-kiong, Jiang Zhengbao, Liu Pengfei
conference: "Arxiv"
year: 2023
bibkey: fu2023evaluate
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2302.04166"}
  - {name: "Code", url: "https://github.com/jinlanfu/GPTScore"}
tags: ['Applications', 'GPT', 'Has Code', 'Language Modeling', 'Model Architecture', 'Tools']
---
Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high45;caliber text images and other outputs through the utilization of large pre45;trained models. Nevertheless assessing the quality of the generation is an even more arduous task than the generation itself and this issue has not been given adequate consideration recently. This paper proposes a novel evaluation framework GPTScore which utilizes the emergent abilities (e.g. zero45;shot instruction) of generative pre45;trained models to score generated texts. There are 19 pre45;trained models explored in this paper ranging in size from 80M (e.g. FLAN45;T545;small) to 175B (e.g. GPT3). Experimental results on four text generation tasks 22 evaluation aspects and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions. This nature helps us overcome several long45;standing challenges in text evaluation45;45;how to achieve customized multi45;faceted evaluation without the need for annotated samples. We make our code publicly available at https://github.com/jinlanfu/GPTScore.
