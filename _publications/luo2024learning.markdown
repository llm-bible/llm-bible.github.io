---
layout: publication
title: Learning Positional Attention For Sequential Recommendation
authors: Luo Fan, Zhang Juan, Xu Shenghui
conference: "Arxiv"
year: 2024
bibkey: luo2024learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.02793"}
  - {name: "Code", url: "https://anonymous.4open.science/"}
tags: ['Attention Mechanism', 'Has Code', 'Model Architecture', 'Survey Paper']
---
Self45;attention45;based networks have achieved remarkable performance in sequential recommendation tasks. A crucial component of these models is positional encoding. In this study we delve into the learned positional embedding demonstrating that it often captures the distance between tokens. Building on this insight we introduce novel attention models that directly learn positional relations. Extensive experiments reveal that our proposed models textbf123;PARec125; and textbf123;FPARec125; outperform previous self45;attention45;based approaches.Our code is available at the link for anonymous review https://anonymous.4open.science/ r/FPARec45;2C55/
