---
layout: publication
title: 'Fragrel: Exploiting Fragment-level Relations In The External Memory Of Large Language Models'
authors: Xihang Yue, Linchao Zhu, Yi Yang
conference: "Arxiv"
year: 2024
bibkey: yue2024exploiting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.03092"}
tags: ['RAG', 'Applications']
---
To process contexts with unlimited length using Large Language Models (LLMs),
recent studies explore hierarchically managing the long text. Only several text
fragments are taken from the external memory and passed into the temporary
working memory, i.e., LLM's context window. However, existing approaches
isolatedly handle the text fragments without considering their structural
connections, thereby suffering limited capability on texts with intensive
inter-relations, e.g., coherent stories and code repositories. This work
attempts to resolve this by exploiting the fragment-level relations in external
memory. First, we formulate the fragment-level relations and present several
instantiations for different text types. Next, we introduce a relation-aware
fragment assessment criteria upon previous independent fragment assessment.
Finally, we present the fragment-connected Hierarchical Memory based LLM. We
validate the benefits of involving these relations on long story understanding,
repository-level code generation, and long-term chatting.
