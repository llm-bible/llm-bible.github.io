---
layout: publication
title: Precision Empowers Excess Distracts Visual Question Answering With Dynamically Infused Knowledge In Language Models
authors: Jhalani Manas, M Annervaz K, Bhattacharyya Pushpak
conference: "Arxiv"
year: 2024
bibkey: jhalani2024precision
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.09994"}
tags: ['Applications', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'RAG', 'Transformer']
---
In the realm of multimodal tasks Visual Question Answering (VQA) plays a crucial role by addressing natural language questions grounded in visual content. Knowledge45;Based Visual Question Answering (KBVQA) advances this concept by adding external knowledge along with images to respond to questions. We introduce an approach for KBVQA augmenting the existing vision45;language transformer encoder45;decoder (OFA) model. Our main contribution involves enhancing questions by incorporating relevant external knowledge extracted from knowledge graphs using a dynamic triple extraction method. We supply a flexible number of triples from the knowledge graph as context tailored to meet the requirements for answering the question. Our model enriched with knowledge demonstrates an average improvement of 4.7537; in Exact Match Score over the state45;of45;the45;art on three different KBVQA datasets. Through experiments and analysis we demonstrate that furnishing variable triples for each question improves the reasoning capabilities of the language model in contrast to supplying a fixed number of triples. This is illustrated even for recent large language models. Additionally we highlight the models generalization capability by showcasing its SOTA45;beating performance on a small dataset achieved through straightforward fine45;tuning.
