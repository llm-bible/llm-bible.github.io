---
layout: publication
title: 'From Generalist To Specialist: A Survey Of Large Language Models For Chemistry'
authors: Yang Han, Ziping Wan, Lu Chen, Kai Yu, Xin Chen
conference: "Arxiv"
year: 2024
bibkey: han2024from
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.19994'}
tags: ['Agentic', 'Training Techniques', 'Applications', 'Tools', 'Survey Paper', 'Reinforcement Learning', 'Pretraining Methods']
---
Large Language Models (LLMs) have significantly transformed our daily life
and established a new paradigm in natural language processing (NLP). However,
the predominant pretraining of LLMs on extensive web-based texts remains
insufficient for advanced scientific discovery, particularly in chemistry. The
scarcity of specialized chemistry data, coupled with the complexity of
multi-modal data such as 2D graph, 3D structure and spectrum, present distinct
challenges. Although several studies have reviewed Pretrained Language Models
(PLMs) in chemistry, there is a conspicuous absence of a systematic survey
specifically focused on chemistry-oriented LLMs. In this paper, we outline
methodologies for incorporating domain-specific chemistry knowledge and
multi-modal information into LLMs, we also conceptualize chemistry LLMs as
agents using chemistry tools and investigate their potential to accelerate
scientific research. Additionally, we conclude the existing benchmarks to
evaluate chemistry ability of LLMs. Finally, we critically examine the current
challenges and identify promising directions for future research. Through this
comprehensive survey, we aim to assist researchers in staying at the forefront
of developments in chemistry LLMs and to inspire innovative applications in the
field.
