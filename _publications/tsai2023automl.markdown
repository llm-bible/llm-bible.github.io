---
layout: publication
title: 'Automl-gpt: Large Language Model For Automl'
authors: Yun-da Tsai, Yu-che Tsai, Bo-wei Huang, Chun-pai Yang, Shou-de Lin
conference: "Arxiv"
year: 2023
bibkey: tsai2023automl
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2309.01125'}
tags: ['RAG', 'Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Tools', 'GPT', 'Merging']
---
With the emerging trend of GPT models, we have established a framework called
AutoML-GPT that integrates a comprehensive set of tools and libraries. This
framework grants users access to a wide range of data preprocessing techniques,
feature engineering methods, and model selection algorithms. Through a
conversational interface, users can specify their requirements, constraints,
and evaluation metrics. Throughout the process, AutoML-GPT employs advanced
techniques for hyperparameter optimization and model selection, ensuring that
the resulting model achieves optimal performance. The system effectively
manages the complexity of the machine learning pipeline, guiding users towards
the best choices without requiring deep domain knowledge. Through our
experimental results on diverse datasets, we have demonstrated that AutoML-GPT
significantly reduces the time and effort required for machine learning tasks.
Its ability to leverage the vast knowledge encoded in large language models
enables it to provide valuable insights, identify potential pitfalls, and
suggest effective solutions to common challenges faced during model training.
