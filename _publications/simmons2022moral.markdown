---
layout: publication
title: 'Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored To Political Identity'
authors: Gabriel Simmons
conference: "Arxiv"
year: 2022
bibkey: simmons2022moral
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2209.12106"}
tags: ['Transformer', 'Tools', 'GPT', 'Ethics and Bias', 'Model Architecture', 'Pretraining Methods', 'Prompting']
---
Large Language Models (LLMs) have demonstrated impressive capabilities in
generating fluent text, as well as tendencies to reproduce undesirable social
biases. This study investigates whether LLMs reproduce the moral biases
associated with political groups in the United States, an instance of a broader
capability herein termed moral mimicry. This hypothesis is explored in the
GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral
Foundations Theory, it is shown that these LLMs are indeed moral mimics. When
prompted with a liberal or conservative political identity, the models generate
text reflecting corresponding moral biases. This study also explores the
relationship between moral mimicry and model size, and similarity between human
and LLM moral word use.
