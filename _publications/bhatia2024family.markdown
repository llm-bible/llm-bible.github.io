---
layout: publication
title: Fintral A Family Of GPT45;4 Level Multimodal Financial Large Language Models
authors: Bhatia Gagan, Nagoudi El Moatez Billah, Cavusoglu Hasan, Abdul-mageed Muhammad
conference: "Arxiv"
year: 2024
bibkey: bhatia2024family
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.10986"}
  - {name: "Code", url: "https://github.com/UBC&#45;NLP/fintral&#125;"}
tags: ['Efficiency And Optimization', 'GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
We introduce FinTral a suite of state45;of45;the45;art multimodal large language models (LLMs) built upon the Mistral45;7b model and tailored for financial analysis. FinTral integrates textual numerical tabular and image data. We enhance FinTral with domain45;specific pretraining instruction fine45;tuning and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods dubbed FinTral45;DPO45;Tamp;R demonstrates an exceptional zero45;shot performance. It outperforms ChatGPT45;3.5 in all tasks and surpasses GPT45;4 in five out of nine tasks marking a significant advancement in AI45;driven financial technology. We also demonstrate that FinTral has the potential to excel in real45;time analysis and decision45;making in diverse financial contexts. The GitHub repository for FinTral is available at url123;https://github.com/UBC&#45;NLP/fintral&#125;.
