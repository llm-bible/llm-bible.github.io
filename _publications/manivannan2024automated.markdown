---
layout: publication
title: 'Climaqa: An Automated Evaluation Framework For Climate Question Answering Models'
authors: Veeramakali Vignesh Manivannan, Yasaman Jafari, Srikar Eranky, Spencer Ho, Rose Yu, Duncan Watson-parris, Yian Ma, Leon Bergen, Taylor Berg-kirkpatrick
conference: "ICLR 2025"
year: 2024
bibkey: manivannan2024automated
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.16701'}
  - {name: "Code", url: 'https://github.com/Rose-STL-Lab/genie-climaqa'}
tags: ['Attention Mechanism', 'Has Code', 'Model Architecture', 'Applications', 'Tools']
---
The use of Large Language Models (LLMs) in climate science has recently
gained significant attention. However, a critical issue remains: the lack of a
comprehensive evaluation framework capable of assessing the quality and
scientific validity of model outputs. To address this issue, we develop
ClimaGen (Climate QA Generator), an adaptive learning framework that generates
question-answer pairs from graduate textbooks with climate scientists in the
loop. As a result, we present ClimaQA-Gold, an expert-annotated benchmark
dataset alongside ClimaQA-Silver, a large-scale, comprehensive synthetic QA
dataset for climate science. Finally, we develop evaluation strategies and
compare different LLMs on our benchmarks. Our results offer novel insights into
various approaches used to enhance knowledge of climate LLMs. The source code
is publicly available at https://github.com/Rose-STL-Lab/genie-climaqa
