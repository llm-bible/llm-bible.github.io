---
layout: publication
title: 'A Systematic Review On The Evaluation Of Large Language Models In Theory Of Mind Tasks'
authors: Karahan Sarıtaş, Kıvanç Tezören, Yavuz Durmazkeser
conference: "Arxiv"
year: 2025
bibkey: sarıtaş2025systematic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.08796"}
tags: ['Tools', 'Survey Paper', 'Model Architecture', 'Merging', 'Attention Mechanism', 'Prompting']
---
In recent years, evaluating the Theory of Mind (ToM) capabilities of large
language models (LLMs) has received significant attention within the research
community. As the field rapidly evolves, navigating the diverse approaches and
methodologies has become increasingly complex. This systematic review
synthesizes current efforts to assess LLMs' ability to perform ToM tasks, an
essential aspect of human cognition involving the attribution of mental states
to oneself and others. Despite notable advancements, the proficiency of LLMs in
ToM remains a contentious issue. By categorizing benchmarks and tasks through a
taxonomy rooted in cognitive science, this review critically examines
evaluation techniques, prompting strategies, and the inherent limitations of
LLMs in replicating human-like mental state reasoning. A recurring theme in the
literature reveals that while LLMs demonstrate emerging competence in ToM
tasks, significant gaps persist in their emulation of human cognitive
abilities.
