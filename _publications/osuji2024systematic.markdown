---
layout: publication
title: 'A Systematic Review Of Data-to-text NLG'
authors: Chinonso Cynthia Osuji, Thiago Castro Ferreira, Brian Davis
conference: "Arxiv"
year: 2024
bibkey: osuji2024systematic
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2402.08496'}
tags: ['Language Modeling', 'Transformer', 'Model Architecture', 'Training Techniques', 'Tools', 'Applications', 'Survey Paper', 'Reinforcement Learning', 'Pretraining Methods']
---
This systematic review undertakes a comprehensive analysis of current
research on data-to-text generation, identifying gaps, challenges, and future
directions within the field. Relevant literature in this field on datasets,
evaluation metrics, application areas, multilingualism, language models, and
hallucination mitigation methods is reviewed. Various methods for producing
high-quality text are explored, addressing the challenge of hallucinations in
data-to-text generation. These methods include re-ranking, traditional and
neural pipeline architecture, planning architectures, data cleaning, controlled
generation, and modification of models and training techniques. Their
effectiveness and limitations are assessed, highlighting the need for
universally applicable strategies to mitigate hallucinations. The review also
examines the usage, popularity, and impact of datasets, alongside evaluation
metrics, with an emphasis on both automatic and human assessment. Additionally,
the evolution of data-to-text models, particularly the widespread adoption of
transformer models, is discussed. Despite advancements in text quality, the
review emphasizes the importance of research in low-resourced languages and the
engineering of datasets in these languages to promote inclusivity. Finally,
several application domains of data-to-text are highlighted, emphasizing their
relevance in such domains. Overall, this review serves as a guiding framework
for fostering innovation and advancing data-to-text generation.
