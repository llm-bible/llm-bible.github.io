---
layout: publication
title: "A Systematic Review Of Data-to-text NLG"
authors: Osuji Chinonso Cynthia, Ferreira Thiago Castro, Davis Brian
conference: "Arxiv"
year: 2024
bibkey: osuji2024systematic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.08496"}
tags: ['Applications', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Survey Paper', 'Tools', 'Training Techniques', 'Transformer']
---
This systematic review undertakes a comprehensive analysis of current research on data-to-text generation identifying gaps challenges and future directions within the field. Relevant literature in this field on datasets evaluation metrics application areas multilingualism language models and hallucination mitigation methods is reviewed. Various methods for producing high-quality text are explored addressing the challenge of hallucinations in data-to-text generation. These methods include re-ranking traditional and neural pipeline architecture planning architectures data cleaning controlled generation and modification of models and training techniques. Their effectiveness and limitations are assessed highlighting the need for universally applicable strategies to mitigate hallucinations. The review also examines the usage popularity and impact of datasets alongside evaluation metrics with an emphasis on both automatic and human assessment. Additionally the evolution of data-to-text models particularly the widespread adoption of transformer models is discussed. Despite advancements in text quality the review emphasizes the importance of research in low-resourced languages and the engineering of datasets in these languages to promote inclusivity. Finally several application domains of data-to-text are highlighted emphasizing their relevance in such domains. Overall this review serves as a guiding framework for fostering innovation and advancing data-to-text generation.
