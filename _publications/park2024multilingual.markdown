---
layout: publication
title: Multiprageval Multilingual Pragmatic Evaluation Of Large Language Models
authors: Park Dojun, Lee Jiwoo, Park Seohyun, Jeong Hyeyun, Koo Youngeun, Hwang Soonha, Park Seonwoo, Lee Sungeun
conference: "Arxiv"
year: 2024
bibkey: park2024multilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.07736"}
tags: ['Pretraining Methods', 'RAG']
---
As the capabilities of LLMs expand it becomes increasingly important to evaluate them beyond basic knowledge assessment focusing on higher45;level language understanding. This study introduces MultiPragEval a robust test suite designed for the multilingual pragmatic evaluation of LLMs across English German Korean and Chinese. Comprising 1200 question units categorized according to Grices Cooperative Principle and its four conversational maxims MultiPragEval enables an in45;depth assessment of LLMs contextual awareness and their ability to infer implied meanings. Our findings demonstrate that Claude345;Opus significantly outperforms other models in all tested languages establishing a state45;of45;the45;art in the field. Among open45;source models Solar45;10.7B and Qwen1.545;14B emerge as strong competitors. This study not only leads the way in the multilingual evaluation of LLMs in pragmatic inference but also provides valuable insights into the nuanced capabilities necessary for advanced language comprehension in AI systems.
