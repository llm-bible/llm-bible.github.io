---
layout: publication
title: 'Hkcanto-eval: A Benchmark For Evaluating Cantonese Language Understanding And Cultural Comprehension In Llms'
authors: Tsz Chung Cheng, Chung Shing Cheng, Chaak Ming Lau, Eugene Tin-ho Lam, Chun Yat Wong, Hoi On Yu, Cheuk Hei Chong
conference: "Arxiv"
year: 2025
bibkey: cheng2025hkcanto
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.12440'}
  - {name: "Code", url: 'https://github.com/hon9kon9ize/hkeval2025'}
tags: ['Reinforcement Learning', 'Has Code', 'Tools', 'Training Techniques']
---
The ability of language models to comprehend and interact in diverse
linguistic and cultural landscapes is crucial. The Cantonese language used in
Hong Kong presents unique challenges for natural language processing due to its
rich cultural nuances and lack of dedicated evaluation datasets. The
HKCanto-Eval benchmark addresses this gap by evaluating the performance of
large language models (LLMs) on Cantonese language understanding tasks,
extending to English and Written Chinese for cross-lingual evaluation.
HKCanto-Eval integrates cultural and linguistic nuances intrinsic to Hong Kong,
providing a robust framework for assessing language models in realistic
scenarios. Additionally, the benchmark includes questions designed to tap into
the underlying linguistic metaknowledge of the models. Our findings indicate
that while proprietary models generally outperform open-weight models,
significant limitations remain in handling Cantonese-specific linguistic and
cultural knowledge, highlighting the need for more targeted training data and
evaluation methods. The code can be accessed at
https://github.com/hon9kon9ize/hkeval2025
