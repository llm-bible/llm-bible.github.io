---
layout: publication
title: Cotar Chain45;of45;thought Attribution Reasoning With Multi45;level Granularity
authors: Berchansky Moshe, Fleischer Daniel, Wasserblat Moshe, Izsak Peter
conference: "Arxiv"
year: 2024
bibkey: berchansky2024chain
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.10513"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
State45;of45;the45;art performance in QA tasks is currently achieved by systems employing Large Language Models (LLMs) however these models tend to hallucinate information in their responses. One approach focuses on enhancing the generation process by incorporating attribution from the given input to the output. However the challenge of identifying appropriate attributions and verifying their accuracy against a source is a complex task that requires significant improvements in assessing such systems. We introduce an attribution45;oriented Chain45;of45;Thought reasoning method to enhance the accuracy of attributions. This approach focuses the reasoning process on generating an attribution45;centric output. Evaluations on two context45;enhanced question45;answering datasets using GPT45;4 demonstrate improved accuracy and correctness of attributions. In addition the combination of our method with finetuning enhances the response and attribution accuracy of two smaller LLMs showing their potential to outperform GPT45;4 in some cases.
