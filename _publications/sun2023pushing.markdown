---
layout: publication
title: Corex Pushing The Boundaries Of Complex Reasoning Through Multi45;model Collaboration
authors: Sun Qiushi, Yin Zhangyue, Li Xiang, Wu Zhiyong, Qiu Xipeng, Kong Lingpeng
conference: "Arxiv"
year: 2023
bibkey: sun2023pushing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.00280"}
tags: ['Agent', 'Agentic', 'Efficiency And Optimization', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra45;large45;scale training corpora a single LLM can manage typical NLP tasks competently. However its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further we introduce Corex in this paper a suite of novel general45;purpose strategies that transform LLMs into autonomous agents pioneering multi45;model collaborations for complex task45;solving. Inspired by human behaviors Corex is constituted by diverse collaboration paradigms including Debate Review and Retrieve modes which collectively work towards enhancing the factuality faithfulness and reliability of the reasoning process. These paradigms foster task45;agnostic approaches that enable LLMs to think outside the box thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in45;depth analysis demonstrate the cost45;effectiveness of our method facilitating collaboration among different LLMs and promoting annotation efficiency.
