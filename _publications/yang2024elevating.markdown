---
layout: publication
title: Curiousllm\: Elevating Multi-document QA With Reasoning-infused Knowledge Graph Prompting
authors: Yang Zukang, Zhu Zixuan
conference: "Arxiv"
year: 2024
bibkey: yang2024elevating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.09077"}
tags: ['Agentic', 'Applications', 'Fine Tuning', 'Pretraining Methods', 'Prompting', 'Tools', 'Training Techniques']
---
In the field of Question Answering (QA) unifying large language models (LLMs) with external databases has shown great success. However these methods often fall short in providing the advanced reasoning needed for complex QA tasks. To address these issues we improve over a novel approach called Knowledge Graph Prompting (KGP) which combines knowledge graphs with a LLM-based agent to improve reasoning and search accuracy. Nevertheless the original KGP framework necessitates costly fine-tuning with large datasets yet still suffers from LLM hallucination. Therefore we propose a reasoning-infused LLM agent to enhance this framework. This agent mimics human curiosity to ask follow-up questions to more efficiently navigate the search. This simple modification significantly boosts the LLM performance in QA tasks without the high costs and latency associated with the initial KGP framework. Our ultimate goal is to further develop this approach leading to more accurate faster and cost-effective solutions in the QA domain.
