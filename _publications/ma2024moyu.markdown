---
layout: publication
title: MOYU A Theoretical Study on Massive Over-activation Yielded Uplifts in LLMs
authors: Ma Chi, Huang Mincong, Wang Chao, Wang Yujie, Yu Lei
conference: "Arxiv"
year: 2024
bibkey: ma2024moyu
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.12569"}
tags: ['ARXIV', 'Efficiency And Optimization', 'LLM', 'Model Architecture', 'Reinforcement Learning']
---
Massive Over-activation Yielded Uplifts(MOYU) is an inherent property of large language models and dynamic activation(DA) based on the MOYU property is a clever yet under-explored strategy designed to accelerate inference in these models. Existing methods that utilize MOYU often face a significant Impossible Trinity struggling to simultaneously maintain model performance enhance inference speed and extend applicability across various architectures. Due to the theoretical ambiguities surrounding MOYU this paper elucidates the root cause of the MOYU property and outlines the mechanisms behind two primary limitations encountered by current DA methods 1) history-related activation uncertainty and 2) semantic-irrelevant activation inertia. Our analysis not only underscores the limitations of current dynamic activation strategies within large-scale LLaMA models but also proposes opportunities for refining the design of future sparsity schemes.
