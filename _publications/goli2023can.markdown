---
layout: publication
title: Can LLMs Capture Human Preferences
authors: Goli Ali, Singh Amandeep
conference: "Arxiv"
year: 2023
bibkey: goli2023can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.02531"}
tags: ['GPT', 'Model Architecture', 'Prompting', 'RAG', 'Reinforcement Learning', 'Survey Paper', 'Tools']
---
We explore the viability of Large Language Models (LLMs) specifically OpenAIs GPT-3.5 and GPT-4 in emulating human survey respondents and eliciting preferences with a focus on intertemporal choices. Leveraging the extensive literature on intertemporal discounting for benchmarking we examine responses from LLMs across various languages and compare them to human responses exploring preferences between smaller sooner and larger later rewards. Our findings reveal that both GPT models demonstrate less patience than humans with GPT-3.5 exhibiting a lexicographic preference for earlier rewards unlike human decision-makers. Though GPT-4 does not display lexicographic preferences its measured discount rates are still considerably larger than those found in humans. Interestingly GPT models show greater patience in languages with weak future tense references such as German and Mandarin aligning with existing literature that suggests a correlation between language structure and intertemporal preferences. We demonstrate how prompting GPT to explain its decisions a procedure we term chain-of-thought conjoint can mitigate but does not eliminate discrepancies between LLM and human responses. While directly eliciting preferences using LLMs may yield misleading results combining chain-of-thought conjoint with topic modeling aids in hypothesis generation enabling researchers to explore the underpinnings of preferences. Chain-of-thought conjoint provides a structured framework for marketers to use LLMs to identify potential attributes or factors that can explain preference heterogeneity across different customers and contexts.
