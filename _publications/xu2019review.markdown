---
layout: publication
title: 'Review Conversational Reading Comprehension'
authors: Hu Xu, Bing Liu, Lei Shu, Philip S. Yu
conference: "Arxiv"
year: 2019
bibkey: xu2019review
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1902.00821"}
  - {name: "Code", url: "https://github.com/howardhsu/RCRC"}
tags: ['Agentic', 'Model Architecture', 'Training Techniques', 'Survey Paper', 'Reinforcement Learning', 'RAG', 'Pretraining Methods', 'BERT', 'Fine-Tuning', 'Has Code', 'Pre-Training']
---
Inspired by conversational reading comprehension (CRC), this paper studies a
novel task of leveraging reviews as a source to build an agent that can answer
multi-turn questions from potential consumers of online businesses. We first
build a review CRC dataset and then propose a novel task-aware pre-tuning step
running between language model (e.g., BERT) pre-training and domain-specific
fine-tuning. The proposed pre-tuning requires no data annotation, but can
greatly enhance the performance on our end task. Experimental results show that
the proposed approach is highly effective and has competitive performance as
the supervised approach. The dataset is available at
\url\{https://github.com/howardhsu/RCRC\}
