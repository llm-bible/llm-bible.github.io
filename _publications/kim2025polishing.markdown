---
layout: publication
title: 'Polishing Every Facet Of The GEM: Testing Linguistic Competence Of Llms And Humans In Korean'
authors: Sungho Kim, Nayeon Kim, Taehee Jeon, Sangkeun Lee
conference: "Arxiv"
year: 2025
bibkey: kim2025polishing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2506.01237'}
  - {name: "Code", url: 'https://github.com/SungHo3268/KoGEM'}
tags: ['Reinforcement Learning', 'Has Code']
---
We introduce the \\(\underline\{Ko\}rean \underline\{G\}rammar \underline\{E\}valuation Bench\underline\{M\}ark (KoGEM)\\), designed to assess the linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k multiple-choice QA pairs covering five main categories and 16 subcategories. The zero-shot evaluation of 27 LLMs of various sizes and types reveals that while LLMs perform remarkably well on straightforward tasks requiring primarily definitional knowledge, they struggle with tasks that demand the integration of real-world experiential knowledge, such as phonological rules and pronunciation. Furthermore, our in-depth analysis suggests that incorporating such experiential knowledge could enhance the linguistic competence of LLMs. With KoGEM, we not only highlight the limitations of current LLMs in linguistic competence but also uncover hidden facets of LLMs in linguistic competence, paving the way for enhancing comprehensive language understanding. Our code and dataset are available at: https://github.com/SungHo3268/KoGEM.
