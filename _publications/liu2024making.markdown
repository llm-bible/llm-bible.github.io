---
layout: publication
title: Making Them Ask And Answer Jailbreaking Large Language Models In Few Queries Via Disguise And Reconstruction
authors: Liu Tong, Zhang Yingjie, Zhao Zhe, Dong Yinpeng, Meng Guozhu, Chen Kai
conference: "Arxiv"
year: 2024
bibkey: liu2024making
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.18104"}
tags: ['Efficiency And Optimization', 'Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'Responsible AI', 'Security']
---
In recent years large language models (LLMs) have demonstrated notable success across various tasks but the trustworthiness of LLMs is still an open problem. One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs. In this work we pioneer a theoretical foundation in LLMs security by identifying bias vulnerabilities within the safety fine45;tuning and design a black45;box jailbreak method named DRA (Disguise and Reconstruction Attack) which conceals harmful instructions through disguise and prompts the model to reconstruct the original harmful instruction within its completion. We evaluate DRA across various open45;source and closed45;source models showcasing state45;of45;the45;art jailbreak success rates and attack efficiency. Notably DRA boasts a 91.137; attack success rate on OpenAI GPT45;4 chatbot.
