---
layout: publication
title: Supervised Knowledge Makes Large Language Models Better In45;context Learners
authors: Yang Linyi, Zhang Shuibai, Yu Zhuohao, Bao Guangsheng, Wang Yidong, Wang Jindong, Xu Ruochen, Ye Wei, Xie Xing, Chen Weizhu, Zhang Yue
conference: "Arxiv"
year: 2023
bibkey: yang2023supervised
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.15918"}
  - {name: "Code", url: "https://github.com/YangLinyi/Supervised&#45;Knowledge&#45;Makes&#45;Large&#45;Language&#45;Models&#45;Better&#45;In&#45;context&#45;Learners"}
tags: ['Applications', 'GPT', 'Has Code', 'Merging', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Tools']
---
Large Language Models (LLMs) exhibit emerging in45;context learning abilities through prompt engineering. The recent progress in large45;scale generative models has further expanded their use in real45;world language applications. However the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under45;explored. While previous in45;context learning research has focused on enhancing models to adhere to users specific instructions and quality expectations and to avoid undesired outputs little to no work has explored the use of task45;Specific fine45;tuned Language Models (SLMs) to improve LLMs in45;context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it 1) generalizes out45;of45;distribution data 2) elucidates how LLMs benefit from discriminative models and 3) minimizes hallucinations in generative tasks. Using our proposed plug45;in method enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality. We offer a comprehensive suite of resources including 16 curated datasets prompts model checkpoints and LLM outputs across 9 distinct tasks. The code and data are released at https://github.com/YangLinyi/Supervised&#45;Knowledge&#45;Makes&#45;Large&#45;Language&#45;Models&#45;Better&#45;In&#45;context&#45;Learners. Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs.
