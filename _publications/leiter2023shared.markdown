---
layout: publication
title: The Eval4nlp 2023 Shared Task On Prompting Large Language Models As Explainable Metrics
authors: Leiter Christoph, Opitz Juri, Deutsch Daniel, Gao Yang, Dror Rotem, Eger Steffen
conference: "Arxiv"
year: 2023
bibkey: leiter2023shared
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.19792"}
tags: ['Applications', 'Interpretability And Explainability', 'Language Modeling', 'Prompting', 'Training Techniques']
---
With an increasing number of parameters and pre45;training data generative large language models (LLMs) have shown remarkable capabilities to solve tasks with minimal or no task45;related examples. Notably LLMs have been successfully employed as evaluation metrics in text generation tasks. Within this context we introduce the Eval4NLP 2023 shared task that asks participants to explore prompting and score extraction for machine translation (MT) and summarization evaluation. Specifically we propose a novel competition setting in which we select a list of allowed LLMs and disallow fine45;tuning to ensure a focus on prompting. We present an overview of participants approaches and evaluate them on a new reference45;free test set spanning three language pairs for MT and a summarization dataset. Notably despite the tasks restrictions the best45;performing systems achieve results on par with or even surpassing recent reference45;free metrics developed using larger models including GEMBA and Comet45;Kiwi45;XXL. Finally as a separate track we perform a small45;scale human evaluation of the plausibility of explanations given by the LLMs.
