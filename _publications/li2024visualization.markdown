---
layout: publication
title: "Visualization Literacy Of Multimodal Large Language Models: A Comparative Study"
authors: Li Zhimin, Miao Haichao, Pascucci Valerio, Liu Shusen
conference: "Arxiv"
year: 2024
bibkey: li2024visualization
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.10996"}
tags: ['GPT', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning', 'Tools']
---
The recent introduction of multimodal large language models (MLLMs) combine the inherent power of large language models (LLMs) with the renewed capabilities to reason about the multimodal context. The potential usage scenarios for MLLMs significantly outpace their text-only counterparts. Many recent works in visualization have demonstrated MLLMs capability to understand and interpret visualization results and explain the content of the visualization to users in natural language. In the machine learning community the general vision capabilities of MLLMs have been evaluated and tested through various visual understanding benchmarks. However the ability of MLLMs to accomplish specific visualization tasks based on visual perception has not been properly explored and evaluated particularly from a visualization-centric perspective. In this work we aim to fill the gap by utilizing the concept of visualization literacy to evaluate MLLMs. We assess MLLMs performance over two popular visualization literacy evaluation datasets (VLAT and mini-VLAT). Under the framework of visualization literacy we develop a general setup to compare different multimodal large language models (e.g. GPT4-o Claude 3 Opus Gemini 1.5 Pro) as well as against existing human baselines. Our study demonstrates MLLMs competitive performance in visualization literacy where they outperform humans in certain tasks such as identifying correlations clusters and hierarchical structures.
