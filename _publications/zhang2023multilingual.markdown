---
layout: publication
title: MELA Multilingual Evaluation Of Linguistic Acceptability
authors: Zhang Ziyin, Liu Yikang, Huang Weifang, Mao Junyu, Wang Rui, Hu Hai
conference: "Arxiv"
year: 2023
bibkey: zhang2023multilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.09033"}
  - {name: "Code", url: "https://github.com/sjtu&#45;compling/MELA"}
tags: ['GPT', 'Has Code', 'Interpretability And Explainability', 'Model Architecture', 'Training Techniques']
---
In this work we present the largest benchmark to date on linguistic acceptability Multilingual Evaluation of Linguistic Acceptability 45;45; MELA with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark and investigate cross45;lingual transfer in acceptability judgements with XLM45;R. In pursuit of multilingual interpretability we conduct probing experiments with fine45;tuned XLM45;R to explore the process of syntax capability acquisition. Our results show that GPT45;4o exhibits a strong multilingual ability outperforming fine45;tuned XLM45;R while open45;source multilingual models lag behind by a noticeable gap. Cross45;lingual transfer experiments show that transfer in acceptability judgment is non45;trivial 500 Icelandic fine45;tuning examples lead to 23 MCC performance in a completely unrelated language 45;45; Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM45;R on syntax45;related tasks. Our data is available at https://github.com/sjtu&#45;compling/MELA.
