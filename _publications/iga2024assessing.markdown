---
layout: publication
title: Assessing LLMs Suitability for Knowledge Graph Completion
authors: Iga Vasile Ionut Remus, Silaghi Gheorghe Cosmin
conference: "Arxiv"
year: 2024
bibkey: iga2024assessing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.17249"}
tags: ['ARXIV', 'Applications', 'GPT', 'Prompt', 'Tools']
---
Recent work has shown the capability of Large Language Models (LLMs) to solve tasks related to Knowledge Graphs such as Knowledge Graph Completion even in Zero- or Few-Shot paradigms. However they are known to hallucinate answers or output results in a non-deterministic manner thus leading to wrongly reasoned responses even if they satisfy the users demands. To highlight opportunities and challenges in knowledge graphs-related tasks we experiment with three distinguished LLMs namely Mixtral-8x7b-Instruct-v0.1 GPT-3.5-Turbo-0125 and GPT-4o on Knowledge Graph Completion for static knowledge graphs using prompts constructed following the TELeR taxonomy in Zero- and One-Shot contexts on a Task-Oriented Dialogue system use case. When evaluated using both strict and flexible metrics measurement manners our results show that LLMs could be fit for such a task if prompts encapsulate sufficient information and relevant examples.
