---
layout: publication
title: Generative Verifiers Reward Modeling As Next45;token Prediction
authors: Zhang Lunjun, Hosseini Arian, Bansal Hritik, Kazemi Mehran, Kumar Aviral, Agarwal Rishabh
conference: "Arxiv"
year: 2024
bibkey: zhang2024generative
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.15240"}
tags: ['Applications', 'Language Modeling', 'Reinforcement Learning', 'Training Techniques']
---
Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best45;of45;N method where N candidate solutions generated by the LLM are ranked by a verifier and the best one is selected. While LLM45;based verifiers are typically trained as discriminative classifiers to score solutions they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation we instead propose training verifiers using the ubiquitous next45;token prediction objective jointly on verification and solution generation. Compared to standard verifiers such generative verifiers (GenRM) can benefit from several advantages of LLMs they integrate seamlessly with instruction tuning enable chain45;of45;thought reasoning and can utilize additional inference45;time compute via majority voting for better verification. We demonstrate that when using Gemma45;based verifiers on algorithmic and grade45;school math reasoning tasks GenRM outperforms discriminative verifiers and LLM45;as45;a45;Judge showing a 1645;6437; improvement in the percentage of problems solved with Best45;of45;N. Furthermore we show that GenRM scales favorably across dataset size model capacity and inference45;time compute.
