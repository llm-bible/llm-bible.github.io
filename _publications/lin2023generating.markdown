---
layout: publication
title: 'Generating With Confidence: Uncertainty Quantification For Black-box Large Language Models'
authors: Zhen Lin, Shubhendu Trivedi, Jimeng Sun
conference: "Arxiv"
year: 2023
bibkey: lin2023generating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2305.19187'}
  - {name: "Code", url: 'https://github.com/zlin7/UQ-NLG'}
tags: ['Has Code', 'Uncategorized']
---
Large language models (LLMs) specializing in natural language generation
(NLG) have recently started exhibiting promising capabilities across a variety
of domains. However, gauging the trustworthiness of responses generated by LLMs
remains an open challenge, with limited research on uncertainty quantification
(UQ) for NLG. Furthermore, existing literature typically assumes white-box
access to language models, which is becoming unrealistic either due to the
closed-source nature of the latest LLMs or computational constraints. In this
work, we investigate UQ in NLG for *black-box* LLMs. We first differentiate
*uncertainty* vs *confidence*: the former refers to the ``dispersion'' of the
potential predictions for a fixed input, and the latter refers to the
confidence on a particular prediction/generation. We then propose and compare
several confidence/uncertainty measures, applying them to *selective NLG* where
unreliable results could either be ignored or yielded for further assessment.
Experiments were carried out with several popular LLMs on question-answering
datasets (for evaluation purposes). Results reveal that a simple measure for
the semantic dispersion can be a reliable predictor of the quality of LLM
responses, providing valuable insights for practitioners on uncertainty
management when adopting LLMs. The code to replicate our experiments is
available at https://github.com/zlin7/UQ-NLG.
