---
layout: publication
title: PARIKSHA A Large45;scale Investigation Of Human45;llm Evaluator Agreement On Multilingual And Multi45;cultural Data
authors: Watts Ishaan, Gumma Varun, Yadavalli Aditya, Seshadri Vivek, Swaminathan Manohar, Sitaram Sunayana
conference: "Arxiv"
year: 2024
bibkey: watts2024pariksha
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.15053"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
Evaluation of multilingual Large Language Models (LLMs) is challenging due to a variety of factors 45;45; the lack of benchmarks with sufficient linguistic diversity contamination of popular benchmarks into LLM pre45;training data and the lack of local cultural nuances in translated benchmarks. In this work we study human and LLM45;based evaluation in a multilingual multi45;cultural setting. We evaluate 30 models across 10 Indic languages by conducting 90K human evaluations and 30K LLM45;based evaluations and find that models such as GPT45;4o and Llama45;3 70B consistently perform best for most Indic languages. We build leaderboards for two evaluation settings 45; pairwise comparison and direct assessment and analyse the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. We also check for various biases in human and LLM45;based evaluation and find evidence of self45;bias in the GPT45;based evaluator. Our work presents a significant step towards scaling up multilingual evaluation of LLMs.
