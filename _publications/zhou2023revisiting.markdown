---
layout: publication
title: 'Revisiting Automated Prompting: Are We Actually Doing Better?'
authors: Yulin Zhou, Yiren Zhao, Ilia Shumailov, Robert Mullins, Yarin Gal
conference: "Arxiv"
year: 2023
bibkey: zhou2023revisiting
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2304.03609'}
tags: ['Few-Shot', 'Training Techniques', 'Fine-Tuning', 'Prompting', 'Pretraining Methods']
---
Current literature demonstrates that Large Language Models (LLMs) are great
few-shot learners, and prompting significantly increases their performance on a
range of downstream tasks in a few-shot learning setting. An attempt to
automate human-led prompting followed, with some progress achieved. In
particular, subsequent work demonstrates automation can outperform fine-tuning
in certain K-shot learning scenarios.
  In this paper, we revisit techniques for automated prompting on six different
downstream tasks and a larger range of K-shot learning settings. We find that
automated prompting does not consistently outperform simple manual prompts. Our
work suggests that, in addition to fine-tuning, manual prompts should be used
as a baseline in this line of research.
