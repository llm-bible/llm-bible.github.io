---
layout: publication
title: Revisiting Automated Prompting&#58; Are We Actually Doing Better?
authors: Zhou Yulin, Zhao Yiren, Shumailov Ilia, Mullins Robert, Gal Yarin
conference: "Arxiv"
year: 2023
bibkey: zhou2023revisiting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.03609"}
tags: ['Few Shot', 'Fine Tuning', 'Pretraining Methods', 'Prompting', 'Training Techniques']
---
Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed with some progress achieved. In particular subsequent work demonstrates automation can outperform fine-tuning in certain K-shot learning scenarios. In this paper we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompts. Our work suggests that in addition to fine-tuning manual prompts should be used as a baseline in this line of research.
