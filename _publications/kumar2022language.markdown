---
layout: publication
title: 'Language Generation Models Can Cause Harm: So What Can We Do About It? An
  Actionable Survey'
authors: Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos,
  Yulia Tsvetkov
conference: Arxiv
year: 2022
citations: 19
bibkey: kumar2022language
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2210.07700'}]
tags: [Interpretability and Explainability, Survey Paper, Prompting]
---
Recent advances in the capacity of large language models to generate
human-like text have resulted in their increased adoption in user-facing
settings. In parallel, these improvements have prompted a heated discourse
around the risks of societal harms they introduce, whether inadvertent or
malicious. Several studies have explored these harms and called for their
mitigation via development of safer, fairer models. Going beyond enumerating
the risks of harms, this work provides a survey of practical methods for
addressing potential threats and societal harms from language generation
models. We draw on several prior works' taxonomies of language model risks to
present a structured overview of strategies for detecting and ameliorating
different kinds of risks/harms of language generators. Bridging diverse strands
of research, this survey aims to serve as a practical guide for both LM
researchers and practitioners, with explanations of different mitigation
strategies' motivations, their limitations, and open problems for future
research.