---
layout: publication
title: Combo Of Thinking And Observing For Outside45;knowledge VQA
authors: Si Qingyi, Mo Yuchen, Lin Zheng, Ji Huishan, Wang Weiping
conference: "Arxiv"
year: 2023
bibkey: si2023combo
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.06407"}
  - {name: "Code", url: "https://github.com/PhoebusSi/Thinking&#45;while&#45;Observing"}
tags: ['Applications', 'Has Code', 'Multimodal Models', 'Reinforcement Learning', 'Tools']
---
Outside45;knowledge visual question answering is a challenging task that requires both the acquisition and the use of open45;ended real45;world knowledge. Some existing solutions draw external knowledge into the cross45;modality space which overlooks the much vaster textual knowledge in natural45;language space while others transform the image into a text that further fuses with the textual knowledge into the natural45;language space and completely abandons the use of visual features. In this paper we are inspired to constrain the cross45;modality space into the same space of natural45;language space which makes the visual features preserved directly and the model still benefits from the vast knowledge in natural45;language space. To this end we propose a novel framework consisting of a multimodal encoder a textual encoder and an answer decoder. Such structure allows us to introduce more types of knowledge including explicit and implicit multimodal and textual knowledge. Extensive experiments validate the superiority of the proposed method which outperforms the state45;of45;the45;art by 6.1737; accuracy. We also conduct comprehensive ablations of each component and systematically study the roles of varying types of knowledge. Codes and knowledge data can be found at https://github.com/PhoebusSi/Thinking&#45;while&#45;Observing.
