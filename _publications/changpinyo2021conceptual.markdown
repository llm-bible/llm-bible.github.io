---
layout: publication
title: 'Conceptual 12M: Pushing Web-scale Image-text Pre-training To Recognize Long-tail
  Visual Concepts'
authors: Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut
conference: Arxiv
year: 2021
citations: 350
bibkey: changpinyo2021conceptual
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2102.08981'}]
tags: [Pre-Training]
---
The availability of large-scale image captioning and visual question
answering datasets has contributed significantly to recent successes in
vision-and-language pre-training. However, these datasets are often collected
with overrestrictive requirements inherited from their original target tasks
(e.g., image caption generation), which limit the resulting dataset scale and
diversity. We take a step further in pushing the limits of vision-and-language
pre-training data by relaxing the data collection pipeline used in Conceptual
Captions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M
(CC12M), a dataset with 12 million image-text pairs specifically meant to be
used for vision-and-language pre-training. We perform an analysis of this
dataset and benchmark its effectiveness against CC3M on multiple downstream
tasks with an emphasis on long-tail visual recognition. Our results clearly
illustrate the benefit of scaling up pre-training data for vision-and-language
tasks, as indicated by the new state-of-the-art results on both the nocaps and
Conceptual Captions benchmarks.