---
layout: publication
title: Testing and Evaluation of Large Language Models Correctness Non-Toxicity and Fairness
authors: Wang Wenxuan
conference: "Arxiv"
year: 2024
bibkey: wang2024testing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.00551"}
tags: ['Applications', 'Bias Mitigation', 'Ethics And Bias', 'Fairness', 'Fine Tuning', 'GPT', 'Model Architecture', 'Tools']
---
Large language models (LLMs) such as ChatGPT have rapidly penetrated into peoples work and daily lives over the past few years due to their extraordinary conversational skills and intelligence. ChatGPT has become the fastest-growing software in terms of user numbers in human history and become an important foundational model for the next generation of artificial intelligence applications. However the generations of LLMs are not entirely reliable often producing content with factual errors biases and toxicity. Given their vast number of users and wide range of application scenarios these unreliable responses can lead to many serious negative impacts. This thesis introduces the exploratory works in the field of language model reliability during the PhD study focusing on the correctness non-toxicity and fairness of LLMs from both software testing and natural language processing perspectives. First to measure the correctness of LLMs we introduce two testing frameworks FactChecker and LogicAsker to evaluate factual knowledge and logical reasoning accuracy respectively. Second for the non-toxicity of LLMs we introduce two works for red-teaming LLMs. Third to evaluate the fairness of LLMs we introduce two evaluation frameworks BiasAsker and XCulturalBench to measure the social bias and cultural bias of LLMs respectively.
