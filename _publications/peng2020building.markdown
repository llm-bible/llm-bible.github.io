---
layout: publication
title: SOLOIST Building Task Bots At Scale With Transfer Learning And Machine Teaching
authors: Peng Baolin, Li Chunyuan, Li Jinchao, Shayandeh Shahin, Liden Lars, Gao Jianfeng
conference: "Arxiv"
year: 2020
bibkey: peng2020building
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2005.05298"}
  - {name: "Code", url: "https://aka.ms/soloist"}
tags: ['Fine Tuning', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
We present a new method SOLOIST that uses transfer learning and machine teaching to build task bots at scale. We parameterize classical modular task45;oriented dialog systems using a Transformer45;based auto45;regressive language model which subsumes different dialog modules into a single neural model. We pre45;train on heterogeneous dialog corpora a task45;grounded response generation model which can generate dialog responses grounded in user goals and real45;world knowledge for task completion. The pre45;trained model can be efficiently adapted to accomplish new tasks with a handful of task45;specific dialogs via machine teaching where training samples are generated by human teachers interacting with the system. Experiments show that (i) SOLOIST creates new state45;of45;the45;art on well45;studied task45;oriented dialog benchmarks including CamRest676 and MultiWOZ; (ii) in the few45;shot fine45;tuning settings SOLOIST significantly outperforms existing methods and (iii) the use of machine teaching substantially reduces the labeling cost of fine45;tuning. The pre45;trained models and codes are available at https://aka.ms/soloist.
