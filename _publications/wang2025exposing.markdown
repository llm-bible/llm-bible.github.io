---
layout: publication
title: 'The Illusionist''s Prompt: Exposing The Factual Vulnerabilities Of Large Language Models With Linguistic Nuances'
authors: Yining Wang, Yuquan Wang, Xi Li, Mi Zhang, Geng Hong, Min Yang
conference: "Arxiv"
year: 2025
bibkey: wang2025exposing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.02865"}
tags: ['Security', 'Model Architecture', 'Tools', 'GPT', 'Prompting']
---
As Large Language Models (LLMs) continue to advance, they are increasingly
relied upon as real-time sources of information by non-expert users. To ensure
the factuality of the information they provide, much research has focused on
mitigating hallucinations in LLM responses, but only in the context of formal
user queries, rather than maliciously crafted ones. In this study, we introduce
The Illusionist's Prompt, a novel hallucination attack that incorporates
linguistic nuances into adversarial queries, challenging the factual accuracy
of LLMs against five types of fact-enhancing strategies. Our attack
automatically generates highly transferrable illusory prompts to induce
internal factual errors, all while preserving user intent and semantics.
Extensive experiments confirm the effectiveness of our attack in compromising
black-box LLMs, including commercial APIs like GPT-4o and Gemini-2.0, even with
various defensive mechanisms.
