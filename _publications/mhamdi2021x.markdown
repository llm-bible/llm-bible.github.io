---
layout: publication
title: X45;METRA45;ADA Cross45;lingual Meta45;transfer Learning Adaptation To Natural Language Understanding And Question Answering
authors: M'hamdi Meryem, Kim Doo Soon, Dernoncourt Franck, Bui Trung, Ren Xiang, May Jonathan
conference: "Arxiv"
year: 2021
bibkey: mhamdi2021x
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2104.09696"}
tags: ['Applications', 'Attention Mechanism', 'BERT', 'Efficiency And Optimization', 'Fine Tuning', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools']
---
Multilingual models such as M45;BERT and XLM45;R have gained increasing popularity due to their zero45;shot cross45;lingual transfer learning capabilities. However their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently meta45;learning has garnered attention as a promising technique for enhancing transfer learning under low45;resource scenarios particularly for cross45;lingual transfer in Natural Language Understanding (NLU). In this work we propose X45;METRA45;ADA a cross45;lingual MEta45;TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML an optimization45;based meta45;learning approach to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross45;lingual NLU tasks multilingual task45;oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine45;tuning reaching competitive performance on both tasks for most languages. Our analysis reveals that X45;METRA45;ADA can leverage limited data for faster adaptation.
