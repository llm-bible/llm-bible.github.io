---
layout: publication
title: 'Evaluating The Consistency Of LLM Evaluators'
authors: Noah Lee, Jiwoo Hong, James Thorne
conference: "Arxiv"
year: 2024
bibkey: lee2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.00543"}
tags: ['Uncategorized']
---
Large language models (LLMs) have shown potential as general evaluators along
with the evident benefits of speed and cost. While their correlation against
human annotators has been widely studied, consistency as evaluators is still
understudied, raising concerns about the reliability of LLM evaluators. In this
paper, we conduct extensive studies on the two aspects of consistency in LLM
evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on
different scoring scales and criterion granularity with open-source and
proprietary models. Our comprehensive analysis demonstrates that strong
proprietary models are not necessarily consistent evaluators, highlighting the
importance of considering consistency in assessing the capability of LLM
evaluators.
