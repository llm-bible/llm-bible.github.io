---
layout: publication
title: Multi45;task Learning With Multi45;head Attention For Multi45;choice Reading Comprehension
authors: Wan Hui
conference: "Arxiv"
year: 2020
bibkey: wan2020multi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2003.04992"}
tags: ['Applications', 'Attention Mechanism', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Multiple45;choice Machine Reading Comprehension (MRC) is an important and challenging Natural Language Understanding (NLU) task in which a machine must choose the answer to a question from a set of choices with the question placed in context of text passages or dialog. In the last a couple of years the NLU field has been revolutionized with the advent of models based on the Transformer architecture which are pretrained on massive amounts of unsupervised data and then fine45;tuned for various supervised learning NLU tasks. Transformer models have come to dominate a wide variety of leader45;boards in the NLU field; in the area of MRC the current state45;of45;the45;art model on the DREAM dataset (seeSunet al. 2019) fine tunes Albert a large pretrained Transformer45;based model and addition45;ally combines it with an extra layer of multi45;head attention between context and question45;answerZhuet al. 2020.The purpose of this note is to document a new state45;of45;the45;art result in the DREAM task which is accomplished by additionally performing multi45;task learning on two MRC multi45;choice reading comprehension tasks (RACE and DREAM).
