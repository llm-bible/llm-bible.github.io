---
layout: publication
title: Emergent Analogical Reasoning In Large Language Models
authors: Webb Taylor, Holyoak Keith J., Lu Hongjing
conference: "Arxiv"
year: 2022
bibkey: webb2022emergent
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2212.09196"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero45;shot without any direct training. In human cognition this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text45;davinci45;003 variant of GPT45;3) on a range of analogical tasks including a non45;visual matrix reasoning task based on the rule structure of Ravens Standard Progressive Matrices. We found that GPT45;3 displayed a surprisingly strong capacity for abstract pattern induction matching or even surpassing human capabilities in most settings; preliminary tests of GPT45;4 indicated even better performance. Our results indicate that large language models such as GPT45;3 have acquired an emergent ability to find zero45;shot solutions to a broad range of analogy problems.
