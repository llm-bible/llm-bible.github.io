---
layout: publication
title: A Chain45;of45;thought Prompting Approach With Llms For Evaluating Students Formative Assessment Responses In Science
authors: Cohn Clayton, Hutchins Nicole, Le Tuan, Biswas Gautam
conference: "Arxiv"
year: 2024
bibkey: cohn2024chain
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.14565"}
tags: ['GPT', 'Interpretability And Explainability', 'Model Architecture', 'Pretraining Methods', 'Prompting']
---
This paper explores the use of large language models (LLMs) to score and explain short45;answer assessments in K45;12 science. While existing methods can score more structured math and computer science assessments they often do not provide explanations for the scores. Our study focuses on employing GPT45;4 for automated assessment in middle school Earth Science combining few45;shot and active learning with chain45;of45;thought reasoning. Using a human45;in45;the45;loop approach we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our methods pros and cons sheds light on the potential for human45;in45;the45;loop techniques to enhance automated grading for open45;ended science assessments.
