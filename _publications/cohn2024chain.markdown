---
layout: publication
title: A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students Formative Assessment Responses in Science
authors: Cohn Clayton, Hutchins Nicole, Le Tuan, Biswas Gautam
conference: "Arxiv"
year: 2024
bibkey: cohn2024chain
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.14565"}
tags: ['ARXIV', 'Few Shot', 'GPT', 'Interpretability And Explainability', 'LLM', 'Model Architecture', 'Pretraining Methods', 'Prompting']
---
This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments they often do not provide explanations for the scores. Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science combining few-shot and active learning with chain-of-thought reasoning. Using a human-in-the-loop approach we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our methods pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.
