---
layout: publication
title: 'Appropriateness Is All You Need!'
authors: Kempt Hendrik, Lavie Alon, Nagel Saskia K.
conference: "Arxiv"
year: 2023
bibkey: kempt2023appropriateness
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.14553"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Merging', 'Model Architecture', 'Reinforcement Learning', 'Responsible AI']
---
'The strive to make AI applications safe has led to the development of safety-measures as the main or even sole normative requirement of their permissible use. Similar can be attested to the latest version of chatbots, such as chatGPT. In this view, if they are safe, they are supposed to be permissible to deploy. This approach, which we call safety-normativity, is rather limited in solving the emerging issues that chatGPT and other chatbots have caused thus far. In answering this limitation, in this paper we argue for limiting chatbots in the range of topics they can chat about according to the normative concept of appropriateness. We argue that rather than looking for safety in a chatbot''s utterances to determine what they may and may not say, we ought to assess those utterances according to three forms of appropriateness: technical-discursive, social, and moral. We then spell out what requirements for chatbots follow from these forms of appropriateness to avoid the limits of previous accounts: positionality, acceptability, and value alignment (PAVA). With these in mind, we may be able to determine what a chatbot may and may not say. Lastly, one initial suggestion is to use challenge sets, specifically designed for appropriateness, as a validation method.'
