---
layout: publication
title: Self45;checker Plug45;and45;play Modules For Fact45;checking With Large Language Models
authors: Miaoran Li, Baolin Peng, Michel Galley, Jianfeng Gao, Zhu Zhang
conference: "Arxiv"
year: 2023
bibkey: li2023self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2305.14623v2"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Tools']
---
Fact45;checking is an essential task in NLP that is commonly utilized for validating the factual accuracy of claims. Prior work has mainly focused on fine45;tuning pre45;trained languages models on specific datasets which can be computationally intensive and time45;consuming. With the rapid development of large language models (LLMs) such as ChatGPT and GPT45;3 researchers are now exploring their in45;context learning capabilities for a wide range of tasks. In this paper we aim to assess the capacity of LLMs for fact45;checking by introducing Self45;Checker a framework comprising a set of plug45;and45;play modules that facilitate fact45;checking by purely prompting LLMs in an almost zero45;shot setting. This framework provides a fast and efficient way to construct fact45;checking systems in low45;resource environments. Empirical results demonstrate the potential of Self45;Checker in utilizing LLMs for fact45;checking. However there is still significant room for improvement compared to SOTA fine45;tuned models which suggests that LLM adoption could be a promising approach for future fact45;checking research.
