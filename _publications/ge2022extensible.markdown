---
layout: publication
title: Extensible Prompts For Language Models On Zero45;shot Language Style Customization
authors: Ge Tao, Hu Jing, Dong Li, Mao Shaoguang, Xia Yan, Wang Xun, Chen Si-qing, Wei Furu
conference: "Arxiv"
year: 2022
bibkey: ge2022extensible
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2212.00616"}
tags: ['Pretraining Methods', 'Prompting', 'Reinforcement Learning']
---
We propose eXtensible Prompt (X45;Prompt) for prompting a large language model (LLM) beyond natural language (NL). X45;Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words. Registering new imaginary words allows us to instruct the LLM to comprehend concepts that are difficult to describe with NL words thereby making a prompt more descriptive. Also these imaginary words are designed to be out45;of45;distribution (OOD) robust so that they can be (re)used like NL words in various prompts distinguishing X45;Prompt from soft prompt that is for fitting in45;distribution data. We propose context45;augmented learning (CAL) to learn imaginary words for general usability enabling them to work properly in OOD (unseen) prompts. We experiment X45;Prompt for zero45;shot language style customization as a case study. The promising results of X45;Prompt demonstrate its potential to facilitate advanced interaction beyond the natural language interface bridging the communication gap between humans and LLMs.
