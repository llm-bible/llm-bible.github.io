---
layout: publication
title: Lets Think Frame By Frame With VIP A Video Infilling And Prediction Dataset For Evaluating Video Chain45;of45;thought
authors: Vaishnavi Himakunthala, Andy Ouyang, Daniel Rose, Ryan He, Alex Mei, Yujie Lu, Chinmay Sonar, Michael Saxon, William Yang Wang
conference: "Arxiv"
year: 2023
bibkey: himakunthala2023think
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2305.13903v3"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'RAG', 'Security']
---
Despite exciting recent results showing vision45;language systems capacity to reason about images using natural language their capacity for video reasoning remains under45;explored. We motivate framing video reasoning as the sequential understanding of a small number of keyframes thereby leveraging the power and robustness of vision45;language while alleviating the computational complexities of processing videos. To evaluate this novel application we introduce VIP an inference45;time challenge dataset designed to explore models reasoning capabilities through video chain45;of45;thought. Inspired by visually descriptive scene plays we propose two formats for keyframe description unstructured dense captions and structured scene descriptions that identify the focus action mood objects and setting (FAMOuS) of the keyframe. To evaluate video reasoning we propose two tasks Video Infilling and Video Prediction which test abilities to generate multiple intermediate keyframes and predict future keyframes respectively. We benchmark GPT45;4 GPT45;3 and VICUNA on VIP demonstrate the performance gap in these complex video reasoning tasks and encourage future work to prioritize language models for efficient and generalized video reasoning.
