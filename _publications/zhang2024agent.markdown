---
layout: publication
title: Agent45;pro Learning To Evolve Via Policy45;level Reflection And Optimization
authors: Zhang Wenqi, Tang Ke, Wu Hai, Wang Mengna, Shen Yongliang, Hou Guiyang, Tan Zeqi, Li Peng, Zhuang Yueting, Lu Weiming
conference: "Arxiv"
year: 2024
bibkey: zhang2024agent
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.17574"}
tags: ['Agentic', 'Applications', 'Efficiency And Optimization', 'Prompting']
---
Large Language Models (LLMs) exhibit robust problem45;solving capabilities for diverse tasks. However most LLM45;based agents are designed as specific task solvers with sophisticated prompt engineering rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors inherently incapacitating to address complex dynamic scenarios e.g. large interactive games. In light of this we propose Agent45;Pro an LLM45;based Agent with Policy45;level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically it involves a dynamic belief generation and reflection process for policy evolution. Rather than action45;level reflection Agent45;Pro iteratively reflects on past trajectories and beliefs fine45;tuning its irrational beliefs for a better policy. Moreover a depth45;first search is employed for policy optimization ensuring continual enhancement in policy payoffs. Agent45;Pro is evaluated across two games Blackjack and Texas Holdem outperforming vanilla LLM and specialized models. Our results show Agent45;Pro can learn and evolve in complex and dynamic scenes which also benefits numerous LLM45;based applications.
