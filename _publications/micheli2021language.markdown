---
layout: publication
title: Language Models Are Few45;shot Butlers
authors: Micheli Vincent, Fleuret Fran√ßois
conference: "Arxiv"
year: 2021
bibkey: micheli2021language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2104.07972"}
tags: ['Agentic', 'GPT', 'Language Modeling', 'Pretraining Methods', 'Reinforcement Learning']
---
Pretrained language models demonstrate strong performance in most NLP tasks when fine45;tuned on small task45;specific datasets. Hence these autoregressive models constitute ideal agents to operate in text45;based environments where language understanding and generative capabilities are essential. Nonetheless collecting expert demonstrations in such environments is a time45;consuming endeavour. We introduce a two45;stage procedure to learn from a small set of demonstrations and further improve by interacting with an environment. We show that language models fine45;tuned with only 1.237; of the expert demonstrations and a simple reinforcement learning algorithm achieve a 5137; absolute improvement in success rate over existing methods in the ALFWorld environment.
