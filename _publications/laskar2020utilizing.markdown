---
layout: publication
title: Utilizing Bidirectional Encoder Representations From Transformers For Answer Selection
authors: Laskar Md Tahmid Rahman, Hoque Enamul, Huang Jimmy Xiangji
conference: "Arxiv"
year: 2020
bibkey: laskar2020utilizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2011.07208"}
tags: ['Applications', 'BERT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Pre45;training a transformer45;based model for the language modeling task in a large dataset and then fine45;tuning it for downstream tasks has been found very useful in recent years. One major advantage of such pre45;trained language models is that they can effectively absorb the context of each word in a sentence. However for tasks such as the answer selection task the pre45;trained language models have not been extensively used yet. To investigate their effectiveness in such tasks in this paper we adopt the pre45;trained Bidirectional Encoder Representations from Transformer (BERT) language model and fine45;tune it on two Question Answering (QA) datasets and three Community Question Answering (CQA) datasets for the answer selection task. We find that fine45;tuning the BERT model for the answer selection task is very effective and observe a maximum improvement of 13.137; in the QA datasets and 18.737; in the CQA datasets compared to the previous state45;of45;the45;art.
