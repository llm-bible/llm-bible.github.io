---
layout: publication
title: Laviplanguage45;grounded Visual Prompts
authors: Kunananthaseelan Nilakshan, Zhang Jing, Harandi Mehrtash
conference: "Arxiv"
year: 2023
bibkey: kunananthaseelan2023visual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.10945"}
tags: ['Fine Tuning', 'Prompting', 'Tools']
---
We introduce a language45;grounded visual prompting method to adapt the visual encoder of vision45;language models for downstream tasks. By capitalizing on language integration we devise a parameter45;efficient strategy to adjust the input of the visual encoder eliminating the need to modify or add to the models parameters. Due to this design choice our algorithm can operate even in black45;box scenarios showcasing adaptability in situations where access to the models parameters is constrained. We will empirically demonstrate that compared to prior art grounding visual prompts with language enhances both the accuracy and speed of adaptation. Moreover our algorithm excels in base45;to45;novel class generalization overcoming limitations of visual prompting and exhibiting the capacity to generalize beyond seen classes. We thoroughly assess and evaluate our method across a variety of image recognition datasets such as EuroSAT UCF101 DTD and CLEVR spanning different learning situations including few45;shot learning base45;to45;novel class generalization and transfer learning.
