---
layout: publication
title: Pragmatic Competence Evaluation Of Large Language Models For Korean
authors: Park Dojun, Lee Jiwoo, Jeong Hyeyun, Park Seohyun, Lee Sungeun
conference: "Arxiv"
year: 2024
bibkey: park2024pragmatic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.12675"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'RAG']
---
The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple45;choice questions (MCQs) a format inherently suited for automated evaluation. Our study extends this evaluation to explore LLMs pragmatic competence45;45;a facet previously underexamined before the advent of sophisticated LLMs specifically in the context of Korean. We employ two distinct evaluation setups the conventional MCQ format adapted for automatic evaluation and Open45;Ended Questions (OEQs) assessed by human experts to examine LLMs narrative response capabilities without predefined options. Our findings reveal that GPT45;4 excels scoring 81.11 and 85.69 in the MCQ and OEQ setups respectively with HyperCLOVA X an LLM optimized for Korean closely following especially in the OEQ setup demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT45;4. Furthermore while few45;shot learning strategies generally enhance LLM performance Chain45;of45;Thought (CoT) prompting introduces a bias toward literal interpretations hindering accurate pragmatic inference. Considering the growing expectation for LLMs to understand and produce language that aligns with human communicative norms our findings emphasize the importance for advancing LLMs abilities to grasp and convey sophisticated meanings beyond mere literal interpretations.
