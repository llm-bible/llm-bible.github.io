---
layout: publication
title: 'Iimedgpt: Promoting Large Language Model Capabilities Of Medical Tasks By Efficient Human Preference Alignment'
authors: Yiming Zhang, Zheng Chang, Wentao Cai, Mengxing Ren, Kang Yuan, Yining Sun, Zenghui Ding
conference: "Arxiv"
year: 2025
bibkey: zhang2025promoting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.02869"}
tags: ['Pre-Training', 'GPT', 'Efficiency and Optimization', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
Recent researches of large language models(LLM), which is pre-trained on
massive general-purpose corpora, have achieved breakthroughs in responding
human queries. However, these methods face challenges including limited data
insufficiency to support extensive pre-training and can not align responses
with users' instructions. To address these issues, we introduce a medical
instruction dataset, CMedINS, containing six medical instructions derived from
actual medical tasks, which effectively fine-tunes LLM in conjunction with
other data. Subsequently, We launch our medical model, IIMedGPT, employing an
efficient preference alignment method, Direct preference Optimization(DPO). The
results show that our final model outperforms existing medical models in
medical dialogue.Datsets, Code and model checkpoints will be released upon
acceptance.
