---
layout: publication
title: Mt5 A Massively Multilingual Pre45;trained Text45;to45;text Transformer
authors: Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-rfou, Aditya Siddhant, Aditya Barua, Colin Raffel
conference: "Arxiv"
year: 2020
bibkey: xue2020massively
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2010.11934v3"}
tags: ['Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques', 'Transformer']
---
The recent Text45;to45;Text Transfer Transformer (T5) leveraged a unified text45;to45;text format and scale to attain state45;of45;the45;art results on a wide variety of English45;language NLP tasks. In this paper we introduce mT5 a multilingual variant of T5 that was pre45;trained on a new Common Crawl45;based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state45;of45;the45;art performance on many multilingual benchmarks. We also describe a simple technique to prevent accidental translation in the zero45;shot setting where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.
