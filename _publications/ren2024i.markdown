---
layout: publication
title: I Learn Better If You Speak My Language Understanding The Superior Performance Of Fine45;tuning Large Language Models With Llm45;generated Responses
authors: Ren Xuan, Wu Biao, Liu Lingqiao
conference: "Arxiv"
year: 2024
bibkey: ren2024i
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.11192"}
tags: ['Pretraining Methods', 'Training Techniques']
---
This paper explores an intriguing observation fine45;tuning a large language model (LLM) with responses generated by a LLM often yields better results than using responses generated by humans. We conduct an in45;depth investigation to understand why this occurs. Contrary to the common belief that these instances is simply due to the more detailed nature of LLM45;generated content our study identifies another contributing factor an LLM is inherently more familiar with LLM generated responses. This familiarity is evidenced by lower perplexity before fine45;tuning. We design a series of experiments to understand the impact of the familiarity and our conclusion reveals that this familiarity significantly impacts learning performance. Training with LLM45;generated responses not only enhances performance but also helps maintain the models capabilities in other tasks after fine45;tuning on a specific task.
