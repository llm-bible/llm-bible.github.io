---
layout: publication
title: "Promptbench: A Unified Library For Evaluation Of Large Language Models"
authors: Zhu Kaijie, Zhao Qinlin, Chen Hao, Wang Jindong, Xie Xing
conference: "Arxiv"
year: 2023
bibkey: zhu2023unified
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.07910"}
  - {name: "Code", url: "https://github.com/microsoft/promptbench"}
tags: ['Applications', 'Has Code', 'Prompting', 'Reinforcement Learning', 'Security', 'Tools']
---
The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper we introduce PromptBench a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers prompt construction prompt engineering dataset and model loading adversarial prompt attack dynamic evaluation protocols and analysis tools. PromptBench is designed to be an open general and flexible codebase for research purposes that can facilitate original study in creating new benchmarks deploying downstream applications and designing new evaluation protocols. The code is available at https://github.com/microsoft/promptbench and will be continuously supported.
