---
layout: publication
title: 'Promptbench: A Unified Library For Evaluation Of Large Language Models'
authors: Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie
conference: "Arxiv"
year: 2023
bibkey: zhu2023unified
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2312.07910'}
  - {name: "Code", url: 'https://github.com/microsoft/promptbench'}
tags: ['Has Code', 'Security', 'Applications', 'Tools', 'Prompting', 'Reinforcement Learning']
---
The evaluation of large language models (LLMs) is crucial to assess their
performance and mitigate potential security risks. In this paper, we introduce
PromptBench, a unified library to evaluate LLMs. It consists of several key
components that are easily used and extended by researchers: prompt
construction, prompt engineering, dataset and model loading, adversarial prompt
attack, dynamic evaluation protocols, and analysis tools. PromptBench is
designed to be an open, general, and flexible codebase for research purposes
that can facilitate original study in creating new benchmarks, deploying
downstream applications, and designing new evaluation protocols. The code is
available at: https://github.com/microsoft/promptbench and will be continuously
supported.
