---
layout: publication
title: DaG LLM ver 1.0 Pioneering Instruction-Tuned Language Modeling for Korean NLP
authors: Jang Dongjun, Lee Sangah, Byun Sungjoo, Kim Jinwoong, Seo Jean, Kim Minseok, Kim Soyeon, Oh Chaeyoung, Kim Jaeyoon, Jo Hyemi, Shin Hyopil
conference: "Arxiv"
year: 2023
bibkey: jang2023dag
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.13784"}
tags: ['ARXIV', 'LLM', 'Language Modeling', 'Pretraining Methods']
---
This paper presents the DaG LLM (David and Goliath Large Language Model) a language model specialized for Korean and fine-tuned through Instruction Tuning across 41 tasks within 13 distinct categories.
