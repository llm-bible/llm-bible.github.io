---
layout: publication
title: 'Dag LLM Ver 1.0: Pioneering Instruction-tuned Language Modeling For Korean NLP'
authors: Jang Dongjun, Lee Sangah, Byun Sungjoo, Kim Jinwoong, Seo Jean, Kim Minseok, Kim Soyeon, Oh Chaeyoung, Kim Jaeyoon, Jo Hyemi, Shin Hyopil
conference: "Arxiv"
year: 2023
bibkey: jang2023dag
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.13784"}
tags: ['Language Modeling', 'Pretraining Methods']
---
This paper presents the DaG LLM (David and Goliath Large Language Model), a language model specialized for Korean and fine-tuned through Instruction Tuning across 41 tasks within 13 distinct categories.
