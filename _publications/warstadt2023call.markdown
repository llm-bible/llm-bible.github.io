---
layout: publication
title: Call for Papers -- The BabyLM Challenge Sample-efficient pretraining on a developmentally plausible corpus
authors: Warstadt Alex, Choshen Leshem, Mueller Aaron, Williams Adina, Wilcox Ethan, Zhuang Chengxu
conference: "Arxiv"
year: 2023
bibkey: warstadt2023call
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2301.11796"}
tags: ['Applications', 'Fine Tuning', 'Language Modeling', 'Pretraining Methods', 'Tools', 'Training Techniques']
---
We present the call for papers for the BabyLM Challenge Sample-efficient pretraining on a developmentally plausible corpus. This shared task is intended for participants with an interest in small scale language modeling human language acquisition low-resource NLP and cognitive modeling. In partnership with CoNLL and CMCL we provide a platform for approaches to pretraining with a limited-size corpus sourced from data inspired by the input to children. The task has three tracks two of which restrict the training data to pre-released datasets of 10M and 100M words and are dedicated to explorations of approaches such as architectural variations self-supervised objectives or curriculum learning. The final track only restricts the amount of text used allowing innovation in the choice of the data its domain and even its modality (i.e. data from sources other than text is welcome). We will release a shared evaluation pipeline which scores models on a variety of benchmarks and tasks including targeted syntactic evaluations and natural language understanding.
