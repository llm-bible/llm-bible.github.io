---
layout: publication
title: 'Synthetic Data Generation Using Large Language Models: Advances In Text And Code'
authors: Mihai Nadas, Laura Diosan, Andreea Tomescu
conference: "Arxiv"
year: 2025
bibkey: nadas2025synthetic
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.14023'}
tags: ['Agentic', 'RAG', 'Applications', 'Training Techniques', 'Tools', 'Prompting', 'Multimodal Models', 'Survey Paper', 'Reinforcement Learning', 'Ethics and Bias']
---
Large language models (LLMs) have unlocked new possibilities for generating
synthetic training data in both natural language and code. By producing
artificial but task-relevant examples, these models can significantly augment
or even replace real-world datasets, especially when labeled data is scarce or
sensitive. This paper surveys recent advances in using LLMs to create synthetic
text and code, emphasizing prompt-based generation, retrieval-augmented
pipelines, and iterative self-refinement. We show how these methods enrich
low-resource tasks such as classification and question answering, as well as
code-centric applications such as instruction tuning, code translation, and bug
repair, by enabling automated verification of functional correctness. Alongside
potential benefits like cost-effectiveness, broad coverage, and controllable
diversity, we address challenges such as factual inaccuracies in generated
text, lack of stylistic realism, and the risk of bias amplification. Proposed
mitigations include filtering and weighting outputs and reinforcement learning
with execution feedback for code. We conclude with open research directions
like automated prompt engineering, cross-modal data synthesis, and robust
evaluation frameworks, highlighting the importance of LLM-generated synthetic
data in advancing AI while emphasizing ethical and quality safeguards.
