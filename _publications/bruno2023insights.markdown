---
layout: publication
title: Insights Into Classifying And Mitigating Llmsx27; Hallucinations
authors: Bruno Alessandro, Mazzeo Pier Luigi, Chetouani Aladine, Tliba Marouane, Kerkouri Mohamed Amine
conference: "Arxiv"
year: 2023
bibkey: bruno2023insights
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.08117"}
tags: ['Applications', 'Language Modeling', 'Merging', 'Reinforcement Learning']
---
The widespread adoption of large language models (LLMs) across diverse AI applications is proof of the outstanding achievements obtained in several tasks such as text mining text generation and question answering. However LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as Hallucinations. They manifest in text generation systems particularly in question-answering systems reliant on LLMs potentially resulting in false or misleading information propagation. This paper delves into the underlying causes of AI hallucination and elucidates its significance in artificial intelligence. In particular Hallucination classification is tackled over several tasks (Machine Translation Question and Answer Dialog Systems Summarisation Systems Knowledge Graph with LLMs and Visual Question Answer). Additionally we explore potential strategies to mitigate hallucinations aiming to enhance the overall reliability of LLMs. Our research addresses this critical issue within the HeReFaNMi (Health-Related Fake News Mitigation) project generously supported by NGI Search dedicated to combating Health-Related Fake News dissemination on the Internet. This endeavour represents a concerted effort to safeguard the integrity of information dissemination in an age of evolving AI technologies.
