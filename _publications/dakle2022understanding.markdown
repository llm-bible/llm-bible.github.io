---
layout: publication
title: Understanding BLOOM An Empirical Study On Diverse NLP Tasks
authors: Dakle Parag Pravin, Rallabandi Saikrishna, Raghavan Preethi
conference: "Arxiv"
year: 2022
bibkey: dakle2022understanding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.14865"}
tags: ['Applications', 'BERT', 'GPT', 'Language Modeling', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
We view the landscape of large language models (LLMs) through the lens of the recently released BLOOM model to understand the performance of BLOOM and other decoder45;only LLMs compared to BERT45;style encoder45;only models. We achieve this by evaluating the smaller BLOOM model variants (textit123;350m/560m125; and textit123;1b3/1b7125;) on several NLP benchmark datasets and popular leaderboards. We make the following observations (1) BLOOM performance does not scale with parameter size unlike other LLMs like GPT and BERT. Experiments fine45;tuning BLOOM models show that the 560m variant performs similarly to or better than the 1b7 variant (2) Zero45;shot cross45;lingual and multi45;lingual fine45;tuning experiments show that BLOOM is at par or worse than monolingual GPT45;2 models and (3) Toxicity analysis of prompt45;based text generation using the RealToxicityPrompts dataset shows that the text generated by BLOOM is at least 1737; less toxic than GPT45;2 and GPT45;3 models.
