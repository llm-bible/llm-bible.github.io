---
layout: publication
title: 'Zero-shot RTL Code Generation With Attention Sink Augmented Large Language Models'
authors: Selim Sandal, Ismail Akturk
conference: "Arxiv"
year: 2024
bibkey: sandal2024zero
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2401.08683'}
tags: ['Attention Mechanism', 'Transformer', 'Efficiency and Optimization', 'Model Architecture', 'Tools', 'Applications', 'Fine-Tuning', 'Prompting', 'Reinforcement Learning']
---
The design and optimization of hardware have traditionally been
resource-intensive, demanding considerable expertise and dependence on
established design automation tools. This paper discusses the possibility of
exploiting large language models to streamline the code generation process in
hardware design. In contrast to earlier studies, this paper aims to use large
language models that accepts high-level design specifications through a single
prompt to generate corresponding Register-Transfer Level (RTL) code. The
ability to use large language models on RTL code generation not only expedites
design iteration cycles but also facilitates the exploration of design spaces
that have computational challenges for conventional techniques. Through our
evaluation, we demonstrate the shortcoming of existing attention mechanisms,
and present the abilities of language models to produce functional, optimized,
and industry-standard compliant RTL code when a novel attention mechanism is
used. These findings underscore the expanding role of large language models in
shaping the future landscape of architectural exploration and automation in
hardware design.
