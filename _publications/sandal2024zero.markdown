---
layout: publication
title: Zero45;shot RTL Code Generation With Attention Sink Augmented Large Language Models
authors: Sandal Selim, Akturk Ismail
conference: "Arxiv"
year: 2024
bibkey: sandal2024zero
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.08683"}
tags: ['Applications', 'Attention Mechanism', 'Efficiency And Optimization', 'Fine Tuning', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Tools', 'Transformer']
---
The design and optimization of hardware have traditionally been resource45;intensive demanding considerable expertise and dependence on established design automation tools. This paper discusses the possibility of exploiting large language models to streamline the code generation process in hardware design. In contrast to earlier studies this paper aims to use large language models that accepts high45;level design specifications through a single prompt to generate corresponding Register45;Transfer Level (RTL) code. The ability to use large language models on RTL code generation not only expedites design iteration cycles but also facilitates the exploration of design spaces that have computational challenges for conventional techniques. Through our evaluation we demonstrate the shortcoming of existing attention mechanisms and present the abilities of language models to produce functional optimized and industry45;standard compliant RTL code when a novel attention mechanism is used. These findings underscore the expanding role of large language models in shaping the future landscape of architectural exploration and automation in hardware design.
