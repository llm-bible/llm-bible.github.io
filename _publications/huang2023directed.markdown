---
layout: publication
title: Directed Acyclic Transformer Pre45;training For High45;quality Non45;autoregressive Text Generation
authors: Huang Fei, Ke Pei, Huang Minlie
conference: "Arxiv"
year: 2023
bibkey: huang2023directed
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.11791"}
tags: ['Applications', 'Attention Mechanism', 'Ethics And Bias', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques', 'Transformer']
---
Non45;AutoRegressive (NAR) text generation models have drawn much attention because of their significantly faster decoding speed and good generation quality in machine translation. However in a wider range of text generation tasks existing NAR models lack proper pre45;training making them still far behind the pre45;trained autoregressive models. In this paper we propose Pre45;trained Directed Acyclic Transformer (PreDAT) and a novel pre45;training task to promote prediction consistency in NAR generation. Experiments on five text generation tasks show that our PreDAT remarkably outperforms existing pre45;trained NAR models (+4.2 scores on average) and even achieves better results than pre45;trained autoregressive baselines in n45;gram45;based metrics along with 17 times speedup in throughput. Further analysis shows that PreDAT benefits from the unbiased prediction order that alleviates the error accumulation problem in autoregressive generation which provides new insights into the advantages of NAR generation.
