---
layout: publication
title: F-eval\: Assessing Fundamental Abilities With Refined Evaluation Methods
authors: Sun Yu, Chen Keyu, Wang Shujie, Li Peiji, Guo Qipeng, Yan Hang, Qiu Xipeng, Huang Xuanjing, Lin Dahua
conference: "Arxiv"
year: 2024
bibkey: sun2024f
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.14869"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Large language models (LLMs) garner significant attention for their unprecedented performance leading to an increasing number of researches evaluating LLMs. However these evaluation benchmarks are limited to assessing the instruction-following capabilities overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However in the absence of references large models have shown limited ability to discern subtle differences. To bridge the gap we propose F-Eval a bilingual evaluation benchmark to evaluate the fundamental abilities including expression commonsense and logic. The tasks in F-Eval include multi-choice objective tasks open-ended objective tasks reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks we devise new evaluation methods serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced LLMs. Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators. Additionally we discuss the influence of different model sizes dimensions and normalization methods. We anticipate that F-Eval will facilitate the study of LLMs fundamental abilities.
