---
layout: publication
title: Birbal An Efficient 7B Instruct45;model Fine45;tuned With Curated Datasets
authors: Jindal Ashvini Kumar, Rajpoot Pawan Kumar, Parikh Ankur
conference: "Arxiv"
year: 2024
bibkey: jindal2024efficient
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.02247"}
tags: ['Efficiency And Optimization', 'Ethics And Bias', 'Training Techniques']
---
LLMOps incur significant costs due to hardware requirements hindering their widespread accessibility. Additionally a lack of transparency in model training methods and data contributes to the majority of models being non45;reproducible. To tackle these challenges the LLM Efficiency Challenge was introduced at NeurIPS Workshop aiming to adapt foundation models on a diverse set of tasks via fine45;tuning on a single GPU (RTX 4090 or A100 with 40GB) within a 2445;hour timeframe. In this system description paper we introduce Birbal our Mistral45;7B based winning model fine45;tuned on a single RTX 4090 for 16 hours. Birbals success lies in curating high45;quality instructions covering diverse tasks resulting in a 3537; performance improvement over second45;best Qwen45;14B based submission.
