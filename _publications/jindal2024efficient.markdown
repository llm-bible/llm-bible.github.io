---
layout: publication
title: Birbal&#58; An Efficient 7B Instruct-model Fine-tuned With Curated Datasets
authors: Jindal Ashvini Kumar, Rajpoot Pawan Kumar, Parikh Ankur
conference: "Arxiv"
year: 2024
bibkey: jindal2024efficient
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.02247"}
tags: ['Efficiency And Optimization', 'Ethics And Bias', 'Fine Tuning', 'Pretraining Methods', 'Training Techniques']
---
LLMOps incur significant costs due to hardware requirements hindering their widespread accessibility. Additionally a lack of transparency in model training methods and data contributes to the majority of models being non-reproducible. To tackle these challenges the LLM Efficiency Challenge was introduced at NeurIPS Workshop aiming to adapt foundation models on a diverse set of tasks via fine-tuning on a single GPU (RTX 4090 or A100 with 40GB) within a 24-hour timeframe. In this system description paper we introduce Birbal our Mistral-7B based winning model fine-tuned on a single RTX 4090 for 16 hours. Birbals success lies in curating high-quality instructions covering diverse tasks resulting in a 3537; performance improvement over second-best Qwen-14B based submission.
