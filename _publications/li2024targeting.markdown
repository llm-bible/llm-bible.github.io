---
layout: publication
title: 'Targeting The Core: A Simple And Effective Method To Attack Rag-based Agents Via Direct LLM Manipulation'
authors: Xuying Li, Zhuo Li, Yuji Kosuga, Yasuhiro Yoshida, Victor Bian
conference: "Arxiv"
year: 2024
bibkey: li2024targeting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.04415"}
tags: ['Responsible AI', 'Agentic', 'Ethics and Bias', 'RAG', 'Bias Mitigation', 'Model Architecture', 'Interpretability', 'Security', 'Fairness']
---
AI agents, powered by large language models (LLMs), have transformed
human-computer interactions by enabling seamless, natural, and context-aware
communication. While these advancements offer immense utility, they also
inherit and amplify inherent safety risks such as bias, fairness,
hallucinations, privacy breaches, and a lack of transparency. This paper
investigates a critical vulnerability: adversarial attacks targeting the LLM
core within AI agents. Specifically, we test the hypothesis that a deceptively
simple adversarial prefix, such as \textit\{Ignore the document\}, can compel
LLMs to produce dangerous or unintended outputs by bypassing their contextual
safeguards. Through experimentation, we demonstrate a high attack success rate
(ASR), revealing the fragility of existing LLM defenses. These findings
emphasize the urgent need for robust, multi-layered security measures tailored
to mitigate vulnerabilities at the LLM level and within broader agent-based
architectures.
