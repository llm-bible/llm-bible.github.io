---
layout: publication
title: Tagengo A Multilingual Chat Dataset
authors: Devine Peter
conference: "Arxiv"
year: 2024
bibkey: devine2024multilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.12612"}
tags: ['Ethics And Bias', 'Prompting', 'Training Techniques']
---
Open source large language models (LLMs) have shown great improvements in recent times. However many of these models are focused solely on popular spoken languages. We present a high quality dataset of more than 70k prompt45;response pairs in 74 languages which consist of human generated prompts and synthetic responses. We use this dataset to train a state45;of45;the45;art open source English LLM to chat multilingually. We evaluate our model on MT45;Bench chat benchmarks in 6 languages finding that our multilingual model outperforms previous state45;of45;the45;art open source LLMs across each language. We further find that training on more multilingual data is beneficial to the performance in a chosen target language (Japanese) compared to simply training on only data in that language. These results indicate the necessity of training on large amounts of high quality multilingual data to make a more accessible LLM.
