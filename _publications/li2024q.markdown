---
layout: publication
title: Q45;adapter Training Your LLM Adapter As A Residual Q45;function
authors: Li Yi-chen, Zhang Fuxiang, Qiu Wenjie, Yuan Lei, Jia Chengxing, Zhang Zongzhang, Yu Yang
conference: "Arxiv"
year: 2024
bibkey: li2024q
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.03856"}
tags: ['Agentic', 'Reinforcement Learning', 'Responsible AI', 'Training Techniques']
---
We consider the problem of adapting Large Language Models (LLMs) pre45;trained with Reinforcement Learning from Human Feedback (RLHF) to downstream preference data. Naive approaches to achieve this could be supervised fine45;tuning on preferred responses or reinforcement learning with a learned reward model. However the LLM runs the risk of forgetting its initial knowledge as the fine45;tuning progresses. To customize the LLM while preserving its existing capabilities this paper proposes a novel method named as Q45;Adapter. We start by formalizing LLM adaptation as a problem of maximizing the linear combination of two rewards one of which corresponds to the reward optimized by the pre45;trained LLM and the other to the downstream preference data. Although both rewards are unknown we show that this can be solved by directly learning a new module from the preference data that approximates the emph123;residual Q45;function125;. We consider this module to be an adapter because the original pre45;trained LLM together with it can form the optimal customised LLM. Empirically experiments on a range of domain45;specific tasks and safety alignment tasks illustrate the superiority of Q45;Adapter in both anti45;forgetting and learning from new preferences.
