---
layout: publication
title: Humaneval45;xl A Multilingual Code Generation Benchmark For Cross45;lingual Natural Language Generalization
authors: Peng Qiwei, Chai Yekun, Li Xuhong
conference: "Arxiv"
year: 2024
bibkey: peng2024humaneval
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.16694"}
  - {name: "Code", url: "https://github.com/FloatAI/humaneval&#45;xl&#125;"}
tags: ['Applications', 'Has Code', 'Prompting', 'RAG', 'Reinforcement Learning', 'Tools']
---
Large language models (LLMs) have made significant progress in generating codes from textual prompts. However existing benchmarks have mainly concentrated on translating English prompts to multilingual codes or have been constrained to very limited natural languages (NLs). These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code leaving a critical gap in the evaluation of multilingual LLMs. In response we introduce HumanEval45;XL a massively multilingual code generation benchmark specifically crafted to address this deficiency. HumanEval45;XL establishes connections between 23 NLs and 12 programming languages (PLs) and comprises of a collection of 22080 prompts with an average of 8.33 test cases. By ensuring parallel data across multiple NLs and PLs HumanEval45;XL offers a comprehensive evaluation platform for multilingual LLMs allowing the assessment of the understanding of different NLs. Our work serves as a pioneering step towards filling the void in evaluating NL generalization in the area of multilingual code generation. We make our evaluation code and data publicly available at url123;https://github.com/FloatAI/humaneval&#45;xl&#125;.
