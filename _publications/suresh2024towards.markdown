---
layout: publication
title: Towards Smaller Faster Decoder45;only Transformers Architectural Variants And Their Implications
authors: Suresh Sathya Krishnan, P Shunmugapriya
conference: "Arxiv"
year: 2024
bibkey: suresh2024towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.14462"}
tags: ['Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
In recent times the research on Large Language Models (LLMs) has grown exponentially predominantly focusing on models underpinned by the transformer architecture as established by 1 and further developed through the decoder45;only variations by 2. Contemporary efforts in this field primarily aim to enhance model capabilities by scaling up both the architecture and data volumes utilized during training. However the exploration into reduce these model sizes while preserving their efficacy remains scant. In this study we introduce three modifications to the decoder45;only transformer architecture namely ParallelGPT (pgpt) LinearGPT (lgpt) and ConvGPT (cgpt). These variants demonstrate comparable performance to the conventional architecture in language generation yet benefit from reduced model sizes and faster training processes. We open45;source the model weights and the complete codebase for these implementation for further research.
