---
layout: publication
title: Advancing Large Multi45;modal Models With Explicit Chain45;of45;reasoning And Visual Question Generation
authors: Uehara Kohei, Goswami Nabarun, Wang Hanqin, Baba Toshiaki, Tanaka Kohtaro, Hashimoto Tomohiro, Wang Kai, Ito Rei, Naoya Takagi, Umagami Ryo, Wen Yingyi, Anakewat Tanachai, Harada Tatsuya
conference: "Arxiv"
year: 2024
bibkey: uehara2024advancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.10005"}
tags: ['Applications', 'Security', 'Training Techniques']
---
The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of large Vision45;and45;Language Models (VLMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to develop a VLM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge thereby enhancing the robustness and explicability of the reasoning process. To this end we developed a novel dataset generated by a Large Language Model (LLM) designed to promote chain45;of45;thought reasoning combined with a question45;asking mechanism. The dataset covers a range of tasks from common ones like caption generation to specialized VQA tasks that require expert knowledge. Furthermore using the dataset we created we fine45;tuned an existing VLM. This training enabled the models to generate questions and perform iterative reasoning during inference. The results demonstrated a stride toward a more robust accurate and interpretable VLM capable of reasoning explicitly and seeking information proactively when confronted with ambiguous visual input.
