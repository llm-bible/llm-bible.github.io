---
layout: publication
title: 'Llms Meet Multimodal Generation And Editing: A Survey'
authors: Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, Jifeng Dai, Yong Zhang, Wei Xue, Qifeng Liu, Yike Guo, Qifeng Chen
conference: "Arxiv"
year: 2024
bibkey: he2024llms
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.19334"}
  - {name: "Code", url: "https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation"}
tags: ['Responsible AI', 'Agentic', 'Survey Paper', 'Applications', 'RAG', 'Reinforcement Learning', 'Merging', 'Has Code', 'Multimodal Models']
---
With the recent advancement in large language models (LLMs), there is a
growing interest in combining LLMs with multimodal learning. Previous surveys
of multimodal large language models (MLLMs) mainly focus on multimodal
understanding. This survey elaborates on multimodal generation and editing
across various domains, comprising image, video, 3D, and audio. Specifically,
we summarize the notable advancements with milestone works in these fields and
categorize these studies into LLM-based and CLIP/T5-based methods. Then, we
summarize the various roles of LLMs in multimodal generation and exhaustively
investigate the critical technical components behind these methods and the
multimodal datasets utilized in these studies. Additionally, we dig into
tool-augmented multimodal agents that can leverage existing generative models
for human-computer interaction. Lastly, we discuss the advancements in the
generative AI safety field, investigate emerging applications, and discuss
future prospects. Our work provides a systematic and insightful overview of
multimodal generation and processing, which is expected to advance the
development of Artificial Intelligence for Generative Content (AIGC) and world
models. A curated list of all related papers can be found at
https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation
