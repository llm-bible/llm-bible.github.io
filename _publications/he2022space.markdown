---
layout: publication
title: SPACE45;3 Unified Dialog Model Pre45;training For Task45;oriented Dialog Understanding And Generation
authors: He Wanwei, Dai Yinpei, Yang Min, Sun Jian, Huang Fei, Si Luo, Li Yongbin
conference: "Arxiv"
year: 2022
bibkey: he2022space
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2209.06664"}
tags: ['Efficiency And Optimization', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Recently pre45;training methods have shown remarkable success in task45;oriented dialog (TOD) systems. However most existing pre45;trained models for TOD focus on either dialog understanding or dialog generation but not both. In this paper we propose SPACE45;3 a novel unified semi45;supervised pre45;trained conversation model learning from large45;scale dialog corpora with limited annotations which can be effectively fine45;tuned on a wide range of downstream dialog tasks. Specifically SPACE45;3 consists of four successive components in a single transformer to maintain a task45;flow in TOD systems (i) a dialog encoding module to encode dialog history (ii) a dialog understanding module to extract semantic vectors from either user queries or system responses (iii) a dialog policy module to generate a policy vector that contains high45;level semantics of the response and (iv) a dialog generation module to produce appropriate responses. We design a dedicated pre45;training objective for each component. Concretely we pre45;train the dialog encoding module with span mask language modeling to learn contextualized dialog information. To capture the structured dialog semantics we pre45;train the dialog understanding module via a novel tree45;induced semi45;supervised contrastive learning objective with the help of extra dialog annotations. In addition we pre45;train the dialog policy module by minimizing the L2 distance between its output policy vector and the semantic vector of the response for policy optimization. Finally the dialog generation model is pre45;trained by language modeling. Results show that SPACE45;3 achieves state45;of45;the45;art performance on eight downstream dialog benchmarks including intent prediction dialog state tracking and end45;to45;end dialog modeling. We also show that SPACE45;3 has a stronger few45;shot ability than existing models under the low45;resource setting.
