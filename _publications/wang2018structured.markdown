---
layout: publication
title: Structured Triplet Learning With Pos45;tag Guided Attention For Visual Question Answering
authors: Wang Zhe, Liu Xiaoyi, Chen Liangjian, Wang Limin, Qiao Yu, Xie Xiaohui, Fowlkes Charless
conference: "Arxiv"
year: 2018
bibkey: wang2018structured
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1801.07853"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture']
---
Visual question answering (VQA) is of significant interest due to its potential to be a strong test of image understanding systems and to probe the connection between language and vision. Despite much recent progress general VQA is far from a solved problem. In this paper we focus on the VQA multiple45;choice task and provide some good practices for designing an effective VQA model that can capture language45;vision interactions and perform joint reasoning. We explore mechanisms of incorporating part45;of45;speech (POS) tag guided attention convolutional n45;grams triplet attention interactions between the image question and candidate answer and structured learning for triplets based on image45;question pairs. We evaluate our models on two popular datasets Visual7W and VQA Real Multiple Choice. Our final model achieves the state45;of45;the45;art performance of 68.237; on Visual7W and a very competitive performance of 69.637; on the test45;standard split of VQA Real Multiple Choice.
