---
layout: publication
title: Do Language Models Enjoy Their Own Stories Prompting Large Language Models For Automatic Story Evaluation
authors: Chhun Cyril, Suchanek Fabian M., Clavel Chlo√©
conference: "Arxiv"
year: 2024
bibkey: chhun2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.13769"}
tags: ['Interpretability And Explainability', 'Prompting']
---
Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways but they are challenging tasks which require high45;level human abilities such as creativity reasoning and deep understanding. Meanwhile Large Language Models (LLM) now achieve state45;of45;the45;art performance on many NLP tasks. In this paper we study whether LLMs can be used as substitutes for human annotators for ASE. We perform an extensive analysis of the correlations between LLM ratings other automatic measures and human annotations and we explore the influence of prompting on the results and the explainability of LLM behaviour. Most notably we find that LLMs outperform current automatic measures for system45;level evaluation but still struggle at providing satisfactory explanations for their answers.
