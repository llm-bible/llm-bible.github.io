---
layout: publication
title: 'Mitigating Training Imbalance In LLM Fine-tuning Via Selective Parameter Merging'
authors: Yiming Ju, Ziyi Ni, Xingrun Xing, Zhixiong Zeng, Hanyu Zhao, Siqi Fan, Zheng Zhang
conference: "Arxiv"
year: 2024
bibkey: ju2024mitigating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.03743"}
tags: ['Fine-Tuning', 'RAG', 'Merging', 'Training Techniques', 'Pretraining Methods']
---
Supervised fine-tuning (SFT) is crucial for adapting Large Language Models
(LLMs) to specific tasks. In this work, we demonstrate that the order of
training data can lead to significant training imbalances, potentially
resulting in performance degradation. Consequently, we propose to mitigate this
imbalance by merging SFT models fine-tuned with different data orders, thereby
enhancing the overall effectiveness of SFT. Additionally, we introduce a novel
technique, "parameter-selection merging," which outperforms traditional
weighted-average methods on five datasets. Further, through analysis and
ablation studies, we validate the effectiveness of our method and identify the
sources of performance improvements.
