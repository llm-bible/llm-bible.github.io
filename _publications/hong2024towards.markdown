---
layout: publication
title: "Towards Building A Robust Knowledge Intensive Question Answering Model With Large Language Models"
authors: Hong Hong Xingyun, Shao Shao Yan, Wang Wang Zhilin, Duan Duan Manni, Xiongnan Jin
conference: "Arxiv"
year: 2024
bibkey: hong2024towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.05385"}
tags: ['Applications', 'Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Security', 'Training Techniques']
---
The development of LLMs has greatly enhanced the intelligence and fluency of question answering while the emergence of retrieval enhancement has enabled models to better utilize external information. However the presence of noise and errors in retrieved information poses challenges to the robustness of LLMs. In this work to evaluate the models performance under multiple interferences we first construct a dataset based on machine reading comprehension datasets simulating various scenarios including critical information absence noise and conflicts. To address the issue of model accuracy decline caused by noisy external information we propose a data augmentation-based fine-tuning method to enhance LLMs robustness against noise. Additionally contrastive learning approach is utilized to preserve the models discrimination capability of external information. We have conducted experiments on both existing LLMs and our approach the results are evaluated by GPT-4 which indicates that our proposed methods improve model robustness while strengthening the models discrimination capability.
