---
layout: publication
title: 'Autonomous Multi-objective Optimization Using Large Language Model'
authors: Yuxiao Huang, Shenghao Wu, Wenjie Zhang, Jibin Wu, Liang Feng, Kay Chen Tan
conference: "Arxiv"
year: 2024
bibkey: huang2024autonomous
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.08987"}
tags: ['Fine-Tuning', 'Tools', 'Efficiency and Optimization', 'Applications', 'RAG', 'Reinforcement Learning', 'Security', 'Training Techniques', 'Pretraining Methods', 'Prompting']
---
Multi-objective optimization problems (MOPs) are ubiquitous in real-world
applications, presenting a complex challenge of balancing multiple conflicting
objectives. Traditional evolutionary algorithms (EAs), though effective, often
rely on domain-specific expertise and iterative fine-tuning, hindering
adaptability to unseen MOPs. In recent years, the advent of Large Language
Models (LLMs) has revolutionized software engineering by enabling the
autonomous generation and refinement of programs. Leveraging this breakthrough,
we propose a new LLM-based framework that autonomously designs EA operators for
solving MOPs. The proposed framework includes a robust testing module to refine
the generated EA operator through error-driven dialogue with LLMs, a dynamic
selection strategy along with informative prompting-based crossover and
mutation to fit textual optimization pipeline. Our approach facilitates the
design of EA operators without the extensive demands for expert intervention,
thereby speeding up the innovation of EA operators. Empirical studies across
various MOP categories validate the robustness and superior performance of our
proposed framework.
