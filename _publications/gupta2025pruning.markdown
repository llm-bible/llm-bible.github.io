---
layout: publication
title: 'Pruning As A Defense: Reducing Memorization In Large Language Models'
authors: Mansi Gupta, Nikhar Waghela, Sarthak Gupta, Shourya Goel, Sanjif Shanmugavelu
conference: "Arxiv"
year: 2025
bibkey: gupta2025pruning
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.15796'}
tags: ['Efficiency and Optimization', 'Security', 'Training Techniques', 'Prompting', 'Pruning']
---
Large language models have been shown to memorize significant portions of
their training data, which they can reproduce when appropriately prompted. This
work investigates the impact of simple pruning techniques on this behavior. Our
findings reveal that pruning effectively reduces the extent of memorization in
LLMs, demonstrating its potential as a foundational approach for mitigating
membership inference attacks.
