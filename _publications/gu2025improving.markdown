---
layout: publication
title: 'Improving Adversarial Transferability In Mllms Via Dynamic Vision-language Alignment Attack'
authors: Chenhe Gu, Jindong Gu, Andong Hua, Yao Qin
conference: "Arxiv"
year: 2025
bibkey: gu2025improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.19672"}
tags: ['Security', 'Model Architecture', 'Multimodal Models', 'GPT', 'Attention Mechanism']
---
Multimodal Large Language Models (MLLMs), built upon LLMs, have recently
gained attention for their capabilities in image recognition and understanding.
However, while MLLMs are vulnerable to adversarial attacks, the transferability
of these attacks across different models remains limited, especially under
targeted attack setting. Existing methods primarily focus on vision-specific
perturbations but struggle with the complex nature of vision-language modality
alignment. In this work, we introduce the Dynamic Vision-Language Alignment
(DynVLA) Attack, a novel approach that injects dynamic perturbations into the
vision-language connector to enhance generalization across diverse
vision-language alignment of different models. Our experimental results show
that DynVLA significantly improves the transferability of adversarial examples
across various MLLMs, including BLIP2, InstructBLIP, MiniGPT4, LLaVA, and
closed-source models such as Gemini.
