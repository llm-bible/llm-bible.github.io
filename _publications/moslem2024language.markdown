---
layout: publication
title: Language Modelling Approaches To Adaptive Machine Translation
authors: Moslem Yasmin
conference: "Arxiv"
year: 2024
bibkey: moslem2024language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.14559"}
tags: ['Applications', 'Efficiency And Optimization', 'Fine Tuning', 'Language Modeling', 'Reinforcement Learning']
---
Consistency is a key requirement of high45;quality translation. It is especially important to adhere to pre45;approved terminology and adapt to corrected translations in domain45;specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However in45;domain data scarcity is common in translation settings due to the lack of specialised datasets and terminology or inconsistency and inaccuracy of available in45;domain translations. In such scenarios where there is insufficient in45;domain data to fine45;tune MT models producing translations that are consistent with the relevant context is challenging. While real45;time adaptation can make use of smaller amounts of in45;domain data to improve the translation on the fly it remains challenging due to supported context limitations and efficiency constraints. Large language models (LLMs) have recently shown interesting capabilities of in45;context learning where they learn to replicate certain input45;output text generation patterns without further fine45;tuning. Such capabilities have opened new horizons for domain45;specific data augmentation and real45;time adaptive MT. This work attempts to address two main relevant questions 1) in scenarios involving human interaction and continuous feedback can we employ language models to improve the quality of adaptive MT at inference time and 2) in the absence of sufficient in45;domain data can we use pre45;trained large45;scale language models to improve the process of MT domain adaptation
