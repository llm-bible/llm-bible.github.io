---
layout: publication
title: 'Rankers, Judges, And Assistants: Towards Understanding The Interplay Of Llms In Information Retrieval Evaluation'
authors: Krisztian Balog, Donald Metzler, Zhen Qin
conference: "Arxiv"
year: 2025
bibkey: balog2025towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.19092"}
tags: ['Ethics and Bias', 'Applications']
---
Large language models (LLMs) are increasingly integral to information
retrieval (IR), powering ranking, evaluation, and AI-assisted content creation.
This widespread adoption necessitates a critical examination of potential
biases arising from the interplay between these LLM-based components. This
paper synthesizes existing research and presents novel experiment designs that
explore how LLM-based rankers and assistants influence LLM-based judges. We
provide the first empirical evidence of LLM judges exhibiting significant bias
towards LLM-based rankers. Furthermore, we observe limitations in LLM judges'
ability to discern subtle system performance differences. Contrary to some
previous findings, our preliminary study does not find evidence of bias against
AI-generated content. These results highlight the need for a more holistic view
of the LLM-driven information ecosystem. To this end, we offer initial
guidelines and a research agenda to ensure the reliable use of LLMs in IR
evaluation.
