---
layout: publication
title: Cogvideo Large45;scale Pretraining For Text45;to45;video Generation Via Transformers
authors: Hong Wenyi, Ding Ming, Zheng Wendi, Liu Xinghan, Tang Jie
conference: "Arxiv"
year: 2022
bibkey: hong2022large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2205.15868"}
tags: ['GPT', 'Interpretability And Explainability', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Large45;scale pretrained transformers have created milestones in text (GPT45;3) and text45;to45;image (DALL45;E and CogView) generation. Its application to video generation is still facing many challenges The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text45;video datasets hinder the model understanding complex movement semantics. In this work we present 9B45;parameter transformer CogVideo trained by inheriting a pretrained text45;to45;image model CogView2. We also propose multi45;frame45;rate hierarchical training strategy to better align text and video clips. As (probably) the first open45;source large45;scale pretrained text45;to45;video model CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.
