---
layout: publication
title: 'Trillion 7B Technical Report'
authors: Sungjun Trillion Labs Han, Juyoung Trillion Labs Suk, Suyeong Trillion Labs An, Hyungguk Trillion Labs Kim, Kyuseok Trillion Labs Kim, Wonsuk Trillion Labs Yang, Seungtaek Trillion Labs Choi, Jamin Trillion Labs Shin
conference: "Arxiv"
year: 2025
bibkey: han2025trillion
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.15431'}
tags: ['Attention Mechanism', 'Training Techniques', 'Model Architecture']
---
We introduce Trillion-7B, the most token-efficient Korean-centric
multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)
mechanism enables highly efficient and effective knowledge transfer from
English to target languages like Korean and Japanese. Combined with optimized
data mixtures, language-specific filtering, and tailored tokenizer
construction, Trillion-7B achieves competitive performance while dedicating
only 10% of its 2T training tokens to multilingual data and requiring just
59.4K H100 GPU hours (\$148K) for full training. Comprehensive evaluations
across 27 benchmarks in four languages demonstrate Trillion-7B's robust
multilingual performance and exceptional cross-lingual consistency.
