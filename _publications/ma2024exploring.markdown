---
layout: publication
title: Exploring The Role Of Transliteration In In45;context Learning For Low45;resource Languages Written In Non45;latin Scripts
authors: Ma Chunlan, Liu Yihong, Ye Haotian, Sch√ºtze Hinrich
conference: "Arxiv"
year: 2024
bibkey: ma2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.02320"}
tags: ['Prompting', 'RAG']
---
Decoder45;only large language models (LLMs) excel in high45;resource languages across various tasks through few45;shot or even zero45;shot in45;context learning (ICL). However their performance often does not transfer well to low45;resource languages especially those written in non45;Latin scripts. Inspired by recent work that leverages transliteration in encoder45;only models we investigate whether transliteration is also effective in improving LLMs performance for low45;resource languages written in non45;Latin scripts. To this end we propose three prompt templates where the target45;language text is represented in (1) its original script (2) Latin script or (3) both. We apply these methods to several representative LLMs of different sizes on various tasks including text classification and sequential labeling. Our findings show that the effectiveness of transliteration varies by task type and model size. For instance all models benefit from transliterations for sequential labeling (with increases of up to 2537;).
