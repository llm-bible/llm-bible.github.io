---
layout: publication
title: Protllm An Interleaved Protein45;language LLM With Protein45;as45;word Pre45;training
authors: Zhuo Le, Chi Zewen, Xu Minghao, Huang Heyan, Zheng Heqi, He Conghui, Mao Xian-ling, Zhang Wentao
conference: "Arxiv"
year: 2024
bibkey: zhuo2024interleaved
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.07920"}
tags: ['Applications', 'Language Modeling', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
We propose ProtLLM a versatile cross45;modal large language model (LLM) for both protein45;centric and protein45;language tasks. ProtLLM features a unique dynamic protein mounting mechanism enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides we propose the protein45;as45;word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally we construct a large45;scale interleaved protein45;text dataset named InterPT for pre45;training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic supervised protein45;centric tasks and explore its novel protein45;language applications. Experimental results demonstrate that ProtLLM not only achieves superior performance against protein45;specialized baselines on protein45;centric tasks but also induces zero45;shot and in45;context learning capabilities on protein45;language tasks.
