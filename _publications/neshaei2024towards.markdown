---
layout: publication
title: Towards Modeling Learner Performance With Large Language Models
authors: Neshaei Seyed Parsa, Davis Richard Lee, Hazimeh Adam, Lazarevski Bojan, Dillenbourg Pierre, KÃ¤ser Tanja
conference: "Arxiv"
year: 2024
bibkey: neshaei2024towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.14661"}
tags: ['Pretraining Methods', 'Prompting', 'Reinforcement Learning']
---
Recent work exploring the capabilities of pre45;trained large language models (LLMs) has demonstrated their ability to act as general pattern machines by completing complex token sequences representing a wide array of tasks including time45;series prediction and robot control. This paper investigates whether the pattern recognition and sequence modeling capabilities of LLMs can be extended to the domain of knowledge tracing a critical component in the development of intelligent tutoring systems (ITSs) that tailor educational experiences by predicting learner performance over time. In an empirical evaluation across multiple real45;world datasets we compare two approaches to using LLMs for this task zero45;shot prompting and model fine45;tuning with existing non45;LLM approaches to knowledge tracing. While LLM45;based approaches do not achieve state45;of45;the45;art performance fine45;tuned LLMs surpass the performance of naive baseline models and perform on par with standard Bayesian Knowledge Tracing approaches across multiple metrics. These findings suggest that the pattern recognition capabilities of LLMs can be used to model complex learning trajectories opening a novel avenue for applying LLMs to educational contexts. The paper concludes with a discussion of the implications of these findings for future research suggesting that further refinements and a deeper understanding of LLMs predictive mechanisms could lead to enhanced performance in knowledge tracing tasks.
