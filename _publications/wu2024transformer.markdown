---
layout: publication
title: Transformer45;based Causal Language Models Perform Clustering
authors: Wu Xinbo, Varshney Lav R.
conference: "Arxiv"
year: 2024
bibkey: wu2024transformer
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.12151"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction45;following capability via additional training for instruction45;following tasks. However the mechanisms responsible for effective instruction45;following capabilities remain inadequately understood. Here we introduce a simplified instruction45;following task and use synthetic datasets to analyze a Transformer45;based causal language model. Our findings suggest that the model learns task45;specific information by clustering data within its hidden space with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting. Furthermore we present inspired applications regarding pre45;training and alignment.
