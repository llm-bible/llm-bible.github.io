---
layout: publication
title: Improving The Faithfulness Of Attention45;based Explanations With Task45;specific Information For Text Classification
authors: Chrysostomou George, Aletras Nikolaos
conference: "Arxiv"
year: 2021
bibkey: chrysostomou2021improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2105.02657"}
tags: ['Attention Mechanism', 'Interpretability And Explainability', 'Model Architecture', 'Transformer']
---
Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to improve performance in various tasks while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace 2019; Serrano and Smith 2019; Wiegreffe and Pinter 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg 2020) across encoders and tasks. In this paper we seek to improve the faithfulness of attention45;based explanations for text classification. We achieve this by proposing a new family of Task45;Scaling (TaSc) mechanisms that learn task45;specific non45;contextualised information to scale the original attention weights. Evaluation tests for explanation faithfulness show that the three proposed variants of TaSc improve attention45;based explanations across two attention mechanisms five encoders and five text classification datasets without sacrificing predictive performance. Finally we demonstrate that TaSc consistently provides more faithful attention45;based explanations compared to three widely45;used interpretability techniques.
