---
layout: publication
title: 'Implicit In-context Learning: Evidence From Artificial Language Experiments'
authors: Xiaomeng Ma, Qihui Xu
conference: "Arxiv"
year: 2025
bibkey: ma2025implicit
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.24190"}
tags: ['GPT', 'Prompting', 'In-Context Learning', 'Model Architecture']
---
Humans acquire language through implicit learning, absorbing complex patterns
without explicit awareness. While LLMs demonstrate impressive linguistic
capabilities, it remains unclear whether they exhibit human-like pattern
recognition during in-context learning at inferencing level. We adapted three
classic artificial language learning experiments spanning morphology,
morphosyntax, and syntax to systematically evaluate implicit learning at
inferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini.
Our results reveal linguistic domain-specific alignment between models and
human behaviors, o3-mini aligns better in morphology while both models align in
syntax.
