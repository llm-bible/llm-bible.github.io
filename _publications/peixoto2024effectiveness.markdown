---
layout: publication
title: 'On The Effectiveness Of Llms For Manual Test Verifications'
authors: Myron David Lucena Campos Peixoto, Davy De Medeiros Baia, Nathalia Nascimento, Paulo Alencar, Baldoino Fonseca, MÃ¡rcio Ribeiro
conference: "Arxiv"
year: 2024
bibkey: peixoto2024effectiveness
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2409.12405'}
tags: ['Reinforcement Learning', 'GPT', 'Fine-Tuning', 'Model Architecture']
---
Background: Manual testing is vital for detecting issues missed by automated
tests, but specifying accurate verifications is challenging. Aims: This study
aims to explore the use of Large Language Models (LLMs) to produce
verifications for manual tests. Method: We conducted two independent and
complementary exploratory studies. The first study involved using 2
closed-source and 6 open-source LLMs to generate verifications for manual test
steps and evaluate their similarity to original verifications. The second study
involved recruiting software testing professionals to assess their perception
and agreement with the generated verifications compared to the original ones.
Results: The open-source models Mistral-7B and Phi-3-mini-4k demonstrated
effectiveness and consistency comparable to closed-source models like
Gemini-1.5-flash and GPT-3.5-turbo in generating manual test verifications.
However, the agreement level among professional testers was slightly above 40%,
indicating both promise and room for improvement. While some LLM-generated
verifications were considered better than the originals, there were also
concerns about AI hallucinations, where verifications significantly deviated
from expectations. Conclusion: We contributed by generating a dataset of 37,040
test verifications using 8 different LLMs. Although the models show potential,
the relatively modest 40% agreement level highlights the need for further
refinement. Enhancing the accuracy, relevance, and clarity of the generated
verifications is crucial to ensure greater reliability in real-world testing
scenarios.
