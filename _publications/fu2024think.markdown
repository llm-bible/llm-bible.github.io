---
layout: publication
title: 'Think Or Remember? Detecting And Directing Llms Towards Memorization Or Generalization'
authors: Yi-fu Fu, Yu-chieh Tu, Tzu-ling Cheng, Cheng-yu Lin, Yi-ting Yang, Heng-yi Liu, Keng-te Liao, Da-cheng Juan, Shou-de Lin
conference: "Arxiv"
year: 2024
bibkey: fu2024think
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.18497"}
tags: ['RAG', 'Training Techniques']
---
In this paper, we explore the foundational mechanisms of memorization and
generalization in Large Language Models (LLMs), inspired by the functional
specialization observed in the human brain. Our investigation serves as a case
study leveraging specially designed datasets and experimental-scale LLMs to lay
the groundwork for understanding these behaviors. Specifically, we aim to first
enable LLMs to exhibit both memorization and generalization by training with
the designed dataset, then (a) examine whether LLMs exhibit neuron-level
spatial differentiation for memorization and generalization, (b) predict these
behaviors using model internal representations, and (c) steer the behaviors
through inference-time interventions. Our findings reveal that neuron-wise
differentiation of memorization and generalization is observable in LLMs, and
targeted interventions can successfully direct their behavior.
