---
layout: publication
title: Diversedialogue A Methodology For Designing Chatbots With Human-like Diversity
authors: Lin Xiaoyu, Yu Xinkai, Aich Ankit, Giorgi Salvatore, Ungar Lyle
conference: "Arxiv"
year: 2024
bibkey: lin2024methodology
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.00262"}
tags: ['Applications', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG']
---
Large Language Models (LLMs) which simulate human users are frequently employed to evaluate chatbots in applications such as tutoring and customer service. Effective evaluation necessitates a high degree of human-like diversity within these simulations. In this paper we demonstrate that conversations generated by GPT-4o mini when used as simulated human participants systematically differ from those between actual humans across multiple linguistic features. These features include topic variation lexical attributes and both the average behavior and diversity (variance) of the language used. To address these discrepancies we propose an approach that automatically generates prompts for user simulations by incorporating features derived from real human interactions such as age gender emotional tone and the topics discussed. We assess our approach using differential language analysis combined with deep linguistic inquiry. Our method of prompt optimization tailored to target specific linguistic features shows significant improvements. Specifically it enhances the human-likeness of LLM chatbot conversations increasing their linguistic diversity. On average we observe a 54 percent reduction in the error of average features between human and LLM-generated conversations. This method of constructing chatbot sets with human-like diversity holds great potential for enhancing the evaluation process of user-facing bots.
