---
layout: publication
title: "E-bench: Towards Evaluating The Ease-of-use Of Large Language Models"
authors: Zhang Zhenyu, Hao Bingguang, Li Jinpeng, Zhang Zekai, Zhao Dongyan
conference: "Arxiv"
year: 2024
bibkey: zhang2024e
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.10950"}
tags: ['Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'TACL']
---
Most large language models (LLMs) are sensitive to prompts and another synonymous expression or a typo may lead to unexpected results for the model. Composing an optimal prompt for a specific demand lacks theoretical support and relies entirely on human experimentation which poses a considerable obstacle to popularizing generative artificial intelligence. However there is no systematic analysis of the stability of LLMs in resisting prompt perturbations in real-world scenarios. In this work we propose to evaluate the ease-of-use of LLMs and construct E-Bench simulating the actual situation of human use from synonymous perturbation (including paraphrasing simplification and colloquialism) and typographical perturbation (such as typing). On this basis we also discuss the combination of these two types of perturbation and analyze the main reasons for performance degradation. Experimental results indicate that with the increase of model size although the ease-of-use are significantly improved there is still a long way to go to build a sufficiently user-friendly model.
