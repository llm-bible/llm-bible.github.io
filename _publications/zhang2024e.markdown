---
layout: publication
title: E45;bench Towards Evaluating The Ease45;of45;use Of Large Language Models
authors: Zhang Zhenyu, Hao Bingguang, Li Jinpeng, Zhang Zekai, Zhao Dongyan
conference: "Arxiv"
year: 2024
bibkey: zhang2024e
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.10950"}
tags: ['Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'TACL']
---
Most large language models (LLMs) are sensitive to prompts and another synonymous expression or a typo may lead to unexpected results for the model. Composing an optimal prompt for a specific demand lacks theoretical support and relies entirely on human experimentation which poses a considerable obstacle to popularizing generative artificial intelligence. However there is no systematic analysis of the stability of LLMs in resisting prompt perturbations in real45;world scenarios. In this work we propose to evaluate the ease45;of45;use of LLMs and construct E45;Bench simulating the actual situation of human use from synonymous perturbation (including paraphrasing simplification and colloquialism) and typographical perturbation (such as typing). On this basis we also discuss the combination of these two types of perturbation and analyze the main reasons for performance degradation. Experimental results indicate that with the increase of model size although the ease45;of45;use are significantly improved there is still a long way to go to build a sufficiently user45;friendly model.
