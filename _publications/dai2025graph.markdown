---
layout: publication
title: 'Graph-based Multimodal Contrastive Learning For Chart Question Answering'
authors: Yue Dai, Soyeon Caren Han, Wei Liu
conference: "Arxiv"
year: 2025
bibkey: dai2025graph
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.04303"}
tags: ['Transformer', 'Tools', 'Applications', 'Model Architecture', 'Reinforcement Learning', 'Pretraining Methods', 'Multimodal Models', 'Prompting']
---
Chart question answering (ChartQA) is challenged by the heterogeneous
composition of chart elements and the subtle data patterns they encode. This
work introduces a novel joint multimodal scene graph framework that explicitly
models the relationships among chart components and their underlying
structures. The framework integrates both visual and textual graphs to capture
structural and semantic characteristics, while a graph contrastive learning
strategy aligns node representations across modalities enabling their seamless
incorporation into a transformer decoder as soft prompts. Moreover, a set of
tailored Chain of Thought (CoT) prompts is proposed to enhance multimodal large
language models (MLLMs) in zero-s ot scenarios by mitigating hallucinations.
Extensive evaluations on benchmarks including ChartQA, OpenCQA, and ChartX
demonstrate significant performance improvements and validate the efficacy of
the proposed approach.
