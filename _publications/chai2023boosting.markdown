---
layout: publication
title: 'Graphllm: Boosting Graph Reasoning Ability Of Large Language Model'
authors: Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang
conference: "Arxiv"
year: 2023
bibkey: chai2023boosting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.05845"}
tags: ['TACL', 'RAG', 'ACL']
---
The advancement of Large Language Models (LLMs) has remarkably pushed the
boundaries towards artificial general intelligence (AGI), with their
exceptional ability on understanding diverse types of information, including
but not limited to images and audio. Despite this progress, a critical gap
remains in empowering LLMs to proficiently understand and reason on graph data.
Recent studies underscore LLMs' underwhelming performance on fundamental graph
reasoning tasks. In this paper, we endeavor to unearth the obstacles that
impede LLMs in graph reasoning, pinpointing the common practice of converting
graphs into natural language descriptions (Graph2Text) as a fundamental
bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering
end-to-end approach that synergistically integrates graph learning models with
LLMs. This synergy equips LLMs with the ability to proficiently interpret and
reason on graph data, harnessing the superior expressive power of graph
learning models. Our empirical evaluations across four fundamental graph
reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a
substantial average accuracy enhancement of 54.44%, alongside a noteworthy
context reduction of 96.45% across various graph reasoning tasks.
