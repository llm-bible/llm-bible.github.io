---
layout: publication
title: Language Features Matter Effective Language Representations For Vision45;language Tasks
authors: Burns Andrea, Tan Reuben, Saenko Kate, Sclaroff Stan, Plummer Bryan A.
conference: "Arxiv"
year: 2019
bibkey: burns2019language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1908.06327"}
tags: ['Applications', 'Attention Mechanism', 'BERT', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
Shouldnt language and vision features be treated equally in vision45;language (VL) tasks Many VL approaches treat the language component as an afterthought using simple language models that are either built upon fixed word embeddings trained on text45;only data or are learned from scratch. We believe that language features deserve more attention and conduct experiments which compare different word embeddings language models and embedding augmentation steps on five common VL tasks image45;sentence retrieval image captioning visual question answering phrase grounding and text45;to45;clip retrieval. Our experiments provide some striking results; an average embedding language model outperforms an LSTM on retrieval45;style tasks; state45;of45;the45;art representations such as BERT perform relatively poorly on vision45;language tasks. From this comprehensive set of experiments we propose a set of best practices for incorporating the language component of VL tasks. To further elevate language features we also show that knowledge in vision45;language problems can be transferred across tasks to gain performance with multi45;task training. This multi45;task training is applied to a new Graph Oriented Vision45;Language Embedding (GrOVLE) which we adapt from Word2Vec using WordNet and an original visual45;language graph built from Visual Genome providing a ready45;to45;use vision45;language embedding http://ai.bu.edu/grovle.
