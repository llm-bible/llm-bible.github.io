---
layout: publication
title: Causal Prompting Debiasing Large Language Model Prompting Based On Front45;door Adjustment
authors: Zhang Congzhi, Zhang Linhai, Wu Jialong, Zhou Deyu, He Yulan
conference: "Arxiv"
year: 2024
bibkey: zhang2024causal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.02738"}
tags: ['Ethics And Bias', 'Pretraining Methods', 'Prompting', 'Training Techniques']
---
Despite the notable advancements of existing prompting methods such as In45;Context Learning and Chain45;of45;Thought for Large Language Models (LLMs) they still face challenges related to various biases. Traditional debiasing methods primarily focus on the model training stage including approaches based on data augmentation and reweighting yet they struggle with the complex biases inherent in LLMs. To address such limitations the causal relationship behind the prompting methods is uncovered using a structural causal model and a novel causal prompting method based on front45;door adjustment is proposed to effectively mitigate LLMs biases. In specific causal intervention is achieved by designing the prompts without accessing the parameters and logits of LLMs. The chain45;of45;thought generated by LLM is employed as the mediator variable and the causal effect between input prompts and output answers is calculated through front45;door adjustment to mitigate model biases. Moreover to accurately represent the chain45;of45;thoughts and estimate the causal effects contrastive learning is used to fine45;tune the encoder of chain45;of45;thought by aligning its space with that of the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance across seven natural language processing datasets on both open45;source and closed45;source LLMs.
