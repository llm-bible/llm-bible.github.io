---
layout: publication
title: 'Fairness In Llm-generated Surveys'
authors: Andr√©s Abeliuk, Vanessa Gaete, Naim Bro
conference: "Arxiv"
year: 2025
bibkey: abeliuk2025fairness
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.15351"}
tags: ['Training Techniques', 'Fairness', 'Survey Paper', 'Tools', 'Language Modeling', 'Bias Mitigation', 'Ethics and Bias', 'Applications']
---
Large Language Models (LLMs) excel in text generation and understanding,
especially in simulating socio-political and economic patterns, serving as an
alternative to traditional surveys. However, their global applicability remains
questionable due to unexplored biases across socio-demographic and geographic
contexts. This study examines how LLMs perform across diverse populations by
analyzing public surveys from Chile and the United States, focusing on
predictive accuracy and fairness metrics. The results show performance
disparities, with LLM consistently outperforming on U.S. datasets. This bias
originates from the U.S.-centric training data, remaining evident after
accounting for socio-demographic differences. In the U.S., political identity
and race significantly influence prediction accuracy, while in Chile, gender,
education, and religious affiliation play more pronounced roles. Our study
presents a novel framework for measuring socio-demographic biases in LLMs,
offering a path toward ensuring fairer and more equitable model performance
across diverse socio-cultural contexts.
