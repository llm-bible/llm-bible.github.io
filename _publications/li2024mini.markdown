---
layout: publication
title: Mini45;gemini Mining The Potential Of Multi45;modality Vision Language Models
authors: Li Yanwei, Zhang Yuechen, Wang Chengyao, Zhong Zhisheng, Chen Yixin, Chu Ruihang, Liu Shaoteng, Jia Jiaya
conference: "Arxiv"
year: 2024
bibkey: li2024mini
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.18814"}
  - {name: "Code", url: "https://github.com/dvlab&#45;research/MiniGemini"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning', 'Tools']
---
In this work we introduce Mini45;Gemini a simple and effective framework enhancing multi45;modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning a performance gap persists compared to advanced models like GPT45;4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any45;to45;any workflow from three aspects i.e. high45;resolution visual tokens high45;quality data and VLM45;guided generation. To enhance visual tokens we propose to utilize an additional visual encoder for high45;resolution refinement without increasing the visual token count. We further construct a high45;quality dataset that promotes precise image comprehension and reasoning45;based generation expanding the operational scope of current VLMs. In general Mini45;Gemini further mines the potential of VLMs and empowers current frameworks with image understanding reasoning and generation simultaneously. Mini45;Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero45;shot benchmarks and even surpasses the developed private models. Code and models are available at https://github.com/dvlab&#45;research/MiniGemini.
