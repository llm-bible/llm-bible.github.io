---
layout: publication
title: 'Plamo-100b: A Ground-up Language Model Designed For Japanese Proficiency'
authors: Preferred Elements, :, Kenshin Abe, Kaizaburo Chubachi, Yasuhiro Fujita, Yuta Hirokawa, Kentaro Imajo, Toshiki Kataoka, Hiroyoshi Komatsu, Hiroaki Mikami, Tsuguo Mogami, Shogo Murai, Kosuke Nakago, Daisuke Nishino, Toru Ogawa, Daisuke Okanohara, Yoshihiko Ozaki, Shotaro Sano, Shuji Suzuki, Tianqi Xu, Toshihiko Yanase
conference: "Arxiv"
year: 2024
bibkey: elements2024plamo
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.07563"}
tags: ['Fine-Tuning', 'Efficiency and Optimization', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods']
---
We introduce PLaMo-100B, a large-scale language model designed for Japanese
proficiency. The model was trained from scratch using 2 trillion tokens, with
architecture such as QK Normalization and Z-Loss to ensure training stability
during the training process. Post-training techniques, including Supervised
Fine-Tuning and Direct Preference Optimization, were applied to refine the
model's performance. Benchmark evaluations suggest that PLaMo-100B performs
well, particularly in Japanese-specific tasks, achieving results that are
competitive with frontier models like GPT-4. The base model is available at
https://huggingface.co/pfnet/plamo-100b.
