---
layout: publication
title: 'Evalita-llm: Benchmarking Large Language Models On Italian'
authors: Bernardo Magnini, Roberto Zanoli, Michele Resta, Martin Cimmino, Paolo Albano, Marco Madeddu, Viviana Patti
conference: "Arxiv"
year: 2025
bibkey: magnini2025evalita
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.02289'}
tags: ['Ethics and Bias', 'Prompting']
---
We describe Evalita-LLM, a new benchmark designed to evaluate Large Language
Models (LLMs) on Italian tasks. The distinguishing and innovative features of
Evalita-LLM are the following: (i) all tasks are native Italian, avoiding
issues of translating from Italian and potential cultural biases; (ii) in
addition to well established multiple-choice tasks, the benchmark includes
generative tasks, enabling more natural interaction with LLMs; (iii) all tasks
are evaluated against multiple prompts, this way mitigating the model
sensitivity to specific prompts and allowing a fairer and objective evaluation.
We propose an iterative methodology, where candidate tasks and candidate
prompts are validated against a set of LLMs used for development. We report
experimental results from the benchmark's development phase, and provide
performance statistics for several state-of-the-art LLMs.
