---
layout: publication
title: Memllm Finetuning Llms To Use An Explicit Read45;write Memory
authors: Modarressi Ali, Köksal Abdullatif, Imani Ayyoob, Fayyaz Mohsen, Schütze Hinrich
conference: "Arxiv"
year: 2024
bibkey: modarressi2024finetuning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.11672"}
tags: ['Interpretability And Explainability', 'Language Modeling', 'RAG']
---
While current large language models (LLMs) demonstrate some capabilities in knowledge45;intensive tasks they are limited by relying on their parameters as an implicit storage mechanism. As a result they struggle with infrequent knowledge and temporal degradation. In addition the uninterpretable nature of parametric memorization makes it challenging to understand and prevent hallucination. Parametric memory pools and model editing are only partial solutions. Retrieval Augmented Generation (RAG) unicode123;x2013125; though non45;parametric unicode123;x2013125; has its own limitations it lacks structure complicates interpretability and makes it hard to effectively manage stored knowledge. In this paper we introduce MemLLM a novel method of enhancing LLMs by integrating a structured and explicit read45;and45;write memory module. MemLLM tackles the aforementioned challenges by enabling dynamic interaction with the memory and improving the LLMs capabilities in using stored knowledge. Our experiments indicate that MemLLM enhances the LLMs performance and interpretability in language modeling in general and knowledge45;intensive tasks in particular. We see MemLLM as an important step towards making LLMs more grounded and factual through memory augmentation.
