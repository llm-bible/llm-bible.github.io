---
layout: publication
title: Token45;level Direct Preference Optimization
authors: Zeng Yongcheng, Liu Guoqing, Ma Weiyu, Yang Ning, Zhang Haifeng, Wang Jun
conference: "Arxiv"
year: 2024
bibkey: zeng2024token
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.11999"}
  - {name: "Code", url: "https://github.com/Vance0124/Token&#45;level&#45;Direct&#45;Preference&#45;Optimization"}
tags: ['Efficiency And Optimization', 'Has Code', 'Reinforcement Learning']
---
Fine45;tuning pre45;trained Large Language Models (LLMs) is essential to align them with human values and intentions. This process often utilizes methods like pairwise comparisons and KL divergence against a reference LLM focusing on the evaluation of full answers generated by the models. However the generation of these responses occurs in a token level following a sequential auto45;regressive fashion. In this paper we introduce Token45;level Direct Preference Optimization (TDPO) a novel approach to align LLMs with human preferences by optimizing policy at the token level. Unlike previous methods which face challenges in divergence efficiency TDPO incorporates forward KL divergence constraints for each token improving alignment and diversity. Utilizing the Bradley45;Terry model for a token45;based reward system TDPO enhances the regulation of KL divergence while preserving simplicity without the need for explicit reward modeling. Experimental results across various text tasks demonstrate TDPOs superior performance in balancing alignment with generation diversity. Notably fine45;tuning with TDPO strikes a better balance than DPO in the controlled sentiment generation and single45;turn dialogue datasets and significantly improves the quality of generated responses compared to both DPO and PPO45;based RLHF methods. Our code is open45;sourced at https://github.com/Vance0124/Token&#45;level&#45;Direct&#45;Preference&#45;Optimization.
