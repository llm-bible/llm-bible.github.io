---
layout: publication
title: 'VITA-1.5: Towards Gpt-4o Level Real-time Vision And Speech Interaction'
authors: Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He
conference: "Arxiv"
year: 2025
bibkey: fu2025vita
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.01957"}
tags: ['GPT', 'Applications', 'Model Architecture', 'Training Techniques', 'Multimodal Models']
---
Recent Multimodal Large Language Models (MLLMs) have typically focused on
integrating visual and textual modalities, with less emphasis placed on the
role of speech in enhancing interaction. However, speech plays a crucial role
in multimodal dialogue systems, and implementing high-performance in both
vision and speech tasks remains a significant challenge due to the fundamental
modality differences. In this paper, we propose a carefully designed
multi-stage training methodology that progressively trains LLM to understand
both visual and speech information, ultimately enabling fluent vision and
speech interaction. Our approach not only preserves strong vision-language
capacity, but also enables efficient speech-to-speech dialogue capabilities
without separate ASR and TTS modules, significantly accelerating multimodal
end-to-end response speed. By comparing our method against state-of-the-art
counterparts across benchmarks for image, video, and speech tasks, we
demonstrate that our model is equipped with both strong visual and speech
capabilities, making near real-time vision and speech interaction.
