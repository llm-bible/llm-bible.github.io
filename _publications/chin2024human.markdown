---
layout: publication
title: Human-Centered LLM-Agent User Interface A Position Paper
authors: Chin Daniel, Wang Yuxuan, Xia Gus
conference: "Arxiv"
year: 2024
bibkey: chin2024human
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.13050"}
tags: ['Agentic', 'Applications', 'Fine Tuning', 'GPT', 'Model Architecture', 'Multimodal Models', 'Prompting', 'Reinforcement Learning', 'Tools']
---
Large Language Model (LLM) -in-the-loop applications have been shown to effectively interpret the human users commands make plans and operate external tools/systems accordingly. Still the operation scope of the LLM agent is limited to passively following the user requiring the user to frame his/her needs with regard to the underlying tools/systems. We note that the potential of an LLM-Agent User Interface (LAUI) is much greater. A user mostly ignorant to the underlying tools/systems should be able to work with a LAUI to discover an emergent workflow. Contrary to the conventional way of designing an explorable GUI to teach the user a predefined set of ways to use the system in the ideal LAUI the LLM agent is initialized to be proficient with the system proactively studies the user and his/her needs and proposes new interaction schemes to the user. To illustrate LAUI we present Flute X GPT a concrete example using an LLM agent a prompt manager and a flute-tutoring multi-modal software-hardware system to facilitate the complex real-time user experience of learning to play the flute.
