---
layout: publication
title: 'How Deep Is Love In Llms'' Hearts? Exploring Semantic Size In Human-like Cognition'
authors: Yao Yao, Yifei Yang, Xinbei Ma, Dongjie Yang, Zhuosheng Zhang, Zuchao Li, Hai Zhao
conference: "Arxiv"
year: 2025
bibkey: yao2025how
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.00330'}
tags: ['Attention Mechanism', 'Training Techniques', 'Model Architecture', 'Reinforcement Learning', 'Ethics and Bias']
---
How human cognitive abilities are formed has long captivated researchers.
However, a significant challenge lies in developing meaningful methods to
measure these complex processes. With the advent of large language models
(LLMs), which now rival human capabilities in various domains, we are presented
with a unique testbed to investigate human cognition through a new lens. Among
the many facets of cognition, one particularly crucial aspect is the concept of
semantic size, the perceived magnitude of both abstract and concrete words or
concepts. This study seeks to investigate whether LLMs exhibit similar
tendencies in understanding semantic size, thereby providing insights into the
underlying mechanisms of human cognition. We begin by exploring metaphorical
reasoning, comparing how LLMs and humans associate abstract words with concrete
objects of varying sizes. Next, we examine LLMs' internal representations to
evaluate their alignment with human cognitive processes. Our findings reveal
that multi-modal training is crucial for LLMs to achieve more human-like
understanding, suggesting that real-world, multi-modal experiences are
similarly vital for human cognitive development. Lastly, we examine whether
LLMs are influenced by attention-grabbing headlines with larger semantic sizes
in a real-world web shopping scenario. The results show that multi-modal LLMs
are more emotionally engaged in decision-making, but this also introduces
potential biases, such as the risk of manipulation through clickbait headlines.
Ultimately, this study offers a novel perspective on how LLMs interpret and
internalize language, from the smallest concrete objects to the most profound
abstract concepts like love. The insights gained not only improve our
understanding of LLMs but also provide new avenues for exploring the cognitive
abilities that define human intelligence.
