---
layout: publication
title: TAB-VCR Tags And Attributes Based Visual Commonsense Reasoning Baselines
authors: Lin Jingxiang, Jain Unnat, Schwing Alexander G.
conference: "Arxiv"
year: 2019
bibkey: lin2019tab
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1910.14671"}
tags: ['Applications', 'Attention Mechanism', 'Efficiency And Optimization', 'Ethics And Bias', 'Model Architecture', 'Pruning', 'RAG', 'Reinforcement Learning']
---
Reasoning is an important ability that we learn from a very early age. Yet reasoning is extremely hard for algorithms. Despite impressive recent progress that has been reported on tasks that necessitate reasoning such as visual question answering and visual dialog models often exploit biases in datasets. To develop models with better reasoning abilities recently the new visual commonsense reasoning (VCR) task has been introduced. Not only do models have to answer questions but also do they have to provide a reason for the given answer. The proposed baseline achieved compelling results leveraging a meticulously designed model composed of LSTM modules and attention nets. Here we show that a much simpler model obtained by ablating and pruning the existing intricate baseline can perform better with half the number of trainable parameters. By associating visual features with attribute information and better text to image grounding we obtain further improvements for our simpler amp; effective baseline TAB-VCR. We show that this approach results in a 5.337; 4.437; and 6.537; absolute improvement over the previous state-of-the-art on question answering answer justification and holistic VCR.
