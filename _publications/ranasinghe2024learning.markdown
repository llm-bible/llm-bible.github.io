---
layout: publication
title: Learning To Localize Objects Improves Spatial Reasoning In Visual45;llms
authors: Ranasinghe Kanchana, Shukla Satya Narayan, Poursaeed Omid, Ryoo Michael S., Lin Tsung-yu
conference: "Arxiv"
year: 2024
bibkey: ranasinghe2024learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.07449"}
tags: ['Applications', 'Reinforcement Learning', 'Tools']
---
Integration of Large Language Models (LLMs) into visual domain tasks resulting in visual45;LLMs (V45;LLMs) has enabled exceptional performance in vision45;language tasks particularly for visual question answering (VQA). However existing V45;LLMs (e.g. BLIP45;2 LLaVA) demonstrate weak spatial reasoning and localization awareness. Despite generating highly descriptive and elaborate textual answers these models fail at simple tasks like distinguishing a left vs right location. In this work we explore how image45;space coordinate based instruction fine45;tuning objectives could inject spatial awareness into V45;LLMs. We discover optimal coordinate representations data45;efficient instruction fine45;tuning objectives and pseudo45;data generation strategies that lead to improved spatial awareness in V45;LLMs. Additionally our resulting model improves VQA across image and video domains reduces undesired hallucination and generates better contextual object descriptions. Experiments across 5 vision45;language tasks involving 14 different datasets establish the clear performance improvements achieved by our proposed framework.
