---
layout: publication
title: Redefining Hallucination in LLMs Towards a psychology-informed framework for mitigating misinformation
authors: Berberette Elijah, Hutchins Jack, Sadovnik Amir
conference: "Arxiv"
year: 2024
bibkey: berberette2024redefining
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.01769"}
tags: ['ARXIV', 'Ethics And Bias', 'LLM', 'Tools']
---
In recent years large language models (LLMs) have become incredibly popular with ChatGPT for example being used by over a billion users. While these models exhibit remarkable language understanding and logical prowess a notable challenge surfaces in the form of hallucinations. This phenomenon results in LLMs outputting misinformation in a confident manner which can lead to devastating consequences with such a large user base. However we question the appropriateness of the term hallucination in LLMs proposing a psychological taxonomy based on cognitive biases and other psychological phenomena. Our approach offers a more fine-grained understanding of this phenomenon allowing for targeted solutions. By leveraging insights from how humans internally resolve similar challenges we aim to develop strategies to mitigate LLM hallucinations. This interdisciplinary approach seeks to move beyond conventional terminology providing a nuanced understanding and actionable pathways for improvement in LLM reliability.
