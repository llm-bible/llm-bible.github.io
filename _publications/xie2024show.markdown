---
layout: publication
title: Show-o One Single Transformer to Unify Multimodal Understanding and Generation
authors: Xie Jinheng, Mao Weijia, Bai Zechen, Zhang David Junhao, Wang Weihao, Lin Kevin Qinghong, Gu Yuchao, Chen Zhijie, Yang Zhenheng, Shou Mike Zheng
conference: "Arxiv"
year: 2024
bibkey: xie2024show
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.12528"}
  - {name: "Code", url: "https://github.com/showlab/Show-o"}
tags: ['GPT', 'Has Code', 'Language Modeling', 'Merging', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
We present a unified transformer i.e. Show-o that unifies multimodal understanding and generation. Unlike fully autoregressive models Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering text-to-image generation text-guided inpainting/extrapolation and mixed-modality generation. Across various benchmarks it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at https://github.com/showlab/Show-o.
