---
layout: publication
title: Patched MOA Optimizing Inference For Diverse Software Development Tasks
authors: Sharma Asankhaya
conference: "Arxiv"
year: 2024
bibkey: sharma2024patched
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.18521"}
  - {name: "Code", url: "https://github.com/codelion/optillm"}
tags: ['Agentic', 'Efficiency And Optimization', 'GPT', 'Has Code', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
This paper introduces Patched MOA (Mixture of Agents) an inference optimization technique that significantly enhances the performance of large language models (LLMs) across diverse software development tasks. We evaluate three inference optimization algorithms 45; Best of N Mixture of Agents and Monte Carlo Tree Search and demonstrate that Patched MOA can boost the performance of smaller models to surpass that of larger more expensive models. Notably our approach improves the gpt45;4o45;mini models performance on the Arena45;Hard45;Auto benchmark by 15.5237; outperforming gpt45;445;turbo at a fraction of the cost. We also apply Patched MOA to various software development workflows showing consistent improvements in task completion rates. Our method is model45;agnostic transparent to end45;users and can be easily integrated into existing LLM pipelines. This work contributes to the growing field of LLM optimization offering a cost45;effective solution for enhancing model performance without the need for fine45;tuning or larger models. Our implementation is open45;source and available at https://github.com/codelion/optillm.
