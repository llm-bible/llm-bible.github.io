---
layout: publication
title: 'Abstract2appendix: Academic Reviews Enhance LLM Long-context Capabilities'
authors: Shengzhi Li, Kittipat Kampa, Rongyu Lin, Bohang Li, Shichao Pei
conference: "Arxiv"
year: 2024
bibkey: li2024academic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.05232"}
tags: ['Fine-Tuning', 'Efficiency and Optimization', 'GPT', 'Survey Paper', 'RAG', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods']
---
Large language models (LLMs) have shown remarkable performance across various
tasks, yet their ability to handle long-context reading remains challenging.
This study explores the effectiveness of leveraging high-quality academic peer
review data for fine-tuning LLMs to enhance their long-context capabilities. We
compare the Direct Preference Optimization (DPO) method with the Supervised
Fine-Tuning (SFT) method, demonstrating DPO's superiority and data efficiency.
Our experiments show that the fine-tuned model achieves a 4.04-point
improvement over phi-3 and a 2.6% increase on the Qasper benchmark using only
2000 samples. Despite facing limitations in data scale and processing costs,
this study underscores the potential of DPO and high-quality data in advancing
LLM performance.
  Additionally, the zero-shot benchmark results indicate that aggregated
high-quality human reviews are overwhelmingly preferred over LLM-generated
responses, even for the most capable models like GPT-4o. This suggests that
high-quality human reviews are extremely rich in information, reasoning, and
long-context retrieval, capabilities that even the most advanced models have
not fully captured. These findings highlight the high utility of leveraging
human reviews to further advance the field.
