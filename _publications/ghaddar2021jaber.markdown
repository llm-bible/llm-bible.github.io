---
layout: publication
title: JABER And SABER Junior And Senior Arabic Bert
authors: Ghaddar Abbas, Wu Yimeng, Rashid Ahmad, Bibi Khalil, Rezagholizadeh Mehdi, Xing Chao, Wang Yasheng, Xinyu Duan, Wang Zhefeng, Huai Baoxing, Jiang Xin, Liu Qun, Langlais Philippe
conference: "Arxiv"
year: 2021
bibkey: ghaddar2021jaber
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2112.04329"}
tags: ['Applications', 'BERT', 'Model Architecture']
---
Language45;specific pre45;trained models have proven to be more accurate than multilingual ones in a monolingual evaluation setting Arabic is no exception. However we found that previously released Arabic BERT models were significantly under45;trained. In this technical report we present JABER and SABER Junior and Senior Arabic BERt respectively our pre45;trained language model prototypes dedicated for Arabic. We conduct an empirical study to systematically evaluate the performance of models across a diverse set of existing Arabic NLU tasks. Experimental results show that JABER and SABER achieve state45;of45;the45;art performances on ALUE a new benchmark for Arabic Language Understanding Evaluation as well as on a well45;established NER benchmark.
