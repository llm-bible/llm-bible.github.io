---
layout: publication
title: Ragged Edges: The Double-edged Sword Of Retrieval-augmented Chatbots
authors: Feldman Philip, Foulds James R., Pan Shimei
conference: "Arxiv"
year: 2024
bibkey: feldman2024ragged
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.01193"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Prompting', 'RAG', 'Reinforcement Learning']
---
Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical as seen in recent court cases where ChatGPTs use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases but can still be misled when prompts directly contradict the models pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and discuss implications for the development of more trustworthy LLMs.
