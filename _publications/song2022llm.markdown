---
layout: publication
title: Llm45;planner Few45;shot Grounded Planning For Embodied Agents With Large Language Models
authors: Song Chan Hee, Wu Jiaman, Washington Clayton, Sadler Brian M., Chao Wei-lun, Su Yu
conference: "Arxiv"
year: 2022
bibkey: song2022llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2212.04088"}
tags: ['Agentic', 'Efficiency And Optimization', 'Pretraining Methods', 'Training Techniques']
---
This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually45;perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work we propose a novel method LLM45;Planner that harnesses the power of large language models to do few45;shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few45;shot performance Despite using less than 0.537; of paired training data LLM45;Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few45;shot setting. Our work opens the door for developing versatile and sample45;efficient embodied agents that can quickly learn many tasks. Website https://dki&#45;lab.github.io/LLM&#45;Planner
