---
layout: publication
title: On The Automatic Generation And Simplification Of Childrens Stories
authors: Valentini Maria, Weber Jennifer, Salcido Jesus, Wright TÃ©a, Colunga Eliana, Kann Katharina
conference: "Arxiv"
year: 2023
bibkey: valentini2023automatic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.18502"}
tags: ['Pretraining Methods', 'Reinforcement Learning']
---
With recent advances in large language models (LLMs) the concept of automatically generating childrens educational materials has become increasingly realistic. Working toward the goal of age45;appropriate simplicity in generated educational texts we first examine the ability of several popular LLMs to generate stories with properly adjusted lexical and readability levels. We find that in spite of the growing capabilities of LLMs they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups. As a second experiment we explore the ability of state45;of45;the45;art lexical simplification models to generalize to the domain of childrens stories and thus create an efficient pipeline for their automatic generation. In order to test these models we develop a dataset of child45;directed lexical simplification instances with examples taken from the LLM45;generated stories in our first experiment. We find that while the strongest45;performing current lexical simplification models do not perform as well on material designed for children due to their reliance on large language models behind the scenes some models that still achieve fairly strong results on general data can mimic or even improve their performance on children45;directed data with proper fine45;tuning which we conduct using our newly created child45;directed simplification dataset.
