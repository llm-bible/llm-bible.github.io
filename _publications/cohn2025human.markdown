---
layout: publication
title: 'Cotal: Human-in-the-loop Prompt Engineering, Chain-of-thought Reasoning, And Active Learning For Generalizable Formative Assessment Scoring'
authors: Clayton Cohn, Nicole Hutchins, Ashwin T S, Gautam Biswas
conference: "Arxiv"
year: 2025
bibkey: cohn2025human
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.02323'}
tags: ['Interpretability and Explainability', 'RAG', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
Large language models (LLMs) have created new opportunities to assist
teachers and support student learning. Methods such as chain-of-thought (CoT)
prompting enable LLMs to grade formative assessments in science, providing
scores and relevant feedback to students. However, the extent to which these
methods generalize across curricula in multiple domains (such as science,
computing, and engineering) remains largely untested. In this paper, we
introduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based
approach to formative assessment scoring that (1) leverages Evidence-Centered
Design (ECD) principles to develop curriculum-aligned formative assessments and
rubrics, (2) applies human-in-the-loop prompt engineering to automate response
scoring, and (3) incorporates teacher and student feedback to iteratively
refine assessment questions, grading rubrics, and LLM prompts for automated
grading. Our findings demonstrate that CoTAL improves GPT-4's scoring
performance, achieving gains of up to 24.5% over a non-prompt-engineered
baseline. Both teachers and students view CoTAL as effective in scoring and
explaining student responses, each providing valuable refinements to enhance
grading accuracy and explanation quality.
