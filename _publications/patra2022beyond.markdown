---
layout: publication
title: Beyond English45;centric Bitexts For Better Multilingual Language Representation Learning
authors: Patra Barun, Singhal Saksham, Huang Shaohan, Chi Zewen, Dong Li, Wei Furu, Chaudhary Vishrav, Song Xia
conference: "Arxiv"
year: 2022
bibkey: patra2022beyond
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2210.14867"}
tags: ['Applications', 'Masked Language Model', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
In this paper we elaborate upon recipes for building multilingual representation models that are not only competitive with existing state45;of45;the45;art models but are also more parameter efficient thereby promoting better adoption in resource45;constrained scenarios and practical applications. We show that going beyond English45;centric bitexts coupled with a novel sampling strategy aimed at reducing under45;utilization of training data substantially boosts performance across model sizes for both Electra and MLM pre45;training objectives. We introduce XY45;LENT X45;Y bitext enhanced Language ENcodings using Transformers which not only achieves state45;of45;the45;art performance over 5 cross45;lingual tasks within all model size bands is also competitive across bands. Our XY45;LENT XL variant outperforms XLM45;RXXL and exhibits competitive performance with mT5 XXL while being 5x and 6x smaller respectively. We then show that our proposed method helps ameliorate the curse of multilinguality with the XY45;LENT XL achieving 99.337; GLUE performance and 98.537; SQuAD 2.0 performance compared to a SoTA English only model in the same size band. We then analyze our models performance on extremely low resource languages and posit that scaling alone may not be sufficient for improving the performance in this scenario
