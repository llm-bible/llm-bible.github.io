---
layout: publication
title: 'Teuken-7b-base & Teuken-7b-instruct: Towards European Llms'
authors: Mehdi Ali, Michael Fromm, Klaudia Thellmann, Jan Ebert, Alexander Arno Weber, Richard Rutmann, Charvi Jain, Max Lübbering, Daniel Steinigen, Johannes Leveling, Katrin Klug, Jasper Schulze Buschhoff, Lena Jurkschat, Hammam Abdelwahab, Benny Jörg Stein, Karl-heinz Sylla, Pavel Denisov, Nicolo' Brandizzi, Qasid Saleem, Anirban Bhowmick, Lennard Helmer, Chelsea John, Pedro Ortiz Suarez, Malte Ostendorff, Alex Jude, Lalith Manjunath, Samuel Weinbach, Carolin Penke, Oleg Filatov, Shima Asaadi, Fabio Barth, Rafet Sifa, Fabian Küch, Andreas Herten, René Jäkel, Georg Rehm, Stefan Kesselheim, Joachim Köhler, Nicolas Flores-herr
conference: "Arxiv"
year: 2024
bibkey: ali2024teuken
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.03730'}
tags: ['Reinforcement Learning', 'Efficiency and Optimization', 'Training Techniques']
---
We present two multilingual LLMs designed to embrace Europe's linguistic
diversity by supporting all 24 official languages of the European Union.
Trained on a dataset comprising around 60% non-English data and utilizing a
custom multilingual tokenizer, our models address the limitations of existing
LLMs that predominantly focus on English or a few high-resource languages. We
detail the models' development principles, i.e., data composition, tokenizer
optimization, and training methodologies. The models demonstrate competitive
performance across multilingual benchmarks, as evidenced by their performance
on European versions of ARC, HellaSwag, MMLU, and TruthfulQA.
