---
layout: publication
title: 'Robin: A Suite Of Multi-scale Vision-language Models And The CHIRP Evaluation Benchmark'
authors: Alexis Roger, Prateek Humane, Daniel Z. Kaplan, Kshitij Gupta, Qi Sun, George Adamopoulos, Jonathan Siu Chi Lim, Quentin Anthony, Edwin Fennell, Irina Rish
conference: "Arxiv"
year: 2025
bibkey: roger2025suite
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.09672"}
tags: ['Arxiv', 'Training Techniques', 'Multimodal Models']
---
The proliferation of Vision-Language Models (VLMs) in the past several years
calls for rigorous and comprehensive evaluation methods and benchmarks. This
work analyzes existing VLM evaluation techniques, including automated metrics,
AI-based assessments, and human evaluations across diverse tasks. We first
introduce Robin - a novel suite of VLMs that we built by combining Large
Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use
Robin to identify shortcomings of current evaluation approaches across scales.
Next, to overcome the identified limitations, we introduce CHIRP - a new long
form response benchmark we developed for more robust and complete VLM
evaluation. We provide open access to the Robin training code, model suite, and
CHIRP benchmark to promote reproducibility and advance VLM research.
