---
layout: publication
title: Dialogue You Can Trust Human And AI Perspectives On Generated Conversations
authors: Ebubechukwu Ike, Takeuchi Johane, Ceravola Antonello, Joublin Frank
conference: "Arxiv"
year: 2024
bibkey: ebubechukwu2024dialogue
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.01808"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Tools']
---
As dialogue systems and chatbots increasingly integrate into everyday interactions the need for efficient and accurate evaluation methods becomes paramount. This study explores the comparative performance of human and AI assessments across a range of dialogue scenarios focusing on seven key performance indicators (KPIs) Coherence Innovation Concreteness Goal Contribution Commonsense Contradiction Incorrect Fact and Redundancy. Utilizing the GPT45;4o API we generated a diverse dataset of conversations and conducted a two45;part experimental analysis. In Experiment 1 we evaluated multi45;party conversations on Coherence Innovation Concreteness and Goal Contribution revealing that GPT models align closely with human judgments. Notably both human and AI evaluators exhibited a tendency towards binary judgment rather than linear scaling highlighting a shared challenge in these assessments. Experiment 2 extended the work of Finch et al. (2023) by focusing on dyadic dialogues and assessing Commonsense Contradiction Incorrect Fact and Redundancy. The results indicate that while GPT45;4o demonstrates strong performance in maintaining factual accuracy and commonsense reasoning it still struggles with reducing redundancy and self45;contradiction. Our findings underscore the potential of GPT models to closely replicate human evaluation in dialogue systems while also pointing to areas for improvement. This research offers valuable insights for advancing the development and implementation of more refined dialogue evaluation methodologies contributing to the evolution of more effective and human45;like AI communication tools.
