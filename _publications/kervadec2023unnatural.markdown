---
layout: publication
title: Unnatural Language Processing How Do Language Models Handle Machine45;generated Prompts
authors: Kervadec Corentin, Franzon Francesca, Baroni Marco
conference: "Arxiv"
year: 2023
bibkey: kervadec2023unnatural
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.15829"}
tags: ['Attention Mechanism', 'Efficiency And Optimization', 'Model Architecture', 'Prompting', 'Training Techniques']
---
Language model prompt optimization research has shown that semantically and grammatically well45;formed manually crafted prompts are routinely outperformed by automatically generated token sequences with no apparent meaning or syntactic structure including sequences of vectors from a models embedding space. We use machine45;generated prompts to probe how models respond to input that is not composed of natural language expressions. We study the behavior of models of different sizes in multiple semantic tasks in response to both continuous and discrete machine45;generated prompts and compare it to the behavior in response to human45;generated natural45;language prompts. Even when producing a similar output machine45;generated and human prompts trigger different response patterns through the network processing pathways including different perplexities different attention and output entropy distributions and different unit activation profiles. We provide preliminary insight into the nature of the units activated by different prompt types suggesting that only natural language prompts recruit a genuinely linguistic circuit.
