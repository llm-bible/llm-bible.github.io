---
layout: publication
title: 'Do Bert-like Bidirectional Models Still Perform Better On Text Classification In The Era Of Llms?'
authors: Junyan Zhang, Yiming Huang, Shuliang Liu, Yubo Gao, Xuming Hu
conference: "Arxiv"
year: 2025
bibkey: zhang2025do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.18215"}
tags: ['Fine-Tuning', 'Tools', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods', 'BERT']
---
The rapid adoption of LLMs has overshadowed the potential advantages of traditional BERT-like models in text classification. This study challenges the prevailing "LLM-centric" trend by systematically comparing three category methods, i.e., BERT-like models fine-tuning, LLM internal state utilization, and zero-shot inference across six high-difficulty datasets. Our findings reveal that BERT-like models often outperform LLMs. We further categorize datasets into three types, perform PCA and probing experiments, and identify task-specific model strengths: BERT-like models excel in pattern-driven tasks, while LLMs dominate those requiring deep semantics or world knowledge. Based on this, we propose TaMAS, a fine-grained task selection strategy, advocating for a nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.
