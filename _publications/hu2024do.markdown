---
layout: publication
title: 'Do Language Models Understand The Cognitive Tasks Given To Them? Investigations With The N-back Paradigm'
authors: Xiaoyang Hu, Richard L. Lewis
conference: "Arxiv"
year: 2024
bibkey: hu2024do
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.18120'}
tags: ['Attention Mechanism', 'GPT', 'Prompting', 'Model Architecture']
---
Cognitive tasks originally developed for humans are now increasingly used to study language models. While applying these tasks is often straightforward, interpreting their results can be challenging. In particular, when a model underperforms, it is often unclear whether this results from a limitation in the cognitive ability being tested or a failure to understand the task itself. A recent study argues that GPT 3.5's declining performance on 2-back and 3-back tasks reflects a working memory capacity limit similar to humans (Gong et al., 2024). By analyzing a range of open-source language models of varying performance levels on these tasks, we show that the poor performance is due at least in part to a limitation in task comprehension and task set maintenance. We challenge the best-performing model with progressively harder versions of the task (up to 10-back) and experiment with alternative prompting strategies, before analyzing model attentions. Our larger aim is to contribute to the ongoing conversation around refining methodologies for the cognitive evaluation of language models.
