---
layout: publication
title: Chain45;of45;specificity An Iteratively Refining Method For Eliciting Knowledge From Large Language Models
authors: Wei Kaiwen, Zhang Jingyuan, Zhang Hongzhi, Zhang Fuzheng, Zhang Di, Jin Li, Yu Yue
conference: "Arxiv"
year: 2024
bibkey: wei2024chain
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.15526"}
tags: ['Applications', 'Reinforcement Learning']
---
Large Language Models (LLMs) exhibit remarkable generative capabilities enabling the generation of valuable information. Despite these advancements previous research found that LLMs sometimes struggle with adhering to specific constraints (e.g. in specific place or at specific time) at times even overlooking them which leads to responses that are either too generic or not fully satisfactory. Existing approaches attempted to address this issue by decomposing or rewriting input instructions yet they fall short in adequately emphasizing specific constraints and in unlocking the underlying knowledge (e.g. programming within the context of software development). In response this paper proposes a simple yet effective method named Chain45;of45;Specificity (CoS). Specifically CoS iteratively emphasizes the specific constraints in the input instructions unlocks knowledge within LLMs and refines responses. Experiments conducted on publicly available and self45;build complex datasets demonstrate that CoS outperforms existing methods in enhancing generated content especially for the specificity. Besides as the number of specific constraints increase other baselines falter while CoS still performs well. Moreover we show that distilling responses generated by CoS effectively enhances the ability of smaller models to follow the constrained instructions. Resources of this paper will be released for further research.
