---
layout: publication
title: 'Sysllm: Generating Synthesized Policy Summaries For Reinforcement Learning Agents Using Large Language Models'
authors: Sahar Admoni, Omer Ben-porat, Ofra Amir
conference: "Arxiv"
year: 2025
bibkey: admoni2025generating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.10509'}
tags: ['Agentic', 'ACL', 'Agent', 'Applications', 'Reinforcement Learning', 'TACL']
---
Policies generated by Reinforcement Learning (RL) algorithms can be difficult
to describe to users, as they result from the interplay between complex reward
structures and neural network-based representations. This combination often
leads to unpredictable behaviors, making policies challenging to analyze and
posing significant obstacles to fostering human trust in real-world
applications. Global policy summarization methods aim to describe agent
behavior through a demonstration of actions in a subset of world-states.
However, users can only watch a limited number of demonstrations, restricting
their understanding of policies. Moreover, those methods overly rely on user
interpretation, as they do not synthesize observations into coherent patterns.
In this work, we present SySLLM (Synthesized Summary using LLMs), a novel
method that employs synthesis summarization, utilizing large language models'
(LLMs) extensive world knowledge and ability to capture patterns, to generate
textual summaries of policies. Specifically, an expert evaluation demonstrates
that the proposed approach generates summaries that capture the main insights
generated by experts while not resulting in significant hallucinations.
Additionally, a user study shows that SySLLM summaries are preferred over
demonstration-based policy summaries and match or surpass their performance in
objective agent identification tasks.
