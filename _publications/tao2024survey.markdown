---
layout: publication
title: A Survey On Self45;evolution Of Large Language Models
authors: Tao Zhengwei, Lin Ting-en, Chen Xiancai, Li Hangyu, Wu Yuchuan, Li Yongbin, Jin Zhi, Huang Fei, Tao Dacheng, Zhou Jingren
conference: "Arxiv"
year: 2024
bibkey: tao2024survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.14387"}
  - {name: "Code", url: "https://github.com/AlibabaResearch/DAMO&#45;ConvAI/tree/main/Awesome&#45;Self&#45;Evolution&#45;of&#45;LLM"}
tags: ['Agentic', 'Applications', 'Has Code', 'Pretraining Methods', 'Survey Paper', 'Tools', 'Training Techniques']
---
Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue self45;evolution approaches that enable LLM to autonomously acquire refine and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work we present a comprehensive survey of self45;evolution approaches in LLMs. We first propose a conceptual framework for self45;evolution and outline the evolving process as iterative cycles composed of four phases experience acquisition experience refinement updating and evaluation. Second we categorize the evolution objectives of LLMs and LLM45;based agents; then we summarize the literature and provide taxonomy and insights for each module. Lastly we pinpoint existing challenges and propose future directions to improve self45;evolution frameworks equipping researchers with critical insights to fast45;track the development of self45;evolving LLMs. Our corresponding GitHub repository is available at https://github.com/AlibabaResearch/DAMO&#45;ConvAI/tree/main/Awesome&#45;Self&#45;Evolution&#45;of&#45;LLM
