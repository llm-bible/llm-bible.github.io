---
layout: publication
title: 'A Survey On Self-evolution Of Large Language Models'
authors: Tao Zhengwei, Lin Ting-en, Chen Xiancai, Li Hangyu, Wu Yuchuan, Li Yongbin, Jin Zhi, Huang Fei, Tao Dacheng, Zhou Jingren
conference: "Arxiv"
year: 2024
bibkey: tao2024survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.14387"}
  - {name: "Code", url: "https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM"}
tags: ['Agentic', 'Applications', 'Has Code', 'Pretraining Methods', 'Survey Paper', 'Tools', 'Training Techniques']
---
'Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs. Our corresponding GitHub repository is available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM'
