---
layout: publication
title: Charactereval A Chinese Benchmark For Role-playing Conversational Agent Evaluation
authors: Tu Quan, Fan Shilong, Tian Zihang, Yan Rui
conference: "Arxiv"
year: 2024
bibkey: tu2024charactereval
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.01275"}
  - {name: "Code", url: "https://github.com/morecry/CharacterEval"}
tags: ['Agentic', 'Attention Mechanism', 'GPT', 'Has Code', 'Model Architecture', 'Reinforcement Learning']
---
Recently the advent of large language models (LLMs) has revolutionized generative agents. Among them Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap we introduce CharacterEval a Chinese benchmark for comprehensive RPCA assessment complemented by a tailored high-quality dataset. The dataset comprises 1785 multi-turn role-playing dialogues encompassing 23020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed beginning with initial dialogue extraction via GPT-4 followed by rigorous human-led quality control and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code data source and reward model will be publicly accessible at https://github.com/morecry/CharacterEval.
