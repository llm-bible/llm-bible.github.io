---
layout: publication
title: BRAINTEASER\: Lateral Thinking Puzzles For Large Language Models
authors: Jiang Yifan, Ilievski Filip, Ma Kaixin, Sourati Zhivar
conference: "Arxiv"
year: 2023
bibkey: jiang2023lateral
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.05057"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Security']
---
The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular lateral thinking puzzles have received little attention. To bridge this gap we devise BRAINTEASER a multiple-choice Question Answering task designed to test the models ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark consisting of data collection distractor generation and generation of adversarial examples leading to 1100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.
