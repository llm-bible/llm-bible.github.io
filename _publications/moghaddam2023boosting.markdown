---
layout: publication
title: Boosting Theory45;of45;mind Performance In Large Language Models Via Prompting
authors: Moghaddam Shima Rahimi, Honey Christopher J.
conference: "Arxiv"
year: 2023
bibkey: moghaddam2023boosting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.11490"}
tags: ['Agentic', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
Large language models (LLMs) excel in many tasks in 2023 but they still face challenges in complex reasoning. Theory45;of45;mind (ToM) tasks which require understanding agents beliefs goals and mental states are essential for common45;sense reasoning involving humans making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT45;4 and three GPT45;3.5 variants (Davinci45;2 Davinci45;3 GPT45;3.545;Turbo) and investigates the effectiveness of in45;context learning in improving their ToM comprehension. We evaluated prompts featuring two45;shot chain of thought reasoning and step45;by45;step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci45;2) improved their ToM accuracy via in45;context learning. GPT45;4 performed best in zero45;shot settings reaching nearly 8037; ToM accuracy but still fell short of the 8737; human accuracy on the test set. However when supplied with prompts for in45;context learning all RLHF45;trained LLMs exceeded 8037; ToM accuracy with GPT45;4 reaching 10037;. These results demonstrate that appropriate prompting enhances LLM ToM reasoning and they underscore the context45;dependent nature of LLM cognitive capacities.
