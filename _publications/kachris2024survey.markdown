---
layout: publication
title: 'A Survey On Hardware Accelerators For Large Language Models'
authors: Christoforos Kachris
conference: "Appl. Sci. 2025 15(2) 586;"
year: 2024
bibkey: kachris2024survey
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2401.09890'}
tags: ['Efficiency and Optimization', 'Model Architecture', 'Applications', 'Tools', 'Survey Paper', 'Reinforcement Learning']
---
Large Language Models (LLMs) have emerged as powerful tools for natural
language processing tasks, revolutionizing the field with their ability to
understand and generate human-like text. As the demand for more sophisticated
LLMs continues to grow, there is a pressing need to address the computational
challenges associated with their scale and complexity. This paper presents a
comprehensive survey on hardware accelerators designed to enhance the
performance and energy efficiency of Large Language Models. By examining a
diverse range of accelerators, including GPUs, FPGAs, and custom-designed
architectures, we explore the landscape of hardware solutions tailored to meet
the unique computational demands of LLMs. The survey encompasses an in-depth
analysis of architecture, performance metrics, and energy efficiency
considerations, providing valuable insights for researchers, engineers, and
decision-makers aiming to optimize the deployment of LLMs in real-world
applications.
