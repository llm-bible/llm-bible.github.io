---
layout: publication
title: Deepening Hidden Representations From Pre45;trained Language Models
authors: Yang Junjie, Zhao Hai
conference: "Arxiv"
year: 2019
bibkey: yang2019deepening
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1911.01940"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Transformer45;based pre45;trained language models have proven to be effective for learning contextualized language representation. However current approaches only take advantage of the output of the encoders final layer when fine45;tuning the downstream tasks. We argue that only taking single layers output restricts the power of pre45;trained representation. Thus we deepen the representation learned by the model by fusing the hidden representation in terms of an explicit HIdden Representation Extractor (HIRE) which automatically absorbs the complementary representation with respect to the output from the final layer. Utilizing RoBERTa as the backbone encoder our proposed improvement over the pre45;trained models is shown effective on multiple natural language understanding tasks and help our model rival with the state45;of45;the45;art models on the GLUE benchmark.
