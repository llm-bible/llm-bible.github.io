---
layout: publication
title: 'Retrieval Backward Attention Without Additional Training: Enhance Embeddings Of Large Language Models Via Repetition'
authors: Yifei Duan, Raphael Shang, Deng Liang, Yongqiang Cai
conference: "Arxiv"
year: 2025
bibkey: duan2025retrieval
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.20726"}
tags: ['Training Techniques', 'Transformer', 'Attention Mechanism', 'Model Architecture']
---
Language models can be viewed as functions that embed text into Euclidean
space, where the quality of the embedding vectors directly determines model
performance, training such neural networks involves various uncertainties. This
paper focuses on improving the performance of pre-trained language models in
zero-shot settings through a simple and easily implementable method. We propose
a novel backward attention mechanism to enhance contextual information
encoding. Evaluated on the Chinese Massive Text Embedding Benchmark (C-MTEB),
our approach achieves significant improvements across multiple tasks, providing
valuable insights for advancing zero-shot learning capabilities.
