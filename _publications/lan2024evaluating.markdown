---
layout: publication
title: 'Criticeval: Evaluating Large Language Model As Critic'
authors: Lan Tian, Zhang Wenwei, Xu Chen, Huang Heyan, Lin Dahua, Chen Kai, Mao Xian-ling
conference: "Arxiv"
year: 2024
bibkey: lan2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.13764"}
tags: ['Applications', 'GPT', 'Model Architecture']
---
Critique ability, i.e., the capability of Large Language Models (LLMs) to
identify and rectify flaws in responses, is crucial for their applications in
self-improvement and scalable oversight. While numerous studies have been
proposed to evaluate critique ability of LLMs, their comprehensiveness and
reliability are still limited. To overcome this problem, we introduce
CriticEval, a novel benchmark designed to comprehensively and reliably evaluate
critique ability of LLMs. Specifically, to ensure the comprehensiveness,
CriticEval evaluates critique ability from four dimensions across nine diverse
task scenarios. It evaluates both scalar-valued and textual critiques,
targeting responses of varying quality. To ensure the reliability, a large
number of critiques are annotated to serve as references, enabling GPT-4 to
evaluate textual critiques reliably. Extensive evaluations of open-source and
closed-source LLMs first validate the reliability of evaluation in CriticEval.
Then, experimental results demonstrate the promising potential of open-source
LLMs, the effectiveness of critique datasets and several intriguing
relationships between the critique ability and some critical factors, including
task types, response qualities and critique dimensions. Datasets and evaluation
toolkit for CriticEval will be publicly released.
