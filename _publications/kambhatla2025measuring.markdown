---
layout: publication
title: 'Measuring Diversity Of Synthetic Prompts And Data Generated With Fine-grained Persona Prompting'
authors: Gauri Kambhatla, Chantal Shaib, Venkata Govindarajan
conference: "Arxiv"
year: 2025
bibkey: kambhatla2025measuring
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.17390'}
tags: ['Training Techniques', 'Fine-Tuning', 'Prompting', 'Pre-Training', 'Pretraining Methods']
---
Fine-grained personas have recently been used for generating 'diverse' synthetic data for pre-training and supervised fine-tuning of Large Language Models (LLMs). In this work, we measure the diversity of persona-driven synthetically generated prompts and responses with a suite of lexical diversity and redundancy metrics. Firstly, we find that synthetic prompts/instructions are significantly less diverse than human-written ones. Next, we sample responses from LLMs of different sizes with fine-grained and coarse persona descriptions to investigate how much fine-grained detail in persona descriptions contribute to generated text diversity. We find that while persona-prompting does improve lexical diversity (especially with larger models), fine-grained detail in personas doesn't increase diversity noticeably.
