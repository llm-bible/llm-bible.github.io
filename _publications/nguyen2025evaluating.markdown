---
layout: publication
title: 'Evaluating Graphical Perception With Multimodal Llms'
authors: Rami Huu Nguyen, Kenichi Maeda, Mahsa Geshvadi, Daniel Haehn
conference: "Arxiv"
year: 2025
bibkey: nguyen2025evaluating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.04221'}
tags: ['Prompting', 'Multimodal Models']
---
Multimodal Large Language Models (MLLMs) have remarkably progressed in
analyzing and understanding images. Despite these advancements, accurately
regressing values in charts remains an underexplored area for MLLMs. For
visualization, how do MLLMs perform when applied to graphical perception tasks?
Our paper investigates this question by reproducing Cleveland and McGill's
seminal 1984 experiment and comparing it against human task performance. Our
study primarily evaluates fine-tuned and pretrained models and zero-shot
prompting to determine if they closely match human graphical perception. Our
findings highlight that MLLMs outperform human task performance in some cases
but not in others. We highlight the results of all experiments to foster an
understanding of where MLLMs succeed and fail when applied to data
visualization.
