---
layout: publication
title: Inference To The Best Explanation In Large Language Models
authors: Dalal Dhairya, Valentino Marco, Freitas André, Buitelaar Paul
conference: "Arxiv"
year: 2024
bibkey: dalal2024inference
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.10767"}
tags: ['Applications', 'GPT', 'Interpretability And Explainability', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
While Large Language Models (LLMs) have found success in real45;world applications their underlying explanatory process is still poorly understood. This paper proposes IBE45;Eval a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs explanations. IBE45;Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including consistency parsimony coherence and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA) where textit123;IBE45;Eval125; is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e. GPT 3.5 and Llama 2). The experiments reveal that IBE45;Eval can successfully identify the best explanation with up to 7737; accuracy (≈ 2737; above random) improving upon a GPT 3.545;as45;a45;Judge baseline (≈+1737;) while being intrinsically more efficient and interpretable. Additional analyses suggest that despite model45;specific variances LLM45;generated explanations tend to conform to IBE criteria and that IBE45;Eval is significantly correlated with human judgment opening up opportunities for future development of automated explanation verification tools.
