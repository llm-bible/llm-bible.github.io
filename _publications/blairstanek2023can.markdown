---
layout: publication
title: BLT Can Large Language Models Handle Basic Legal Text
authors: Blair-stanek Andrew, Holzenberger Nils, Van Durme Benjamin
conference: "Arxiv"
year: 2023
bibkey: blairstanek2023can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.09693"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
We find that the best publicly available LLMs like GPT45;4 Claude and 123;PaLM 2125; currently perform poorly at basic legal text handling. We introduce a benchmark consisting of tasks that lawyers and paralegals would expect LLMs to handle zero45;shot such as looking up the text at a line of a witness deposition or at a subsection of a contract. LLMs poor performance on this benchmark casts into doubt their reliability as45;is for legal practice. However fine45;tuning for these tasks brings even a smaller model to near45;perfect performance on our test set and also raises performance on a related legal task. These results suggest that many simple behaviors needed for a domain may not be present in foundational LLMs without additional engagement from subject matter experts.
