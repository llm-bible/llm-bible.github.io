---
layout: publication
title: Saullm45;54b amp; Saullm45;141b Scaling Up Domain Adaptation For The Legal Domain
authors: Colombo Pierre, Pires Telmo, Boudiaf Malik, Melo Rui, Culver Dominic, Morgado Sofia, Malaboeuf Etienne, Hautreux Gabriel, Charpentier Johanne, Desa Michael
conference: "Arxiv"
year: 2024
bibkey: colombo2024saullm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.19584"}
tags: ['Fine Tuning', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
In this paper we introduce SaulLM45;54B and SaulLM45;141B two large language models (LLMs) tailored for the legal sector. These models which feature architectures of 54 billion and 141 billion parameters respectively are based on the Mixtral architecture. The development of SaulLM45;54B and SaulLM45;141B is guided by large45;scale domain adaptation divided into three strategies (1) the exploitation of continued pretraining involving a base corpus that includes over 540 billion of legal tokens (2) the implementation of a specialized legal instruction45;following protocol and (3) the alignment of model outputs with human preferences in legal interpretations. The integration of synthetically generated data in the second and third steps enhances the models capabilities in interpreting and processing legal texts effectively reaching state45;of45;the45;art performance and outperforming previous open45;source models on LegalBench45;Instruct. This work explores the trade45;offs involved in domain45;specific adaptation at this scale offering insights that may inform future studies on domain adaptation using strong decoder models. Building upon SaulLM45;7B this study refines the approach to produce an LLM better equipped for legal tasks. We are releasing base instruct and aligned versions on top of SaulLM45;54B and SaulLM45;141B under the MIT License to facilitate reuse and collaborative research.
