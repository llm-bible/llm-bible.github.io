---
layout: publication
title: 'Measuring Psychological Depth In Language Models'
authors: Harel-canada Fabrice, Zhou Hanyu, Mupalla Sreya, Yildiz Zeynep, Sahai Amit, Peng Nanyun
conference: "Arxiv"
year: 2024
bibkey: harelcanada2024measuring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.12680"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'RAG', 'Tools']
---
Evaluations of creative stories generated by large language models (LLMs) often focus on objective properties of the text such as its style coherence and toxicity. While these metrics are indispensable they do not speak to a storys subjective psychological impact from a readers perspective. We introduce the Psychological Depth Scale (PDS) a novel framework rooted in literary theory that measures an LLMs ability to produce authentic and narratively complex stories that provoke emotion empathy and engagement. We empirically validate our framework by showing that humans can consistently evaluate stories based on PDS (0.72 Krippendorffs alpha). We also explore techniques for automating the PDS to easily scale future analyses. GPT-4o combined with a novel Mixture-of-Personas (MoP) prompting strategy achieves an average Spearman correlation of (0.51) with human judgment while Llama-3-70B scores as high as 0.68 for empathy. Finally we compared the depth of stories authored by both humans and LLMs. Surprisingly GPT-4 stories either surpassed or were statistically indistinguishable from highly-rated human-written stories sourced from Reddit. By shifting the focus from text to reader the Psychological Depth Scale is a validated automated and systematic means of measuring the capacity of LLMs to connect with humans through the stories they tell.
