---
layout: publication
title: Langbite\: A Platform For Testing Bias In Large Language Models
authors: Morales Sergio, Claris√≥ Robert, Cabot Jordi
conference: "Arxiv"
year: 2024
bibkey: morales2024platform
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.18558"}
tags: ['Applications', 'Ethics And Bias', 'Prompting', 'Tools']
---
The integration of Large Language Models (LLMs) into various software applications raises concerns about their potential biases. Typically those models are trained on a vast amount of data scrapped from forums websites social media and other internet sources which may instill harmful and discriminating behavior into the model. To address this issue we present LangBiTe a testing platform to systematically assess the presence of biases within an LLM. LangBiTe enables development teams to tailor their test scenarios and automatically generate and execute the test cases according to a set of user-defined ethical requirements. Each test consists of a prompt fed into the LLM and a corresponding test oracle that scrutinizes the LLMs response for the identification of biases. LangBite provides users with the bias evaluation of LLMs and end-to-end traceability between the initial ethical requirements and the insights obtained.
