---
layout: publication
title: 'Langbite: A Platform For Testing Bias In Large Language Models'
authors: Sergio Morales, Robert Claris√≥, Jordi Cabot
conference: "Arxiv"
year: 2024
bibkey: morales2024platform
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.18558"}
tags: ['Tools', 'Ethics and Bias', 'ACL', 'Prompting', 'Applications']
---
The integration of Large Language Models (LLMs) into various software
applications raises concerns about their potential biases. Typically, those
models are trained on a vast amount of data scrapped from forums, websites,
social media and other internet sources, which may instill harmful and
discriminating behavior into the model. To address this issue, we present
LangBiTe, a testing platform to systematically assess the presence of biases
within an LLM. LangBiTe enables development teams to tailor their test
scenarios, and automatically generate and execute the test cases according to a
set of user-defined ethical requirements. Each test consists of a prompt fed
into the LLM and a corresponding test oracle that scrutinizes the LLM's
response for the identification of biases. LangBite provides users with the
bias evaluation of LLMs, and end-to-end traceability between the initial
ethical requirements and the insights obtained.
