---
layout: publication
title: 'From Text To Transformation: A Comprehensive Review Of Large Language Models'' Versatility'
authors: Kaur Pravneet, Kashyap Gautam Siddharth, Kumar Ankit, Nafis Md Tabrez, Kumar Sandeep, Shokeen Vikrant
conference: "Arxiv"
year: 2024
bibkey: kaur2024from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.16142"}
tags: ['Applications', 'BERT', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Survey Paper', 'Transformer']
---
This groundbreaking study explores the expanse of Large Language Models
(LLMs), such as Generative Pre-Trained Transformer (GPT) and Bidirectional
Encoder Representations from Transformers (BERT) across varied domains ranging
from technology, finance, healthcare to education. Despite their established
prowess in Natural Language Processing (NLP), these LLMs have not been
systematically examined for their impact on domains such as fitness, and
holistic well-being, urban planning, climate modelling as well as disaster
management. This review paper, in addition to furnishing a comprehensive
analysis of the vast expanse and extent of LLMs' utility in diverse domains,
recognizes the research gaps and realms where the potential of LLMs is yet to
be harnessed. This study uncovers innovative ways in which LLMs can leave a
mark in the fields like fitness and wellbeing, urban planning, climate
modelling and disaster response which could inspire future researches and
applications in the said avenues.
