---
layout: publication
title: 'Multilingual Machine Translation With Open Large Language Models At Practical Scale: An Empirical Study'
authors: Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, Bin Wang
conference: "Arxiv"
year: 2025
bibkey: cui2025multilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.02481"}
tags: ['Training Techniques', 'Model Architecture', 'Tools', 'GPT', 'Pretraining Methods', 'Applications']
---
Large language models (LLMs) have shown continuously improving multilingual
capabilities, and even small-scale open-source models have demonstrated rapid
performance enhancement. In this paper, we systematically explore the abilities
of open LLMs with less than ten billion parameters to handle multilingual
machine translation (MT) tasks. We conduct comprehensive evaluations on six
popular LLMs and find that models like Gemma2-9B exhibit impressive
multilingual translation capabilities. We then introduce the Parallel-First
Monolingual-Second (PFMS) data mixing strategy in the continual pretraining
stage to further enhance the MT performance and present GemmaX2-28, a 9B model
achieving top-tier multilingual translation performance across 28 languages.
Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)
models such as TowerInstruct and XALMA and achieves competitive performance
with Google Translate and GPT-4-turbo.
