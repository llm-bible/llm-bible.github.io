---
layout: publication
title: Readonce Transformers Reusable Representations Of Text For Transformers
authors: Lin Shih-ting, Sabharwal Ashish, Khot Tushar
conference: "Arxiv"
year: 2020
bibkey: lin2020readonce
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2010.12854"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
We present ReadOnce Transformers an approach to convert a transformer45;based model into one that can build an information45;capturing task45;independent and compressed representation of text. The resulting representation is reusable across different examples and tasks thereby requiring a document shared across many examples or tasks to only be emph123;read once125;. This leads to faster training and evaluation of models. Additionally we extend standard text45;to45;text transformer models to Representation+Text45;to45;text models and evaluate on multiple downstream tasks multi45;hop QA abstractive QA and long45;document summarization. Our one45;time computed representation results in a 2x45;5x speedup compared to standard text45;to45;text models while the compression also allows existing language models to handle longer documents without the need for designing new pre45;trained models.
