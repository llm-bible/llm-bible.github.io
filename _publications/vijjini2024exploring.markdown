---
layout: publication
title: Exploring Safety45;utility Trade45;offs In Personalized Language Models
authors: Vijjini Anvesh Rao, Chowdhury Somnath Basu Roy, Chaturvedi Snigdha
conference: "Arxiv"
year: 2024
bibkey: vijjini2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.11107"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Responsible AI', 'Tools']
---
As large language models (LLMs) become increasingly integrated into daily applications it is essential to ensure they operate fairly across diverse user demographics. In this work we show that LLMs suffer from personalization bias where their performance is impacted when they are personalized to a users identity. We quantify personalization bias by evaluating the performance of LLMs along two axes 45; safety and utility. We measure safety by examining how benign LLM responses are to unsafe prompts with and without personalization. We measure utility by evaluating the LLMs performance on various tasks including general knowledge mathematical abilities programming and reasoning skills. We find that various LLMs ranging from open45;source models like Llama (Touvron et al. 2023) and Mistral (Jiang et al. 2023) to API45;based ones like GPT45;3.5 and GPT45;4o (Ouyang et al. 2022) exhibit significant variance in performance in terms of safety45;utility trade45;offs depending on the users identity. Finally we discuss several strategies to mitigate personalization bias using preference tuning and prompt45;based defenses.
