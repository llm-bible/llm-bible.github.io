---
layout: publication
title: Auditing Counterfire Evaluating Advanced Counterargument Generation With Evidence And Style
authors: Verma Preetika, Jaidka Kokil, Churina Svetlana
conference: "Arxiv"
year: 2024
bibkey: verma2024auditing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.08498"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
We audited large language models (LLMs) for their ability to create evidence45;based and stylistic counter45;arguments to posts from the Reddit ChangeMyView dataset. We benchmarked their rhetorical quality across a host of qualitative and quantitative metrics and then ultimately evaluated them on their persuasive abilities as compared to human counter45;arguments. Our evaluation is based on Counterfire a new dataset of 32000 counter45;arguments generated from large language models (LLMs) GPT45;3.5 Turbo and Koala and their fine45;tuned variants and PaLM 2 with varying prompts for evidence use and argumentative style. GPT45;3.5 Turbo ranked highest in argument quality with strong paraphrasing and style adherence particularly in reciprocity style arguments. However the stylistic counter45;arguments still fall short of human persuasive standards where people also preferred reciprocal to evidence45;based rebuttals. The findings suggest that a balance between evidentiality and stylistic elements is vital to a compelling counter45;argument. We close with a discussion of future research directions and implications for evaluating LLM outputs.
