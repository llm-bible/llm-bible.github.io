---
layout: publication
title: Diverse And Fine45;grained Instruction45;following Ability Exploration With Synthetic Data
authors: Gu Zihui, Sun Xingwu, Lian Fengzong, Kang Zhanhui, Xu Cheng-zhong, Fan Ju
conference: "AAAI"
year: 2024
bibkey: gu2024diverse
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.03942"}
tags: ['Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Instruction45;following is particularly crucial for large language models (LLMs) to support diverse user requests. While existing work has made progress in aligning LLMs with human preferences evaluating their capabilities on instruction following remains a challenge due to complexity and diversity of real45;world user instructions. While existing evaluation methods focus on general skills they suffer from two main shortcomings i.e. lack of fine45;grained task45;level evaluation and reliance on singular instruction expression. To address these problems this paper introduces DINGO a fine45;grained and diverse instruction45;following evaluation dataset that has two main advantages (1) DINGO is based on a manual annotated fine45;grained and multi45;level category tree with 130 nodes derived from real45;world user requests; (2) DINGO includes diverse instructions generated by both GPT45;4 and human experts. Through extensive experiments we demonstrate that DINGO can not only provide more challenging and comprehensive evaluation for LLMs but also provide task45;level fine45;grained directions to further improve LLMs.
