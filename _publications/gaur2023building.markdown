---
layout: publication
title: Building Trustworthy NeuroSymbolic AI Systems Consistency Reliability Explainability and Safety
authors: Gaur Manas, Sheth Amit
conference: "Arxiv"
year: 2023
bibkey: gaur2023building
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.06798"}
tags: ['Applications', 'Attention Mechanism', 'GPT', 'Interpretability And Explainability', 'Model Architecture', 'Reinforcement Learning', 'Responsible AI', 'Tools']
---
Explainability and Safety engender Trust. These require a model to exhibit consistency and reliability. To achieve these it is necessary to use and analyze data and knowledge with statistical and symbolic AI methods relevant to the AI application - neither alone will do. Consequently we argue and seek to demonstrate that the NeuroSymbolic AI approach is better suited for making AI a trusted AI system. We present the CREST framework that shows how Consistency Reliability user-level Explainability and Safety are built on NeuroSymbolic methods that use data and knowledge to support requirements for critical applications such as health and well-being. This article focuses on Large Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs have garnered substantial attention from researchers due to their versatility in handling a broad array of natural language processing (NLP) scenarios. For example ChatGPT and Googles MedPaLM have emerged as highly promising platforms for providing information in general and health-related queries respectively. Nevertheless these models remain black boxes despite incorporating human feedback and instruction-guided tuning. For instance ChatGPT can generate unsafe responses despite instituting safety guardrails. CREST presents a plausible approach harnessing procedural and graph-based knowledge within a NeuroSymbolic framework to shed light on the challenges associated with LLMs.
