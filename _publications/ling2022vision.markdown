---
layout: publication
title: Vision45;language Pre45;training For Multimodal Aspect45;based Sentiment Analysis
authors: Ling Yan, Yu Jianfei, Xia Rui
conference: "Arxiv"
year: 2022
bibkey: ling2022vision
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.07955"}
  - {name: "Code", url: "https://github.com/NUSTM/VLP&#45;MABSA"}
tags: ['Attention Mechanism', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Tools', 'Training Techniques']
---
As an important task in sentiment analysis Multimodal Aspect45;Based Sentiment Analysis (MABSA) has attracted increasing attention in recent years. However previous approaches either (i) use separately pre45;trained visual and textual models which ignore the crossmodal alignment or (ii) use vision45;language models pre45;trained with general pre45;training tasks which are inadequate to identify finegrained aspects opinions and their alignments across modalities. To tackle these limitations we propose a task45;specific Vision45;Language Pre45;training framework for MABSA (VLPMABSA) which is a unified multimodal encoder45;decoder architecture for all the pretraining and downstream tasks. We further design three types of task45;specific pre45;training tasks from the language vision and multimodal modalities respectively. Experimental results show that our approach generally outperforms the state45;of45;the45;art approaches on three MABSA subtasks. Further analysis demonstrates the effectiveness of each pretraining task. The source code is publicly released at https://github.com/NUSTM/VLP&#45;MABSA.
