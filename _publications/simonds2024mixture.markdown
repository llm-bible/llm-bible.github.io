---
layout: publication
title: 'Modem: Mixture Of Domain Expert Models'
authors: Toby Simonds, Kemal Kurniawan, Jey Han Lau
conference: "Arxiv"
year: 2024
bibkey: simonds2024mixture
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.07490"}
tags: ['Prompting', 'BERT', 'Efficiency and Optimization', 'Model Architecture']
---
We propose a novel approach to enhancing the performance and efficiency of
large language models (LLMs) by combining domain prompt routing with
domain-specialized models. We introduce a system that utilizes a BERT-based
router to direct incoming prompts to the most appropriate domain expert model.
These expert models are specifically tuned for domains such as health,
mathematics and science. Our research demonstrates that this approach can
significantly outperform general-purpose models of comparable size, leading to
a superior performance-to-cost ratio across various benchmarks. The
implications of this study suggest a potential paradigm shift in LLM
development and deployment. Rather than focusing solely on creating
increasingly large, general-purpose models, the future of AI may lie in
developing ecosystems of smaller, highly specialized models coupled with
sophisticated routing systems. This approach could lead to more efficient
resource utilization, reduced computational costs, and superior overall
performance.
