---
layout: publication
title: Verification And Refinement Of Natural Language Explanations Through Llm-symbolic Theorem Proving
authors: Quan Xin, Valentino Marco, Dennis Louise A., Freitas Andr√©
conference: "Arxiv"
year: 2024
bibkey: quan2024verification
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.01379"}
tags: ['Interpretability And Explainability', 'Reinforcement Learning', 'Tools']
---
Natural language explanations have become a proxy for evaluating explainable and multi-step Natural Language Inference (NLI) models. However assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets a process that is time-consuming and prone to logical errors. To address existing limitations this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs). Specifically we present a neuro-symbolic framework named Explanation-Refiner that augments a TP with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI. In turn the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning autoformalisation and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of human-annotated explanations of variable complexity in different domains.
