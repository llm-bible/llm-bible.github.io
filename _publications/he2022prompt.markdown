---
layout: publication
title: Hyperprompt Prompt45;based Task45;conditioning Of Transformers
authors: He Yun, Zheng Huaixiu Steven, Tay Yi, Gupta Jai, Du Yu, Aribandi Vamsi, Zhao Zhe, Li Yaguang, Chen Zhao, Metzler Donald, Cheng Heng-tze, Chi Ed H.
conference: "Arxiv"
year: 2022
bibkey: he2022prompt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2203.00759"}
tags: ['Applications', 'Attention Mechanism', 'Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Transformer']
---
Prompt45;Tuning is a new paradigm for finetuning pre45;trained language models in a parameter45;efficient way. Here we explore the use of HyperNetworks to generate hyper45;prompts we propose HyperPrompt a novel architecture for prompt45;based task45;conditioning of self45;attention in Transformers. The hyper45;prompts are end45;to45;end learnable via generation by a HyperNetwork. HyperPrompt allows the network to learn task45;specific feature maps where the hyper45;prompts serve as task global memories for the queries to attend to at the same time enabling flexible information sharing among tasks. We show that HyperPrompt is competitive against strong multi45;task learning baselines with as few as 0.1437; of additional task45;conditioning parameters achieving great parameter and computational efficiency. Through extensive empirical experiments we demonstrate that HyperPrompt can achieve superior performances over strong T5 multi45;task learning baselines and parameter45;efficient adapter variants including Prompt45;Tuning and HyperFormer++ on Natural Language Understanding benchmarks of GLUE and SuperGLUE across many model sizes.
