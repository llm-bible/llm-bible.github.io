---
layout: publication
title: Llms Can Find Mathematical Reasoning Mistakes By Pedagogical Chain45;of45;thought
authors: Jiang Zhuoxuan, Peng Haoyuan, Feng Shanshan, Li Fan, Li Dongsheng
conference: "Arxiv"
year: 2024
bibkey: jiang2024llms
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.06705"}
tags: ['Merging', 'Prompting', 'Reinforcement Learning']
---
Self45;correction is emerging as a promising approach to mitigate the issue of hallucination in Large Language Models (LLMs). To facilitate effective self45;correction recent research has proposed mistake detection as its initial step. However current literature suggests that LLMs often struggle with reliably identifying reasoning mistakes when using simplistic prompting strategies. To address this challenge we introduce a unique prompting strategy termed the Pedagogical Chain45;of45;Thought (PedCoT) which is specifically designed to guide the identification of reasoning mistakes particularly mathematical reasoning mistakes. PedCoT consists of pedagogical principles for prompts (PPP) design two45;stage interaction process (TIP) and grounded PedCoT prompts all inspired by the educational theory of the Bloom Cognitive Model (BCM). We evaluate our approach on two public datasets featuring math problems of varying difficulty levels. The experiments demonstrate that our zero45;shot prompting strategy significantly outperforms strong baselines. The proposed method can achieve the goal of reliable mathematical mistake identification and provide a foundation for automatic math answer grading. The results underscore the significance of educational theory serving as domain knowledge in guiding prompting strategy design for addressing challenging tasks with LLMs effectively.
