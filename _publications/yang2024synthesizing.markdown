---
layout: publication
title: Synthesizing Text45;to45;sql Data From Weak And Strong Llms
authors: Yang Jiaxi, Hui Binyuan, Yang Min, Yang Jian, Lin Junyang, Zhou Chang
conference: "Arxiv"
year: 2024
bibkey: yang2024synthesizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.03256"}
tags: ['Pretraining Methods', 'Prompting']
---
The capability gap between open45;source and closed45;source large language models (LLMs) remains a challenge in text45;to45;SQL tasks. In this paper we introduce a synthetic data approach that combines data produced by larger more powerful models (strong models) with error information data generated by smaller not well45;aligned models (weak models). The method not only enhances the domain generalization of text45;to45;SQL models but also explores the potential of error data supervision through preference learning. Furthermore we employ the synthetic data approach for instruction tuning on open45;source LLMs resulting SENSE a specialized text45;to45;SQL model. The effectiveness of SENSE is demonstrated through state45;of45;the45;art results on the SPIDER and BIRD benchmarks bridging the performance gap between open45;source models and methods prompted by closed45;source models.
