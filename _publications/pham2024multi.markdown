---
layout: publication
title: Suri Multi45;constraint Instruction Following For Long45;form Text Generation
authors: Pham Chau Minh, Sun Simeng, Iyyer Mohit
conference: "Arxiv"
year: 2024
bibkey: pham2024multi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.19371"}
  - {name: "Code", url: "https://github.com/chtmp223/suri"}
tags: ['Applications', 'Fine Tuning', 'Has Code', 'Language Modeling', 'Reinforcement Learning']
---
Existing research on instruction following largely focuses on tasks with simple instructions and short responses. In this work we explore multi45;constraint instruction following for generating long45;form text. We create Suri a dataset with 20K human45;written long45;form texts paired with LLM45;generated backtranslated instructions that contain multiple complex constraints. Because of prohibitive challenges associated with collecting human preference judgments on long45;form texts preference45;tuning algorithms such as DPO are infeasible in our setting; thus we propose Instructional ORPO (I45;ORPO) an alignment method based on the ORPO algorithm. Instead of receiving negative feedback from dispreferred responses I45;ORPO obtains negative feedback from synthetically corrupted instructions generated by an LLM. Using Suri we perform supervised and I45;ORPO fine45;tuning on Mistral45;7b45;Instruct45;v0.2. The resulting models Suri45;SFT and Suri45;I45;ORPO generate significantly longer texts (~5K tokens) than base models without significant quality deterioration. Our human evaluation shows that while both SFT and I45;ORPO models satisfy most constraints Suri45;I45;ORPO generations are generally preferred for their coherent and informative incorporation of the constraints. We release our code at https://github.com/chtmp223/suri.
