---
layout: publication
title: 'Pre-training Llms Using Human-like Development Data Corpus'
authors: Khushi Bhardwaj, Raj Sanjay Shah, Sashank Varma
conference: "Arxiv"
year: 2023
bibkey: bhardwaj2023pre
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.04666"}
tags: ['Pre-Training', 'Model Architecture', 'Security', 'Training Techniques', 'BERT']
---
Pre-trained Large Language Models (LLMs) have shown success in a diverse set
of language inference and understanding tasks. The pre-training stage of LLMs
looks at a large corpus of raw textual data. The BabyLM shared task compares
LLM pre-training to human language acquisition, where the number of tokens seen
by 13-year-old kids is magnitudes smaller than the number of tokens seen by
LLMs. In this work, we pre-train and evaluate LLMs on their ability to learn
contextual word representations using roughly the same number of tokens as seen
by children. We provide a strong set of baselines; with different
architectures, evaluation of changes in performance across epochs, and reported
pre-training metrics for the strict small and strict tracks of the task. We
also try to loosely replicate the RoBERTa baseline given by the task organizers
to observe the training robustness to hyperparameter selection and
replicability. We provide the submission details to the strict and strict-small
tracks in this report.
