---
layout: publication
title: Pre45;training Llms Using Human45;like Development Data Corpus
authors: Bhardwaj Khushi, Shah Raj Sanjay, Varma Sashank
conference: "Arxiv"
year: 2023
bibkey: bhardwaj2023pre
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.04666"}
tags: ['BERT', 'Ethics And Bias', 'Model Architecture', 'Security', 'Training Techniques']
---
Pre45;trained Large Language Models (LLMs) have shown success in a diverse set of language inference and understanding tasks. The pre45;training stage of LLMs looks at a large corpus of raw textual data. The BabyLM shared task compares LLM pre45;training to human language acquisition where the number of tokens seen by 1345;year45;old kids is magnitudes smaller than the number of tokens seen by LLMs. In this work we pre45;train and evaluate LLMs on their ability to learn contextual word representations using roughly the same number of tokens as seen by children. We provide a strong set of baselines; with different architectures evaluation of changes in performance across epochs and reported pre45;training metrics for the strict small and strict tracks of the task. We also try to loosely replicate the RoBERTa baseline given by the task organizers to observe the training robustness to hyperparameter selection and replicability. We provide the submission details to the strict and strict45;small tracks in this report.
