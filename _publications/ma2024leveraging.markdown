---
layout: publication
title: Leveraging Large Language Models For Relevance Judgments In Legal Case Retrieval
authors: Ma Shengjie, Chen Chong, Chu Qi, Mao Jiaxin
conference: "Arxiv"
year: 2024
bibkey: ma2024leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.18405"}
tags: ['Few Shot', 'Pretraining Methods', 'RAG']
---
Collecting relevant judgments for legal case retrieval is a challenging and time-consuming task. Accurately judging the relevance between two legal cases requires a considerable effort to read the lengthy text and a high level of domain expertise to extract Legal Facts and make juridical judgments. With the advent of advanced large language models some recent studies have suggested that it is promising to use LLMs for relevance judgment. Nonetheless the method of employing a general large language model for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored. To fill this research gap we devise a novel few-shot workflow tailored to the relevant judgment of legal cases. The proposed workflow breaks down the annotation process into a series of stages imitating the process employed by human annotators and enabling a flexible integration of expert reasoning to enhance the accuracy of relevance judgments. By comparing the relevance judgments of LLMs and human experts we empirically show that we can obtain reliable relevance judgments with the proposed workflow. Furthermore we demonstrate the capacity to augment existing legal case retrieval models through the synthesis of data generated by the large language model.
