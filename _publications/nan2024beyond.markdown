---
layout: publication
title: Beyond The Hype A Dispassionate Look At Vision-language Models In Medical Scenario
authors: Nan Yang, Zhou Huichi, Xing Xiaodan, Yang Guang
conference: "Arxiv"
year: 2024
bibkey: nan2024beyond
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.08704"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Security']
---
Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across diverse tasks garnering significant attention in AI communities. However their performance and reliability in specialized domains such as medicine remain insufficiently assessed. In particular most assessments over-concentrate in evaluating VLMs based on simple Visual Question Answering (VQA) on multi-modality data while ignoring the in-depth characteristic of LVLMs. In this study we introduce RadVUQA a novel Radiological Visual Understanding and Question Answering benchmark to comprehensively evaluate existing LVLMs. RadVUQA mainly validates LVLMs across five dimensions 1) Anatomical understanding assessing the models ability to visually identify biological structures; 2) Multimodal comprehension which involves the capability of interpreting linguistic and visual instructions to produce desired outcomes; 3) Quantitative and spatial reasoning evaluating the models spatial awareness and proficiency in combining quantitative analysis with visual and linguistic information; 4) Physiological knowledge measuring the models capability to comprehend functions and mechanisms of organs and systems; and 5) Robustness which assesses the models capabilities against unharmonised and synthetic data. The results indicate that both generalized LVLMs and medical-specific LVLMs have critical deficiencies with weak multimodal comprehension and quantitative reasoning capabilities. Our findings reveal the large gap between existing LVLMs and clinicians highlighting the urgent need for more robust and intelligent LVLMs. The code and dataset will be available after the acceptance of this paper.
