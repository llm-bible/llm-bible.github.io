---
layout: publication
title: 'Mhgpt: A Lightweight Generative Pre-trained Transformer For Mental Health Text Analysis'
authors: Kim Dae-young, Hwa Rebecca, Rahman Muhammad Mahbubur
conference: "Arxiv"
year: 2024
bibkey: kim2024lightweight
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.08261"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
'This paper introduces mhGPT, a lightweight generative pre-trained transformer trained on mental health-related social media and PubMed articles. Fine-tuned for specific mental health tasks, mhGPT was evaluated under limited hardware constraints and compared with state-of-the-art models like MentaLLaMA and Gemma. Despite having only 1.98 billion parameters and using just 5&#37; of the dataset, mhGPT outperformed larger models and matched the performance of models trained on significantly more data. The key contributions include integrating diverse mental health data, creating a custom tokenizer, and optimizing a smaller architecture for low-resource settings. This research could advance AI-driven mental health care, especially in areas with limited computing power.'
