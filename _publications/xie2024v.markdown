---
layout: publication
title: 'V-DPO: Mitigating Hallucination In Large Vision Language Models Via Vision-guided Direct Preference Optimization'
authors: Yuxi Xie, Guanzhen Li, Xiao Xu, Min-yen Kan
conference: "Arxiv"
year: 2024
bibkey: xie2024v
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.02712'}
  - {name: "Code", url: 'https://github.com/YuxiXie/V-DPO'}
tags: ['Attention Mechanism', 'Has Code', 'Efficiency and Optimization', 'Model Architecture', 'Training Techniques', 'Multimodal Models', 'Reinforcement Learning', 'Ethics and Bias']
---
Large vision-language models (LVLMs) suffer from hallucination, resulting in
misalignment between the output textual response and the input visual content.
Recent research indicates that the over-reliance on the Large Language Model
(LLM) backbone, as one cause of the LVLM hallucination, inherently introduces
bias from language priors, leading to insufficient context attention to the
visual inputs.
  We tackle this issue of hallucination by mitigating such over-reliance
through preference learning. We propose Vision-guided Direct Preference
Optimization (V-DPO) to enhance visual context learning at training time. To
interpret the effectiveness and generalizability of V-DPO on different types of
training data, we construct a synthetic dataset containing both response- and
image-contrast preference pairs, compared against existing human-annotated
hallucination samples. Our approach achieves significant improvements compared
with baseline methods across various hallucination benchmarks. Our analysis
indicates that V-DPO excels in learning from image-contrast preference data,
demonstrating its superior ability to elicit and understand nuances of visual
context. Our code is publicly available at https://github.com/YuxiXie/V-DPO.
