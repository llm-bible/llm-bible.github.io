---
layout: publication
title: "A Survey Of Prompt Engineering Methods In Large Language Models For Different NLP Tasks"
authors: Vatsal Shubham, Dubey Harsh
conference: "Arxiv"
year: 2024
bibkey: vatsal2024survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.12994"}
tags: ['Fine Tuning', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
Large language models (LLMs) have shown remarkable performance on many different Natural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding more to the already existing abilities of LLMs to achieve significant performance gains on various NLP tasks. Prompt engineering requires composing natural language instructions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous state-of-the-art (SoTA) models prompt engineering does not require extensive parameter re-training or fine-tuning based on the given NLP task and thus solely operates on the embedded knowledge of LLMs. Additionally LLM enthusiasts can intelligently extract LLMs knowledge through a basic natural language conversational exchange or prompt engineering allowing more and more people even without deep mathematical machine learning background to experiment with LLMs. With prompt engineering gaining popularity in the last two years researchers have come up with numerous engineering techniques around designing prompts to improve accuracy of information extraction from the LLMs. In this paper we summarize different prompting techniques and club them together based on different NLP tasks that they have been used for. We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task talk about the corresponding LLMs used present a taxonomy diagram and discuss the possible SoTA for specific datasets. In total we read and present a survey of 44 research papers which talk about 39 different prompting methods on 29 different NLP tasks of which most of them have been published in the last two years.
