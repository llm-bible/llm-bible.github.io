---
layout: publication
title: Qilin45;med45;vl Towards Chinese Large Vision45;language Model For General Healthcare
authors: Liu Junling, Wang Ziming, Ye Qichen, Chong Dading, Zhou Peilin, Hua Yining
conference: "Arxiv"
year: 2023
bibkey: liu2023qilin
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.17956"}
tags: ['Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However there is a noticeable lack of models in languages other than English and models that can interpret multi45;modal input which is crucial for global healthcare accessibility. In response this study introduces Qilin45;Med45;VL the first Chinese large vision45;language model designed to integrate the analysis of textual and visual data. Qilin45;Med45;VL combines a pre45;trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two45;stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the models ability to generate medical captions and answer complex medical queries. We also release ChiMed45;VL a dataset consisting of more than 1M image45;text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.
