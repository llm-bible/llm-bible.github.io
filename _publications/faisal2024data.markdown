---
layout: publication
title: Data45;augmentation45;based Dialectal Adaptation For Llms
authors: Faisal Fahim, Anastasopoulos Antonios
conference: "Arxiv"
year: 2024
bibkey: faisal2024data
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.08092"}
  - {name: "Code", url: "https://github.com/ffaisal93/dialect&#95;copa"}
tags: ['Applications', 'BERT', 'Has Code', 'Model Architecture', 'RAG', 'Reinforcement Learning']
---
This report presents GMUNLPs participation to the Dialect45;Copa shared task at VarDial 2024 which focuses on evaluating the commonsense reasoning capabilities of large language models (LLMs) on South Slavic micro45;dialects. The task aims to assess how well LLMs can handle non45;standard dialectal varieties as their performance on standard languages is already well45;established. We propose an approach that combines the strengths of different types of language models and leverages data augmentation techniques to improve task performance on three South Slavic dialects Chakavian Cherkano and Torlak. We conduct experiments using a language45;family45;focused encoder45;based model (BERTic) and a domain45;agnostic multilingual model (AYA45;101). Our results demonstrate that the proposed data augmentation techniques lead to substantial performance gains across all three test datasets in the open45;source model category. This work highlights the practical utility of data augmentation and the potential of LLMs in handling non45;standard dialectal varieties contributing to the broader goal of advancing natural language understanding in low45;resource and dialectal settings. Codehttps://github.com/ffaisal93/dialect&#95;copa
