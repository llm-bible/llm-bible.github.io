---
layout: publication
title: Data-augmentation-based Dialectal Adaptation For Llms
authors: Faisal Fahim, Anastasopoulos Antonios
conference: "Arxiv"
year: 2024
bibkey: faisal2024data
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.08092"}
  - {name: "Code", url: "https://github.com/ffaisal93/dialect_copa"}
tags: ['Applications', 'BERT', 'Has Code', 'Model Architecture', 'RAG', 'Reinforcement Learning']
---
This report presents GMUNLPs participation to the Dialect-Copa shared task at VarDial 2024 which focuses on evaluating the commonsense reasoning capabilities of large language models (LLMs) on South Slavic micro-dialects. The task aims to assess how well LLMs can handle non-standard dialectal varieties as their performance on standard languages is already well-established. We propose an approach that combines the strengths of different types of language models and leverages data augmentation techniques to improve task performance on three South Slavic dialects Chakavian Cherkano and Torlak. We conduct experiments using a language-family-focused encoder-based model (BERTic) and a domain-agnostic multilingual model (AYA-101). Our results demonstrate that the proposed data augmentation techniques lead to substantial performance gains across all three test datasets in the open-source model category. This work highlights the practical utility of data augmentation and the potential of LLMs in handling non-standard dialectal varieties contributing to the broader goal of advancing natural language understanding in low-resource and dialectal settings. Codehttps://github.com/ffaisal93/dialect\_copa
