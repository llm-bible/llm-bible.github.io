---
layout: publication
title: 'Position: Stop Acting Like Language Model Agents Are Normal Agents'
authors: Elija Perrier, Michael Timothy Bennett
conference: "Arxiv"
year: 2025
bibkey: perrier2025stop
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.10420'}
tags: ['Agentic', 'Tools', 'Applications']
---
Language Model Agents (LMAs) are increasingly treated as capable of
autonomously navigating interactions with humans and tools. Their design and
deployment tends to presume they are normal agents capable of sustaining
coherent goals, adapting across contexts and acting with a measure of
intentionality. These assumptions are critical to prospective use cases in
industrial, social and governmental settings. But LMAs are not normal agents.
They inherit the structural problems of the large language models (LLMs) around
which they are built: hallucinations, jailbreaking, misalignment and
unpredictability. In this Position paper we argue LMAs should not be treated as
normal agents, because doing so leads to problems that undermine their utility
and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs.
Despite scaffolding such as external memory and tools, they remain
ontologically stateless, stochastic, semantically sensitive, and linguistically
intermediated. These pathologies destabilise the ontological properties of LMAs
including identifiability, continuity, persistence and and consistency,
problematising their claim to agency. In response, we argue LMA ontological
properties should be measured before, during and after deployment so that the
negative effects of pathologies can be mitigated.
