---
layout: publication
title: 'Contextual Cues In Machine Translation: Investigating The Potential Of Multi-source Input Strategies In Llms And NMT Systems'
authors: Lia Shahnazaryan, Patrick Simianer, Joern Wuebker
conference: "Arxiv"
year: 2025
bibkey: shahnazaryan2025contextual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.07195"}
tags: ['Model Architecture', 'GPT', 'Applications', 'Merging']
---
We explore the impact of multi-source input strategies on machine translation
(MT) quality, comparing GPT-4o, a large language model (LLM), with a
traditional multilingual neural machine translation (NMT) system. Using
intermediate language translations as contextual cues, we evaluate their
effectiveness in enhancing English and Chinese translations into Portuguese.
Results suggest that contextual information significantly improves translation
quality for domain-specific datasets and potentially for linguistically distant
language pairs, with diminishing returns observed in benchmarks with high
linguistic variability. Additionally, we demonstrate that shallow fusion, a
multi-source approach we apply within the NMT system, shows improved results
when using high-resource languages as context for other translation pairs,
highlighting the importance of strategic context language selection.
