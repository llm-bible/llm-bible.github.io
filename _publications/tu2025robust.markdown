---
layout: publication
title: 'Rbft: Robust Fine-tuning For Retrieval-augmented Generation Against Retrieval Defects'
authors: Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai
conference: "Arxiv"
year: 2025
bibkey: tu2025robust
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.18365'}
tags: ['RAG', 'Efficiency and Optimization', 'Security', 'Training Techniques', 'Fine-Tuning', 'Reinforcement Learning', 'Pretraining Methods']
---
Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieved from a knowledge base. However, its
effectiveness is fundamentally constrained by the reliability of both the
retriever and the knowledge base. In real-world scenarios, imperfections in
these components often lead to the retrieval of noisy, irrelevant, or
misleading counterfactual information, ultimately undermining the
trustworthiness of RAG systems. To address this challenge, we propose Robust
Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against
retrieval defects through two targeted fine-tuning tasks. Experimental results
demonstrate that RbFT significantly improves the robustness of RAG systems
across diverse retrieval conditions, surpassing existing methods while
maintaining high inference efficiency and compatibility with other robustness
techniques.
