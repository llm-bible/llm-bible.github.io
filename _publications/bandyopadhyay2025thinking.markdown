---
layout: publication
title: 'Thinking Machines: A Survey Of LLM Based Reasoning Strategies'
authors: Dibyanayan Bandyopadhyay, Soham Bhattacharjee, Asif Ekbal
conference: "Arxiv"
year: 2025
bibkey: bandyopadhyay2025thinking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.10814'}
tags: ['Security', 'Survey Paper']
---
Large Language Models (LLMs) are highly proficient in language-based tasks.
Their language capabilities have positioned them at the forefront of the future
AGI (Artificial General Intelligence) race. However, on closer inspection,
Valmeekam et al. (2024); Zecevic et al. (2023); Wu et al. (2024) highlight a
significant gap between their language proficiency and reasoning abilities.
Reasoning in LLMs and Vision Language Models (VLMs) aims to bridge this gap by
enabling these models to think and re-evaluate their actions and responses.
Reasoning is an essential capability for complex problem-solving and a
necessary step toward establishing trust in Artificial Intelligence (AI). This
will make AI suitable for deployment in sensitive domains, such as healthcare,
banking, law, defense, security etc. In recent times, with the advent of
powerful reasoning models like OpenAI O1 and DeepSeek R1, reasoning endowment
has become a critical research topic in LLMs. In this paper, we provide a
detailed overview and comparison of existing reasoning techniques and present a
systematic survey of reasoning-imbued language models. We also study current
challenges and present our findings.
