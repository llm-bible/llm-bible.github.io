---
layout: publication
title: 'Hyacinth6b: A Large Language Model For Traditional Chinese'
authors: Chih-wei Song, Yin-te Tsai
conference: "Arxiv"
year: 2024
bibkey: song2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.13334"}
tags: ['RAG', 'Training Techniques', 'Fine-Tuning']
---
This research's primary motivation of this study is to address the high
hardware and computational demands typically associated with LLMs.Therefore,our
goal is to find a balance between model lightness and performance,striving to
maximize performance while using a comparatively lightweight model. Hyacinth6B
was developed with this objective in mind,aiming to fully leverage the core
capabilities of LLMs without incurring substantial resource costs, effectively
pushing the boundaries of smaller model's performance. The training approach
involves parameter efficient finetuning using the LoRA method.
