---
layout: publication
title: Reliability Check An Analysis Of Gpt45;3s Response To Sensitive Topics And Prompt Wording
authors: Khatun Aisha, Brown Daniel G.
conference: "Arxiv"
year: 2023
bibkey: khatun2023reliability
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.06199"}
  - {name: "Code", url: "https://github.com/tanny411/GPT3&#45;Reliability&#45;Check"}
tags: ['Agentic', 'Applications', 'GPT', 'Has Code', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
Large language models (LLMs) have become mainstream technology with their versatile use cases and impressive performance. Despite the countless out45;of45;the45;box applications LLMs are still not reliable. A lot of work is being done to improve the factual accuracy consistency and ethical standards of these models through fine45;tuning prompting and Reinforcement Learning with Human Feedback (RLHF) but no systematic analysis of the responses of these models to different categories of statements or on their potential vulnerabilities to simple prompting changes is available. In this work we analyze what confuses GPT45;3 how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. We find that GPT45;3 correctly disagrees with obvious Conspiracies and Stereotypes but makes mistakes with common Misconceptions and Controversies. The model responses are inconsistent across prompts and settings highlighting GPT45;3s unreliability. Dataset and code of our analysis is available in https://github.com/tanny411/GPT3&#45;Reliability&#45;Check.
