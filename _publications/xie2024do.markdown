---
layout: publication
title: 'Do Large Language Models Truly Grasp Mathematics? An Empirical Exploration From Cognitive Psychology'
authors: Wei Xie, Shuoyoucheng Ma, Zhenhua Wang, Enze Wang, Kai Chen, Xiaobing Sun, Baosheng Wang
conference: "Arxiv"
year: 2024
bibkey: xie2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.14979"}
tags: ['Fine-Tuning', 'RAG', 'Reinforcement Learning', 'Training Techniques', 'Prompting']
---
The cognitive mechanism by which Large Language Models (LLMs) solve
mathematical problems remains a widely debated and unresolved issue. Currently,
there is little interpretable experimental evidence that connects LLMs'
problem-solving with human cognitive psychology.To determine if LLMs possess
human-like mathematical reasoning, we modified the problems used in the human
Cognitive Reflection Test (CRT). Our results show that, even with the use of
Chains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model
(noted for its reasoning capabilities), have a high error rate when solving
these modified CRT problems. Specifically, the average accuracy rate dropped by
up to 50% compared to the original questions.Further analysis of LLMs'
incorrect answers suggests that they primarily rely on pattern matching from
their training data, which aligns more with human intuition (System 1 thinking)
rather than with human-like reasoning (System 2 thinking). This finding
challenges the belief that LLMs have genuine mathematical reasoning abilities
comparable to humans. As a result, this work may adjust overly optimistic views
on LLMs' progress towards artificial general intelligence.
