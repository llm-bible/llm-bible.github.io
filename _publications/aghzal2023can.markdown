---
layout: publication
title: Can Large Language Models Be Good Path Planners A Benchmark And Investigation On Spatial45;temporal Reasoning
authors: Aghzal Mohamed, Plaku Erion, Yao Ziyu
conference: "Arxiv"
year: 2023
bibkey: aghzal2023can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.03249"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'RAG', 'Reinforcement Learning', 'TACL']
---
Large language models (LLMs) have achieved remarkable success across a wide spectrum of tasks; however they still face limitations in scenarios that demand long45;term planning and spatial reasoning. To facilitate this line of research in this work we propose a new benchmark termed textbf123;P125;ath textbf123;P125;lanning from textbf123;N125;atural textbf123;L125;anguage (textbf123;PPNL125;). Our benchmark evaluates LLMs spatial45;temporal reasoning by formulating path planning tasks that require an LLM to navigate to target locations while avoiding obstacles and adhering to constraints. Leveraging this benchmark we systematically investigate LLMs including GPT45;4 via different few45;shot prompting methodologies as well as BART and T5 of various sizes via fine45;tuning. Our experimental results show the promise of few45;shot GPT45;4 in spatial reasoning when it is prompted to reason and act interleavedly although it still fails to perform long45;term temporal reasoning. In contrast while fine45;tuned LLMs achieved impressive results on in45;distribution reasoning tasks they struggled to generalize to larger environments or environments with more obstacles.
