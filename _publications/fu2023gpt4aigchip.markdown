---
layout: publication
title: Gpt4aigchip Towards Next-generation AI Accelerator Design Automation Via Large Language Models
authors: Fu Yonggan, Zhang Yongan, Yu Zhongzhi, Li Sixu, Ye Zhifan, Li Chaojian, Wan Cheng, Lin Yingyan
conference: "Arxiv"
year: 2023
bibkey: fu2023gpt4aigchip
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.10730"}
tags: ['Fine Tuning', 'GPT', 'In Context Learning', 'Model Architecture', 'Prompting', 'RAG', 'Tools']
---
The remarkable capabilities and intricate nature of Artificial Intelligence (AI) have dramatically escalated the imperative for specialized AI accelerators. Nonetheless designing these accelerators for various AI workloads remains both labor- and time-intensive. While existing design exploration and automation tools can partially alleviate the need for extensive human involvement they still demand substantial hardware expertise posing a barrier to non-experts and stifling AI accelerator development. Motivated by the astonishing potential of large language models (LLMs) for generating high-quality content in response to human language instructions we embark on this work to examine the possibility of harnessing LLMs to automate AI accelerator design. Through this endeavor we develop GPT4AIGChip a framework intended to democratize AI accelerator design by leveraging human natural languages instead of domain-specific languages. Specifically we first perform an in-depth investigation into LLMs limitations and capabilities for AI accelerator design thus aiding our understanding of our current position and garnering insights into LLM-powered automated AI accelerator design. Furthermore drawing inspiration from the above insights we develop a framework called GPT4AIGChip which features an automated demo-augmented prompt-generation pipeline utilizing in-context learning to guide LLMs towards creating high-quality AI accelerator design. To our knowledge this work is the first to demonstrate an effective pipeline for LLM-powered automated AI accelerator generation. Accordingly we anticipate that our insights and framework can serve as a catalyst for innovations in next-generation LLM-powered design automation tools.
