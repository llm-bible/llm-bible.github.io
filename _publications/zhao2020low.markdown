---
layout: publication
title: Low45;resource Knowledge45;grounded Dialogue Generation
authors: Zhao Xueliang, Wu Wei, Tao Chongyang, Xu Can, Zhao Dongyan, Yan Rui
conference: "Arxiv"
year: 2020
bibkey: zhao2020low
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2002.10348"}
tags: ['Agentic', 'Applications', 'Training Techniques']
---
Responding with knowledge has been recognized as an important capability for an intelligent conversational agent. Yet knowledge45;grounded dialogues as training data for learning such a response generation model are difficult to obtain. Motivated by the challenge in practice we consider knowledge45;grounded dialogue generation under a natural assumption that only limited training examples are available. In such a low45;resource setting we devise a disentangled response decoder in order to isolate parameters that depend on knowledge45;grounded dialogues from the entire generation model. By this means the major part of the model can be learned from a large number of ungrounded dialogues and unstructured documents while the remaining small parameters can be well fitted using the limited training examples. Evaluation results on two benchmarks indicate that with only 1/8 training data our model can achieve the state45;of45;the45;art performance and generalize well on out45;of45;domain knowledge.
