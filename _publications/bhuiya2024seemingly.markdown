---
layout: publication
title: Seemingly Plausible Distractors In Multi45;hop Reasoning Are Large Language Models Attentive Readers
authors: Bhuiya Neeladri, Schlegel Viktor, Winkler Stefan
conference: "Arxiv"
year: 2024
bibkey: bhuiya2024seemingly
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.05197"}
tags: ['Pretraining Methods']
---
State45;of45;the45;art Large Language Models (LLMs) are accredited with an increasing number of different capabilities ranging from reading comprehension over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on their multi45;hop reasoning capability the ability to identify and integrate information from multiple textual sources. Given the concerns with the presence of simplifying cues in existing multi45;hop reasoning benchmarks which allow models to circumvent the reasoning requirement we set out to investigate whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi45;hop reasoning but they do so in more subtle ways than what was reported about their fine45;tuned pre45;trained language model (PLM) predecessors. Motivated by this finding we propose a challenging multi45;hop reasoning benchmark by generating seemingly plausible multi45;hop reasoning chains which ultimately lead to incorrect answers. We evaluate multiple open and proprietary state45;of45;the45;art LLMs and find that their performance to perform multi45;hop reasoning is affected as indicated by up to 4537; relative decrease in F1 score when presented with such seemingly plausible alternatives. We conduct a deeper analysis and find evidence that while LLMs tend to ignore misleading lexical cues misleading reasoning paths indeed present a significant challenge.
