---
layout: publication
title: 'Enhancing LLM Evaluations: The Garbling Trick'
authors: William F. Bradley
conference: "Arxiv"
year: 2024
bibkey: bradley2024enhancing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.01533'}
tags: ['Reinforcement Learning']
---
As large language models (LLMs) become increasingly powerful, traditional evaluation metrics tend to saturate, making it challenging to distinguish between models. We propose a general method to transform existing LLM evaluations into a series of progressively more difficult tasks. These enhanced evaluations emphasize reasoning capabilities and can reveal relative performance differences that are not apparent in the original assessments.
  To demonstrate the effectiveness of our approach, we create a new multiple-choice test corpus, extend it into a family of evaluations, and assess a collection of LLMs. Our results offer insights into the comparative abilities of these models, particularly highlighting the differences between base LLMs and more recent "reasoning" models.
