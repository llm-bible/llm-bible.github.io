---
layout: publication
title: Parameter45;efficient Finetuning Of Transformers For Source Code
authors: Ayupov Shamil, Chirkova Nadezhda
conference: "Arxiv"
year: 2022
bibkey: ayupov2022parameter
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2212.05901"}
tags: ['Fine Tuning', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Transformer']
---
Pretrained Transformers achieve state45;of45;the45;art performance in various code45;processing tasks but may be too large to be deployed. As software development tools often incorporate modules for various purposes which may potentially use a single instance of the pretrained model it appears relevant to utilize parameter45;efficient fine45;tuning for the pretrained models of code. In this work we test two widely used approaches adapters and LoRA which were initially tested on NLP tasks on four code45;processing tasks. We find that though the efficient fine45;tuning approaches may achieve comparable or higher performance than the standard full fine45;tuning in code understanding tasks they underperform full fine45;tuning in code45;generative tasks. These results underline the importance of testing efficient fine45;tuning approaches on other domains than NLP and motivate future research in efficient fine45;tuning for source code.
