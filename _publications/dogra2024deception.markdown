---
layout: publication
title: 'Deception In Reinforced Autonomous Agents'
authors: Atharvan Dogra, Krishna Pillutla, Ameet Deshpande, Ananya B Sai, John Nay, Tanmay Rajpurohit, Ashwin Kalyan, Balaraman Ravindran
conference: "Arxiv"
year: 2024
bibkey: dogra2024deception
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2405.04325'}
tags: ['Reinforcement Learning', 'Agentic', 'Security', 'Agent']
---
We explore the ability of large language model (LLM)-based agents to engage
in subtle deception such as strategically phrasing and intentionally
manipulating information to misguide and deceive other agents. This harmful
behavior can be hard to detect, unlike blatant lying or unintentional
hallucination. We build an adversarial testbed mimicking a legislative
environment where two LLMs play opposing roles: a corporate *lobbyist*
proposing amendments to bills that benefit a specific company while evading a
*critic* trying to detect this deception. We use real-world legislative bills
matched with potentially affected companies to ground these interactions. Our
results show that LLM lobbyists initially exhibit limited deception against
strong LLM critics which can be further improved through simple verbal
reinforcement, significantly enhancing their deceptive capabilities, and
increasing deception rates by up to 40 points. This highlights the risk of
autonomous agents manipulating other agents through seemingly neutral language
to attain self-serving goals.
