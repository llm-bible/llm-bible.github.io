---
layout: publication
title: 'Rationale Behind Essay Scores: Enhancing S-llm''s Multi-trait Essay Scoring With Rationale Generated By Llms'
authors: Seongyeub Chu, Jongwoo Kim, Bryan Wong, Munyong Yi
conference: "Arxiv"
year: 2024
bibkey: chu2024rationale
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.14202"}
  - {name: "Code", url: "https://github.com/BBeeChu/RMTS.git"}
tags: ['Agentic', 'Training Techniques', 'Reinforcement Learning', 'Pretraining Methods', 'Fine-Tuning', 'Has Code', 'Interpretability and Explainability', 'Prompting']
---
Existing automated essay scoring (AES) has solely relied on essay text
without using explanatory rationales for the scores, thereby forgoing an
opportunity to capture the specific aspects evaluated by rubric indicators in a
fine-grained manner. This paper introduces Rationale-based Multiple Trait
Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates
prompt-engineering-based large language models (LLMs) with a fine-tuning-based
essay scoring model using a smaller large language model (S-LLM). RMTS uses an
LLM-based trait-wise rationale generation system where a separate LLM agent
generates trait-specific rationales based on rubric guidelines, which the
scoring model uses to accurately predict multi-trait scores. Extensive
experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,
show that RMTS significantly outperforms state-of-the-art models and vanilla
S-LLMs in trait-specific scoring. By assisting quantitative assessment with
fine-grained qualitative rationales, RMTS enhances the trait-wise reliability,
providing partial explanations about essays. The code is available at
https://github.com/BBeeChu/RMTS.git.
