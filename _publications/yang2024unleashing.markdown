---
layout: publication
title: Unleashing The Potential Of Large Language Models For Predictive Tabular Tasks In Data Science
authors: Yang Yazheng, Wang Yuqi, Sen Sankalok, Li Lei, Liu Qi
conference: "Arxiv"
year: 2024
bibkey: yang2024unleashing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.20208"}
tags: ['Few Shot', 'In Context Learning', 'Large Scale Training', 'Prompting', 'Training Techniques']
---
In the domain of data science the predictive tasks of classification regression and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore we investigate the practical application of applying the trained model to zero-shot prediction few-shot prediction and in-context learning scenarios. Through extensive experiments our methodology has shown significant improvements over existing benchmarks. These advancements highlight the efficacy of tailoring LLM training to solve table-related problems in data science thereby establishing a new benchmark in the utilization of LLMs for enhancing tabular intelligence.
