---
layout: publication
title: 'Large Language Models'
authors: Douglas Michael R.
conference: "Arxiv"
year: 2023
bibkey: douglas2023large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.05782"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Survey Paper', 'Transformer']
---
Artificial intelligence is making spectacular progress, and one of the best
examples is the development of large language models (LLMs) such as OpenAI's
GPT series. In these lectures, written for readers with a background in
mathematics or physics, we give a brief history and survey of the state of the
art, and describe the underlying transformer architecture in detail. We then
explore some current ideas on how LLMs work and how models trained to predict
the next word in a text are able to perform other tasks displaying
intelligence.
