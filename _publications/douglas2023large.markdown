---
layout: publication
title: 'Large Language Models'
authors: Michael R. Douglas
conference: "Arxiv"
year: 2023
bibkey: douglas2023large
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2307.05782'}
tags: ['Transformer', 'GPT', 'Model Architecture', 'Survey Paper', 'Reinforcement Learning', 'Pretraining Methods']
---
Artificial intelligence is making spectacular progress, and one of the best
examples is the development of large language models (LLMs) such as OpenAI's
GPT series. In these lectures, written for readers with a background in
mathematics or physics, we give a brief history and survey of the state of the
art, and describe the underlying transformer architecture in detail. We then
explore some current ideas on how LLMs work and how models trained to predict
the next word in a text are able to perform other tasks displaying
intelligence.
