---
layout: publication
title: 'Can Llms \(\textit{understand}\) Math? -- Exploring The Pitfalls In Mathematical Reasoning'
authors: Tiasa Singha Roy, Aditeya Baral, Ayush Rajesh Jhaveri, Yusuf Baig
conference: "Arxiv"
year: 2025
bibkey: roy2025can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.15623"}
tags: ['Tools', 'Reinforcement Learning']
---
Large language models (LLMs) demonstrate considerable potential in various natural language tasks but face significant challenges in mathematical reasoning, particularly in executing precise, multi-step logic. However, current evaluation frameworks judge their performance solely based on accuracy, which only accounts for the final answer. This study explores these pitfalls by employing a novel evaluation framework. We propose an evaluation metric called the MAPLE score, which holistically quantifies reasoning misalignment by integrating error rates, redundancy, and validity.
