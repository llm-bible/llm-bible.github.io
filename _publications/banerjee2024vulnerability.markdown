---
layout: publication
title: 'The Vulnerability Of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?'
authors: Sourav Banerjee, Ayushi Agarwal, Eishkaran Singh
conference: "Arxiv"
year: 2024
bibkey: banerjee2024vulnerability
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.03597'}
tags: ['Ethics and Bias', 'Security', 'Tools', 'Survey Paper']
---
The pursuit of leaderboard rankings in Large Language Models (LLMs) has
created a fundamental paradox: models excel at standardized tests while failing
to demonstrate genuine language understanding and adaptability. Our systematic
analysis of NLP evaluation frameworks reveals pervasive vulnerabilities across
the evaluation spectrum, from basic metrics to complex benchmarks like GLUE and
MMLU. These vulnerabilities manifest through benchmark exploitation, dataset
contamination, and evaluation bias, creating a false perception of progress in
language understanding capabilities. Through extensive review of contemporary
evaluation approaches, we identify significant limitations in static benchmark
designs, human evaluation protocols, and LLM-as-judge frameworks, all of which
compromise the reliability of current performance assessments. As LLM
capabilities evolve and existing benchmarks become redundant, we lay the
groundwork for new evaluation methods that resist manipulation, minimize data
contamination, and assess domain-specific tasks. This requires frameworks that
are adapted dynamically, addressing current limitations and providing a more
accurate reflection of LLM performance.
