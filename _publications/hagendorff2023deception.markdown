---
layout: publication
title: Deception Abilities Emerged In Large Language Models
authors: Hagendorff Thilo
conference: "Arxiv"
year: 2023
bibkey: hagendorff2023deception
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.16513"}
tags: ['Agentic', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus aligning them with human values is of great importance. However given the steady increase in reasoning abilities future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state45;of45;the45;art LLMs such as GPT45;4 but were non45;existent in earlier LLMs. We conduct a series of experiments showing that state45;of45;the45;art LLMs are able to understand and induce false beliefs in other agents that their performance in complex deception scenarios can be amplified utilizing chain45;of45;thought reasoning and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum revealing hitherto unknown machine behavior in LLMs our study contributes to the nascent field of machine psychology.
