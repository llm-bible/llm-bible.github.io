---
layout: publication
title: 'Towards Smarter Hiring: Are Zero-shot And Few-shot Pre-trained Llms Ready For HR Spoken Interview Transcript Analysis?'
authors: Subhankar Maity, Aniket Deroy, Sudeshna Sarkar
conference: "Arxiv"
year: 2025
bibkey: maity2025towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.05683"}
tags: ['Few-Shot', 'GPT', 'Model Architecture', 'Reinforcement Learning']
---
This research paper presents a comprehensive analysis of the performance of
prominent pre-trained large language models (LLMs), including GPT-4 Turbo,
GPT-3.5 Turbo, text-davinci-003, text-babbage-001, text-curie-001,
text-ada-001, llama-2-7b-chat, llama-2-13b-chat, and llama-2-70b-chat, in
comparison to expert human evaluators in providing scores, identifying errors,
and offering feedback and improvement suggestions to candidates during mock HR
(Human Resources) interviews. We introduce a dataset called HURIT (Human
Resource Interview Transcripts), which comprises 3,890 HR interview transcripts
sourced from real-world HR interview scenarios. Our findings reveal that
pre-trained LLMs, particularly GPT-4 Turbo and GPT-3.5 Turbo, exhibit
commendable performance and are capable of producing evaluations comparable to
those of expert human evaluators. Although these LLMs demonstrate proficiency
in providing scores comparable to human experts in terms of human evaluation
metrics, they frequently fail to identify errors and offer specific actionable
advice for candidate performance improvement in HR interviews. Our research
suggests that the current state-of-the-art pre-trained LLMs are not fully
conducive for automatic deployment in an HR interview assessment. Instead, our
findings advocate for a human-in-the-loop approach, to incorporate manual
checks for inconsistencies and provisions for improving feedback quality as a
more suitable strategy.
