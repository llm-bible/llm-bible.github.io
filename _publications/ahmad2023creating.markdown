---
layout: publication
title: Creating Trustworthy LLMs Dealing with Hallucinations in Healthcare AI
authors: Ahmad Muhammad Aurangzeb, Yaramis Ilker, Roy Taposh Dutta
conference: "Arxiv"
year: 2023
bibkey: ahmad2023creating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.01463"}
tags: ['ARXIV', 'Ethics And Bias']
---
Large language models have proliferated across multiple domains in as short period of time. There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality coherence and hallucinations. Give the high stakes nature of healthcare many researchers have even cautioned against its usage until these issues are resolved. The key to the implementation and deployment of LLMs in healthcare is to make these models trustworthy transparent (as much possible) and explainable. In this paper we describe the key elements in creating reliable trustworthy and unbiased models as a necessary condition for their adoption in healthcare. Specifically we focus on the quantification validation and mitigation of hallucinations in the context in healthcare. Lastly we discuss how the future of LLMs in healthcare may look like.
