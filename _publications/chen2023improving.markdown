---
layout: publication
title: 'Soulchat: Improving Llms'' Empathy, Listening, And Comfort Abilities Through Fine-tuning With Multi-turn Empathy Conversations'
authors: Yirong Chen, Xiaofen Xing, Jingkai Lin, Huimin Zheng, Zhenyu Wang, Qi Liu, Xiangmin Xu
conference: "Arxiv"
year: 2023
bibkey: chen2023improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.00273"}
tags: ['Pretraining Methods', 'Training Techniques', 'Fine-Tuning', 'Reinforcement Learning']
---
Large language models (LLMs) have been widely applied in various fields due
to their excellent capability for memorizing knowledge and chain of thought
(CoT). When these language models are applied in the field of psychological
counseling, they often rush to provide universal advice. However, when users
seek psychological support, they need to gain empathy, trust, understanding and
comfort, rather than just reasonable advice. To this end, we constructed a
multi-turn empathetic conversation dataset of more than 2 million samples, in
which the input is the multi-turn conversation context, and the target is
empathetic responses that cover expressions such as questioning, comfort,
recognition, listening, trust, emotional support, etc. Experiments have shown
that the empathy ability of LLMs can be significantly enhanced when finetuning
by using multi-turn dialogue history and responses that are closer to the
expression of a psychological consultant.
