---
layout: publication
title: 'Advancing Ai-scientist Understanding: Making LLM Think Like A Physicist With Interpretable Reasoning'
authors: Yinggan Xu, Hana Kimlee, Yijia Xiao, Di Luo
conference: "Arxiv"
year: 2025
bibkey: xu2025advancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.01911"}
tags: ['Agentic', 'Tools', 'Ethics and Bias', 'Interpretability and Explainability', 'Interpretability']
---
Large Language Models (LLMs) are playing an expanding role in physics
research by enhancing reasoning, symbolic manipulation, and numerical
computation. However, ensuring the reliability and interpretability of their
outputs remains a significant challenge. In our framework, we conceptualize the
collaboration between AI and human scientists as a dynamic interplay among
three modules: the reasoning module, the interpretation module, and the
AI-scientist interaction module. Recognizing that effective physics reasoning
demands rigorous logical consistency, quantitative precision, and deep
integration with established theoretical models, we introduce the
interpretation module to improve the understanding of AI-generated outputs,
which is not previously explored in the literature. This module comprises
multiple specialized agents, including summarizers, model builders, UI
builders, and testers, which collaboratively structure LLM outputs within a
physically grounded framework, by constructing a more interpretable science
model. A case study demonstrates that our approach enhances transparency,
facilitates validation, and strengthens AI-augmented reasoning in scientific
discovery.
