---
layout: publication
title: 'Towards Optimizing A Retrieval Augmented Generation Using Large Language Model On Academic Data'
authors: Anum Afzal, Juraj Vladika, Gentrit Fazlija, Andrei Staradubets, Florian Matthes
conference: "Arxiv"
year: 2024
bibkey: afzal2024towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.08438"}
tags: ['Tools', 'GPT', 'Efficiency and Optimization', 'RAG', 'Model Architecture', 'Merging']
---
Given the growing trend of many organizations integrating Retrieval Augmented
Generation (RAG) into their operations, we assess RAG on domain-specific data
and test state-of-the-art models across various optimization techniques. We
incorporate four optimizations; Multi-Query, Child-Parent-Retriever, Ensemble
Retriever, and In-Context-Learning, to enhance the functionality and
performance in the academic domain. We focus on data retrieval, specifically
targeting various study programs at a large technical university. We
additionally introduce a novel evaluation approach, the RAG Confusion Matrix
designed to assess the effectiveness of various configurations within the RAG
framework. By exploring the integration of both open-source (e.g., Llama2,
Mistral) and closed-source (GPT-3.5 and GPT-4) Large Language Models, we offer
valuable insights into the application and optimization of RAG frameworks in
domain-specific contexts. Our experiments show a significant performance
increase when including multi-query in the retrieval phase.
