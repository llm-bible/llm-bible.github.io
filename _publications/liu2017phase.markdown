---
layout: publication
title: Phase Conductor On Multi45;layered Attentions For Machine Comprehension
authors: Liu Rui, Wei Wei, Mao Weiguang, Chikina Maria
conference: "Arxiv"
year: 2017
bibkey: liu2017phase
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1710.10504"}
tags: ['Attention Mechanism', 'Merging', 'Model Architecture']
---
Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question45;aware passage attention model and self45;matching attention model. Our research proposes phase conductor (PhaseCond) for attention models in two meaningful ways. First PhaseCond an architecture of multi45;layered attention models consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow. Second we extend and improve the dot45;product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives. We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset showing that our model significantly outperforms both state45;of45;the45;art single45;layered and multiple45;layered attention models. We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi45;layered attention models.
