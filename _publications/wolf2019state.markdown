---
layout: publication
title: 'Huggingface''s Transformers: State-of-the-art Natural Language Processing'
authors: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick Von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush
conference: "Arxiv"
year: 2019
bibkey: wolf2019state
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1910.03771"}
  - {name: "Code", url: "https://github.com/huggingface/transformers"}
tags: ['Transformer', 'Tools', 'Applications', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques', 'Has Code', 'Pretraining Methods']
---
Recent progress in natural language processing has been driven by advances in
both model architecture and model pretraining. Transformer architectures have
facilitated building higher-capacity models and pretraining has made it
possible to effectively utilize this capacity for a wide variety of tasks.
\textit\{Transformers\} is an open-source library with the goal of opening up
these advances to the wider machine learning community. The library consists of
carefully engineered state-of-the art Transformer architectures under a unified
API. Backing this library is a curated collection of pretrained models made by
and available for the community. \textit\{Transformers\} is designed to be
extensible by researchers, simple for practitioners, and fast and robust in
industrial deployments. The library is available at
\url\{https://github.com/huggingface/transformers\}.
