---
layout: publication
title: 'Exploring Large Language Models For Relevance Judgments In Tetun'
authors: Gabriel De Jesus, SÃ©rgio Nunes
conference: "Arxiv"
year: 2024
bibkey: dejesus2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.07299"}
tags: ['Reinforcement Learning']
---
The Cranfield paradigm has served as a foundational approach for developing
test collections, with relevance judgments typically conducted by human
assessors. However, the emergence of large language models (LLMs) has
introduced new possibilities for automating these tasks. This paper explores
the feasibility of using LLMs to automate relevance assessments, particularly
within the context of low-resource languages. In our study, LLMs are employed
to automate relevance judgment tasks, by providing a series of query-document
pairs in Tetun as the input text. The models are tasked with assigning
relevance scores to each pair, where these scores are then compared to those
from human annotators to evaluate the inter-annotator agreement levels. Our
investigation reveals results that align closely with those reported in studies
of high-resource languages.
