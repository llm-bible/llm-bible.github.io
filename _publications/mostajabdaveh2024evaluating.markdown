---
layout: publication
title: 'Evaluating LLM Reasoning In The Operations Research Domain With ORQA'
authors: Mahdi Mostajabdaveh, Timothy T. Yu, Samarendra Chandan Bindu Dash, Rindranirina Ramamonjison, Jabo Serge Byusa, Giuseppe Carenini, Zirui Zhou, Yong Zhang
conference: "Arxiv"
year: 2024
bibkey: mostajabdaveh2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.17874"}
tags: ['Efficiency and Optimization', 'Applications', 'Reinforcement Learning']
---
In this paper, we introduce and apply Operations Research Question Answering
(ORQA), a new benchmark designed to assess the generalization capabilities of
Large Language Models (LLMs) in the specialized technical domain of Operations
Research (OR). This benchmark evaluates whether LLMs can emulate the knowledge
and reasoning skills of OR experts when confronted with diverse and complex
optimization problems. The dataset, developed by OR experts, features
real-world optimization problems that demand multistep reasoning to construct
their mathematical models. Our evaluations of various open source LLMs, such as
LLaMA 3.1, DeepSeek, and Mixtral, reveal their modest performance, highlighting
a gap in their ability to generalize to specialized technical domains. This
work contributes to the ongoing discourse on LLMs generalization capabilities,
offering valuable insights for future research in this area. The dataset and
evaluation code are publicly available.
