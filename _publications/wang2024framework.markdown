---
layout: publication
title: 'Testagent: A Framework For Domain-adaptive Evaluation Of Llms Via Dynamic Benchmark Construction And Exploratory Interaction'
authors: Wanying Wang, Zeyu Ma, Pengfei Liu, Mingang Chen
conference: "Arxiv"
year: 2024
bibkey: wang2024framework
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.11507'}
tags: ['Agentic', 'RAG', 'Tools', 'Fine-Tuning', 'Reinforcement Learning']
---
As large language models (LLMs) are increasingly deployed to various vertical domains, automatically evaluating their performance across different domains remains a critical challenge. Current evaluation methods often rely on static and resource-intensive datasets that are not aligned with real-world requirements and lack cross-domain adaptability. To address these limitations, we revisit the evaluation process and introduce two key concepts: \textbf\{Benchmark+\}, which extends the traditional question-answer benchmark into a more flexible ``strategy-criterion'' format; and \textbf\{Assessment+\}, which enhances the interaction process to facilitate deeper exploration and comprehensive analysis from multiple perspectives. We propose \textbf\{\textsc\{TestAgent\}\}, an agent-based evaluation framework that implements these concepts using retrieval-augmented generation and reinforcement learning. \textsc\{TestAgent\} enables automatic dynamic benchmark generation and in-depth assessment across diverse vertical domains. Experiments on tasks ranging from constructing multiple vertical domain evaluations to transforming static benchmarks into dynamic forms demonstrate the effectiveness of \textsc\{TestAgent\}. This work provides a novel perspective on automatic evaluation methods for domain-specific LLMs, offering a pathway for domain-adaptive dynamic benchmark construction and exploratory assessment.
