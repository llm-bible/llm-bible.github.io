---
layout: publication
title: Multimodal Attention For Neural Machine Translation
authors: "Ozan Caglayan, Lo\xEFc Barrault, Fethi Bougares"
conference: Arxiv
year: 2016
citations: 39
bibkey: caglayan2016multimodal
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1609.03976'}]
tags: [Multimodal Models, Transformer, Attention Mechanism]
---
The attention mechanism is an important part of the neural machine
translation (NMT) where it was reported to produce richer source representation
compared to fixed-length encoding sequence-to-sequence models. Recently, the
effectiveness of attention has also been explored in the context of image
captioning. In this work, we assess the feasibility of a multimodal attention
mechanism that simultaneously focus over an image and its natural language
description for generating a description in another language. We train several
variants of our proposed attention mechanism on the Multi30k multilingual image
captioning dataset. We show that a dedicated attention for each modality
achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT
baseline.