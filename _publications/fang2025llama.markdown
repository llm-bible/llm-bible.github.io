---
layout: publication
title: 'Llama-omni2: Llm-based Real-time Spoken Chatbot With Autoregressive Streaming Speech Synthesis'
authors: Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, Yang Feng
conference: "Arxiv"
year: 2025
bibkey: fang2025llama
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.02625"}
tags: ['Applications', 'Pretraining Methods', 'GPT']
---
Real-time, intelligent, and natural speech interaction is an essential part
of the next-generation human-computer interaction. Recent advancements have
showcased the potential of building intelligent spoken chatbots based on large
language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of
speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable
of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built
upon the Qwen2.5 series models, integrating a speech encoder and an
autoregressive streaming speech decoder. Despite being trained on only 200K
multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong
performance on several spoken question answering and speech instruction
following benchmarks, surpassing previous state-of-the-art SpeechLMs like
GLM-4-Voice, which was trained on millions of hours of speech data.
