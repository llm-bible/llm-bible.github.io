---
layout: publication
title: 'LLM Can Be A Dangerous Persuader: Empirical Study Of Persuasion Safety In Large Language Models'
authors: Minqian Liu, Zhiyang Xu, Xinyi Zhang, Heajun An, Sarvech Qadir, Qi Zhang, Pamela J. Wisniewski, Jin-hee Cho, Sang Won Lee, Ruoxi Jia, Lifu Huang
conference: "Arxiv"
year: 2025
bibkey: liu2025llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.10430"}
tags: ['Responsible AI', 'Tools', 'RAG', 'Model Architecture', 'Reinforcement Learning', 'Attention Mechanism']
---
Recent advancements in Large Language Models (LLMs) have enabled them to
approach human-level persuasion capabilities. However, such potential also
raises concerns about the safety risks of LLM-driven persuasion, particularly
their potential for unethical influence through manipulation, deception,
exploitation of vulnerabilities, and many other harmful tactics. In this work,
we present a systematic investigation of LLM persuasion safety through two
critical aspects: (1) whether LLMs appropriately reject unethical persuasion
tasks and avoid unethical strategies during execution, including cases where
the initial persuasion goal appears ethically neutral, and (2) how influencing
factors like personality traits and external pressures affect their behavior.
To this end, we introduce PersuSafety, the first comprehensive framework for
the assessment of persuasion safety which consists of three stages, i.e.,
persuasion scene creation, persuasive conversation simulation, and persuasion
safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and
15 common unethical strategies. Through extensive experiments across 8 widely
used LLMs, we observe significant safety concerns in most LLMs, including
failing to identify harmful persuasion tasks and leveraging various unethical
persuasion strategies. Our study calls for more attention to improve safety
alignment in progressive and goal-driven conversations such as persuasion.
