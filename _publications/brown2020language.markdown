---
layout: publication
title: Language Models Are Few45;shot Learners
authors: Brown Tom B., Mann Benjamin, Ryder Nick, Subbiah Melanie, Kaplan Jared, Dhariwal Prafulla, Neelakantan Arvind, Shyam Pranav, Sastry Girish, Askell Amanda, Agarwal Sandhini, Herbert-voss Ariel, Krueger Gretchen, Henighan Tom, Child Rewon, Ramesh Aditya, Ziegler Daniel M., Wu Jeffrey, Winter Clemens, Hesse Christopher, Chen Mark, Sigler Eric, Litwin Mateusz, Gray Scott, Chess Benjamin, Clark Jack, Berner Christopher, Mccandlish Sam, Radford Alec, Sutskever Ilya, Amodei Dario
conference: "Arxiv"
year: 2020
bibkey: brown2020language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2005.14165"}
tags: ['Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre45;training on a large corpus of text followed by fine45;tuning on a specific task. While typically task45;agnostic in architecture this method still requires task45;specific fine45;tuning datasets of thousands or tens of thousands of examples. By contrast humans can generally perform a new language task from only a few examples or from simple instructions 45; something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task45;agnostic few45;shot performance sometimes even reaching competitiveness with prior state45;of45;the45;art fine45;tuning approaches. Specifically we train GPT45;3 an autoregressive language model with 175 billion parameters 10x more than any previous non45;sparse language model and test its performance in the few45;shot setting. For all tasks GPT45;3 is applied without any gradient updates or fine45;tuning with tasks and few45;shot demonstrations specified purely via text interaction with the model. GPT45;3 achieves strong performance on many NLP datasets including translation question45;answering and cloze tasks as well as several tasks that require on45;the45;fly reasoning or domain adaptation such as unscrambling words using a novel word in a sentence or performing 345;digit arithmetic. At the same time we also identify some datasets where GPT45;3s few45;shot learning still struggles as well as some datasets where GPT45;3 faces methodological issues related to training on large web corpora. Finally we find that GPT45;3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT45;3 in general.
