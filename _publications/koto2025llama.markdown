---
layout: publication
title: 'Llama-3.1-sherkala-8b-chat: An Open Large Language Model For Kazakh'
authors: Fajri Koto, Rituraj Joshi, Nurdaulet Mukhituly, Yuxia Wang, Zhuohan Xie, Rahul Pal, Daniil Orel, Parvez Mullah, Diana Turmakhan, Maiya Goloburda, Mohammed Kamran, Samujjwal Ghosh, Bokang Jia, Jonibek Mansurov, Mukhammed Togmanov, Debopriyo Banerjee, Nurkhan Laiyk, Akhmed Sakip, Xudong Han, Ekaterina Kochmar, Alham Fikri Aji, Aaryamonvikram Singh, Alok Anil Jadhav, Satheesh Katipomu, Samta Kamboj, Monojit Choudhury, Gurpreet Gosal, Gokul Ramakrishnan, Biswajit Mishra, Sarath Chandran, Avraham Sheinin, Natalia Vassilieva, Neha Sengupta, Larry Murray, Preslav Nakov
conference: "Arxiv"
year: 2025
bibkey: koto2025llama
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.01493"}
tags: ['Fine-Tuning', 'Responsible AI', 'Applications', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods']
---
Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a
state-of-the-art instruction-tuned open generative large language model (LLM)
designed for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM
advancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model,
Sherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian,
and Turkish. With 8 billion parameters, it demonstrates strong knowledge and
reasoning abilities in Kazakh, significantly outperforming existing open Kazakh
and multilingual models of similar scale while achieving competitive
performance in English. We release Sherkala-Chat (8B) as an open-weight
instruction-tuned model and provide a detailed overview of its training,
fine-tuning, safety alignment, and evaluation, aiming to advance research and
support diverse real-world applications.
