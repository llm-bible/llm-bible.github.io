---
layout: publication
title: 'A Survey On Llm-based Code Generation For Low-resource And Domain-specific Programming Languages'
authors: Sathvik Joel, Jie Jw Wu, Fatemeh H. Fard
conference: "Arxiv"
year: 2024
bibkey: joel2024survey
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.03981'}
tags: ['ACL', 'RAG', 'Efficiency and Optimization', 'Model Architecture', 'Applications', 'Survey Paper', 'Reinforcement Learning', 'TACL']
---
Large Language Models (LLMs) have shown impressive capabilities in code
generation for popular programming languages. However, their performance on
Low-Resource Programming Languages (LRPLs) and Domain-Specific Languages (DSLs)
remains a significant challenge, affecting millions of developers-3.5 million
users in Rust alone-who cannot fully utilize LLM capabilities. LRPLs and DSLs
encounter unique obstacles, including data scarcity and, for DSLs, specialized
syntax that is poorly represented in general-purpose datasets.
  Addressing these challenges is crucial, as LRPLs and DSLs enhance development
efficiency in specialized domains, such as finance and science. While several
surveys discuss LLMs in software engineering, none focus specifically on the
challenges and opportunities associated with LRPLs and DSLs. Our survey fills
this gap by systematically reviewing the current state, methodologies, and
challenges in leveraging LLMs for code generation in these languages. We
filtered 111 papers from over 27,000 published studies between 2020 and 2024 to
evaluate the capabilities and limitations of LLMs in LRPLs and DSLs. We report
the LLMs used, benchmarks, and metrics for evaluation, strategies for enhancing
performance, and methods for dataset collection and curation.
  We identified four main evaluation techniques and several metrics for
assessing code generation in LRPLs and DSLs. Our analysis categorizes
improvement methods into six groups and summarizes novel architectures proposed
by researchers. Despite various techniques and metrics, a standard approach and
benchmark dataset for evaluating code generation in LRPLs and DSLs are lacking.
This survey serves as a resource for researchers and practitioners at the
intersection of LLMs, software engineering, and specialized programming
languages, laying the groundwork for future advancements in code generation for
LRPLs and DSLs.
