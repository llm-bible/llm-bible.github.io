---
layout: publication
title: 'Finetuning Llms For Comparative Assessment Tasks'
authors: Vatsal Raina, Adian Liusie, Mark Gales
conference: "Arxiv"
year: 2024
bibkey: raina2024finetuning
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2409.15979'}
tags: ['Reinforcement Learning', 'Tools', 'Training Techniques']
---
Automated assessment in natural language generation is a challenging task.
Instruction-tuned large language models (LLMs) have shown promise in
reference-free evaluation, particularly through comparative assessment.
However, the quadratic computational complexity of pairwise comparisons limits
its scalability. To address this, efficient comparative assessment has been
explored by applying comparative strategies on zero-shot LLM probabilities. We
propose a framework for finetuning LLMs for comparative assessment to align the
model's output with the target distribution of comparative probabilities. By
training on soft probabilities, our approach improves state-of-the-art
performance while maintaining high performance with an efficient subset of
comparisons.
