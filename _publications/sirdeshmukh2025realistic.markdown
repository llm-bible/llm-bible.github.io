---
layout: publication
title: 'Multichallenge: A Realistic Multi-turn Conversation Evaluation Benchmark Challenging To Frontier Llms'
authors: Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, Chen Xing
conference: "Arxiv"
year: 2025
bibkey: sirdeshmukh2025realistic
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.17399'}
tags: ['RAG', 'Applications']
---
We present MultiChallenge, a pioneering benchmark evaluating large language
models (LLMs) on conducting multi-turn conversations with human users, a
crucial yet underexamined capability for their applications. MultiChallenge
identifies four categories of challenges in multi-turn conversations that are
not only common and realistic among current human-LLM interactions, but are
also challenging to all current frontier LLMs. All 4 challenges require
accurate instruction-following, context allocation, and in-context reasoning at
the same time. We also develop LLM as judge with instance-level rubrics to
facilitate an automatic evaluation method with fair agreement with experienced
human raters. Despite achieving near-perfect scores on existing multi-turn
evaluation benchmarks, all frontier models have less than 50% accuracy on
MultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving
just a 41.4% average accuracy.
