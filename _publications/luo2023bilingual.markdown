---
layout: publication
title: Taiyi A Bilingual Fine45;tuned Large Language Model For Diverse Biomedical Tasks
authors: Luo Ling, Ning Jinzhong, Zhao Yingwen, Wang Zhijun, Ding Zeyuan, Chen Peng, Fu Weiru, Han Qinyu, Xu Guangtao, Qiu Yunzhi, Pan Dinghao, Li Jiru, Li Hao, Feng Wenduo, Tu Senbo, Liu Yuqi, Yang Zhihao, Wang Jian, Sun Yuanyuan, Lin Hongfei
conference: "Journal of the American Medical Informatics Association"
year: 2023
bibkey: luo2023bilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.11608"}
tags: ['Applications', 'RAG']
---
Objective Most existing fine45;tuned biomedical large language models (LLMs) focus on enhancing performance in monolingual biomedical question answering and conversation tasks. To investigate the effectiveness of the fine45;tuned LLMs on diverse biomedical NLP tasks in different languages We present Taiyi a bilingual fine45;tuned LLM for diverse biomedical tasks. Materials and Methods We first curated a comprehensive collection of 140 existing biomedical text mining datasets (102 English and 38 Chinese datasets) across over 10 task types. Subsequently a two45;stage strategy is proposed for supervised fine45;tuning to optimize the model performance across varied tasks. Results Experimental results on 13 test sets covering named entity recognition relation extraction text classification question answering tasks demonstrate that Taiyi achieves superior performance compared to general LLMs. The case study involving additional biomedical NLP tasks further shows Taiyis considerable potential for bilingual biomedical multi45;tasking. Conclusion Leveraging rich high45;quality biomedical corpora and developing effective fine45;tuning strategies can significantly improve the performance of LLMs within the biomedical domain. Taiyi shows the bilingual multi45;tasking capability through supervised fine45;tuning. However those tasks such as information extraction that are not generation tasks in nature remain challenging for LLM45;based generative approaches and they still underperform the conventional discriminative approaches of smaller language models.
