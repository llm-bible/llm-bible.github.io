---
layout: publication
title: "Interpretable User Satisfaction Estimation For Conversational Systems With Large Language Models"
authors: Lin Ying-chun, Neville Jennifer, Stokes Jack W., Yang Longqi, Safavi Tara, Wan Mengting, Counts Scott, Suri Siddharth, Andersen Reid, Xu Xiaofeng, Gupta Deepak, Jauhar Sujay Kumar, Song Xia, Buscher Georg, Tiwary Saurabh, Hecht Brent, Teevan Jaime
conference: "Arxiv"
year: 2024
bibkey: lin2024interpretable
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.12388"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Tools']
---
Accurate and interpretable user satisfaction estimation (USE) is critical for understanding evaluating and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. The resulting method Supervised Prompting for User satisfaction Rubrics (SPUR) not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.
