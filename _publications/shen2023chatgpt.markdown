---
layout: publication
title: In ChatGPT We Trust Measuring and Characterizing the Reliability of ChatGPT
authors: Shen Xinyue, Chen Zeyuan, Backes Michael, Zhang Yang
conference: "Arxiv"
year: 2023
bibkey: shen2023chatgpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.08979"}
tags: ['Applications', 'Arxiv']
---
The way users acquire information is undergoing a paradigm shift with the advent of ChatGPT. Unlike conventional search engines ChatGPT retrieves knowledge from the model itself and generates answers for users. ChatGPTs impressive question-answering (QA) capability has attracted more than 100 million users within a short period of time but has also raised concerns regarding its reliability. In this paper we perform the first large-scale measurement of ChatGPTs reliability in the generic QA scenario with a carefully curated set of 5695 questions across ten datasets and eight domains. We find that ChatGPTs reliability varies across different domains especially underperforming in law and science questions. We also demonstrate that system roles originally designed by OpenAI to allow users to steer ChatGPTs behavior can impact ChatGPTs reliability in an imperceptible way. We further show that ChatGPT is vulnerable to adversarial examples and even a single character change can negatively affect its reliability in certain cases. We believe that our study provides valuable insights into ChatGPTs reliability and underscores the need for strengthening the reliability and security of large language models (LLMs).
