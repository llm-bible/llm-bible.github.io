---
layout: publication
title: 'Speculative Contrastive Decoding'
authors: Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou
conference: "Arxiv"
year: 2023
bibkey: yuan2023speculative
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.08981"}
tags: ['RAG', 'Efficiency and Optimization', 'Ethics and Bias']
---
Large language models~(LLMs) exhibit exceptional performance in language
tasks, yet their auto-regressive inference is limited due to high computational
requirements and is sub-optimal due to the exposure bias. Inspired by
speculative decoding and contrastive decoding, we introduce Speculative
Contrastive Decoding~(SCD), a straightforward yet powerful decoding approach
that leverages predictions from smaller language models~(LMs) to achieve both
decoding acceleration and quality improvement. Extensive evaluations and
analyses on four diverse language tasks demonstrate the effectiveness of SCD,
showing that decoding efficiency and quality can compatibly benefit from one
smaller LM.
