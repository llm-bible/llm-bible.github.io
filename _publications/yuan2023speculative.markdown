---
layout: publication
title: Speculative Contrastive Decoding
authors: Yuan Hongyi, Lu Keming, Huang Fei, Yuan Zheng, Zhou Chang
conference: "Arxiv"
year: 2023
bibkey: yuan2023speculative
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.08981"}
tags: ['Efficiency And Optimization', 'Ethics And Bias', 'RAG']
---
Large language models~(LLMs) exhibit exceptional performance in language tasks yet their auto45;regressive inference is limited due to high computational requirements and is sub45;optimal due to the exposure bias. Inspired by speculative decoding and contrastive decoding we introduce Speculative Contrastive Decoding~(SCD) a straightforward yet powerful decoding approach that leverages predictions from smaller language models~(LMs) to achieve both decoding acceleration and quality improvement. Extensive evaluations and analyses on four diverse language tasks demonstrate the effectiveness of SCD showing that decoding efficiency and quality can compatibly benefit from one smaller LM.
