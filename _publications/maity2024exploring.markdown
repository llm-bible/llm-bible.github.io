---
layout: publication
title: Exploring The Capabilities Of Prompted Large Language Models In Educational And Assessment Applications
authors: Maity Subhankar, Deroy Aniket, Sarkar Sudeshna
conference: "Arxiv"
year: 2024
bibkey: maity2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.11579"}
tags: ['Applications', 'Efficiency And Optimization', 'Fine Tuning', 'Merging', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Tools']
---
In the era of generative artificial intelligence (AI) the fusion of large language models (LLMs) offers unprecedented opportunities for innovation in the field of modern education. We embark on an exploration of prompted LLMs within the context of educational and assessment applications to uncover their potential. Through a series of carefully crafted research questions we investigate the effectiveness of prompt45;based techniques in generating open45;ended questions from school45;level textbooks assess their efficiency in generating open45;ended questions from undergraduate45;level technical textbooks and explore the feasibility of employing a chain45;of45;thought inspired multi45;stage prompting approach for language45;agnostic multiple45;choice question (MCQ) generation. Additionally we evaluate the ability of prompted LLMs for language learning exemplified through a case study in the low45;resource Indian language Bengali to explain Bengali grammatical errors. We also evaluate the potential of prompted LLMs to assess human resource (HR) spoken interview transcripts. By juxtaposing the capabilities of LLMs with those of human experts across various educational tasks and domains our aim is to shed light on the potential and limitations of LLMs in reshaping educational practices.
