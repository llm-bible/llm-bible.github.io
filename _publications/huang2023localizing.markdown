---
layout: publication
title: 'Acegpt, Localizing Large Language Models In Arabic'
authors: Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Juncai He, Ziche Liu, Zhiyi Zhang, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, Jinchao Xu
conference: "Arxiv"
year: 2023
bibkey: huang2023localizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.12053"}
  - {name: "Code", url: "https://github.com/FreedomIntelligence/AceGPT"}
tags: ['Agentic', 'Training Techniques', 'Model Architecture', 'Reinforcement Learning', 'GPT', 'Pretraining Methods', 'Fine-Tuning', 'Has Code', 'Pre-Training']
---
This paper is devoted to the development of a localized Large Language Model
(LLM) specifically for Arabic, a language imbued with unique cultural
characteristics inadequately addressed by current mainstream models.
Significant concerns emerge when addressing cultural sensitivity and local
values. To address this, the paper proposes a comprehensive solution that
includes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT)
utilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside
Reinforcement Learning with AI Feedback (RLAIF) employing a reward model
attuned to local culture and values. The goal is to cultivate culturally
cognizant and value-aligned Arabic LLMs capable of accommodating the diverse,
application-specific needs of Arabic-speaking communities.
  Comprehensive evaluations reveal that the resulting model, dubbed `AceGPT',
sets the state-of-the-art standard for open Arabic LLMs across various
benchmarks. Codes, data, and models are in
https://github.com/FreedomIntelligence/AceGPT.
