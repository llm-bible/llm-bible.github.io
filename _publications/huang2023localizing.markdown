---
layout: publication
title: Acegpt Localizing Large Language Models In Arabic
authors: Huang Huang, Yu Fei, Zhu Jianqing, Sun Xuening, Cheng Hao, Song Dingjie, Chen Zhihong, Alharthi Abdulmohsen, An Bang, He Juncai, Liu Ziche, Zhang Zhiyi, Chen Junying, Li Jianquan, Wang Benyou, Zhang Lian, Sun Ruoyu, Wan Xiang, Li Haizhou, Xu Jinchao
conference: "Arxiv"
year: 2023
bibkey: huang2023localizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.12053"}
  - {name: "Code", url: "https://github.com/FreedomIntelligence/AceGPT"}
tags: ['Agentic', 'Fine Tuning', 'GPT', 'Has Code', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
This paper is devoted to the development of a localized Large Language Model (LLM) specifically for Arabic a language imbued with unique cultural characteristics inadequately addressed by current mainstream models. Significant concerns emerge when addressing cultural sensitivity and local values. To address this the paper proposes a comprehensive solution that includes further pre45;training with Arabic texts Supervised Fine45;Tuning (SFT) utilizing native Arabic instructions and GPT45;4 responses in Arabic alongside Reinforcement Learning with AI Feedback (RLAIF) employing a reward model attuned to local culture and values. The goal is to cultivate culturally cognizant and value45;aligned Arabic LLMs capable of accommodating the diverse application45;specific needs of Arabic45;speaking communities. Comprehensive evaluations reveal that the resulting model dubbed AceGPT sets the state45;of45;the45;art standard for open Arabic LLMs across various benchmarks. Codes data and models are in https://github.com/FreedomIntelligence/AceGPT.
