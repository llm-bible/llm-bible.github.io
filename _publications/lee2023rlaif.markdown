---
layout: publication
title: RLAIF Vs. RLHF Scaling Reinforcement Learning From Human Feedback With AI Feedback
authors: Lee Harrison, Phatale Samrat, Mansoor Hassan, Mesnard Thomas, Ferret Johan, Lu Kellie, Bishop Colton, Hall Ethan, Carbune Victor, Rastogi Abhinav, Prakash Sushant
conference: "Proceedings of the"
year: 2023
bibkey: lee2023rlaif
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.00267"}
tags: ['Agentic', 'Applications', 'Reinforcement Learning', 'Training Techniques']
---
Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences but gathering high45;quality preference labels is expensive. RL from AI Feedback (RLAIF) introduced in Bai et al. offers a promising alternative that trains the reward model (RM) on preferences generated by an off45;the45;shelf LLM. Across the tasks of summarization helpful dialogue generation and harmless dialogue generation we show that RLAIF achieves comparable performance to RLHF. Furthermore we take a step towards self45;improvement by demonstrating that RLAIF can outperform a supervised fine45;tuned baseline even when the AI labeler is the same size as the policy or even the exact same checkpoint as the initial policy. Finally we introduce direct45;RLAIF (d45;RLAIF) 45; a technique that circumvents RM training by obtaining rewards directly from an off45;the45;shelf LLM during RL which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on45;par with using human feedback offering a potential solution to the scalability limitations of RLHF.
