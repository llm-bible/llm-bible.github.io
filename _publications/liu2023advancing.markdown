---
layout: publication
title: MMC Advancing Multimodal Chart Understanding With Large45;scale Instruction Tuning
authors: Liu Fuxiao, Wang Xiaoyang, Yao Wenlin, Chen Jianshu, Song Kaiqiang, Cho Sangwoo, Yacoob Yaser, Yu Dong
conference: "Arxiv"
year: 2023
bibkey: liu2023advancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.10774"}
  - {name: "Code", url: "https://github.com/FuxiaoLiu/MMC"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'RAG', 'Reinforcement Learning', 'Tools']
---
With the rapid development of large language models (LLMs) and their integration into large multimodal models (LMMs) there has been impressive progress in zero45;shot completion of user45;oriented vision45;language tasks. However a gap remains in the domain of chart image understanding due to the distinct abstract components in charts. To address this we introduce a large45;scale MultiModal Chart Instruction (textbf123;MMC45;Instruction125;) dataset comprising 600k instances supporting diverse tasks and chart types. Leveraging this data we develop MultiModal Chart Assistant (textbf123;MMCA125;) an LMM that achieves state45;of45;the45;art performance on existing chart QA benchmarks. Recognizing the need for a comprehensive evaluation of LMM chart understanding we also propose a MultiModal Chart Benchmark (textbf123;MMC45;Benchmark125;) a comprehensive human45;annotated benchmark with nine distinct tasks evaluating reasoning capabilities over charts. Extensive experiments on MMC45;Benchmark reveal the limitations of existing LMMs on correctly interpreting charts even for the most recent GPT45;4V model. Our work provides an instruction45;tuning methodology and benchmark to advance multimodal understanding of charts. Code and data are available at https://github.com/FuxiaoLiu/MMC.
