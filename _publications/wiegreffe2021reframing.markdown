---
layout: publication
title: Reframing Human45;ai Collaboration For Generating Free45;text Explanations
authors: Wiegreffe Sarah, Hessel Jack, Swayamdipta Swabha, Riedl Mark, Choi Yejin
conference: "Arxiv"
year: 2021
bibkey: wiegreffe2021reframing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2112.08674"}
tags: ['GPT', 'Interpretability And Explainability', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning']
---
Large language models are increasingly capable of generating fluent45;appearing text with relatively little task45;specific supervision. But can these models accurately explain classification decisions We consider the task of generating free45;text explanations using human45;written examples in a few45;shot manner. We find that (1) authoring higher quality prompts results in higher quality generations; and (2) surprisingly in a head45;to45;head comparison crowdworkers often prefer explanations generated by GPT45;3 to crowdsourced explanations in existing datasets. Our human studies also show however that while models often produce factual grammatical and sufficient explanations they have room to improve along axes such as providing novel information and supporting the label. We create a pipeline that combines GPT45;3 with a supervised filter that incorporates binary acceptability judgments from humans in the loop. Despite the intrinsic subjectivity of acceptability judgments we demonstrate that acceptability is partially correlated with various fine45;grained attributes of explanations. Our approach is able to consistently filter GPT45;345;generated explanations deemed acceptable by humans.
