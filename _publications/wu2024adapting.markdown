---
layout: publication
title: Adapting Large Language Models For Document45;level Machine Translation
authors: Wu Minghao, Vu Thuy-trang, Qu Lizhen, Foster George, Haffari Gholamreza
conference: "Arxiv"
year: 2024
bibkey: wu2024adapting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.06468"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Prompting', 'Training Techniques']
---
Large language models (LLMs) have significantly advanced various natural language processing (NLP) tasks. Recent research indicates that moderately45;sized LLMs often outperform larger ones after task45;specific fine45;tuning. This study focuses on adapting LLMs for document45;level machine translation (DocMT) for specific language pairs. We first investigate the impact of prompt strategies on translation performance and then conduct extensive experiments using two fine45;tuning methods three LLM backbones and 18 translation tasks across nine language pairs. Our results show that specialized models can sometimes surpass GPT45;4 in translation performance but still face issues like off45;target translation due to error propagation in decoding. We provide an in45;depth analysis of these LLMs tailored for DocMT examining translation errors discourse phenomena training strategies the scaling law of parallel documents recent test set evaluations and zero45;shot crosslingual transfer. Our findings highlight the strengths and limitations of LLM45;based DocMT models and provide a foundation for future research.
