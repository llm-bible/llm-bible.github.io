---
layout: publication
title: 'Language In Vivo Vs. In Silico: Size Matters But Larger Language Models Still Do Not Comprehend Language On A Par With Humans'
authors: Dentella Vittoria, Guenther Fritz, Leivada Evelina
conference: "Arxiv"
year: 2024
bibkey: dentella2024language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.14883"}
tags: ['GPT', 'Model Architecture', 'Prompting', 'Scaling Laws', 'Uncategorized']
---
Understanding the limits of language is a prerequisite for Large Language Models (LLMs) to act as theories of natural language. LLM performance in some language tasks presents both quantitative and qualitative differences from that of humans, however it remains to be determined whether such differences are amenable to model size. This work investigates the critical role of model scaling, determining whether increases in size make up for such differences between humans and models. We test three LLMs from different families (Bard, 137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a grammaticality judgment task featuring anaphora, center embedding, comparatives, and negative polarity. N=1,200 judgments are collected and scored for accuracy, stability, and improvements in accuracy upon repeated presentation of a prompt. Results of the best performing LLM, ChatGPT-4, are compared to results of n=80 humans on the same stimuli. We find that increased model size may lead to better performance, but LLMs are still not sensitive to (un)grammaticality as humans are. It seems possible but unlikely that scaling alone can fix this issue. We interpret these results by comparing language learning in vivo and in silico, identifying three critical differences concerning (i) the type of evidence, (ii) the poverty of the stimulus, and (iii) the occurrence of semantic hallucinations due to impenetrable linguistic reference.
