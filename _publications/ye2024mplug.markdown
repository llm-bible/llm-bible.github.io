---
layout: publication
title: Mplug45;owl3 Towards Long Image45;sequence Understanding In Multi45;modal Large Language Models
authors: Ye Jiabo, Xu Haiyang, Liu Haowei, Hu Anwen, Yan Ming, Qian Qi, Zhang Ji, Huang Fei, Zhou Jingren
conference: "Arxiv"
year: 2024
bibkey: ye2024mplug
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.04840"}
tags: ['Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning']
---
Multi45;modal Large Language Models (MLLMs) have demonstrated remarkable capabilities in executing instructions for a variety of single45;image tasks. Despite this progress significant challenges remain in modeling long image sequences. In this work we introduce the versatile multi45;modal large language model mPLUG45;Owl3 which enhances the capability for long image45;sequence understanding in scenarios that incorporate retrieved image45;text knowledge interleaved image45;text and lengthy videos. Specifically we propose novel hyper attention blocks to efficiently integrate vision and language into a common language45;guided semantic space thereby facilitating the processing of extended multi45;image scenarios. Extensive experimental results suggest that mPLUG45;Owl3 achieves state45;of45;the45;art performance among models with a similar size on single45;image multi45;image and video benchmarks. Moreover we propose a challenging long visual sequence evaluation named Distractor Resistance to assess the ability of models to maintain focus amidst distractions. Finally with the proposed architecture mPLUG45;Owl3 demonstrates outstanding performance on ultra45;long visual sequence inputs. We hope that mPLUG45;Owl3 can contribute to the development of more efficient and powerful multimodal large language models.
