---
layout: publication
title: "Challenges And Thrills Of Legal Arguments"
authors: Pallaprolu Anurag, Vaidya Radha, Attawar Aditya Swaroop
conference: "Arxiv"
year: 2020
bibkey: pallaprolu2020challenges
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2006.03773"}
tags: ['Attention Mechanism', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
State-of-the-art attention based models mostly centered around the transformer architecture solve the problem of sequence-to-sequence translation using the so-called scaled dot-product attention. While this technique is highly effective for estimating inter-token attention it does not answer the question of inter-sequence attention when we deal with conversation-like scenarios. We propose an extension HumBERT that attempts to perform continuous contextual argument generation using locally trained transformers.
