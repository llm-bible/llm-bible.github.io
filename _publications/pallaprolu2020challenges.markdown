---
layout: publication
title: Challenges And Thrills Of Legal Arguments
authors: Pallaprolu Anurag, Vaidya Radha, Attawar Aditya Swaroop
conference: "Arxiv"
year: 2020
bibkey: pallaprolu2020challenges
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2006.03773"}
tags: ['Attention Mechanism', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
State45;of45;the45;art attention based models mostly centered around the transformer architecture solve the problem of sequence45;to45;sequence translation using the so45;called scaled dot45;product attention. While this technique is highly effective for estimating inter45;token attention it does not answer the question of inter45;sequence attention when we deal with conversation45;like scenarios. We propose an extension HumBERT that attempts to perform continuous contextual argument generation using locally trained transformers.
