---
layout: publication
title: Gpt45;who An Information Density45;based Machine45;generated Text Detector
authors: Venkatraman Saranya, Uchendu Adaku, Lee Dongwon
conference: "Arxiv"
year: 2023
bibkey: venkatraman2023gpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.06202"}
  - {name: "Code", url: "https://github.com/saranya&#45;venkatraman/gpt&#45;who"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
The Uniform Information Density (UID) principle posits that humans prefer to spread information evenly during language production. We examine if this UID principle can help capture differences between Large Language Models (LLMs)45;generated and human45;generated texts. We propose GPT45;who the first psycholinguistically45;inspired domain45;agnostic statistical detector. This detector employs UID45;based features to model the unique statistical signature of each LLM and human author for accurate detection. We evaluate our method using 4 large45;scale benchmark datasets and find that GPT45;who outperforms state45;of45;the45;art detectors (both statistical45; amp; non45;statistical) such as GLTR GPTZero DetectGPT OpenAI detector and ZeroGPT by over 2037; across domains. In addition to better performance it is computationally inexpensive and utilizes an interpretable representation of text articles. We find that GPT45;who can distinguish texts generated by very sophisticated LLMs even when the overlying text is indiscernible. UID45;based measures for all datasets and code are available at https://github.com/saranya&#45;venkatraman/gpt&#45;who.
