---
layout: publication
title: LLM Evaluators Recognize And Favor Their Own Generations
authors: Panickssery Arjun, Bowman Samuel R., Feng Shi
conference: "Arxiv"
year: 2024
bibkey: panickssery2024llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.13076"}
tags: ['Ethics And Bias', 'GPT', 'Interpretability And Explainability', 'Model Architecture', 'Reinforcement Learning', 'Responsible AI']
---
Self45;evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling constitutional AI and self45;refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self45;preference where an LLM evaluator scores its own outputs higher than others while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores or is it just a coincidence In this paper we investigate if self45;recognition capability contributes to self45;preference. We discover that out of the box LLMs such as GPT45;4 and Llama 2 have non45;trivial accuracy at distinguishing themselves from other LLMs and humans. By fine45;tuning LLMs we discover a linear correlation between self45;recognition capability and the strength of self45;preference bias; using controlled experiments we show that the causal explanation resists straightforward confounders. We discuss how self45;recognition can interfere with unbiased evaluations and AI safety more generally.
