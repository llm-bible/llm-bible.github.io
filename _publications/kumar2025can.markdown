---
layout: publication
title: 'Can Llms Simulate Personas With Reversed Performance? A Benchmark For Counterfactual Instruction Following'
authors: Sai Adith Senthil Kumar, Hao Yan, Saipavan Perepa, Murong Yue, Ziyu Yao
conference: "Arxiv"
year: 2025
bibkey: kumar2025can
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.06460'}
tags: ['RAG', 'Applications']
---
Large Language Models (LLMs) are now increasingly widely used to simulate
personas in virtual environments, leveraging their instruction-following
capability. However, we discovered that even state-of-the-art LLMs cannot
simulate personas with reversed performance (e.g., student personas with low
proficiency in educational settings), which impairs the simulation diversity
and limits the practical applications of the simulated environments. In this
work, using mathematical reasoning as a representative scenario, we propose the
first benchmark dataset for evaluating LLMs on simulating personas with
reversed performance, a capability that we dub "counterfactual instruction
following". We evaluate both open-weight and closed-source LLMs on this task
and find that LLMs, including the OpenAI o1 reasoning model, all struggle to
follow counterfactual instructions for simulating reversedly performing
personas. Intersectionally simulating both the performance level and the race
population of a persona worsens the effect even further. These results
highlight the challenges of counterfactual instruction following and the need
for further research.
