---
layout: publication
title: Multi45;stage Pre45;training Enhanced By Chatgpt For Multi45;scenario Multi45;domain Dialogue Summarization
authors: Zhou Weixiao, Li Gengyao, Cheng Xianfu, Liang Xinnian, Zhu Junnan, Zhai Feifei, Li Zhoujun
conference: "Arxiv"
year: 2023
bibkey: zhou2023multi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.10285"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Training Techniques']
---
Dialogue summarization involves a wide range of scenarios and domains. However existing methods generally only apply to specific scenarios or domains. In this study we propose a new pre45;trained model specifically designed for multi45;scenario multi45;domain dialogue summarization. It adopts a multi45;stage pre45;training strategy to reduce the gap between the pre45;training objective and fine45;tuning objective. Specifically we first conduct domain45;aware pre45;training using large45;scale multi45;scenario multi45;domain dialogue data to enhance the adaptability of our pre45;trained model. Then we conduct task45;oriented pre45;training using large45;scale multi45;scenario multi45;domain dialogue45;summary parallel data annotated by ChatGPT to enhance the dialogue summarization ability of our pre45;trained model. Experimental results on three dialogue summarization datasets from different scenarios and domains indicate that our pre45;trained model significantly outperforms previous state45;of45;the45;art models in full fine45;tuning zero45;shot and few45;shot settings.
