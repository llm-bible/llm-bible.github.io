---
layout: publication
title: 'Proswitch: Knowledge-guided Instruction Tuning To Switch Between Professional And Non-professional Responses'
authors: Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yongfeng Huang, Heng Chang, Yueting Zhuang
conference: "Arxiv"
year: 2024
bibkey: zong2024knowledge
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2403.09131'}
tags: ['Reinforcement Learning', 'Language Modeling', 'Applications']
---
Large Language Models (LLMs) have demonstrated efficacy in various linguistic
applications, including question answering and controlled text generation.
However, studies into their ability to switch between opposite styles of
responses in professional domains remain underexplored. This study introduces a
novel approach, named ProSwitch, which enables a language model to switch
between professional and non-professional answers, by tuning and evaluating
through the guidance of domain and style knowledge. ProSwitch unfolds in three
phases: LLM-augmented preparation to collect domain knowledge and QA pairs,
instruction tuning to optimize LLMs with multiple levels of knowledge, and
comprehensive evaluation to assess both style discrimination and
reference-based quality of the generated text. Comparative analysis of
ProSwitch against general and specialized LLMs reveals that our approach
outperforms baselines in switching between professional and non-professional
responses.
