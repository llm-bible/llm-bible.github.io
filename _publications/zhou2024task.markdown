---
layout: publication
title: DETAIL Task Demonstration Attribution For Interpretable In45;context Learning
authors: Zhou Zijian, Lin Xiaoqiang, Xu Xinyi, Prakash Alok, Rus Daniela, Low Bryan Kian Hsiang
conference: "Arxiv"
year: 2024
bibkey: zhou2024task
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.14899"}
tags: ['Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Transformer']
---
In45;context learning (ICL) allows transformer45;based language models that are pre45;trained on general text to quickly learn a specific task with a few task demonstrations without updating their parameters significantly boosting their flexibility and generality. ICL possesses many distinct characteristics from conventional machine learning thereby requiring new approaches to interpret this learning paradigm. Taking the viewpoint of recent works showing that transformers learn in context by formulating an internal optimizer we propose an influence function45;based attribution technique DETAIL that addresses the specific characteristics of ICL. We empirically verify the effectiveness of our approach for demonstration attribution while being computationally efficient. Leveraging the results we then show how DETAIL can help improve model performance in real45;world scenarios through demonstration reordering and curation. Finally we experimentally prove the wide applicability of DETAIL by showing our attribution scores obtained on white45;box models are transferable to black45;box models in improving model performance.
