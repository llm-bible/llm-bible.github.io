---
layout: publication
title: 'Tigerllm - A Family Of Bangla Large Language Models'
authors: Nishat Raihan, Marcos Zampieri
conference: "Arxiv"
year: 2025
bibkey: raihan2025tigerllm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.10995"}
tags: ['Language Modeling', 'Model Architecture', 'GPT', 'Reinforcement Learning']
---
The development of Large Language Models (LLMs) remains heavily skewed towards English and a few other high-resource languages. This linguistic disparity is particularly evident for Bangla - the 5th most spoken language. A few initiatives attempted to create open-source Bangla LLMs with performance still behind high-resource languages and limited reproducibility. To address this gap, we introduce TigerLLM - a family of Bangla LLMs. Our results demonstrate that these models surpass all open-source alternatives and also outperform larger proprietary models like GPT3.5 across standard benchmarks, establishing TigerLLM as the new baseline for future Bangla language modeling.
