---
layout: publication
title: Hrot Hybrid Prompt Strategy And Retrieval Of Thought For Table45;text Hybrid Question Answering
authors: Luo Tongxu, Lei Fangyu, Lei Jiahe, Liu Weihao, He Shihu, Zhao Jun, Liu Kang
conference: "Arxiv"
year: 2023
bibkey: luo2023hybrid
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.12669"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
Answering numerical questions over hybrid contents from the given tables and text(TextTableQA) is a challenging task. Recently Large Language Models (LLMs) have gained significant attention in the NLP community. With the emergence of large language models In45;Context Learning and Chain45;of45;Thought prompting have become two particularly popular research topics in this field. In this paper we introduce a new prompting strategy called Hybrid prompt strategy and Retrieval of Thought for TextTableQA. Through In45;Context Learning we prompt the model to develop the ability of retrieval thinking when dealing with hybrid data. Our method achieves superior performance compared to the fully45;supervised SOTA on the MultiHiertt dataset in the few45;shot setting.
