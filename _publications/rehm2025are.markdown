---
layout: publication
title: 'Are Multilingual Language Models An Off-ramp For Under-resourced Languages? Will We Arrive At Digital Language Equality In Europe In 2030?'
authors: Georg Rehm, Annika Gr√ºtzner-zahn, Fabio Barth
conference: "Arxiv"
year: 2025
bibkey: rehm2025are
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.12886"}
tags: ['Training Techniques', 'Pre-Training', 'Applications', 'Reinforcement Learning']
---
Large language models (LLMs) demonstrate unprecedented capabilities and
define the state of the art for almost all natural language processing (NLP)
tasks and also for essentially all Language Technology (LT) applications. LLMs
can only be trained for languages for which a sufficient amount of pre-training
data is available, effectively excluding many languages that are typically
characterised as under-resourced. However, there is both circumstantial and
empirical evidence that multilingual LLMs, which have been trained using data
sets that cover multiple languages (including under-resourced ones), do exhibit
strong capabilities for some of these under-resourced languages. Eventually,
this approach may have the potential to be a technological off-ramp for those
under-resourced languages for which "native" LLMs, and LLM-based technologies,
cannot be developed due to a lack of training data. This paper, which
concentrates on European languages, examines this idea, analyses the current
situation in terms of technology support and summarises related work. The
article concludes by focusing on the key open questions that need to be
answered for the approach to be put into practice in a systematic way.
