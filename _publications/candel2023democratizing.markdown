---
layout: publication
title: H2ogpt Democratizing Large Language Models
authors: Candel Arno, Mckinney Jon, Singer Philipp, Pfeiffer Pascal, Jeblick Maximilian, Prabhu Prithvi, Gambera Jeff, Landry Mark, Bansal Shivam, Chesler Ryan, Lee Chun Ming, Conde Marcos V., Stetsenko Pasha, Grellier Olivier, Ambati Srisatish
conference: "Arxiv"
year: 2023
bibkey: candel2023democratizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.08161"}
tags: ['Applications', 'Bias Mitigation', 'Ethics And Bias', 'Fairness', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
Applications built on top of Large Language Models (LLMs) such as GPT45;4 represent a revolution in AI due to their human45;level capabilities in natural language processing. However they also pose many significant risks such as the presence of biased private or harmful text and the unauthorized inclusion of copyrighted material. We introduce h2oGPT a suite of open45;source code repositories for the creation and use of LLMs based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the worlds best truly open45;source alternative to closed45;source approaches. In collaboration with and as part of the incredible and unstoppable open45;source community we open45;source several fine45;tuned h2oGPT models from 7 to 40 Billion parameters ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 10037; private document search using natural language. Open45;source language models help boost AI development and make it more accessible and trustworthy. They lower entry hurdles allowing people and groups to tailor these models to their needs. This openness increases innovation transparency and fairness. An open45;source strategy is needed to share AI benefits fairly and H2O.ai will continue to democratize AI and LLMs.
