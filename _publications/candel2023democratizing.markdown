---
layout: publication
title: 'H2ogpt: Democratizing Large Language Models'
authors: Arno Candel, Jon Mckinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Prithvi Prabhu, Jeff Gambera, Mark Landry, Shivam Bansal, Ryan Chesler, Chun Ming Lee, Marcos V. Conde, Pasha Stetsenko, Olivier Grellier, Srisatish Ambati
conference: "Arxiv"
year: 2023
bibkey: candel2023democratizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.08161"}
tags: ['Transformer', 'GPT', 'Applications', 'Ethics and Bias', 'Bias Mitigation', 'Model Architecture', 'Reinforcement Learning', 'Interpretability', 'Pretraining Methods', 'Fairness']
---
Applications built on top of Large Language Models (LLMs) such as GPT-4
represent a revolution in AI due to their human-level capabilities in natural
language processing. However, they also pose many significant risks such as the
presence of biased, private, or harmful text, and the unauthorized inclusion of
copyrighted material.
  We introduce h2oGPT, a suite of open-source code repositories for the
creation and use of LLMs based on Generative Pretrained Transformers (GPTs).
The goal of this project is to create the world's best truly open-source
alternative to closed-source approaches. In collaboration with and as part of
the incredible and unstoppable open-source community, we open-source several
fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial
use under fully permissive Apache 2.0 licenses. Included in our release is
100% private document search using natural language.
  Open-source language models help boost AI development and make it more
accessible and trustworthy. They lower entry hurdles, allowing people and
groups to tailor these models to their needs. This openness increases
innovation, transparency, and fairness. An open-source strategy is needed to
share AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.
