---
layout: publication
title: LM4HPC Towards Effective Language Model Application in High-Performance Computing
authors: Chen Le, Lin Pei-hung, Vanderbruggen Tristan, Liao Chunhua, Emani Murali, De Supinski Bronis
conference: "Arxiv"
year: 2023
bibkey: chen2023lm4hpc
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.14979"}
tags: ['Efficiency And Optimization', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Tools']
---
In recent years language models (LMs) such as GPT-4 have been widely used in multiple domains including natural language processing visualization and so on. However applying them for analyzing and optimizing high-performance computing (HPC) software is still challenging due to the lack of HPC-specific support. In this paper we design the LM4HPC framework to facilitate the research and development of HPC software analyses and optimizations using LMs. Tailored for supporting HPC datasets AI models and pipelines our framework is built on top of a range of components from different levels of the machine learning software stack with Hugging Face-compatible APIs. Using three representative tasks we evaluated the prototype of our framework. The results show that LM4HPC can help users quickly evaluate a set of state-of-the-art models and generate insightful leaderboards.
