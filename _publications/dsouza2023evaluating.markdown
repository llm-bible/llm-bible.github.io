---
layout: publication
title: Evaluating Prompt45;based Question Answering For Object Prediction In The Open Research Knowledge Graph
authors: D'souza Jennifer, Hrou Moussab, Auer SÃ¶ren
conference: "Arxiv"
year: 2023
bibkey: dsouza2023evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.12900"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
There have been many recent investigations into prompt45;based training of transformer language models for new text genres in low45;resource settings. The prompt45;based training approach has been found to be effective in generalizing pre45;trained or fine45;tuned models for transfer to resource45;scarce settings. This work for the first time reports results on adopting prompt45;based training of transformers for textit123;scholarly knowledge graph object prediction125;. The work is unique in the following two main aspects. 1) It deviates from the other works proposing entity and relation extraction pipelines for predicting objects of a scholarly knowledge graph. 2) While other works have tested the method on text genera relatively close to the general knowledge domain we test the method for a significantly different domain i.e. scholarly knowledge in turn testing the linguistic probabilistic and factual generalizability of these large45;scale transformer models. We find that (i) per expectations transformer models when tested out45;of45;the45;box underperform on a new domain of data (ii) prompt45;based training of the models achieve performance boosts of up to 4037; in a relaxed evaluation setting and (iii) testing the models on a starkly different domain even with a clever training objective in a low resource setting makes evident the domain knowledge capture gap offering an empirically45;verified incentive for investing more attention and resources to the scholarly domain in the context of transformer models.
