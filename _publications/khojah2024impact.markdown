---
layout: publication
title: 'The Impact Of Prompt Programming On Function-level Code Generation'
authors: Ranim Khojah, Francisco Gomes De Oliveira Neto, Mazen Mohamad, Philipp Leitner
conference: "Arxiv"
year: 2024
bibkey: khojah2024impact
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.20545"}
tags: ['GPT', 'Applications', 'Model Architecture', 'Few-Shot', 'Prompting']
---
Large Language Models (LLMs) are increasingly used by software engineers for
code generation. However, limitations of LLMs such as irrelevant or incorrect
code have highlighted the need for prompt programming (or prompt engineering)
where engineers apply specific prompt techniques (e.g., chain-of-thought or
input-output examples) to improve the generated code. Despite this, the impact
of different prompt techniques -- and their combinations -- on code generation
remains underexplored. In this study, we introduce CodePromptEval, a dataset of
7072 prompts designed to evaluate five prompt techniques (few-shot, persona,
chain-of-thought, function signature, list of packages) and their effect on the
correctness, similarity, and quality of complete functions generated by three
LLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt
techniques significantly influence the generated code, combining multiple
techniques does not necessarily improve the outcome. Additionally, we observed
a trade-off between correctness and quality when using prompt techniques. Our
dataset and replication package enable future research on improving
LLM-generated code and evaluating new prompt techniques.
