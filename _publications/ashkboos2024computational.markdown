---
layout: publication
title: 'Computational Bottlenecks Of Training Small-scale Large Language Models'
authors: Saleh Ashkboos, Iman Mirzadeh, Keivan Alizadeh, Mohammad Hossein Sekhavat, Moin Nabi, Mehrdad Farajtabar, Fartash Faghri
conference: "Arxiv"
year: 2024
bibkey: ashkboos2024computational
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.19456'}
tags: ['Attention Mechanism', 'Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Reinforcement Learning']
---
While large language models (LLMs) dominate the AI landscape, Small-scale
large Language Models (SLMs) are gaining attention due to cost and efficiency
demands from consumers. However, there is limited research on the training
behavior and computational requirements of SLMs. In this study, we explore the
computational bottlenecks of training SLMs (up to 2B parameters) by examining
the effects of various hyperparameters and configurations, including GPU type,
batch size, model size, communication protocol, attention type, and the number
of GPUs. We assess these factors on popular cloud services using metrics such
as loss per dollar and tokens per second. Our findings aim to support the
broader adoption and optimization of language model training for low-resource
AI research institutes.
