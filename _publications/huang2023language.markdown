---
layout: publication
title: Language Is Not All You Need Aligning Perception With Language Models
authors: Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei
conference: "Arxiv"
year: 2023
bibkey: huang2023language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2302.14045v2"}
tags: ['Applications', 'Multimodal Models', 'Prompting', 'Reinforcement Learning']
---
A big convergence of language multimodal perception action and world modeling is a key step toward artificial general intelligence. In this work we introduce Kosmos45;1 a Multimodal Large Language Model (MLLM) that can perceive general modalities learn in context (i.e. few45;shot) and follow instructions (i.e. zero45;shot). Specifically we train Kosmos45;1 from scratch on web45;scale multimodal corpora including arbitrarily interleaved text and images image45;caption pairs and text data. We evaluate various settings including zero45;shot few45;shot and multimodal chain45;of45;thought prompting on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos45;1 achieves impressive performance on (i) language understanding generation and even OCR45;free NLP (directly fed with document images) (ii) perception45;language tasks including multimodal dialogue image captioning visual question answering and (iii) vision tasks such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross45;modal transfer i.e. transfer knowledge from language to multimodal and from multimodal to language. In addition we introduce a dataset of Raven IQ test which diagnoses the nonverbal reasoning capability of MLLMs.
