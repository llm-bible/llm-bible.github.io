---
layout: publication
title: 'Logical Reasoning Over Natural Language As Knowledge Representation: A Survey'
authors: Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria
conference: "Arxiv"
year: 2023
bibkey: yang2023logical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2303.12023"}
tags: ['Pretraining Methods', 'Model Architecture', 'Transformer', 'Survey Paper']
---
Logical reasoning is central to human cognition and intelligence. It includes
deductive, inductive, and abductive reasoning. Past research of logical
reasoning within AI uses formal language as knowledge representation and
symbolic reasoners. However, reasoning with formal language has proved
challenging (e.g., brittleness and knowledge-acquisition bottleneck). This
paper provides a comprehensive overview on a new paradigm of logical reasoning,
which uses natural language as knowledge representation and pretrained language
models as reasoners, including philosophical definition and categorization of
logical reasoning, advantages of the new paradigm, benchmarks and methods,
challenges of the new paradigm, possible future directions, and relation to
related NLP fields. This new paradigm is promising since it not only alleviates
many challenges of formal representation but also has advantages over
end-to-end neural methods. This survey focus on transformer-based LLMs
explicitly working on deductive, inductive, and abductive reasoning over
English representation.
