---
layout: publication
title: Exploring Automated Distractor Generation For Math Multiple45;choice Questions Via Large Language Models
authors: Feng Wanyong, Lee Jaewook, Mcnichols Hunter, Scarlatos Alexander, Smith Digory, Woodhead Simon, Ornelas Nancy Otero, Lan Andrew
conference: "Arxiv"
year: 2024
bibkey: feng2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.02124"}
tags: ['Applications', 'Reinforcement Learning']
---
Multiple45;choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer grade and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors i.e. incorrect options that are designed to target common errors or misconceptions among real students. To date the task of crafting high45;quality distractors largely remains a labor and time45;intensive process for teachers and learning content designers which has limited scalability. In this work we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)45;based approaches from in45;context learning to fine45;tuning. We conduct extensive experiments using a real45;world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors they are less adept at anticipating common errors or misconceptions among real students.
