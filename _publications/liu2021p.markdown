---
layout: publication
title: P45;tuning V2 Prompt Tuning Can Be Comparable To Fine45;tuning Universally Across Scales And Tasks
authors: Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang
conference: "Arxiv"
year: 2021
bibkey: liu2021p
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2110.07602v3"}
  - {name: "Code", url: "https://github.com/THUDM/P&#45;tuning&#45;v2"}
tags: ['Has Code', 'Pretraining Methods', 'Prompting', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
Prompt tuning which only tunes continuous prompts with a frozen language model substantially reduces per45;task storage and memory usage at training. However in the context of NLU prior work reveals that prompt tuning does not perform well for normal45;sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.137;45;337; tuned parameters. Our method P45;Tuning v2 is an implementation of Deep Prompt Tuning cite123;li2021prefixqin2021learning125; optimized and adapted for NLU. Given the universality and simplicity of P45;Tuning v2 we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P&#45;tuning&#45;v2.
