---
layout: publication
title: Both Matter Enhancing The Emotional Intelligence Of Large Language Models Without Compromising The General Intelligence
authors: Zhao Weixiang, Li Zhuojun, Wang Shilong, Wang Yang, Hu Yulin, Zhao Yanyan, Wei Chen, Qin Bing
conference: "Arxiv"
year: 2024
bibkey: zhao2024both
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.10073"}
tags: ['Pretraining Methods', 'Reinforcement Learning']
---
Emotional Intelligence (EI) consisting of emotion perception emotion cognition and emotion expression plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine45;tuning on EI45;related classification or regression tasks. However this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end we first introduce textsc123;EiBench125; a large45;scale collection of EI45;related tasks in the text45;to45;text formation with task instructions that covers all three aspects of EI which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel underline123;textbf123;Mo125;125;dular underline123;textbf123;E125;125;motional underline123;textbf123;I125;125;ntelligence enhancement method (textbf123;MoEI125;) consisting of Modular Parameter Expansion and intra45;inter modulation is proposed to comprehensively enhance the EI of LLMs without compromise their GI. Extensive experiments on two representative LLM45;based assistants Flan45;T5 and LLaMA45;245;Chat demonstrate the effectiveness of MoEI to improving EI while maintain GI.
