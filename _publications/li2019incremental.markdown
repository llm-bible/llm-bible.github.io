---
layout: publication
title: Incremental Transformer With Deliberation Decoder For Document Grounded Conversations
authors: Li Zekang, Niu Cheng, Meng Fandong, Feng Yang, Li Qian, Zhou Jie
conference: "Arxiv"
year: 2019
bibkey: li2019incremental
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1907.08854"}
tags: ['Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
Document Grounded Conversations is a task to generate dialogue responses when chatting about the content of a given document. Obviously document knowledge plays a critical role in Document Grounded Conversations while existing dialogue models do not exploit this kind of knowledge effectively enough. In this paper we propose a novel Transformer45;based architecture for multi45;turn document grounded conversations. In particular we devise an Incremental Transformer to encode multi45;turn utterances along with knowledge in related documents. Motivated by the human cognitive process we design a two45;pass decoder (Deliberation Decoder) to improve context coherence and knowledge correctness. Our empirical study on a real45;world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance.
