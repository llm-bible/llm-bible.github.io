---
layout: publication
title: 'Large Language Model Enhanced Recommender Systems: A Survey'
authors: Qidong Liu, Xiangyu Zhao, Yuhao Wang, Yejing Wang, Zijian Zhang, Yuqi Sun, Xiang Li, Maolin Wang, Pengyue Jia, Chong Chen, Wei Huang, Feng Tian
conference: "Arxiv"
year: 2024
bibkey: liu2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.13432"}
tags: ['Survey Paper', 'Reinforcement Learning', 'RAG', 'RecSys', 'Applications']
---
Large Language Model (LLM) has transformative potential in various domains,
including recommender systems (RS). There have been a handful of research that
focuses on empowering the RS by LLM. However, previous efforts mainly focus on
LLM as RS, which may face the challenge of intolerant inference costs by LLM.
Recently, the integration of LLM into RS, known as LLM-Enhanced Recommender
Systems (LLMERS), has garnered significant interest due to its potential to
address latency and memory constraints in real-world applications. This paper
presents a comprehensive survey of the latest research efforts aimed at
leveraging LLM to enhance RS capabilities. We identify a critical shift in the
field with the move towards incorporating LLM into the online system, notably
by avoiding their use during inference. Our survey categorizes the existing
LLMERS approaches into three primary types based on the component of the RS
model being augmented: Knowledge Enhancement, Interaction Enhancement, and
Model Enhancement. We provide an in-depth analysis of each category, discussing
the methodologies, challenges, and contributions of recent studies.
Furthermore, we highlight several promising research directions that could
further advance the field of LLMERS.
