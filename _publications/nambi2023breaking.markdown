---
layout: publication
title: 'Breaking Language Barriers With A LEAP: Learning Strategies For Polyglot Llms'
authors: Nambi Akshay, Balloli Vaibhav, Ranjit Mercy, Ganu Tanuja, Ahuja Kabir, Sitaram Sunayana, Bali Kalika
conference: "Arxiv"
year: 2023
bibkey: nambi2023breaking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.17740"}
tags: ['GPT', 'Model Architecture', 'Prompting']
---
Large language models (LLMs) are at the forefront of transforming numerous
domains globally. However, their inclusivity and effectiveness remain limited
for non-Latin scripts and low-resource languages. This paper tackles the
imperative challenge of enhancing the multilingual performance of LLMs,
specifically focusing on Generative models. Through systematic investigation
and evaluation of diverse languages using popular question-answering (QA)
datasets, we present novel techniques that unlock the true potential of LLMs in
a polyglot landscape. Our approach encompasses three key strategies that yield
remarkable improvements in multilingual proficiency. First, by meticulously
optimizing prompts tailored for polyglot LLMs, we unlock their latent
capabilities, resulting in substantial performance boosts across languages.
Second, we introduce a new hybrid approach that synergizes GPT generation with
multilingual embeddings and achieves significant multilingual performance
improvement on critical tasks like QA and retrieval. Finally, to further propel
the performance of polyglot LLMs, we introduce a novel learning algorithm that
dynamically selects the optimal prompt strategy, LLM model, and embeddings per
query. This dynamic adaptation maximizes the efficacy of LLMs across languages,
outperforming best static and random strategies. Our results show substantial
advancements in multilingual understanding and generation across a diverse
range of languages.
