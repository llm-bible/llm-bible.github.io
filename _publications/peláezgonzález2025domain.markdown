---
layout: publication
title: 'A Domain-based Taxonomy Of Jailbreak Vulnerabilities In Large Language Models'
authors: Carlos Peláez-gonzález, Andrés Herrera-poyatos, Cristina Zuheros, David Herrera-poyatos, Virilo Tejedor, Francisco Herrera
conference: "Arxiv"
year: 2025
bibkey: peláezgonzález2025domain
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.04976"}
tags: ['Prompting', 'Security', 'Training Techniques', 'Reinforcement Learning']
---
The study of large language models (LLMs) is a key area in open-world machine
learning. Although LLMs demonstrate remarkable natural language processing
capabilities, they also face several challenges, including consistency issues,
hallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the
crafting of prompts that bypass alignment safeguards, leading to unsafe outputs
that compromise the integrity of LLMs. This work specifically focuses on the
challenge of jailbreak vulnerabilities and introduces a novel taxonomy of
jailbreak attacks grounded in the training domains of LLMs. It characterizes
alignment failures through generalization, objectives, and robustness gaps. Our
primary contribution is a perspective on jailbreak, framed through the
different linguistic domains that emerge during LLM training and alignment.
This viewpoint highlights the limitations of existing approaches and enables us
to classify jailbreak attacks on the basis of the underlying model deficiencies
they exploit. Unlike conventional classifications that categorize attacks based
on prompt construction methods (e.g., prompt templating), our approach provides
a deeper understanding of LLM behavior. We introduce a taxonomy with four
categories -- mismatched generalization, competing objectives, adversarial
robustness, and mixed attacks -- offering insights into the fundamental nature
of jailbreak vulnerabilities. Finally, we present key lessons derived from this
taxonomic study.
