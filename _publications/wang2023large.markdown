---
layout: publication
title: Recmind Large Language Model Powered Agent For Recommendation
authors: Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang
conference: "Arxiv"
year: 2023
bibkey: wang2023large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2308.14296v3"}
tags: ['Agentic', 'Pretraining Methods', 'RAG', 'Tools']
---
While the recommendation system (RS) has advanced significantly through deep learning current RS approaches usually train and fine45;tune models on task45;specific datasets limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus we designed an LLM45;powered autonomous recommender agent RecMind which is capable of leveraging external knowledge utilizing tools with careful planning to provide zero45;shot personalized recommendations. We propose a Self45;Inspiring algorithm to improve the planning ability. At each intermediate step the LLM self45;inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the models ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMinds performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few45;shot LLM45;based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.
