---
layout: publication
title: ICE45;GRT Instruction Context Enhancement By Generative Reinforcement Based Transformers
authors: Zheng Chen, Sun Ke, Tang Da, Ma Yukun, Zhang Yuyu, Xi Chenguang, Zhou Xun
conference: "Arxiv"
year: 2024
bibkey: zheng2024ice
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.02072"}
tags: ['Agentic', 'Efficiency And Optimization', 'Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA encounter limitations in domain45;specific tasks with these models often lacking depth and accuracy in specialized areas and exhibiting a decrease in general capabilities when fine45;tuned particularly analysis ability in small sized models. To address these gaps we introduce ICE45;GRT utilizing Reinforcement Learning from Human Feedback (RLHF) grounded in Proximal Policy Optimization (PPO) demonstrating remarkable ability in in45;domain scenarios without compromising general task performance. Our exploration of ICE45;GRT highlights its understanding and reasoning ability to not only generate robust answers but also to provide detailed analyses of the reasons behind the answer. This capability marks a significant progression beyond the scope of Supervised Fine45;Tuning models. The success of ICE45;GRT is dependent on several crucial factors including Appropriate Data Reward Size Scaling KL45;Control Advantage Normalization etc. The ICE45;GRT model exhibits state45;of45;the45;art performance in domain45;specific tasks and across 12 general Language tasks against equivalent size and even larger size LLMs highlighting the effectiveness of our approach. We provide a comprehensive analysis of the ICE45;GRT underscoring the significant advancements it brings to the field of LLM.
