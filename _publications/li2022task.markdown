---
layout: publication
title: 'Task-specific Pre-training And Prompt Decomposition For Knowledge Graph Population With Language Models'
authors: Tianyi Li, Wenyu Huang, Nikos Papasarantopoulos, Pavlos Vougiouklis, Jeff Z. Pan
conference: "Arxiv"
year: 2022
bibkey: li2022task
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2208.12539"}
tags: ['Training Techniques', 'Model Architecture', 'BERT', 'Prompting', 'Pre-Training', 'Applications']
---
We present a system for knowledge graph population with Language Models,
evaluated on the Knowledge Base Construction from Pre-trained Language Models
(LM-KBC) challenge at ISWC 2022. Our system involves task-specific pre-training
to improve LM representation of the masked object tokens, prompt decomposition
for progressive generation of candidate objects, among other methods for
higher-quality retrieval. Our system is the winner of track 1 of the LM-KBC
challenge, based on BERT LM; it achieves 55.0% F-1 score on the hidden test set
of the challenge.
