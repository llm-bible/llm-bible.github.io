---
layout: publication
title: 'Med42 -- Evaluating Fine-tuning Strategies For Medical Llms: Full-parameter Vs. Parameter-efficient Approaches'
authors: Cl√©ment Christophe, Praveen K Kanithi, Prateek Munjal, Tathagata Raha, Nasir Hayat, Ronnie Rajan, Ahmed Al-mahrooqi, Avani Gupta, Muhammad Umar Salman, Gurpreet Gosal, Bhargav Kanakiya, Charles Chen, Natalia Vassilieva, Boulbaba Ben Amor, Marco Af Pimentel, Shadab Khan
conference: "Arxiv"
year: 2024
bibkey: christophe2024evaluating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2404.14779'}
tags: ['RAG', 'Training Techniques', 'Applications', 'Model Architecture', 'Fine-Tuning', 'Pretraining Methods']
---
This study presents a comprehensive analysis and comparison of two
predominant fine-tuning methodologies - full-parameter fine-tuning and
parameter-efficient tuning - within the context of medical Large Language
Models (LLMs). We developed and refined a series of LLMs, based on the Llama-2
architecture, specifically designed to enhance medical knowledge retrieval,
reasoning, and question-answering capabilities. Our experiments systematically
evaluate the effectiveness of these tuning strategies across various well-known
medical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of
72% on the US Medical Licensing Examination (USMLE) datasets, setting a new
standard in performance for openly available medical LLMs. Through this
comparative analysis, we aim to identify the most effective and efficient
method for fine-tuning LLMs in the medical domain, thereby contributing
significantly to the advancement of AI-driven healthcare applications.
