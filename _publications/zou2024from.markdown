---
layout: publication
title: 'From Seconds To Hours: Reviewing Multimodal Large Language Models On Comprehensive Long Video Understanding'
authors: Heqing Xiao Jie Zou, Tianze Xiao Jie Luo, Guiyang Xiao Jie Xie, Xiao Jie Victor, Zhang, Fengmao Lv, Guangcong Wang, Junyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang
conference: "Arxiv"
year: 2024
bibkey: zou2024from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.18938"}
tags: ['RAG', 'Training Techniques', 'Survey Paper', 'Multimodal Models']
---
The integration of Large Language Models (LLMs) with visual encoders has
recently shown promising performance in visual understanding tasks, leveraging
their inherent capability to comprehend and generate human-like text for visual
reasoning. Given the diverse nature of visual data, MultiModal Large Language
Models (MM-LLMs) exhibit variations in model designing and training for
understanding images, short videos, and long videos. Our paper focuses on the
substantial differences and unique challenges posed by long video understanding
compared to static image and short video understanding. Unlike static images,
short videos encompass sequential frames with both spatial and within-event
temporal information, while long videos consist of multiple events with
between-event and long-term temporal information. In this survey, we aim to
trace and summarize the advancements of MM-LLMs from image understanding to
long video understanding. We review the differences among various visual
understanding tasks and highlight the challenges in long video understanding,
including more fine-grained spatiotemporal details, dynamic events, and
long-term dependencies. We then provide a detailed summary of the advancements
in MM-LLMs in terms of model design and training methodologies for
understanding long videos. Finally, we compare the performance of existing
MM-LLMs on video understanding benchmarks of various lengths and discuss
potential future directions for MM-LLMs in long video understanding.
