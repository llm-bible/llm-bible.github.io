---
layout: publication
title: 'Hudex: Integrating Hallucination Detection And Explainability For Enhancing The Reliability Of LLM Responses'
authors: Sujeong Lee, Hayoung Lee, Seongsoo Heo, Wonik Choi
conference: "Arxiv"
year: 2025
bibkey: lee2025integrating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.08109"}
tags: ['Interpretability', 'Model Architecture', 'GPT', 'Interpretability and Explainability']
---
Recent advances in large language models (LLMs) have shown promising
improvements, often surpassing existing methods across a wide range of
downstream tasks in natural language processing. However, these models still
face challenges, which may hinder their practical applicability. For example,
the phenomenon of hallucination is known to compromise the reliability of LLMs,
especially in fields that demand high factual precision. Current benchmarks
primarily focus on hallucination detection and factuality evaluation but do not
extend beyond identification. This paper proposes an explanation enhanced
hallucination-detection model, coined as HuDEx, aimed at enhancing the
reliability of LLM-generated responses by both detecting hallucinations and
providing detailed explanations. The proposed model provides a novel approach
to integrate detection with explanations, and enable both users and the LLM
itself to understand and reduce errors. Our measurement results demonstrate
that the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in
hallucination detection accuracy, while maintaining reliable explanations.
Furthermore, the proposed model performs well in both zero-shot and other test
environments, showcasing its adaptability across diverse benchmark datasets.
The proposed approach further enhances the hallucination detection research by
introducing a novel approach to integrating interpretability with hallucination
detection, which further enhances the performance and reliability of evaluating
hallucinations in language models.
