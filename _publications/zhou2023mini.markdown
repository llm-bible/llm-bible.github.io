---
layout: publication
title: 'Mini-giants: "small" Language Models And Open Source Win-win'
authors: Zhengping Zhou, Lezhi Li, Xinxi Chen, Andy Li
conference: "Arxiv"
year: 2023
bibkey: zhou2023mini
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2307.08189'}
tags: ['Reinforcement Learning', 'GPT', 'Model Architecture']
---
ChatGPT is phenomenal. However, it is prohibitively expensive to train and
refine such giant models. Fortunately, small language models are flourishing
and becoming more and more competent. We call them "mini-giants". We argue that
open source community like Kaggle and mini-giants will win-win in many ways,
technically, ethically and socially. In this article, we present a brief yet
rich background, discuss how to attain small language models, present a
comparative study of small language models and a brief discussion of evaluation
methods, discuss the application scenarios where small language models are most
needed in the real world, and conclude with discussion and outlook.
