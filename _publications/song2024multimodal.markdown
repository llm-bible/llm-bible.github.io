---
layout: publication
title: Moma Multimodal LLM Adapter For Fast Personalized Image Generation
authors: Song Kunpeng, Zhu Yizhe, Liu Bingchen, Yan Qing, Elgammal Ahmed, Yang Xiao
conference: "Arxiv"
year: 2024
bibkey: song2024multimodal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.05674"}
tags: ['Attention Mechanism', 'Merging', 'Model Architecture', 'Multimodal Models', 'Prompting', 'RAG', 'Tools', 'Training Techniques']
---
In this paper we present MoMA an open45;vocabulary training45;free personalized image model that boasts flexible zero45;shot capabilities. As foundational text45;to45;image models rapidly evolve the demand for robust image45;to45;image translation grows. Addressing this need MoMA specializes in subject45;driven personalized image generation. Utilizing an open45;source Multimodal Large Language Model (MLLM) we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features facilitating an image diffusion model. To better leverage the generated features we further introduce a novel self45;attention shortcut method that efficiently transfers image features to an image diffusion model improving the resemblance of the target object in generated images. Remarkably as a tuning45;free plug45;and45;play module our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity enhanced identity45;preservation and prompt faithfulness. Our work is open45;source thereby providing universal access to these advancements.
