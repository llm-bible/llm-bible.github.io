---
layout: publication
title: Language Grounded Qformer For Efficient Vision Language Understanding
authors: Choraria Moulik, Sekhar Nitesh, Wu Yue, Zhang Xu, Singhal Prateek, Varshney Lav R.
conference: "Arxiv"
year: 2023
bibkey: choraria2023language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.07449"}
tags: ['Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Large45;scale pretraining and instruction tuning have been successful for training general45;purpose language models with broad competencies. However extending to general45;purpose vision45;language models is challenging due to the distributional diversity in visual inputs. A recent line of work explores vision45;language instruction tuning taking inspiration from the Query Transformer (QFormer) approach proposed in BLIP45;2 models for bridging frozen modalities. However these approaches rely heavily on large45;scale multi45;modal pretraining for representation learning before eventual finetuning incurring a huge computational overhead poor scaling and limited accessibility. To that end we propose a more efficient method for QFormer45;based vision45;language alignment and demonstrate the effectiveness of our strategy compared to existing baselines in improving the efficiency of vision45;language pretraining.
