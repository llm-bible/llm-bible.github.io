---
layout: publication
title: 'A Systematic Evaluation Of LLM Strategies For Mental Health Text Analysis: Fine-tuning Vs. Prompt Engineering Vs. RAG'
authors: Arshia Kermani, Veronica Perez-rosas, Vangelis Metsis
conference: "Arxiv"
year: 2025
bibkey: kermani2025systematic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.24307"}
tags: ['Fine-Tuning', 'Applications', 'RAG', 'Training Techniques', 'Pretraining Methods', 'Prompting']
---
This study presents a systematic comparison of three approaches for the
analysis of mental health text using large language models (LLMs): prompt
engineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA
3, we evaluate these approaches on emotion classification and mental health
condition detection tasks across two datasets. Fine-tuning achieves the highest
accuracy (91% for emotion classification, 80% for mental health conditions) but
requires substantial computational resources and large training sets, while
prompt engineering and RAG offer more flexible deployment with moderate
performance (40-68% accuracy). Our findings provide practical insights for
implementing LLM-based solutions in mental health applications, highlighting
the trade-offs between accuracy, computational requirements, and deployment
flexibility.
