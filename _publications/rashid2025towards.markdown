---
layout: publication
title: 'Towards Cost-effective Reward Guided Text Generation'
authors: Ahmad Rashid, Ruotian Wu, Rongqi Fan, Hongliang Li, Agustinus Kristiadi, Pascal Poupart
conference: "Arxiv"
year: 2025
bibkey: rashid2025towards
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.04517'}
tags: ['Agentic', 'Language Modeling', 'Training Techniques', 'Applications', 'Model Architecture', 'Reinforcement Learning']
---
Reward-guided text generation (RGTG) has emerged as a viable alternative to
offline reinforcement learning from human feedback (RLHF). RGTG methods can
align baseline language models to human preferences without further training
like in standard RLHF methods. However, they rely on a reward model to score
each candidate token generated by the language model at inference, incurring
significant test-time overhead. Additionally, the reward model is usually only
trained to score full sequences, which can lead to sub-optimal choices for
partial sequences. In this work, we present a novel reward model architecture
that is trained, using a Bradley-Terry loss, to prefer the optimal expansion of
a sequence with just a *single call* to the reward model at each step of
the generation process. That is, a score for all possible candidate tokens is
generated simultaneously, leading to efficient inference. We theoretically
analyze various RGTG reward models and demonstrate that prior techniques prefer
sub-optimal sequences compared to our method during inference. Empirically, our
reward model leads to significantly faster inference than other RGTG methods.
It requires fewer calls to the reward model and performs competitively compared
to previous RGTG and offline RLHF methods.
