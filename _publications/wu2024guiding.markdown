---
layout: publication
title: 'Guiding Clinical Reasoning With Large Language Models Via Knowledge Seeds'
authors: Wu Jiageng, Wu Xian, Yang Jie
conference: "Arxiv"
year: 2024
bibkey: wu2024guiding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.06609"}
tags: ['Applications', 'Fairness', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients' diseases, and deciding on appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians. In this study, we introduce a novel framework, In-Context Padding (ICP), designed to enhance LLMs with medical knowledge. Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs. Experiments on two clinical question datasets demonstrate that ICP significantly improves the clinical reasoning ability of LLMs.
