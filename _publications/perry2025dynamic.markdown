---
layout: publication
title: 'Dynamic Knowledge Integration For Enhanced Vision-language Reasoning'
authors: Julian Perry, Surasakdi Siripong, Thanakorn Phonchai
conference: "Arxiv"
year: 2025
bibkey: perry2025dynamic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.08597"}
tags: ['Security', 'Training Techniques', 'Efficiency and Optimization', 'Multimodal Models', 'Reinforcement Learning', 'Pretraining Methods', 'Fine-Tuning', 'Applications']
---
Large Vision-Language Models (LVLMs) have demonstrated impressive
capabilities in multimodal tasks, but their performance is often constrained by
the lack of external knowledge integration, limiting their ability to handle
knowledge-intensive tasks such as visual question answering and reasoning. To
address this challenge, we propose a novel method, Adaptive Knowledge-Guided
Pretraining for Large Vision-Language Models (AKGP-LVLM), which dynamically
incorporates structured and unstructured knowledge into LVLMs during
pretraining and fine-tuning. Our approach employs a knowledge encoder to
represent external knowledge, a retrieval mechanism to select task-relevant
information, and a dynamic adaptor to align multimodal and knowledge
representations effectively. We evaluate our method on four benchmark datasets,
demonstrating significant performance improvements over state-of-the-art
models. Furthermore, human evaluations highlight the superior correctness and
relevance of our model's outputs. Extensive analyses confirm the robustness,
efficiency, and scalability of AKGP-LVLM, making it a compelling solution for
real-world knowledge-intensive tasks.
