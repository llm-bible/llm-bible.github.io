---
layout: publication
title: To Err Is Human But Llamas Can Learn It Too
authors: Luhtaru Agnes, Purason Taido, Vainikko Martin, Del Maksym, Fishel Mark
conference: "Arxiv"
year: 2024
bibkey: luhtaru2024err
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.05493"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting']
---
This study explores enhancing grammatical error correction (GEC) through artificial error generation (AEG) using language models (LMs). Specifically we fine45;tune Llama 245;based LMs for error generation and find that this approach yields synthetic errors akin to human errors. Next we train GEC Llama models with the help of these artificial errors and outperform previous state45;of45;the45;art error correction models with gains ranging between 0.8 and 6 F0.5 points across all tested languages (German Ukrainian and Estonian). Moreover we demonstrate that generating errors by fine45;tuning smaller sequence45;to45;sequence models and prompting large commercial LMs (GPT45;3.5 and GPT45;4) also results in synthetic errors beneficially affecting error generation models.
