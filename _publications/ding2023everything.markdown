---
layout: publication
title: Everything Of Thoughts Defying The Law Of Penrose Triangle For Thought Generation
authors: Ding Ruomeng, Zhang Chaoyun, Wang Lu, Xu Yong, Ma Minghua, Zhang Wei, Qin Si, Rajmohan Saravan, Lin Qingwei, Zhang Dongmei
conference: "Arxiv"
year: 2023
bibkey: ding2023everything
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.04254"}
tags: ['Agentic', 'Efficiency And Optimization', 'Prompting', 'RAG', 'Reinforcement Learning', 'Tools']
---
Recent advancements in Large Language Models (LLMs) have revolutionized decision45;making by breaking down complex problems into more manageable language sequences referred to as thoughts. An effective thought design should consider three key perspectives performance efficiency and flexibility. However existing thought can at most exhibit two of these attributes. To address these limitations we introduce a novel thought prompting approach called Everything of Thoughts (XoT) to defy the law of Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts thereby enhancing LLMs capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS45;LLM collaborative thought revision framework this approach autonomously produces high45;quality comprehensive cognitive mappings with minimal LLM interactions. Additionally XoT empowers LLMs to engage in unconstrained thinking allowing for flexible cognitive mappings for problems with multiple solutions. We evaluate XoT on several challenging multi45;solution problem45;solving tasks including Game of 24 845;Puzzle and Pocket Cube. Our results demonstrate that XoT significantly outperforms existing approaches. Notably XoT can yield multiple solutions with just one LLM call showcasing its remarkable proficiency in addressing complex problems across diverse domains.
