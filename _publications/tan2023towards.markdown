---
layout: publication
title: Towards Robust Temporal Reasoning Of Large Language Models Via A Multi45;hop QA Dataset And Pseudo45;instruction Tuning
authors: Tan Qingyu, Ng Hwee Tou, Bing Lidong
conference: "Arxiv"
year: 2023
bibkey: tan2023towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.09821"}
  - {name: "Code", url: "https://github.com/nusnlp/complex&#45;tr"}
tags: ['Applications', 'Has Code', 'Reinforcement Learning', 'Security']
---
Knowledge in the real world is being updated constantly. However it is costly to frequently update large language models (LLMs). Therefore it is crucial for LLMs to understand the concept of temporal knowledge. However prior works on temporal question answering (TQA) did not emphasize multi45;answer and multi45;hop types of temporal reasoning. In this paper we propose a complex temporal question45;answering dataset Complex45;TR that focuses on multi45;answer and multi45;hop temporal reasoning. Besides we also propose a novel data augmentation strategy to improve the complex temporal reasoning capability and robustness of LLMs. We conducted experiments on multiple temporal QA datasets. Experimental results show that our method is able to improve LLMs performance on temporal QA benchmarks by significant margins. Our code and data are released at https://github.com/nusnlp/complex&#45;tr.
