---
layout: publication
title: ALTER: Augmentation For Large-table-based Reasoning
authors: Zhang Han, Ma Yuheng, Yang Hanfang
conference: "Arxiv"
year: 2024
bibkey: zhang2024augmentation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.03061"}
tags: ['Efficiency And Optimization', 'Pretraining Methods', 'Security', 'Tools']
---
While extensive research has explored the use of large language models (LLMs) for table-based reasoning most approaches struggle with scalability when applied to large tables. To maintain the superior comprehension abilities of LLMs in these scenarios we introduce ALTER(Augmentation for Large-Table-Based Reasoning)-a framework designed to harness the latent augmentation potential in both free-form natural language (NL) questions via the query augmentor and semi-structured tabular data through the table augmentor. By utilizing only a small subset of relevant data from the table and supplementing it with pre-augmented schema semantic and literal information ALTER achieves outstanding performance on table-based reasoning benchmarks. We also provide a detailed analysis of large-table scenarios comparing different methods and various partitioning principles. In these scenarios our method outperforms all other approaches and exhibits robustness and efficiency against perturbations.
