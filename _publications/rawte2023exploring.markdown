---
layout: publication
title: 'Exploring The Relationship Between LLM Hallucinations And Prompt Linguistic Nuances: Readability, Formality, And Concreteness'
authors: Rawte Vipula, Priya Prachi, Tonmoy S. M Towhidul Islam, Zaman S M Mehedi, Sheth Amit, Das Amitava
conference: "Arxiv"
year: 2023
bibkey: rawte2023exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.11064"}
tags: ['Fine Tuning', 'Merging', 'Prompting', 'Reinforcement Learning', 'Uncategorized']
---
As Large Language Models (LLMs) have advanced, they have brought forth new challenges, with one of the prominent issues being LLM hallucination. While various mitigation techniques are emerging to address hallucination, it is equally crucial to delve into its underlying causes. Consequently, in this preliminary exploratory investigation, we examine how linguistic factors in prompts, specifically readability, formality, and concreteness, influence the occurrence of hallucinations. Our experimental results suggest that prompts characterized by greater formality and concreteness tend to result in reduced hallucination. However, the outcomes pertaining to readability are somewhat inconclusive, showing a mixed pattern.
