---
layout: publication
title: 'Llm-inference-bench: Inference Benchmarking Of Large Language Models On AI Accelerators'
authors: Krishna Teja Chitty-venkata, Siddhisanket Raskar, Bharat Kale, Farah Ferdaus, Aditya Tanikanti, Ken Raffenetti, Valerie Taylor, Murali Emani, Venkatram Vishwanath
conference: "Arxiv"
year: 2024
bibkey: chittyvenkata2024llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.00136"}
tags: ['Language Modeling', 'Tools', 'Applications']
---
Large Language Models (LLMs) have propelled groundbreaking advancements
across several domains and are commonly used for text generation applications.
However, the computational demands of these complex models pose significant
challenges, requiring efficient hardware acceleration. Benchmarking the
performance of LLMs across diverse hardware platforms is crucial to
understanding their scalability and throughput characteristics. We introduce
LLM-Inference-Bench, a comprehensive benchmarking suite to evaluate the
hardware inference performance of LLMs. We thoroughly analyze diverse hardware
platforms, including GPUs from Nvidia and AMD and specialized AI accelerators,
Intel Habana and SambaNova. Our evaluation includes several LLM inference
frameworks and models from LLaMA, Mistral, and Qwen families with 7B and 70B
parameters. Our benchmarking results reveal the strengths and limitations of
various models, hardware platforms, and inference frameworks. We provide an
interactive dashboard to help identify configurations for optimal performance
for a given hardware platform.
