---
layout: publication
title: Readctrl Personalizing Text Generation With Readability45;controlled Instruction Learning
authors: Tran Hieu, Yao Zonghai, Li Lingxi, Yu Hong
conference: "Arxiv"
year: 2024
bibkey: tran2024personalizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.09205"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
Content generation conditioning on userss readability is an important application for personalization. In an era of large language models (LLMs) readability45;controlled text generation based on LLMs has become increasingly important. This paper introduces a novel methodology called Readability45;Controlled Instruction Learning (ReadCtrl) which aims to instruction45;tune LLMs to tailor users readability levels. Unlike the traditional methods which primarily focused on categorical readability adjustments typically classified as high medium and low or expert and layperson levels with limited success ReadCtrl introduces a dynamic framework that enables LLMs to generate content at various (near continuous level) complexity levels thereby enhancing their versatility across different applications. Our results show that the ReadCtrl45;Mistral45;7B models significantly outperformed strong baseline models such as GPT45;4 and Claude45;3 with a win rate of 52.137;35.737; against GPT45;4 in human evaluations. Furthermore Read45;Ctrl has shown significant improvements in automatic evaluations as evidenced by better readability metrics (e.g. FOG FKGL) and generation quality metrics (e.g. BLEU SARI SummaC45;Factuality UniEval45;Consistency and Coherence). These results underscore Read45;Ctrls effectiveness and tenacity in producing high45;quality contextually appropriate outputs that closely align with targeted readability levels marking a significant advancement in personalized content generation using LLMs.
