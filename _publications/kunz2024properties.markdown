---
layout: publication
title: 'Properties And Challenges Of Llm-generated Explanations'
authors: Kunz Jenny, Kuhlmann Marco
conference: "Arxiv"
year: 2024
bibkey: kunz2024properties
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.10532"}
tags: ['Fine Tuning', 'Interpretability And Explainability', 'Pretraining Methods', 'Training Techniques']
---
The self-rationalising capabilities of large language models (LLMs) have been
explored in restricted settings, using task/specific data sets. However,
current LLMs do not (only) rely on specifically annotated data; nonetheless,
they frequently explain their outputs. The properties of the generated
explanations are influenced by the pre-training corpus and by the target data
used for instruction fine-tuning. As the pre-training corpus includes a large
amount of human-written explanations "in the wild", we hypothesise that LLMs
adopt common properties of human explanations. By analysing the outputs for a
multi-domain instruction fine-tuning data set, we find that generated
explanations show selectivity and contain illustrative elements, but less
frequently are subjective or misleading. We discuss reasons and consequences of
the properties' presence or absence. In particular, we outline positive and
negative implications depending on the goals and user groups of the
self-rationalising system.
