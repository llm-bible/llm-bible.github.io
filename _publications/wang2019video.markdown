---
layout: publication
title: 'Video Question Generation Via Cross-modal Self-attention Networks Learning'
authors: Yu-siang Wang, Hung-ting Su, Chen-hsi Chang, Zhe-yu Liu, Winston H. Hsu
conference: "Arxiv"
year: 2019
bibkey: wang2019video
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1907.03049"}
tags: ['Multimodal Models', 'Model Architecture', 'Transformer', 'Attention Mechanism']
---
We introduce a novel task, Video Question Generation (Video QG). A Video QG
model automatically generates questions given a video clip and its
corresponding dialogues. Video QG requires a range of skills -- sentence
comprehension, temporal relation, the interplay between vision and language,
and the ability to ask meaningful questions. To address this, we propose a
novel semantic rich cross-modal self-attention (SRCMSA) network to aggregate
the multi-modal and diverse features. To be more precise, we enhance the video
frames semantic by integrating the object-level information, and we jointly
consider the cross-modal attention for the video question generation task.
Excitingly, our proposed model remarkably improves the baseline from 7.58 to
14.48 in the BLEU-4 score on the TVQA dataset. Most of all, we arguably pave a
novel path toward understanding the challenging video input and we provide
detailed analysis in terms of diversity, which ushers the avenues for future
investigations.
