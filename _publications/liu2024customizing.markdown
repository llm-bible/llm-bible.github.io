---
layout: publication
title: Customizing Large Language Model Generation Style Using Parameter45;efficient Finetuning
authors: Liu Xinyue, Diddee Harshita, Ippolito Daphne
conference: "Arxiv"
year: 2024
bibkey: liu2024customizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.04574"}
tags: ['Applications', 'Pretraining Methods', 'Reinforcement Learning']
---
One45;size45;fits45;all large language models (LLMs) are increasingly being used to help people with their writing. However the style these models are trained to write in may not suit all users or use cases. LLMs would be more useful as writing assistants if their idiolect could be customized to match each user. In this paper we explore whether parameter45;efficient finetuning (PEFT) with Low45;Rank Adaptation can effectively guide the style of LLM generations. We use this method to customize LLaMA45;2 to ten different authors and show that the generated text has lexical syntactic and surface alignment with the target author but struggles with content memorization. Our findings highlight the potential of PEFT to support efficient user45;level customization of LLMs.
