---
layout: publication
title: 'MPO: Boosting LLM Agents With Meta Plan Optimization'
authors: Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, Sujian Li
conference: "Arxiv"
year: 2025
bibkey: xiong2025boosting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.02682"}
tags: ['Agentic', 'Efficiency and Optimization', 'Tools', 'RAG', 'Training Techniques']
---
Recent advancements in large language models (LLMs) have enabled LLM-based
agents to successfully tackle interactive planning tasks. However, despite
their successes, existing approaches often suffer from planning hallucinations
and require retraining for each new agent. To address these challenges, we
propose the Meta Plan Optimization (MPO) framework, which enhances agent
planning capabilities by directly incorporating explicit guidance. Unlike
previous methods that rely on complex knowledge, which either require
significant human effort or lack quality assurance, MPO leverages high-level
general guidance through meta plans to assist agent planning and enables
continuous optimization of the meta plans based on feedback from the agent's
task execution. Our experiments conducted on two representative tasks
demonstrate that MPO significantly outperforms existing baselines. Moreover,
our analysis indicates that MPO provides a plug-and-play solution that enhances
both task completion efficiency and generalization capabilities in previous
unseen scenarios.
