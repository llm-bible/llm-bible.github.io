---
layout: publication
title: Course45;correction Safety Alignment Using Synthetic Preferences
authors: Xu Rongwu, Cai Yishuo, Zhou Zhenhong, Gu Renjie, Weng Haiqin, Liu Yan, Zhang Tianwei, Xu Wei, Qiu Han
conference: "Arxiv"
year: 2024
bibkey: xu2024course
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.16637"}
tags: ['Pretraining Methods', 'Reinforcement Learning', 'Responsible AI', 'Security']
---
The risk of harmful content generated by large language models (LLMs) becomes a critical concern. This paper presents a systematic study on assessing and improving LLMs capability to perform the task of textbf123;course45;correction125; ie the model can steer away from generating harmful content autonomously. To start with we introduce the textsc123;C^245;Eval125; benchmark for quantitative assessment and analyze 10 popular LLMs revealing varying proficiency of current safety45;tuned LLMs in course45;correction. To improve we propose fine45;tuning LLMs with preference learning emphasizing the preference for timely course45;correction. Using an automated pipeline we create textsc123;C^245;Syn125; a synthetic dataset with 750K pairwise preferences to teach models the concept of timely course45;correction through data45;driven preference learning. Experiments on 2 LLMs textsc123;Llama245;Chat 7B125; and textsc123;Qwen2 7B125; show that our method effectively enhances course45;correction skills without affecting general performance. Additionally it effectively improves LLMs safety particularly in resisting jailbreak attacks.
