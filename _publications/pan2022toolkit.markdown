---
layout: publication
title: 'Extremebert: A Toolkit For Accelerating Pretraining Of Customized BERT'
authors: Pan Rui, Diao Shizhe, Chen Jianlin, Zhang Tong
conference: "Arxiv"
year: 2022
bibkey: pan2022toolkit
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.17201"}
  - {name: "Code", url: "https://github.com/extreme-bert/extreme-bert"}
tags: ['BERT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
In this paper, we present ExtremeBERT, a toolkit for accelerating and
customizing BERT pretraining. Our goal is to provide an easy-to-use BERT
pretraining toolkit for the research community and industry. Thus, the
pretraining of popular language models on customized datasets is affordable
with limited resources. Experiments show that, to achieve the same or better
GLUE scores, the time cost of our toolkit is over \\(6\times\\) times less for BERT
Base and \\(9\times\\) times less for BERT Large when compared with the original
BERT paper. The documentation and code are released at
https://github.com/extreme-bert/extreme-bert under the Apache-2.0 license.
