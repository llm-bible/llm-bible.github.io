---
layout: publication
title: 'Extremebert: A Toolkit For Accelerating Pretraining Of Customized BERT'
authors: Rui Pan, Shizhe Diao, Jianlin Chen, Tong Zhang
conference: "Arxiv"
year: 2022
bibkey: pan2022toolkit
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.17201"}
  - {name: "Code", url: "https://github.com/extreme-bert/extreme-bert"}
tags: ['Training Techniques', 'Model Architecture', 'Pretraining Methods', 'BERT', 'Has Code']
---
In this paper, we present ExtremeBERT, a toolkit for accelerating and
customizing BERT pretraining. Our goal is to provide an easy-to-use BERT
pretraining toolkit for the research community and industry. Thus, the
pretraining of popular language models on customized datasets is affordable
with limited resources. Experiments show that, to achieve the same or better
GLUE scores, the time cost of our toolkit is over \\(6\times\\) times less for BERT
Base and \\(9\times\\) times less for BERT Large when compared with the original
BERT paper. The documentation and code are released at
https://github.com/extreme-bert/extreme-bert under the Apache-2.0 license.
