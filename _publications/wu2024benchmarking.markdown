---
layout: publication
title: 'Detectrl: Benchmarking Llm-generated Text Detection In Real-world Scenarios'
authors: Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xinyi Yang, Yulin Yuan, Lidia S. Chao
conference: "Arxiv"
year: 2024
bibkey: wu2024benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.23746"}
  - {name: "Code", url: "https://github.com/NLP2CT/DetectRL"}
tags: ['GPT', 'Applications', 'Model Architecture', 'Reinforcement Learning', 'Security', 'Has Code', 'Prompting']
---
Detecting text generated by large language models (LLMs) is of great recent
interest. With zero-shot methods like DetectGPT, detection capabilities have
reached impressive levels. However, the reliability of existing detectors in
real-world applications remains underexplored. In this study, we present a new
benchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection
techniques still underperformed in this task. We collected human-written
datasets from domains where LLMs are particularly prone to misuse. Using
popular LLMs, we generated data that better aligns with real-world
applications. Unlike previous studies, we employed heuristic rules to create
adversarial LLM-generated text, simulating various prompts usages, human
revisions like word substitutions, and writing noises like spelling mistakes.
Our development of DetectRL reveals the strengths and limitations of current
SOTA detectors. More importantly, we analyzed the potential impact of writing
styles, model types, attack methods, the text lengths, and real-world human
writing factors on different types of detectors. We believe DetectRL could
serve as an effective benchmark for assessing detectors in real-world
scenarios, evolving with advanced attack methods, thus providing more stressful
evaluation to drive the development of more efficient detectors. Data and code
are publicly available at: https://github.com/NLP2CT/DetectRL.
