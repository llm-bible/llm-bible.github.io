---
layout: publication
title: Tagengo A Multilingual Chat Dataset
authors: Devine Peter
conference: "Arxiv"
year: 2024
bibkey: devine2024tagengo
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.12612"}
tags: ['Ethics And Bias', 'Prompting', 'Training Techniques']
---
Open source large language models (LLMs) have shown great improvements in recent times. However many of these models are focused solely on popular spoken languages. We present a high quality dataset of more than 70k prompt-response pairs in 74 languages which consist of human generated prompts and synthetic responses. We use this dataset to train a state-of-the-art open source English LLM to chat multilingually. We evaluate our model on MT-Bench chat benchmarks in 6 languages finding that our multilingual model outperforms previous state-of-the-art open source LLMs across each language. We further find that training on more multilingual data is beneficial to the performance in a chosen target language (Japanese) compared to simply training on only data in that language. These results indicate the necessity of training on large amounts of high quality multilingual data to make a more accessible LLM.
