---
layout: publication
title: GPT45;RE In45;context Learning For Relation Extraction Using Large Language Models
authors: Wan Zhen, Cheng Fei, Mao Zhuoyuan, Liu Qianying, Song Haiyue, Li Jiwei, Kurohashi Sadao
conference: "Arxiv"
year: 2023
bibkey: wan2023gpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.02105"}
tags: ['BERT', 'GPT', 'Model Architecture', 'Pretraining Methods']
---
In spite of the potential for ground45;breaking achievements offered by large language models (LLMs) (e.g. GPT45;3) they still lag significantly behind fully45;supervised baselines (e.g. fine45;tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE (1) low relevance regarding entity and relation in retrieved demonstrations for in45;context learning; and (2) the strong inclination to wrongly classify NULL examples into other pre45;defined labels. In this paper we propose GPT45;RE to bridge the gap between LLMs and fully45;supervised baselines. GPT45;RE successfully addresses the aforementioned issues by (1) incorporating task45;specific entity representations in demonstration retrieval; and (2) enriching the demonstrations with gold label45;induced reasoning logic. We evaluate GPT45;RE on four widely45;used RE datasets and observe that GPT45;RE achieves improvements over not only existing GPT45;3 baselines but also fully45;supervised baselines. Specifically GPT45;RE achieves SOTA performances on the Semeval and SciERC datasets and competitive performances on the TACRED and ACE05 datasets.
