---
layout: publication
title: 'Role-playing Evaluation For Large Language Models'
authors: Yassine El Boudouri, Walter Nuninger, Julian Alvarez, Yvan Peter
conference: "Arxiv"
year: 2025
bibkey: boudouri2025role
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.13157"}
  - {name: "Code", url: "https://github.com/yelboudouri/RPEval"}
tags: ['Has Code', 'Ethics and Bias']
---
Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval
