---
layout: publication
title: 'Agent-centric Projection Of Prompting Techniques And Implications For Synthetic Training Data For Large Language Models'
authors: Dhruv Dhamani, Mary Lou Maher
conference: "Arxiv"
year: 2025
bibkey: dhamani2025agent
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.07815'}
tags: ['Agentic', 'Agent', 'Training Techniques', 'Tools', 'Model Architecture', 'Prompting']
---
Recent advances in prompting techniques and multi-agent systems for Large
Language Models (LLMs) have produced increasingly complex approaches. However,
we lack a framework for characterizing and comparing prompting techniques or
understanding their relationship to multi-agent LLM systems. This position
paper introduces and explains the concepts of linear contexts (a single,
continuous sequence of interactions) and non-linear contexts (branching or
multi-path) in LLM systems. These concepts enable the development of an
agent-centric projection of prompting techniques, a framework that can reveal
deep connections between prompting strategies and multi-agent systems. We
propose three conjectures based on this framework: (1) results from non-linear
prompting techniques can predict outcomes in equivalent multi-agent systems,
(2) multi-agent system architectures can be replicated through single-LLM
prompting techniques that simulate equivalent interaction patterns, and (3)
these equivalences suggest novel approaches for generating synthetic training
data. We argue that this perspective enables systematic cross-pollination of
research findings between prompting and multi-agent domains, while providing
new directions for improving both the design and training of future LLM
systems.
