---
layout: publication
title: 'Uro-bench: A Comprehensive Benchmark For End-to-end Spoken Dialogue Models'
authors: Ruiqi Yan, Xiquan Li, Wenxi Chen, Zhikang Niu, Chen Yang, Ziyang Ma, Kai Yu, Xie Chen
conference: "Arxiv"
year: 2025
bibkey: yan2025uro
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.17810"}
tags: ['Uncategorized']
---
In recent years, with advances in large language models (LLMs), end-to-end
spoken dialogue models (SDMs) have made significant strides. Compared to
text-based LLMs, the evaluation of SDMs needs to take speech-related aspects
into account, such as paralinguistic information and speech quality. However,
there is still a lack of comprehensive evaluations for SDMs in speech-to-speech
(S2S) scenarios. To address this gap, we propose URO-Bench, an extensive
benchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers
evaluations about multilingualism, multi-round dialogues, and paralinguistics.
Our benchmark is divided into two difficulty levels: basic track and pro track,
consisting of 16 and 20 datasets respectively, evaluating the model's abilities
in Understanding, Reasoning, and Oral conversation. Evaluations on our proposed
benchmark reveal that current open-source SDMs perform rather well in daily QA
tasks, but lag behind their backbone LLMs in terms of instruction-following
ability and also suffer from catastrophic forgetting. Their performance in
advanced evaluations of paralinguistic information and audio understanding
remains subpar, highlighting the need for further research in this direction.
We hope that URO-Bench can effectively facilitate the development of spoken
dialogue models by providing a multifaceted evaluation of existing models and
helping to track progress in this area.
