---
layout: publication
title: 'Iconicity In Large Language Models'
authors: Anna Marklová, Jiří Milička, Leonid Ryvkin, Ľudmila Lacková Bennet, Libuše Kormaníková
conference: "Arxiv"
year: 2025
bibkey: marklová2025iconicity
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.05643'}
tags: ['GPT', 'Tokenization', 'Model Architecture']
---
Lexical iconicity, a direct relation between a word's meaning and its form,
is an important aspect of every natural language, most commonly manifesting
through sound-meaning associations. Since Large language models' (LLMs') access
to both meaning and sound of text is only mediated (meaning through textual
context, sound through written representation, further complicated by
tokenization), we might expect that the encoding of iconicity in LLMs would be
either insufficient or significantly different from human processing. This
study addresses this hypothesis by having GPT-4 generate highly iconic
pseudowords in artificial languages. To verify that these words actually carry
iconicity, we had their meanings guessed by Czech and German participants
(n=672) and subsequently by LLM-based participants (generated by GPT-4 and
Claude 3.5 Sonnet). The results revealed that humans can guess the meanings of
pseudowords in the generated iconic language more accurately than words in
distant natural languages and that LLM-based participants are even more
successful than humans in this task. This core finding is accompanied by
several additional analyses concerning the universality of the generated
language and the cues that both human and LLM-based participants utilize.
