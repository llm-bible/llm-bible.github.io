---
layout: publication
title: Mixture45;of45;agents Enhances Large Language Model Capabilities
authors: Wang Junlin, Wang Jue, Athiwaratkun Ben, Zhang Ce, Zou James
conference: "Arxiv"
year: 2024
bibkey: wang2024mixture
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.04692"}
tags: ['Agentic', 'Applications', 'GPT', 'Model Architecture', 'RAG']
---
Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture45;of45;Agents (MoA) methodology. In our approach we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state45;of45;art performance on AlpacaEval 2.0 MT45;Bench and FLASK surpassing GPT45;4 Omni. For example our MoA using only open45;source LLMs is the leader of AlpacaEval 2.0 by a substantial gap achieving a score of 65.137; compared to 57.537; by GPT45;4 Omni.
