---
layout: publication
title: 'Llm-assisted Automated Deductive Coding Of Dialogue Data: Leveraging Dialogue-specific Characteristics To Enhance Contextual Understanding'
authors: Ying Na, Shihui Feng
conference: "Arxiv"
year: 2025
bibkey: na2025llm
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.19734'}
tags: ['RAG', 'Model Architecture', 'Tools', 'GPT', 'Prompting', 'Reinforcement Learning']
---
Dialogue data has been a key source for understanding learning processes,
offering critical insights into how students engage in collaborative
discussions and how these interactions shape their knowledge construction. The
advent of Large Language Models (LLMs) has introduced promising opportunities
for advancing qualitative research, particularly in the automated coding of
dialogue data. However, the inherent contextual complexity of dialogue presents
unique challenges for these models, especially in understanding and
interpreting complex contextual information. This study addresses these
challenges by developing a novel LLM-assisted automated coding approach for
dialogue data. The novelty of our proposed framework is threefold: 1) We
predict the code for an utterance based on dialogue-specific characteristics --
communicative acts and communicative events -- using separate prompts following
the role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs
including GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We
leveraged the interrelation between events and acts to implement consistency
checking using GPT-4o. In particular, our contextual consistency checking
provided a substantial accuracy improvement. We also found the accuracy of act
predictions was consistently higher than that of event predictions. This study
contributes a new methodological framework for enhancing the precision of
automated coding of dialogue data as well as offers a scalable solution for
addressing the contextual challenges inherent in dialogue analysis.
