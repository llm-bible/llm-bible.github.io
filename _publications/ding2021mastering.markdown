---
layout: publication
title: Cogview Mastering Text45;to45;image Generation Via Transformers
authors: Ding Ming, Yang Zhuoyi, Hong Wenyi, Zheng Wendi, Zhou Chang, Yin Da, Lin Junyang, Zou Xu, Shao Zhou, Yang Hongxia, Tang Jie
conference: "Arxiv"
year: 2021
bibkey: ding2021mastering
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2105.13290"}
tags: ['Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Text45;to45;Image generation in the general domain has long been an open problem which requires both a powerful generative model and cross45;modal understanding. We propose CogView a 445;billion45;parameter Transformer with VQ45;VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks e.g. style learning super45;resolution text45;image ranking and fashion design and methods to stabilize pretraining e.g. eliminating NaN losses. CogView achieves the state45;of45;the45;art FID on the blurred MS COCO dataset outperforming previous GAN45;based models and a recent similar work DALL45;E.
