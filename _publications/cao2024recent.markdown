---
layout: publication
title: Recent Advances In Text Embedding A Comprehensive Review Of Top-performing Methods On The MTEB Benchmark
authors: Cao Hongliu
conference: "Arxiv"
year: 2024
bibkey: cao2024recent
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.01607"}
tags: ['Applications', 'RAG', 'Training Techniques']
---
Text embedding methods have become increasingly popular in both industrial and academic fields due to their critical role in a variety of natural language processing tasks. The significance of universal text embeddings has been further highlighted with the rise of Large Language Models (LLMs) applications such as Retrieval-Augmented Systems (RAGs). While previous models have attempted to be general-purpose they often struggle to generalize across tasks and domains. However recent advancements in training data quantity quality and diversity; synthetic data generation from LLMs as well as using LLMs as backbones encourage great improvements in pursuing universal text embeddings. In this paper we provide an overview of the recent advances in universal text embedding models with a focus on the top performing text embeddings on Massive Text Embedding Benchmark (MTEB). Through detailed comparison and analysis we highlight the key contributions and limitations in this area and propose potentially inspiring future research directions.
