---
layout: publication
title: '\(\textit{latent}\)-glat: Glancing At Latent Variables For Parallel Text Generation'
authors: Yu Bao, Hao Zhou, Shujian Huang, Dongqi Wang, Lihua Qian, Xinyu Dai, Jiajun Chen, Lei Li
conference: "Arxiv"
year: 2022
bibkey: bao2022glancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.02030"}
tags: ['Efficiency and Optimization', 'GPT', 'Applications', 'Language Modeling', 'Model Architecture', 'Training Techniques', 'Attention Mechanism', 'Pretraining Methods']
---
Recently, parallel text generation has received widespread attention due to
its success in generation efficiency. Although many advanced techniques are
proposed to improve its generation quality, they still need the help of an
autoregressive model for training to overcome the one-to-many multi-modal
phenomenon in the dataset, limiting their applications. In this paper, we
propose \\(\textit\{latent\}\\)-GLAT, which employs the discrete latent variables to
capture word categorical information and invoke an advanced curriculum learning
technique, alleviating the multi-modality problem. Experiment results show that
our method outperforms strong baselines without the help of an autoregressive
model, which further broadens the application scenarios of the parallel
decoding paradigm.
