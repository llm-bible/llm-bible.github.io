---
layout: publication
title: Unveiling The Safety Of Gpt45;4o An Empirical Study Using Jailbreak Attacks
authors: Ying Zonghao, Liu Aishan, Liu Xianglong, Tao Dacheng
conference: "Arxiv"
year: 2024
bibkey: ying2024unveiling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.06302"}
  - {name: "Code", url: "https://github.com/NY1024/Jailbreak&#95;GPT4o&#125;"}
tags: ['Attention Mechanism', 'Efficiency And Optimization', 'GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning', 'Responsible AI', 'Security']
---
The recent release of GPT45;4o has garnered widespread attention due to its powerful general capabilities. While its impressive performance is widely acknowledged its safety aspects have not been sufficiently explored. Given the potential societal impact of risky content generated by advanced generative AI such as GPT45;4o it is crucial to rigorously evaluate its safety. In response to this question this paper for the first time conducts a rigorous evaluation of GPT45;4o against jailbreak attacks. Specifically this paper adopts a series of multi45;modal and uni45;modal jailbreak attacks on 4 commonly used benchmarks encompassing three modalities (ie text speech and image) which involves the optimization of over 4000 initial text queries and the analysis and statistical evaluation of nearly 8000+ response on GPT45;4o. Our extensive experiments reveal several novel observations (1) In contrast to the previous version (such as GPT45;4V) GPT45;4o has enhanced safety in the context of text modality jailbreak; (2) The newly introduced audio modality opens up new attack vectors for jailbreak attacks on GPT45;4o; (3) Existing black45;box multimodal jailbreak attack methods are largely ineffective against GPT45;4o and GPT45;4V. These findings provide critical insights into the safety implications of GPT45;4o and underscore the need for robust alignment guardrails in large models. Our code is available at url123;https://github.com/NY1024/Jailbreak&#95;GPT4o&#125;.
