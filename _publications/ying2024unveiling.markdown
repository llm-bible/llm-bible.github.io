---
layout: publication
title: 'Unveiling The Safety Of Gpt-4o: An Empirical Study Using Jailbreak Attacks'
authors: Ying Zonghao, Liu Aishan, Liu Xianglong, Tao Dacheng
conference: "Arxiv"
year: 2024
bibkey: ying2024unveiling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.06302"}
  - {name: "Code", url: "https://github.com/NY1024/Jailbreak_GPT4o"}
tags: ['Attention Mechanism', 'Efficiency And Optimization', 'GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning', 'Responsible AI', 'Security']
---
The recent release of GPT-4o has garnered widespread attention due to its powerful general capabilities. While its impressive performance is widely acknowledged its safety aspects have not been sufficiently explored. Given the potential societal impact of risky content generated by advanced generative AI such as GPT-4o it is crucial to rigorously evaluate its safety. In response to this question this paper for the first time conducts a rigorous evaluation of GPT-4o against jailbreak attacks. Specifically this paper adopts a series of multi-modal and uni-modal jailbreak attacks on 4 commonly used benchmarks encompassing three modalities (ie text speech and image) which involves the optimization of over 4000 initial text queries and the analysis and statistical evaluation of nearly 8000+ response on GPT-4o. Our extensive experiments reveal several novel observations (1) In contrast to the previous version (such as GPT-4V) GPT-4o has enhanced safety in the context of text modality jailbreak; (2) The newly introduced audio modality opens up new attack vectors for jailbreak attacks on GPT-4o; (3) Existing black-box multimodal jailbreak attack methods are largely ineffective against GPT-4o and GPT-4V. These findings provide critical insights into the safety implications of GPT-4o and underscore the need for robust alignment guardrails in large models. Our code is available at (url)https://github.com/NY1024/Jailbreak\_GPT4o\}."
