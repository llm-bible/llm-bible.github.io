---
layout: publication
title: 'When One LLM Drools, Multi-llm Collaboration Rules'
authors: Shangbin Feng, Wenxuan Ding, Alisa Liu, Zifeng Wang, Weijia Shi, Yike Wang, Zejiang Shen, Xiaochuang Han, Hunter Lang, Chen-yu Lee, Tomas Pfister, Yejin Choi, Yulia Tsvetkov
conference: "Arxiv"
year: 2025
bibkey: feng2025when
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.04506"}
tags: ['Tools', 'Training Techniques', 'Reinforcement Learning']
---
This position paper argues that in many realistic (i.e., complex,
contextualized, subjective) scenarios, one LLM is not enough to produce a
reliable output. We challenge the status quo of relying solely on a single
general-purpose LLM and argue for multi-LLM collaboration to better represent
the extensive diversity of data, skills, and people. We first posit that a
single LLM underrepresents real-world data distributions, heterogeneous skills,
and pluralistic populations, and that such representation gaps cannot be
trivially patched by further training a single LLM. We then organize existing
multi-LLM collaboration methods into a hierarchy, based on the level of access
and information exchange, ranging from API-level, text-level, logit-level, to
weight-level collaboration. Based on these methods, we highlight how multi-LLM
collaboration addresses challenges that a single LLM struggles with, such as
reliability, democratization, and pluralism. Finally, we identify the
limitations of existing multi-LLM methods and motivate future work. We envision
multi-LLM collaboration as an essential path toward compositional intelligence
and collaborative AI development.
