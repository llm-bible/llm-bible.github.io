---
layout: publication
title: Open Artificial Knowledge
authors: Borisov Vadim, Schreiber Richard H.
conference: "Arxiv"
year: 2024
bibkey: borisov2024open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.14371"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'RAG', 'Training Techniques']
---
The tremendous success of chat45;based AI systems like ChatGPT Claude and Gemini stems from Large Language Models (LLMs) trained on vast amount of datasets. However acquiring high45;quality diverse and ethically sourced training data remains a significant challenge. We introduce the Open Artificial Knowledge (OAK) dataset a large45;scale resource of over 500 million tokens (at the moment of writing) designed to address this issue. OAK leverages an ensemble of state45;of45;the45;art LLMs including GPT4o LLaMa345;70B LLaMa345;8B Mixtral45;8x7B Gemma45;7B and Gemma45;245;9B to generate high45;quality text across diverse domains guided by Wikipedias main categories. Our methodology ensures broad knowledge coverage while maintaining coherence and factual accuracy. The OAK dataset aims to foster the development of more capable and aligned language models while addressing critical issues of data scarcity and privacy in LLM training and it is freely available on www.oakdataset.org.
