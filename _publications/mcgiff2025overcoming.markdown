---
layout: publication
title: 'Overcoming Data Scarcity In Generative Language Modelling For Low-resource Languages: A Systematic Review'
authors: Josh Mcgiff, Nikola S. Nikolov
conference: "Arxiv"
year: 2025
bibkey: mcgiff2025overcoming
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.04531"}
tags: ['Training Techniques', 'Model Architecture', 'Survey Paper', 'Tools', 'Reinforcement Learning', 'GPT', 'Pretraining Methods', 'Transformer', 'Prompting']
---
Generative language modelling has surged in popularity with the emergence of
services such as ChatGPT and Google Gemini. While these models have
demonstrated transformative potential in productivity and communication, they
overwhelmingly cater to high-resource languages like English. This has
amplified concerns over linguistic inequality in natural language processing
(NLP). This paper presents the first systematic review focused specifically on
strategies to address data scarcity in generative language modelling for
low-resource languages (LRL). Drawing from 54 studies, we identify, categorise
and evaluate technical approaches, including monolingual data augmentation,
back-translation, multilingual training, and prompt engineering, across
generative tasks. We also analyse trends in architecture choices, language
family representation, and evaluation methods. Our findings highlight a strong
reliance on transformer-based models, a concentration on a small subset of
LRLs, and a lack of consistent evaluation across studies. We conclude with
recommendations for extending these methods to a wider range of LRLs and
outline open challenges in building equitable generative language systems.
Ultimately, this review aims to support researchers and developers in building
inclusive AI tools for underrepresented languages, a necessary step toward
empowering LRL speakers and the preservation of linguistic diversity in a world
increasingly shaped by large-scale language technologies.
