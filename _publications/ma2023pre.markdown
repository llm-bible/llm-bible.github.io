---
layout: publication
title: Pre45;training With Large Language Model45;based Document Expansion For Dense Passage Retrieval
authors: Ma Guangyuan, Wu Xing, Wang Peng, Lin Zijia, Hu Songlin
conference: "Arxiv"
year: 2023
bibkey: ma2023pre
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.08285"}
tags: ['Pretraining Methods', 'RAG', 'Training Techniques']
---
In this paper we systematically study the potential of pre45;training with Large Language Model(LLM)45;based document expansion for dense passage retrieval. Concretely we leverage the capabilities of LLMs for document expansion i.e. query generation and effectively transfer expanded knowledge to retrievers using pre45;training strategies tailored for passage retrieval. These strategies include contrastive learning and bottlenecked query generation. Furthermore we incorporate a curriculum learning strategy to reduce the reliance on LLM inferences. Experimental results demonstrate that pre45;training with LLM45;based document expansion significantly boosts the retrieval performance on large45;scale web45;search tasks. Our work shows strong zero45;shot and out45;of45;domain retrieval abilities making it more widely applicable for retrieval when initializing with no human45;labeled data.
