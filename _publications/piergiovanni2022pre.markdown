---
layout: publication
title: Pre45;training Image45;language Transformers For Open45;vocabulary Tasks
authors: Piergiovanni Aj, Kuo Weicheng, Angelova Anelia
conference: "Arxiv"
year: 2022
bibkey: piergiovanni2022pre
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2209.04372"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
We present a pre45;training approach for vision and language transformer models which is based on a mixture of diverse tasks. We explore both the use of image45;text captioning data in pre45;training which does not need additional supervision as well as object45;aware strategies to pre45;train the model. We evaluate the method on a number of textgenerative vision+language tasks such as Visual Question Answering visual entailment and captioning and demonstrate large gains over standard pre45;training methods.
