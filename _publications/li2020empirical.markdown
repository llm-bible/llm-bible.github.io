---
layout: publication
title: An Empirical Investigation Of Pre45;trained Transformer Language Models For Open45;domain Dialogue Generation
authors: Li Piji
conference: "Arxiv"
year: 2020
bibkey: li2020empirical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2003.04195"}
tags: ['Applications', 'Language Modeling', 'Masked Language Model', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
We present an empirical investigation of pre45;trained Transformer45;based auto45;regressive language models for the task of open45;domain dialogue generation. Training paradigm of pre45;training and fine45;tuning is employed to conduct the parameter learning. Corpora of News and Wikipedia in Chinese and English are collected for the pre45;training stage respectively. Dialogue context and response are concatenated into a single sequence utilized as the input of the models during the fine45;tuning stage. A weighted joint prediction paradigm for both context and response is designed to evaluate the performance of models with or without the loss term for context prediction. Various of decoding strategies such as greedy search beam search top45;k sampling etc. are employed to conduct the response text generation. Extensive experiments are conducted on the typical single45;turn and multi45;turn dialogue corpora such as Weibo Douban Reddit DailyDialog and Persona45;Chat. Detailed numbers of automatic evaluation metrics on relevance and diversity of the generated results for the languages models as well as the baseline approaches are reported.
