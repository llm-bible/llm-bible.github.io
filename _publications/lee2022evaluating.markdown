---
layout: publication
title: Evaluating Human45;language Model Interaction
authors: Lee Mina, Srivastava Megha, Hardy Amelia, Thickstun John, Durmus Esin, Paranjape Ashwin, Gerard-ursin Ines, Li Xiang Lisa, Ladhak Faisal, Rong Frieda, Wang Rose E., Kwon Minae, Park Joon Sung, Cao Hancheng, Lee Tony, Bommasani Rishi, Bernstein Michael, Liang Percy
conference: "Arxiv"
year: 2022
bibkey: lee2022evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2212.09746"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
Many real45;world applications of language models (LMs) such as writing assistance and code autocomplete involve human45;LM interaction. However most benchmarks are non45;interactive in that a model produces output without human involvement. To evaluate human45;LM interaction we develop a new framework Human45;AI Language45;based Interaction Evaluation (HALIE) that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard non45;interactive evaluation HALIE captures (i) the interactive process not only the final output; (ii) the first45;person subjective experience not just a third45;party assessment; and (iii) notions of preference beyond quality (e.g. enjoyment and ownership). We then design five tasks to cover different forms of interaction social dialogue question answering crossword puzzles summarization and metaphor generation. With four state45;of45;the45;art LMs (three variants of OpenAIs GPT45;3 and AI21 Labs Jurassic45;1) we find that better non45;interactive performance does not always translate to better human45;LM interaction. In particular we highlight three cases where the results from non45;interactive and interactive metrics diverge and underscore the importance of human45;LM interaction for LM evaluation.
