---
layout: publication
title: 'Look Ahead Text Understanding And LLM Stitching'
authors: Junlin Julian Piedmont High School, Piedmont, Ca, Usa Jiang, Xin College Of Business, City University Of Hong Kong, Hong Kong, China Li
conference: "Proceedings of the International AAAI Conference on Web and Social Media 18(1) 751-760 (2024)"
year: 2024
bibkey: jiang2024look
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.17836'}
tags: ['Transformer', 'RAG', 'GPT', 'BERT', 'Model Architecture', 'Reinforcement Learning', 'Pretraining Methods']
---
This paper proposes a look ahead text understanding problem with look ahead
section identification (LASI) as an example. This problem may appear in
generative AI as well as human interactions, where we want to understand the
direction of a developing text or conversation. We tackle the problem using
transformer-based LLMs. We show that LASI is more challenging than classic
section identification (SI). We argue that both bidirectional contextual
information (e.g., BERT) and unidirectional predictive ability (e.g., GPT) will
benefit the task. We propose two approaches to stitch together BERT and GPT.
Experiments show that our approach outperforms the established models,
especially when there is noise in the text (which is often the case for
developing text in generative AI). Our paper sheds light on other look ahead
text understanding tasks that are important to social media, such as look ahead
sentiment classification, and points out the opportunities to leverage
pre-trained LLMs through stitching.
