---
layout: publication
title: 'Attacks, Defenses And Evaluations For LLM Conversation Safety: A Survey'
authors: Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
conference: "Arxiv"
year: 2024
bibkey: dong2024defenses
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.09283"}
  - {name: "Code", url: "https://github.com/niconi19/LLM-conversation-safety"}
tags: ['Responsible AI', 'Security', 'Survey Paper', 'RAG', 'Has Code', 'Applications']
---
Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
