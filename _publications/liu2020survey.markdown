---
layout: publication
title: A Survey On Contextual Embeddings
authors: Qi Liu, Matt J. Kusner, Phil Blunsom
conference: Arxiv
year: 2020
citations: 111
bibkey: liu2020survey
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2003.07278'}]
tags: [Pre-Training, BERT, Survey Paper, Quantization]
---
Contextual embeddings, such as ELMo and BERT, move beyond global word
representations like Word2Vec and achieve ground-breaking performance on a wide
range of natural language processing tasks. Contextual embeddings assign each
word a representation based on its context, thereby capturing uses of words
across varied contexts and encoding knowledge that transfers across languages.
In this survey, we review existing contextual embedding models, cross-lingual
polyglot pre-training, the application of contextual embeddings in downstream
tasks, model compression, and model analyses.