---
layout: publication
title: Conversational Question Answering With Reformulations Over Knowledge Graph
authors: Liu Lihui, Hill Blaine, Du Boxin, Wang Fei, Tong Hanghang
conference: "Arxiv"
year: 2023
bibkey: liu2023conversational
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.17269"}
tags: ['Agentic', 'Applications', 'Model Architecture', 'Reinforcement Learning']
---
Conversational question answering (convQA) over knowledge graphs (KGs) involves answering multi45;turn natural language questions about information contained in a KG. State45;of45;the45;art methods of ConvQA often struggle with inexplicit question45;answer pairs. These inputs are easy for human beings to understand given a conversation history but hard for a machine to interpret which can degrade ConvQA performance. To address this problem we propose a reinforcement learning (RL) based model CornNet which utilizes question reformulations generated by large language models (LLMs) to improve ConvQA performance. CornNet adopts a teacher45;student architecture where a teacher model learns question representations using human writing reformulations and a student model to mimic the teacher models output via reformulations generated by LLMs. The learned question representation is then used by an RL model to locate the correct answer in a KG. Extensive experimental results show that CornNet outperforms state45;of45;the45;art convQA models.
