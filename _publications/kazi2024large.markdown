---
layout: publication
title: 'Large Language Models As User-agents For Evaluating Task-oriented-dialogue Systems'
authors: Taaha Kazi, Ruiliang Lyu, Sizhe Zhou, Dilek Hakkani-tur, Gokhan Tur
conference: "Arxiv"
year: 2024
bibkey: kazi2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.09972"}
tags: ['Prompting', 'Agentic', 'Applications', 'Tools']
---
Traditionally, offline datasets have been used to evaluate task-oriented
dialogue (TOD) models. These datasets lack context awareness, making them
suboptimal benchmarks for conversational systems. In contrast, user-agents,
which are context-aware, can simulate the variability and unpredictability of
human conversations, making them better alternatives as evaluators. Prior
research has utilized large language models (LLMs) to develop user-agents. Our
work builds upon this by using LLMs to create user-agents for the evaluation of
TOD systems. This involves prompting an LLM, using in-context examples as
guidance, and tracking the user-goal state. Our evaluation of diversity and
task completion metrics for the user-agents shows improved performance with the
use of better prompts. Additionally, we propose methodologies for the automatic
evaluation of TOD models within this dynamic framework.
