---
layout: publication
title: 'A Reasoning-focused Legal Retrieval Benchmark'
authors: Lucia Zheng, Neel Guha, Javokhir Arifov, Sarah Zhang, Michal Skreta, Christopher D. Manning, Peter Henderson, Daniel E. Ho
conference: "Arxiv"
year: 2025
bibkey: zheng2025reasoning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.03970"}
tags: ['Applications', 'RAG', 'TACL', 'Reinforcement Learning', 'ACL', 'Security']
---
As the legal community increasingly examines the use of large language models
(LLMs) for various legal applications, legal AI developers have turned to
retrieval-augmented LLMs ("RAG" systems) to improve system performance and
robustness. An obstacle to the development of specialized RAG systems is the
lack of realistic legal RAG benchmarks which capture the complexity of both
legal retrieval and downstream legal question-answering. To address this, we
introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.
Our tasks correspond to real-world legal research tasks, and were produced
through annotation processes which resemble legal research. We describe the
construction of these benchmarks and the performance of existing retriever
pipelines. Our results suggest that legal RAG remains a challenging
application, thus motivating future research.
