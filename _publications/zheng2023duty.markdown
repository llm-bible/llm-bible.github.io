---
layout: publication
title: Ddcot Duty45;distinct Chain45;of45;thought Prompting For Multimodal Reasoning In Language Models
authors: Ge Zheng, Bin Yang, Jiajin Tang, Hong-yu Zhou, Sibei Yang
conference: "Arxiv"
year: 2023
bibkey: zheng2023duty
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2310.16436v2"}
tags: ['Interpretability And Explainability', 'Multimodal Models', 'Prompting', 'RAG']
---
A long45;standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently large language models (LLMs) have made remarkable strides in such multi45;step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However the transfer of these advancements to multimodal contexts introduces heightened challenges including but not limited to the impractical need for labor45;intensive annotation and the limitations in terms of flexibility generalizability and explainability. To evoke CoT reasoning in multimodality this work first conducts an in45;depth analysis of these challenges posed by multimodality and presents two key insights keeping critical thinking and letting everyone do their jobs in multimodal CoT reasoning. Furthermore this study proposes a novel DDCoT prompting that maintains a critical attitude through negative45;space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero45;shot prompting and fine45;tuning learning significantly outperforming state45;of45;the45;art methods but also exhibit impressive generalizability and explainability.
