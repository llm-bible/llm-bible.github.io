---
layout: publication
title: Transllama Llm45;based Simultaneous Translation System
authors: Koshkin Roman, Sudoh Katsuhito, Nakamura Satoshi
conference: "Arxiv"
year: 2024
bibkey: koshkin2024llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.04636"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques', 'Transformer']
---
Decoder45;only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless they have limited applications in simultaneous machine translation (SiMT) currently dominated by encoder45;decoder transformers. This study demonstrates that after fine45;tuning on a small dataset comprising causally aligned source and target sentence pairs a pre45;trained open45;source LLM can control input segmentation directly by generating a special wait token. This obviates the need for a separate policy and enables the LLM to perform English45;German and English45;Russian SiMT tasks with BLEU scores that are comparable to those of specific state45;of45;the45;art baselines. We also evaluated closed45;source models such as GPT45;4 which displayed encouraging results in performing the SiMT task without prior training (zero45;shot) indicating a promising avenue for enhancing future SiMT systems.
