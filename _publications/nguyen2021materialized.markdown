---
layout: publication
title: Materialized Knowledge Bases From Commonsense Transformers
authors: Nguyen Tuan-phong, Razniewski Simon
conference: "Proceedings of the First Workshop on Commonsense Representation and Reasoning"
year: 2021
bibkey: nguyen2021materialized
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2112.14815"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Starting from the COMET methodology by Bosselut et al. (2019) generating commonsense knowledge directly from pre45;trained language models has recently received significant attention. Surprisingly up to now no materialized resource of commonsense knowledge generated this way is publicly available. This paper fills this gap and uses the materialized resources to perform a detailed analysis of the potential of this approach in terms of precision and recall. Furthermore we identify common problem cases and outline use cases enabled by materialized resources. We posit that the availability of these resources is important for the advancement of the field as it enables an off45;the45;shelf45;use of the resulting knowledge as well as further analyses on its strengths and weaknesses.
