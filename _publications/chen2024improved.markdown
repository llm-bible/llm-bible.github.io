---
layout: publication
title: Llast: Improved End-to-end Speech Translation System Leveraged By Large Language Models
authors: Chen Xi, Zhang Songyang, Bai Qibing, Chen Kai, Nakamura Satoshi
conference: "Arxiv"
year: 2024
bibkey: chen2024improved
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.15415"}
  - {name: "Code", url: "https://github.com/openaudiolab/LLaST"}
tags: ['Efficiency And Optimization', 'Fine Tuning', 'Has Code', 'Model Architecture', 'RAG', 'Tools', 'Training Techniques']
---
We introduces LLaST a framework for building high-performance Large Language model based Speech-to-text Translation systems. We address the limitations of end-to-end speech translation(E2E ST) models by exploring model architecture design and optimization techniques tailored for LLMs. Our approach includes LLM-based speech translation architecture design ASR-augmented training multilingual data augmentation and dual-LoRA optimization. Our approach demonstrates superior performance on the CoVoST-2 benchmark and showcases exceptional scaling capabilities powered by LLMs. We believe this effective method will serve as a strong baseline for speech translation and provide insights for future improvements of the LLM-based speech translation framework. We release the data code and models in https://github.com/openaudiolab/LLaST."
