---
layout: publication
title: Llast Improved End45;to45;end Speech Translation System Leveraged By Large Language Models
authors: Chen Xi, Zhang Songyang, Bai Qibing, Chen Kai, Nakamura Satoshi
conference: "Arxiv"
year: 2024
bibkey: chen2024improved
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.15415"}
  - {name: "Code", url: "https://github.com/openaudiolab/LLaST"}
tags: ['Efficiency And Optimization', 'Fine Tuning', 'Has Code', 'Model Architecture', 'RAG', 'Tools', 'Training Techniques']
---
We introduces LLaST a framework for building high45;performance Large Language model based Speech45;to45;text Translation systems. We address the limitations of end45;to45;end speech translation(E2E ST) models by exploring model architecture design and optimization techniques tailored for LLMs. Our approach includes LLM45;based speech translation architecture design ASR45;augmented training multilingual data augmentation and dual45;LoRA optimization. Our approach demonstrates superior performance on the CoVoST45;2 benchmark and showcases exceptional scaling capabilities powered by LLMs. We believe this effective method will serve as a strong baseline for speech translation and provide insights for future improvements of the LLM45;based speech translation framework. We release the data code and models in https://github.com/openaudiolab/LLaST.
