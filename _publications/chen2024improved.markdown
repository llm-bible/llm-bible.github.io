---
layout: publication
title: 'Llast: Improved End-to-end Speech Translation System Leveraged By Large Language Models'
authors: Xi Chen, Songyang Zhang, Qibing Bai, Kai Chen, Satoshi Nakamura
conference: "Arxiv"
year: 2024
bibkey: chen2024improved
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.15415"}
  - {name: "Code", url: "https://github.com/openaudiolab/LLaST"}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Tools', 'RAG', 'Fine-Tuning', 'Has Code']
---
We introduces LLaST, a framework for building high-performance Large Language
model based Speech-to-text Translation systems. We address the limitations of
end-to-end speech translation(E2E ST) models by exploring model architecture
design and optimization techniques tailored for LLMs. Our approach includes
LLM-based speech translation architecture design, ASR-augmented training,
multilingual data augmentation, and dual-LoRA optimization. Our approach
demonstrates superior performance on the CoVoST-2 benchmark and showcases
exceptional scaling capabilities powered by LLMs. We believe this effective
method will serve as a strong baseline for speech translation and provide
insights for future improvements of the LLM-based speech translation framework.
We release the data, code and models in https://github.com/openaudiolab/LLaST.
