---
layout: publication
title: 'Llmsteer: Improving Long-context LLM Inference By Steering Attention On Reused Contexts'
authors: Zhuohan Gu, Jiayi Yao, Kuntai Du, Junchen Jiang
conference: "Arxiv"
year: 2024
bibkey: gu2024improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.13009"}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Tools', 'Pretraining Methods', 'Fine-Tuning', 'Attention Mechanism']
---
As large language models (LLMs) show impressive performance on complex tasks,
they still struggle with longer contextual understanding and high computational
costs. To balance efficiency and quality, we introduce LLMSteer, a
fine-tuning-free framework that enhances LLMs through query-independent
attention steering. Tested on popular LLMs and datasets, LLMSteer narrows the
performance gap with baselines by 65.9% and reduces the runtime delay by up to
4.8x compared to recent attention steering methods.
