---
layout: publication
title: 'X-llava: Optimizing Bilingual Large Vision-language Alignment'
authors: Dongjae Shin, Hyeonseok Lim, Inho Won, Changsu Choi, Minjun Kim, Seungwoo Song, Hangyeol Yoo, Sangmin Kim, Kyungtae Lim
conference: "https://aclanthology.org/2024.findings-naacl.158"
year: 2024
bibkey: shin2024x
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2403.11399'}
tags: ['Training Techniques', 'Model Architecture', 'GPT', 'Multimodal Models', 'Pretraining Methods']
---
The impressive development of large language models (LLMs) is expanding into
the realm of large multimodal models (LMMs), which incorporate multiple types
of data beyond text. However, the nature of multimodal models leads to
significant expenses in the creation of training data. Furthermore,
constructing multilingual data for LMMs presents its own set of challenges due
to language diversity and complexity. Therefore, in this study, we propose two
cost-effective methods to solve this problem: (1) vocabulary expansion and
pretraining of multilingual LLM for specific languages, and (2) automatic and
elaborate construction of multimodal datasets using GPT4-V. Based on015 these
methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal
training dataset. Additionally, we developed a bilingual multimodal model that
exhibits excellent performance in both Korean and English, surpassing existing
approaches.
