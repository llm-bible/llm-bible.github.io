---
layout: publication
title: Generic Attention45;model Explainability For Interpreting Bi45;modal And Encoder45;decoder Transformers
authors: Chefer Hila, Gur Shir, Wolf Lior
conference: "Arxiv"
year: 2021
bibkey: chefer2021generic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2103.15679"}
tags: ['Applications', 'Attention Mechanism', 'Interpretability And Explainability', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Transformers are increasingly dominating multi45;modal reasoning tasks such as visual question answering achieving state45;of45;the45;art results thanks to their ability to contextualize information using the self45;attention and co45;attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self45;attention Transformers with co45;attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the models input. In this work we propose the first method to explain prediction by any Transformer45;based architecture including bi45;modal Transformers and Transformers with co45;attentions. We provide generic solutions and apply these to the three most commonly used of these architectures (i) pure self45;attention (ii) self45;attention combined with co45;attention and (iii) encoder45;decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability.
