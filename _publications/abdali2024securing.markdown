---
layout: publication
title: Securing Large Language Models Threats Vulnerabilities and Responsible Practices
authors: Abdali Sara, Anarfi Richard, Barberan Cj, He Jia
conference: "Arxiv"
year: 2024
bibkey: abdali2024securing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.12503"}
tags: ['Pretraining Methods', 'Security']
---
Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks revolutionizing how we approach language understanding and generations. Nevertheless alongside their remarkable utility LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives security and privacy concerns vulnerabilities against adversarial attacks potential harms caused by misuses of LLMs mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.
