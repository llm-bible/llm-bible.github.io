---
layout: publication
title: 'Qualbench: Benchmarking Chinese Llms With Localized Professional Qualifications For Vertical Domain Evaluation'
authors: Mengze Hong, Wailing Ng, Di Jiang, Chen Jason Zhang
conference: "Arxiv"
year: 2025
bibkey: hong2025benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.05225"}
tags: ['Tools', 'GPT', 'Applications', 'RAG', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
The rapid advancement of Chinese large language models (LLMs) underscores the
need for domain-specific evaluations to ensure reliable applications. However,
existing benchmarks often lack coverage in vertical domains and offer limited
insights into the Chinese working context. Leveraging qualification exams as a
unified framework for human expertise evaluation, we introduce QualBench, the
first multi-domain Chinese QA benchmark dedicated to localized assessment of
Chinese LLMs. The dataset includes over 17,000 questions across six vertical
domains, with data selections grounded in 24 Chinese qualifications to closely
align with national policies and working standards. Through comprehensive
evaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with
Chinese LLMs consistently surpassing non-Chinese models, highlighting the
importance of localized domain knowledge in meeting qualification requirements.
The best performance of 75.26% reveals the current gaps in domain coverage
within model capabilities. Furthermore, we present the failure of LLM
collaboration with crowdsourcing mechanisms and suggest the opportunities for
multi-domain RAG knowledge enhancement and vertical domain LLM training with
Federated Learning.
