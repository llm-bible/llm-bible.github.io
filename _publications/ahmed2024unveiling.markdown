---
layout: publication
title: LM4OPT Unveiling The Potential Of Large Language Models In Formulating Mathematical Optimization Problems
authors: Ahmed Tasnim, Choudhury Salimur
conference: "Arxiv"
year: 2024
bibkey: ahmed2024unveiling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.01342"}
tags: ['Efficiency And Optimization', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
In the rapidly evolving field of natural language processing the translation of linguistic descriptions into mathematical formulation of optimization problems presents a formidable challenge demanding intricate understanding and processing capabilities from Large Language Models (LLMs). This study compares prominent LLMs including GPT45;3.5 GPT45;4 and Llama45;245;7b in zero45;shot and one45;shot settings for this task. Our findings show GPT45;4s superior performance particularly in the one45;shot scenario. A central part of this research is the introduction of LM4OPT a progressive fine45;tuning framework for Llama45;245;7b that utilizes noisy embeddings and specialized datasets. However this research highlights a notable gap in the contextual understanding capabilities of smaller models such as Llama45;245;7b compared to larger counterparts especially in processing lengthy and complex input contexts. Our empirical investigation utilizing the NL4Opt dataset unveils that GPT45;4 surpasses the baseline performance established by previous research achieving an F145;score of 0.63 solely based on the problem description in natural language and without relying on any additional named entity information. GPT45;3.5 follows closely both outperforming the fine45;tuned Llama45;245;7b. These findings not only benchmark the current capabilities of LLMs in a novel application area but also lay the groundwork for future improvements in mathematical formulation of optimization problems from natural language input.
