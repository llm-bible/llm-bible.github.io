---
layout: publication
title: Explaining NLP Models Via Minimal Contrastive Editing (mice)
authors: "Alexis Ross, Ana Marasovi\u0107, Matthew E. Peters"
conference: Arxiv
year: 2020
citations: 22
bibkey: ross2020explaining
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2012.13985'}]
tags: [Interpretability and Explainability]
---
Humans have been shown to give contrastive explanations, which explain why an
observed event happened rather than some other counterfactual event (the
contrast case). Despite the influential role that contrastivity plays in how
humans explain, this property is largely missing from current methods for
explaining NLP models. We present Minimal Contrastive Editing (MiCE), a method
for producing contrastive explanations of model predictions in the form of
edits to inputs that change model outputs to the contrast case. Our experiments
across three tasks--binary sentiment classification, topic classification, and
multiple-choice question answering--show that MiCE is able to produce edits
that are not only contrastive, but also minimal and fluent, consistent with
human contrastive edits. We demonstrate how MiCE edits can be used for two use
cases in NLP system development--debugging incorrect model outputs and
uncovering dataset artifacts--and thereby illustrate that producing contrastive
explanations is a promising research direction for model interpretability.