---
layout: publication
title: 'A Survey On Human Preference Learning For Large Language Models'
authors: Jiang Ruili, Chen Kehai, Bai Xuefeng, He Zhixuan, Li Juntao, Yang Muyun, Zhao Tiejun, Nie Liqiang, Zhang Min
conference: "Arxiv"
year: 2024
bibkey: jiang2024survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.11191"}
tags: ['Survey Paper']
---
The recent surge of versatile large language models (LLMs) largely depends on
aligning increasingly capable foundation models with human intentions by
preference learning, enhancing LLMs with excellent applicability and
effectiveness in a wide range of contexts. Despite the numerous related studies
conducted, a perspective on how human preferences are introduced into LLMs
remains limited, which may prevent a deeper comprehension of the relationships
between human preferences and LLMs as well as the realization of their
limitations. In this survey, we review the progress in exploring human
preference learning for LLMs from a preference-centered perspective, covering
the sources and formats of preference feedback, the modeling and usage of
preference signals, as well as the evaluation of the aligned LLMs. We first
categorize the human feedback according to data sources and formats. We then
summarize techniques for human preferences modeling and compare the advantages
and disadvantages of different schools of models. Moreover, we present various
preference usage methods sorted by the objectives to utilize human preference
signals. Finally, we summarize some prevailing approaches to evaluate LLMs in
terms of alignment with human intentions and discuss our outlooks on the human
intention alignment for LLMs.
