---
layout: publication
title: M^123;2125;chat Empowering VLM For Multimodal LLM Interleaved Text45;image Generation
authors: Chi Xiaowei, Zhang Rongyu, Jiang Zhengkai, Liu Yijiang, Wang Yatian, Qi Xingqun, Luo Wenhan, Gao Peng, Zhang Shanghang, Liu Qifeng, Guo Yike
conference: "Arxiv"
year: 2023
bibkey: chi2023empowering
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.17963"}
  - {name: "Code", url: "https://mattie&#45;e.github.io/M2Chat.github.io&#125;"}
tags: ['Applications', 'GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Prompting', 'Reinforcement Learning', 'Tools']
---
While current LLM chatbots like GPT45;4V bridge the gap between human instructions and visual representations to enable text45;image generations they still lack efficient alignment methods for high45;fidelity performance on multiple downstream tasks. In this paper we propose textbf123;M^123;2125;Chat125; a novel unified multimodal LLM framework for generating interleaved text45;image conversation across various scenarios. Specifically we propose an M^123;3125;Adapter that efficiently integrates granular low45;level visual information and high45;level semantic features from multi45;modality prompts. Upon the well45;aligned fused feature M^123;3125;Adapter tailors a learnable gating strategy to balance the model creativity and consistency across various tasks adaptively. Moreover to further enhance the effectiveness of M^123;3125;Adapter while preserving the coherence of semantic context comprehension we introduce a two45;stage M^123;3125;FT fine45;tuning strategy. This strategy optimizes disjoint groups of parameters for image45;text alignment and visual45;instruction respectively. Extensive experiments demonstrate our M^123;2125;Chat surpasses state45;of45;the45;art counterparts across diverse benchmarks showcasing its prowess in interleaving generation storytelling and multimodal dialogue systems. The demo and code are available at red123;https://mattie&#45;e.github.io/M2Chat.github.io&#125;.
