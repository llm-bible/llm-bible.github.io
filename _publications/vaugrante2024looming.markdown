---
layout: publication
title: 'A Looming Replication Crisis In Evaluating Behavior In Language Models? Evidence And Solutions'
authors: Laur√®ne Vaugrante, Mathias Niepert, Thilo Hagendorff
conference: "Arxiv"
year: 2024
bibkey: vaugrante2024looming
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.20303"}
tags: ['Tools', 'GPT', 'Applications', 'Model Architecture', 'Reinforcement Learning', 'Prompting']
---
In an era where large language models (LLMs) are increasingly integrated into
a wide range of everyday applications, research into these models' behavior has
surged. However, due to the novelty of the field, clear methodological
guidelines are lacking. This raises concerns about the replicability and
generalizability of insights gained from research on LLM behavior. In this
study, we discuss the potential risk of a replication crisis and support our
concerns with a series of replication experiments focused on prompt engineering
techniques purported to influence reasoning abilities in LLMs. We tested
GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, and Llama 3-70B, on
the chain-of-thought, EmotionPrompting, ExpertPrompting, Sandbagging, as well
as Re-Reading prompt engineering techniques, using manually double-checked
subsets of reasoning benchmarks including CommonsenseQA, CRT, NumGLUE,
ScienceQA, and StrategyQA. Our findings reveal a general lack of statistically
significant differences across nearly all techniques tested, highlighting,
among others, several methodological weaknesses in previous research. We
propose a forward-looking approach that includes developing robust
methodologies for evaluating LLMs, establishing sound benchmarks, and designing
rigorous experimental frameworks to ensure accurate and reliable assessments of
model outputs.
