---
layout: publication
title: 'COOL, A Context Outlooker, And Its Application To Question Answering And Other Natural Language Processing Tasks'
authors: Fangyi Zhu, See-kiong Ng, St√©phane Bressan
conference: "Arxiv"
year: 2022
bibkey: zhu2022context
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.09593"}
tags: ['Transformer', 'Applications', 'Model Architecture', 'Reinforcement Learning', 'Attention Mechanism', 'Pretraining Methods', 'Multimodal Models']
---
Vision outlooker improves the performance of vision transformers, which
implements a self-attention mechanism by adding an outlook attention, a form of
local attention.
  In natural language processing, as has been the case in computer vision and
other domains, transformer-based models constitute the state-of-the-art for
most processing tasks. In this domain, too, many authors have argued and
demonstrated the importance of local context.
  We present an outlook attention mechanism, COOL, for natural language
processing. COOL, added on top of the self-attention layers of a
transformer-based model, encodes local syntactic context considering word
proximity and more pair-wise constraints than dynamic convolution used by
existing approaches.
  A comparative empirical performance evaluation of an implementation of COOL
with different transformer-based models confirms the opportunity for
improvement over a baseline using the original models alone for various natural
language processing tasks, including question answering. The proposed approach
achieves competitive performance with existing state-of-the-art methods on some
tasks.
