---
layout: publication
title: Llm-rec\: Personalized Recommendation Via Prompting Large Language Models
authors: Lyu Hanjia, Jiang Song, Zeng Hanqing, Xia Yinglong, Wang Qifan, Zhang Si, Chen Ren, Leung Christopher, Tang Jiajie, Luo Jiebo
conference: "Arxiv"
year: 2023
bibkey: lyu2023llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.15780"}
tags: ['Applications', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning']
---
Text-based recommendation holds a wide range of practical applications due to its versatility as textual descriptions can represent nearly any type of item. However directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study we introduce a novel approach coined LLM-Rec which incorporates four distinct prompting strategies of text enrichment for improving personalized text-based recommendations. Our empirical experiments reveal that using LLM-augmented text significantly enhances recommendation quality. Even basic MLP (Multi-Layer Perceptron) models achieve comparable or even better results than complex content-based methods. Notably the success of LLM-Rec lies in its prompting strategies which effectively tap into the language models comprehension of both general and specific item characteristics. This highlights the importance of employing diverse prompts and input augmentation techniques to boost the recommendation effectiveness of LLMs.
