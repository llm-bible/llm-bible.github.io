---
layout: publication
title: 'A Mathematical Abstraction For Balancing The Trade-off Between Creativity And Reality In Large Language Models'
authors: Sinha Ritwik, Song Zhao, Zhou Tianyi
conference: "Arxiv"
year: 2023
bibkey: sinha2023mathematical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.02295"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Large Language Models have become popular for their remarkable capabilities in human-oriented tasks and traditional natural language processing tasks. Its efficient functioning is attributed to the attention mechanism in the Transformer architecture enabling it to concentrate on particular aspects of the input. LLMs are increasingly being used in domains such as generating prose poetry or art which require the model to be creative (e.g. Adobe firefly). LLMs possess advanced language generation abilities that enable them to generate distinctive and captivating content. This utilization of LLMs in generating narratives shows their flexibility and potential for use in domains that extend beyond conventional natural language processing duties. In different contexts we may expect the LLM to generate factually correct answers that match reality; e.g. question-answering systems or online assistants. In such situations being correct is critical to LLMs being trusted in practice. The Bing Chatbot provides its users with the flexibility to select one of the three output modes creative balanced and precise. Each mode emphasizes creativity and factual accuracy differently. In this work we provide a mathematical abstraction to describe creativity and reality based on certain losses. A model trained on these losses balances the trade-off between the creativity and reality of the model.
