---
layout: publication
title: A Survey Of Controllable Text Generation Using Transformer-based Pre-trained
  Language Models
authors: Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song
conference: Arxiv
year: 2022
citations: 107
bibkey: zhang2022survey
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2201.05337'}]
tags: [Transformer, Survey Paper, Language Modeling, Applications]
---
Controllable Text Generation (CTG) is emerging area in the field of natural
language generation (NLG). It is regarded as crucial for the development of
advanced text generation technologies that better meet the specific constraints
in practical applications. In recent years, methods using large-scale
pre-trained language models (PLMs), in particular the widely used
transformer-based PLMs, have become a new paradigm of NLG, allowing generation
of more diverse and fluent text. However, due to the limited level of
interpretability of deep neural networks, the controllability of these methods
need to be guaranteed. To this end, controllable text generation using
transformer-based PLMs has become a rapidly growing yet challenging new
research hotspot. A diverse range of approaches have emerged in the recent 3-4
years, targeting different CTG tasks that require different types of controlled
constraints. In this paper, we present a systematic critical review on the
common tasks, main approaches, and evaluation methods in this area. Finally, we
discuss the challenges that the field is facing, and put forward various
promising future directions. To the best of our knowledge, this is the first
survey paper to summarize the state-of-the-art CTG techniques from the
perspective of Transformer-based PLMs. We hope it can help researchers and
practitioners in the related fields to quickly track the academic and
technological frontier, providing them with a landscape of the area and a
roadmap for future research.