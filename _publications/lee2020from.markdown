---
layout: publication
title: 'LIMEADE: From AI Explanations To Advice Taking'
authors: Benjamin Charles Germain Lee, Doug Downey, Kyle Lo, Daniel S. Weld
conference: "Arxiv"
year: 2020
bibkey: lee2020from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2003.04315"}
tags: ['Tools', 'Interpretability and Explainability', 'Model Architecture', 'Reinforcement Learning', 'Attention Mechanism']
---
Research in human-centered AI has shown the benefits of systems that can
explain their predictions. Methods that allow an AI to take advice from humans
in response to explanations are similarly useful. While both capabilities are
well-developed for transparent learning models (e.g., linear models and
GA\\(^2\\)Ms), and recent techniques (e.g., LIME and SHAP) can generate
explanations for opaque models, little attention has been given to advice
methods for opaque models. This paper introduces LIMEADE, the first general
framework that translates both positive and negative advice (expressed using
high-level vocabulary such as that employed by post-hoc explanations) into an
update to an arbitrary, underlying opaque model. We demonstrate the generality
of our approach with case studies on seventy real-world models across two broad
domains: image classification and text recommendation. We show our method
improves accuracy compared to a rigorous baseline on the image classification
domains. For the text modality, we apply our framework to a neural recommender
system for scientific papers on a public website; our user study shows that our
framework leads to significantly higher perceived user control, trust, and
satisfaction.
