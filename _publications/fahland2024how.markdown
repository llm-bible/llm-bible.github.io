---
layout: publication
title: 'How Well Can A Large Language Model Explain Business Processes As Perceived By Users?'
authors: Dirk Fahland, Fabiana Fournier, Lior Limonad, Inna Skarbovsky, Ava J. E. Swevels
conference: "Data Knowledge Engineering Volume 157 May 2025"
year: 2024
bibkey: fahland2024how
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2401.12846'}
tags: ['Interpretability and Explainability', 'RAG', 'Tools', 'Reinforcement Learning', 'Interpretability']
---
Large Language Models (LLMs) are trained on a vast amount of text to
interpret and generate human-like textual content. They are becoming a vital
vehicle in realizing the vision of the autonomous enterprise, with
organizations today actively adopting LLMs to automate many aspects of their
operations. LLMs are likely to play a prominent role in future AI-augmented
business process management systems, catering functionalities across all system
lifecycle stages. One such system's functionality is Situation-Aware
eXplainability (SAX), which relates to generating causally sound and
human-interpretable explanations. In this paper, we present the SAX4BPM
framework developed to generate SAX explanations. The SAX4BPM suite consists of
a set of services and a central knowledge repository. The functionality of
these services is to elicit the various knowledge ingredients that underlie SAX
explanations. A key innovative component among these ingredients is the causal
process execution view. In this work, we integrate the framework with an LLM to
leverage its power to synthesize the various input ingredients for the sake of
improved SAX explanations. Since the use of LLMs for SAX is also accompanied by
a certain degree of doubt related to its capacity to adequately fulfill SAX
along with its tendency for hallucination and lack of inherent capacity to
reason, we pursued a methodological evaluation of the perceived quality of the
generated explanations. We developed a designated scale and conducted a
rigorous user study. Our findings show that the input presented to the LLMs
aided with the guard-railing of its performance, yielding SAX explanations
having better-perceived fidelity. This improvement is moderated by the
perception of trust and curiosity. More so, this improvement comes at the cost
of the perceived interpretability of the explanation.
