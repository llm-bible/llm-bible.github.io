---
layout: publication
title: 'Understanding The Logical Capabilities Of Large Language Models Via Out-of-context Representation Learning'
authors: Jonathan Shaki, Emanuele La Malfa, Michael Wooldridge, Sarit Kraus
conference: "Arxiv"
year: 2025
bibkey: shaki2025understanding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.10408"}
tags: ['Fine-Tuning', 'Ethics and Bias', 'Training Techniques', 'Pretraining Methods', 'Prompting', 'In-Context Learning']
---
We study the capabilities of Large Language Models (LLM) on binary relations,
a ubiquitous concept in math employed in most reasoning, math and logic
benchmarks. This work focuses on equality, inequality, and inclusion, along
with the properties they satisfy, such as ir/reflexivity, a/symmetry,
transitivity, and logical complexity (e.g., number of reasoning ``hops''). We
propose an alternative to in-context learning that trains only the
representations of newly introduced tokens, namely out-of-context
representation learning. This method mitigates linguistic biases already
present in a model and, differently from in-context learning, does not rely on
external information or illustrations. We argue out-of-context representation
learning as a better alternative to in-context learning and fine-tuning to
evaluate the capabilities of LLMs on logic tasks that are the building blocks
of more complex reasoning benchmarks.
