---
layout: publication
title: Interpreting Pretrained Language Models Via Concept Bottlenecks
authors: Tan Zhen, Cheng Lu, Wang Song, Bo Yuan, Li Jundong, Liu Huan
conference: "Arxiv"
year: 2023
bibkey: tan2023interpreting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.05014"}
tags: ['Attention Mechanism', 'Interpretability And Explainability', 'Model Architecture', 'Reinforcement Learning', 'Security', 'Survey Paper']
---
Pretrained language models (PLMs) have made significant strides in various natural language processing tasks. However the lack of interpretability due to their black45;box nature poses challenges for responsible implementation. Although previous studies have attempted to improve interpretability by using e.g. attention weights in self45;attention layers these weights often lack clarity readability and intuitiveness. In this research we propose a novel approach to interpreting PLMs by employing high45;level meaningful concepts that are easily understandable for humans. For example we learn the concept of Food and investigate how it influences the prediction of a models sentiment towards a restaurant review. We introduce C^3M which combines human45;annotated and machine45;generated concepts to extract hidden neurons designed to encapsulate semantically meaningful and task45;specific concepts. Through empirical evaluations on real45;world datasets we manifest that our approach offers valuable insights to interpret PLM behavior helps diagnose model failures and enhances model robustness amidst noisy concept labels.
