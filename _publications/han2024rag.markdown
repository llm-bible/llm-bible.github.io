---
layout: publication
title: RAG45;QA Arena Evaluating Domain Robustness For Long45;form Retrieval Augmented Question Answering
authors: Han Rujun, Zhang Yuhao, Qi Peng, Xu Yumo, Wang Jenyuan, Liu Lan, Wang William Yang, Min Bonan, Castelli Vittorio
conference: "Arxiv"
year: 2024
bibkey: han2024rag
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.13998"}
tags: ['Applications', 'RAG', 'Reinforcement Learning', 'Security', 'Tools']
---
Question answering based on retrieval augmented generation (RAG45;QA) is an important research topic in NLP and has a wide range of real45;world applications. However most existing datasets for this task are either constructed using a single source corpus or consist of short extractive answers which fall short of evaluating large language model (LLM) based RAG45;QA systems on cross45;domain generalization. To address these limitations we create Long45;form RobustQA (LFRQA) a new dataset comprising human45;written long45;form answers that integrate short extractive answers from multiple documents into a single coherent narrative covering 26K queries and large corpora across seven different domains. We further propose RAG45;QA Arena by directly comparing model45;generated answers against LFRQAs answers using LLMs as evaluators. We show via extensive experiments that RAG45;QA Arena and human judgments on answer quality are highly correlated. Moreover only 41.337; of the most competitive LLMs answers are preferred to LFRQAs answers demonstrating RAG45;QA Arena as a challenging evaluation platform for future research.
