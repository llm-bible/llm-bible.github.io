---
layout: publication
title: 'Optimizing Llms For Resource-constrained Environments: A Survey Of Model Compression Techniques'
authors: Sanjay Surendranath Girija, Shashank Kapoor, Lakshit Arora, Dipen Pradhan, Aman Raj, Ankit Shetgaonkar
conference: "Arxiv"
year: 2025
bibkey: girija2025optimizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.02309"}
tags: ['Efficiency and Optimization', 'Survey Paper', 'Applications', 'Pruning', 'Reinforcement Learning', 'Quantization', 'Distillation']
---
Large Language Models (LLMs) have revolutionized many areas of artificial
intelligence (AI), but their substantial resource requirements limit their
deployment on mobile and edge devices. This survey paper provides a
comprehensive overview of techniques for compressing LLMs to enable efficient
inference in resource-constrained environments. We examine three primary
approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For
each technique, we discuss the underlying principles, present different
variants, and provide examples of successful applications. We also briefly
discuss complementary techniques such as mixture-of-experts and early-exit
strategies. Finally, we highlight promising future directions, aiming to
provide a valuable resource for both researchers and practitioners seeking to
optimize LLMs for edge deployment.
