---
layout: publication
title: 'Contextual Code Switching For Machine Translation Using Language Models'
authors: Arshad Kaji, Manan Shah
conference: "Arxiv"
year: 2023
bibkey: kaji2023contextual
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2312.13179'}
tags: ['Few-Shot', 'Training Techniques', 'Applications', 'Fine-Tuning', 'Prompting', 'Pretraining Methods']
---
Large language models (LLMs) have exerted a considerable impact on diverse
language-related tasks in recent years. Their demonstrated state-of-the-art
performance is achieved through methodologies such as zero-shot or few-shot
prompting. These models undergo training on extensive datasets that encompass
segments of the Internet and subsequently undergo fine-tuning tailored to
specific tasks. Notably, they exhibit proficiency in tasks such as translation,
summarization, question answering, and creative writing, even in the absence of
explicit training for those particular tasks. While they have shown substantial
improvement in the multilingual tasks their performance in the code switching,
especially for machine translation remains relatively uncharted. In this paper,
we present an extensive study on the code switching task specifically for the
machine translation task comparing multiple LLMs. Our results indicate that
despite the LLMs having promising results in the certain tasks, the models with
relatively lesser complexity outperform the multilingual large language models
in the machine translation task. We posit that the efficacy of multilingual
large language models in contextual code switching is constrained by their
training methodologies. In contrast, relatively smaller models, when trained
and fine-tuned on bespoke datasets, may yield superior results in comparison to
the majority of multilingual models.
