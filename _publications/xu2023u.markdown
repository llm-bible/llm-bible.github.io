---
layout: publication
title: U45;llava Unifying Multi45;modal Tasks Via Large Language Model
authors: Xu Jinjin, Xu Liwu, Yang Yuzhe, Li Xiang, Wang Fanyi, Xie Yanchun, Huang Yi-jie, Li Yaqian
conference: "Arxiv"
year: 2023
bibkey: xu2023u
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.05348"}
  - {name: "Code", url: "https://github.com/OPPOMKLab/u&#45;LLaVA"}
tags: ['Has Code', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Recent advancements in multi45;modal large language models (MLLMs) have led to substantial improvements in visual understanding primarily driven by sophisticated modality alignment strategies. However predominant approaches prioritize global or regional comprehension with less focus on fine45;grained pixel45;level tasks. To address this gap we introduce u45;LLaVA an innovative unifying multi45;task framework that integrates pixel regional and global features to refine the perceptual faculties of MLLMs. We commence by leveraging an efficient modality alignment approach harnessing both image and video datasets to bolster the models foundational understanding across diverse visual contexts. Subsequently a joint instruction tuning method with task45;specific projectors and decoders for end45;to45;end downstream training is presented. Furthermore this work contributes a novel mask45;based multi45;task dataset comprising 277K samples crafted to challenge and assess the fine45;grained perception capabilities of MLLMs. The overall framework is simple effective and achieves state45;of45;the45;art performance across multiple benchmarks. We also make our model data and code publicly accessible at https://github.com/OPPOMKLab/u&#45;LLaVA.
