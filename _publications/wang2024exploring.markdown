---
layout: publication
title: Exploring The Potential Of Multimodal LLM With Knowledge45;intensive Multimodal ASR
authors: Wang Minghan, Wang Yuxia, Vu Thuy-trang, Shareghi Ehsan, Haffari Gholamreza
conference: "Arxiv"
year: 2024
bibkey: wang2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.10880"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Multimodal Models', 'Prompting', 'RAG', 'Reinforcement Learning', 'Tools']
---
Recent advancements in multimodal large language models (MLLMs) have made significant progress in integrating information across various modalities yet real45;world applications in educational and scientific domains remain challenging. This paper introduces the Multimodal Scientific ASR (MS45;ASR) task which focuses on transcribing scientific conference videos by leveraging visual information from slides to enhance the accuracy of technical terminologies. Realized that traditional metrics like WER fall short in assessing performance accurately prompting the proposal of severity45;aware WER (SWER) that considers the content type and severity of ASR errors. We propose the Scientific Vision Augmented ASR (SciVASR) framework as a baseline method enabling MLLMs to improve transcript quality through post45;editing. Evaluations of state45;of45;the45;art MLLMs including GPT45;4o show a 4537; improvement over speech45;only baselines highlighting the importance of multimodal information integration.
