---
layout: publication
title: Exploiting Deep Representations For Neural Machine Translation
authors: Zi-yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, Tong Zhang
conference: Arxiv
year: 2018
citations: 20
bibkey: dou2018exploiting
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1810.10181'}]
tags: [Transformer, Attention Mechanism]
---
Advanced neural machine translation (NMT) models generally implement encoder
and decoder as multiple layers, which allows systems to model complex functions
and capture complicated linguistic structures. However, only the top layers of
encoder and decoder are leveraged in the subsequent process, which misses the
opportunity to exploit the useful information embedded in other layers. In this
work, we propose to simultaneously expose all of these signals with layer
aggregation and multi-layer attention mechanisms. In addition, we introduce an
auxiliary regularization term to encourage different layers to capture diverse
information. Experimental results on widely-used WMT14 English-German and WMT17
Chinese-English translation data demonstrate the effectiveness and universality
of the proposed approach.