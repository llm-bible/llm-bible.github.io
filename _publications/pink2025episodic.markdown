---
layout: publication
title: 'Position: Episodic Memory Is The Missing Piece For Long-term LLM Agents'
authors: Mathis Pink, Qinyuan Wu, Vy Ai Vo, Javier Turek, Jianing Mu, Alexander Huth, Mariya Toneva
conference: "Arxiv"
year: 2025
bibkey: pink2025episodic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.06975"}
tags: ['Agentic', 'Tools', 'Reinforcement Learning']
---
As Large Language Models (LLMs) evolve from text-completion tools into fully
fledged agents operating in dynamic environments, they must address the
challenge of continually learning and retaining long-term knowledge. Many
biological systems solve these challenges with episodic memory, which supports
single-shot learning of instance-specific contexts. Inspired by this, we
present an episodic memory framework for LLM agents, centered around five key
properties of episodic memory that underlie adaptive and context-sensitive
behavior. With various research efforts already partially covering these
properties, this position paper argues that now is the right time for an
explicit, integrated focus on episodic memory to catalyze the development of
long-term agents. To this end, we outline a roadmap that unites several
research directions under the goal to support all five properties of episodic
memory for more efficient long-term LLM agents.
