---
layout: publication
title: Mitigating Hallucination In Large Multi45;modal Models Via Robust Instruction Tuning
authors: Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang
conference: "Arxiv"
year: 2023
bibkey: liu2023mitigating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2306.14565v4"}
  - {name: "Code", url: "https://github.com/FuxiaoLiu/LRV&#45;Instruction"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
Despite the promising progress in multi45;modal tasks current large multi45;modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset named Large45;scale Robust Visual (LRV)45;Instruction. Our dataset comprises 400k visual instructions generated by GPT4 covering 16 vision45;and45;language tasks with open45;ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples we design LRV45;Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels (i) Nonexistent Object Manipulation (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs we propose GPT445;Assisted Visual Instruction Evaluation (GAVIE) a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human45;annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions particularly Existent Object and Knowledge Manipulation instructions. Moreover we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG45;Owl on LRV45;Instruction while improving performance on several public datasets compared to state45;of45;the45;art methods. Additionally we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data are available at https://github.com/FuxiaoLiu/LRV&#45;Instruction.
