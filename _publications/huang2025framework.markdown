---
layout: publication
title: 'A Framework For Benchmarking And Aligning Task-planning Safety In Llm-based Embodied Agents'
authors: Yuting Huang, Leilei Ding, Zhipeng Tang, Tianfu Wang, Xinrui Lin, Wuyang Zhang, Mingxiao Ma, Yanyong Zhang
conference: "Arxiv"
year: 2025
bibkey: huang2025framework
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.14650"}
tags: ['Responsible AI', 'Agentic', 'Security', 'Model Architecture', 'Tools', 'Reinforcement Learning', 'GPT']
---
Large Language Models (LLMs) exhibit substantial promise in enhancing
task-planning capabilities within embodied agents due to their advanced
reasoning and comprehension. However, the systemic safety of these agents
remains an underexplored frontier. In this study, we present Safe-BeAl, an
integrated framework for the measurement (SafePlan-Bench) and alignment
(Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench
establishes a comprehensive benchmark for evaluating task-planning safety,
encompassing 2,027 daily tasks and corresponding environments distributed
across 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis
reveals that even in the absence of adversarial inputs or malicious intent,
LLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we
propose Safe-Align, a method designed to integrate physical-world safety
knowledge into LLM-based embodied agents while maintaining task-specific
performance. Experiments across a variety of settings demonstrate that
Safe-BeAl provides comprehensive safety validation, improving safety by 8.55 -
15.22%, compared to embodied agents based on GPT-4, while ensuring successful
task completion.
