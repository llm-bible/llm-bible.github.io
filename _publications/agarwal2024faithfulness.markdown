---
layout: publication
title: Faithfulness Vs. Plausibility: On The (un)reliability Of Explanations From Large Language Models
authors: Agarwal Chirag, Tanneru Sree Harsha, Lakkaraju Himabindu
conference: "Arxiv"
year: 2024
bibkey: agarwal2024faithfulness
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.04614"}
tags: ['Applications', 'Interpretability And Explainability', 'Reinforcement Learning', 'Tools']
---
Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs) which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However there is little to no understanding of their faithfulness. In this work we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations primarily driven by the demand for user-friendly interfaces may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.
