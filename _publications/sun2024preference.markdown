---
layout: publication
title: 'PMMT: Preference Alignment In Multilingual Machine Translation Via LLM Distillation'
authors: Shuqiao Sun, Yutong Yao, Peiwen Wu, Feijun Jiang, Kaifu Zhang
conference: "Arxiv"
year: 2024
bibkey: sun2024preference
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.11410'}
tags: ['Efficiency and Optimization', 'WMT', 'Distillation', 'Applications', 'Reinforcement Learning']
---
Translation is important for cross-language communication, and many efforts
have been made to improve its accuracy. However, less investment is conducted
in aligning translations with human preferences, such as translation tones or
styles. In this paper, a new method is proposed to effectively generate
large-scale multilingual parallel corpora with specific translation preferences
using Large Language Models (LLMs). Meanwhile, an automatic pipeline is
designed to distill human preferences into smaller Machine Translation (MT)
models for efficiently and economically supporting large-scale calls in online
services. Experiments indicate that the proposed method takes the lead in
translation tasks with aligned human preferences by a large margin. Meanwhile,
on popular public benchmarks like WMT and Flores, on which our models were not
trained, the proposed method also shows a competitive performance compared to
SOTA works.
