---
layout: publication
title: 'Human Parity On Commonsenseqa: Augmenting Self-attention With External Attention'
authors: Yichong Xu et al.
conference: IJCAI 2022 (Long oral)
year: 2021
citations: 18
bibkey: xu2021human
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2112.03254'}]
tags: [Transformer, Applications, Attention Mechanism]
---
Most of today's AI systems focus on using self-attention mechanisms and
transformer architectures on large amounts of diverse data to achieve
impressive performance gains. In this paper, we propose to augment the
transformer architecture with an external attention mechanism to bring external
knowledge and context to bear. By integrating external information into the
prediction process, we hope to reduce the need for ever-larger models and
increase the democratization of AI systems. We find that the proposed external
attention mechanism can significantly improve the performance of existing AI
systems, allowing practitioners to easily customize foundation AI models to
many diverse downstream applications. In particular, we focus on the task of
Commonsense Reasoning, demonstrating that the proposed external attention
mechanism can augment existing transformer models and significantly improve the
model's reasoning capabilities. The proposed system, Knowledgeable External
Attention for commonsense Reasoning (KEAR), reaches human parity on the open
CommonsenseQA research benchmark with an accuracy of 89.4% in comparison to
the human accuracy of 88.9%.