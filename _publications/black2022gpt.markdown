---
layout: publication
title: Gpt45;neox45;20b An Open45;source Autoregressive Language Model
authors: Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach
conference: "Arxiv"
year: 2022
bibkey: black2022gpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2204.06745v1"}
  - {name: "Code", url: "https://github.com/EleutherAI/gpt&#45;neox"}
tags: ['GPT', 'Has Code', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
We introduce GPT45;NeoX45;20B a 20 billion parameter autoregressive language model trained on the Pile whose weights will be made freely and openly available to the public through a permissive license. It is to the best of our knowledge the largest dense autoregressive model that has publicly available weights at the time of submission. In this work we describe model123;125;s architecture and training and evaluate its performance on a range of language45;understanding mathematics and knowledge45;based tasks. We find that GPT45;NeoX45;20B is a particularly powerful few45;shot reasoner and gains far more in performance when evaluated five45;shot than similarly sized GPT45;3 and FairSeq models. We open45;source the training and evaluation code as well as the model weights at https://github.com/EleutherAI/gpt&#45;neox.
