---
layout: publication
title: 'Exploring The Robustness Of Language Models For Tabular Question Answering Via Attention Analysis'
authors: Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao
conference: "Arxiv"
year: 2024
bibkey: bhandari2024exploring
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2406.12719'}
tags: ['Attention Mechanism', 'Security', 'Training Techniques', 'Applications', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Ethics and Bias', 'In-Context Learning']
---
Large Language Models (LLMs), already shown to ace various text comprehension tasks, have also remarkably been shown to tackle table comprehension tasks without specific training. Building on earlier studies of LLMs for tabular tasks, we probe how in-context learning (ICL), model scale, instruction tuning, and domain bias affect Tabular QA (TQA) robustness by testing LLMs, under diverse augmentations and perturbations, on diverse domains: Wikipedia-based \\(\textbf\{WTQ\}\\), financial \\(\textbf\{TAT-QA\}\\), and scientific \\(\textbf\{SCITAB\}\\). Although instruction tuning and larger, newer LLMs deliver stronger, more robust TQA performance, data contamination and reliability issues, especially on \\(\textbf\{WTQ\}\\), remain unresolved. Through an in-depth attention analysis, we reveal a strong correlation between perturbation-induced shifts in attention dispersion and the drops in performance, with sensitivity peaking in the model's middle layers. We highlight the need for improved interpretable methodologies to develop more reliable LLMs for table comprehension.
