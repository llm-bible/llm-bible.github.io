---
layout: publication
title: 'To Err Is AI : A Case Study Informing LLM Flaw Reporting Practices'
authors: Sean Mcgregor, Allyson Ettinger, Nick Judd, Paul Albee, Liwei Jiang, Kavel Rao, Will Smith, Shayne Longpre, Avijit Ghosh, Christopher Fiorelli, Michelle Hoang, Sven Cattell, Nouha Dziri
conference: "Arxiv"
year: 2024
bibkey: mcgregor2024err
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.12104'}
tags: ['Responsible AI']
---
In August of 2024, 495 hackers generated evaluations in an open-ended bug
bounty targeting the Open Language Model (OLMo) from The Allen Institute for
AI. A vendor panel staffed by representatives of OLMo's safety program
adjudicated changes to OLMo's documentation and awarded cash bounties to
participants who successfully demonstrated a need for public disclosure
clarifying the intent, capacities, and hazards of model deployment. This paper
presents a collection of lessons learned, illustrative of flaw reporting best
practices intended to reduce the likelihood of incidents and produce safer
large language models (LLMs). These include best practices for safety reporting
processes, their artifacts, and safety program staffing.
