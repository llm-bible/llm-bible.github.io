---
layout: publication
title: Fmm45;attack A Flow45;based Multi45;modal Adversarial Attack On Video45;based Llms
authors: Li Jinmin, Gao Kuofeng, Bai Yang, Zhang Jingyun, Xia Shu-tao, Wang Yisen
conference: "Arxiv"
year: 2024
bibkey: li2024fmm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.13507"}
  - {name: "Code", url: "https://github.com/THU&#45;Kingmin/FMM&#45;Attack"}
tags: ['Has Code', 'Prompting', 'Responsible AI', 'Security']
---
Despite the remarkable performance of video45;based large language models (LLMs) their adversarial threat remains unexplored. To fill this gap we propose the first adversarial attack tailored for video45;based LLMs by crafting flow45;based multi45;modal adversarial perturbations on a small fraction of frames within a video dubbed FMM45;Attack. Extensive experiments show that our attack can effectively induce video45;based LLMs to generate incorrect answers when videos are added with imperceptible adversarial perturbations. Intriguingly our FMM45;Attack can also induce garbling in the model output prompting video45;based LLMs to hallucinate. Overall our observations inspire a further understanding of multi45;modal robustness and safety45;related feature alignment across different modalities which is of great importance for various large multi45;modal models. Our code is available at https://github.com/THU&#45;Kingmin/FMM&#45;Attack.
