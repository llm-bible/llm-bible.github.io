---
layout: publication
title: 'HRET: A Self-evolving LLM Evaluation Toolkit For Korean'
authors: Hanwool Lee, Soo Yong Kim, Dasol Choi, Sangwon Baek, Seunghyeok Hong, Ilgyun Jeong, Inseon Hwang, Naeun Lee, Guijin Son
conference: "Arxiv"
year: 2025
bibkey: lee2025self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.22968"}
tags: ['Tools', 'Model Architecture', 'Reinforcement Learning']
---
Recent advancements in Korean large language models (LLMs) have spurred
numerous benchmarks and evaluation methodologies, yet the lack of a
standardized evaluation framework has led to inconsistent results and limited
comparability. To address this, we introduce HRET Haerae Evaluation Toolkit, an
open-source, self-evolving evaluation framework tailored specifically for
Korean LLMs. HRET unifies diverse evaluation methods, including logit-based
scoring, exact-match, language-inconsistency penalization, and LLM-as-a-Judge
assessments. Its modular, registry-based architecture integrates major
benchmarks (HAE-RAE Bench, KMMLU, KUDGE, HRM8K) and multiple inference backends
(vLLM, HuggingFace, OpenAI-compatible endpoints). With automated pipelines for
continuous evolution, HRET provides a robust foundation for reproducible, fair,
and transparent Korean NLP research.
