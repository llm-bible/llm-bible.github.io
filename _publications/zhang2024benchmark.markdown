---
layout: publication
title: 'Cjeval: A Benchmark For Assessing Large Language Models Using Chinese Junior High School Exam Data'
authors: Qian-wen Zhang, Haochen Wang, Fang Li, Siyu An, Lingfeng Qiao, Liangcai Gao, Di Yin, Xing Sun
conference: "Arxiv"
year: 2024
bibkey: zhang2024benchmark
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.16202"}
tags: ['Training Techniques', 'Tools', 'Reinforcement Learning', 'Pretraining Methods', 'Fine-Tuning', 'Interpretability and Explainability', 'Applications']
---
Online education platforms have significantly transformed the dissemination
of educational resources by providing a dynamic and digital infrastructure.
With the further enhancement of this transformation, the advent of Large
Language Models (LLMs) has elevated the intelligence levels of these platforms.
However, current academic benchmarks provide limited guidance for real-world
industry scenarios. This limitation arises because educational applications
require more than mere test question responses. To bridge this gap, we
introduce CJEval, a benchmark based on Chinese Junior High School Exam
Evaluations. CJEval consists of 26,136 samples across four application-level
educational tasks covering ten subjects. These samples include not only
questions and answers but also detailed annotations such as question types,
difficulty levels, knowledge concepts, and answer explanations. By utilizing
this benchmark, we assessed LLMs' potential applications and conducted a
comprehensive analysis of their performance by fine-tuning on various
educational tasks. Extensive experiments and discussions have highlighted the
opportunities and challenges of applying LLMs in the field of education.
