---
layout: publication
title: An LLM Feature45;based Framework For Dialogue Constructiveness Assessment
authors: Zhou Lexin, Farag Youmna, Vlachos Andreas
conference: "Arxiv"
year: 2024
bibkey: zhou2024llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.14760"}
tags: ['Applications', 'Interpretability And Explainability', 'Pretraining Methods', 'Prompting', 'Tools', 'Training Techniques']
---
Research on dialogue constructiveness assessment focuses on (i) analysing conversational factors that influence individuals to take specific actions win debates change their perspectives or broaden their open45;mindedness and (ii) predicting constructive outcomes following dialogues for such use cases. These objectives can be achieved by training either interpretable feature45;based models (which often involve costly human annotations) or neural models such as pre45;trained language models (which have empirically shown higher task accuracy but lack interpretability). We propose a novel LLM feature45;based framework that combines the strengths of feature45;based and neural approaches while mitigating their downsides in assessing dialogue constructiveness. The framework first defines a set of dataset45;independent and interpretable linguistic features which can be extracted by both prompting an LLM and simple heuristics. Such features are then used to train LLM feature45;based models. We apply this framework to three datasets of dialogue constructiveness and find that our LLM feature45;based models significantly outperform standard feature45;based models and neural models and tend to learn more robust prediction rules instead of relying on superficial shortcuts (as seen with neural models). Further we demonstrate that interpreting these LLM feature45;based models can yield valuable insights into what makes a dialogue constructive.
