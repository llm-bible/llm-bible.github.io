---
layout: publication
title: 'Are Large Language Models Capable Of Generating Human-level Narratives?'
authors: Tian Yufei, Huang Tenghao, Liu Miri, Jiang Derek, Spangher Alexander, Chen Muhao, May Jonathan, Peng Nanyun
conference: "Arxiv"
year: 2024
bibkey: tian2024are
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.13248"}
tags: ['Pretraining Methods', 'RAG', 'Tools']
---
This paper investigates the capability of LLMs in storytelling focusing on narrative development and plot progression. We introduce a novel computational framework to analyze narratives through three discourse-level aspects i) story arcs ii) turning points and iii) affective dimensions including arousal and valence. By leveraging expert and automatic annotations we uncover significant discrepancies between the LLM- and human- written stories. While human-written stories are suspenseful arousing and diverse in narrative structures LLM stories are homogeneously positive and lack tension. Next we measure narrative reasoning skills as a precursor to generative capacities concluding that most LLMs fall short of human abilities in discourse understanding. Finally we show that explicit integration of aforementioned discourse features can enhance storytelling as is demonstrated by over 4037; improvement in neural storytelling in terms of diversity suspense and arousal.
