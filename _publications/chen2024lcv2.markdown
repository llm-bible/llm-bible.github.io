---
layout: publication
title: LCV2 An Efficient Pretraining-Free Framework for Grounded Visual Question Answering
authors: Chen Yuhan, Su Lumei, Chen Lihua, Lin Zhiwei
conference: "Arxiv"
year: 2024
bibkey: chen2024lcv2
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.15842"}
tags: ['Applications', 'Multimodal Models', 'Pretraining Methods', 'Prompting', 'Tools', 'Training Techniques']
---
In this paper the LCV2 modular method is proposed for the Grounded Visual Question Answering task in the vision-language multimodal domain. This approach relies on a frozen large language model (LLM) as intermediate mediator between the off-the-shelf VQA model and the off-the-shelf visual grounding (VG) model where the LLM transforms and conveys textual information between the two modules based on a designed prompt. LCV2 establish an integrated plug-and-play framework without the need for any pre-training process. This framework can be deployed for VQA Grounding tasks under low computational resources. The modularized model within the framework allows application with various state-of-the-art pre-trained models exhibiting significant potential to be advance with the times. Experimental implementations were conducted under constrained computational and memory resources evaluating the proposed methods performance on benchmark datasets including GQA CLEVR and VizWiz-VQA-Grounding. Comparative analyses with baseline methods demonstrate the robust competitiveness of LCV2.
