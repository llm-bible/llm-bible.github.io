---
layout: publication
title: St45;moe Designing Stable And Transferable Sparse Expert Models
authors: Zoph Barret, Bello Irwan, Kumar Sameer, Du Nan, Huang Yanping, Dean Jeff, Shazeer Noam, Fedus William
conference: "Arxiv"
year: 2022
bibkey: zoph2022st
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2202.08906"}
tags: ['Applications', 'Fine Tuning', 'Model Architecture', 'Pretraining Methods', 'Security', 'Training Techniques', 'Transformer']
---
Scale has opened new frontiers in natural language processing 45;45; but at a high cost. In response Mixture45;of45;Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state45;of45;the45;art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine45;tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters with a computational cost comparable to a 32B dense encoder45;decoder Transformer (Stable and Transferable Mixture45;of45;Experts or ST45;MoE45;32B). For the first time a sparse model achieves state45;of45;the45;art performance in transfer learning across a diverse set of tasks including reasoning (SuperGLUE ARC Easy ARC Challenge) summarization (XSum CNN45;DM) closed book question answering (WebQA Natural Questions) and adversarially constructed tasks (Winogrande ANLI R3).
