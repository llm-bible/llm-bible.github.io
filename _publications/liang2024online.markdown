---
layout: publication
title: 'Online Training Of Large Language Models: Learn While Chatting'
authors: Liang Juhao, Wang Ziwei, Ma Zhuoheng, Li Jianquan, Zhang Zhiyi, Wu Xiangbo, Wang Benyou
conference: "Arxiv"
year: 2024
bibkey: liang2024online
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.04790"}
tags: ['Agentic', 'Reinforcement Learning', 'Tools', 'Training Techniques', 'Uncategorized']
---
Large Language Models(LLMs) have dramatically revolutionized the field of
Natural Language Processing(NLP), offering remarkable capabilities that have
garnered widespread usage. However, existing interaction paradigms between LLMs
and users are constrained by either inflexibility, limitations in
customization, or a lack of persistent learning. This inflexibility is
particularly evident as users, especially those without programming skills,
have restricted avenues to enhance or personalize the model. Existing
frameworks further complicate the model training and deployment process due to
their computational inefficiencies and lack of user-friendly interfaces. To
overcome these challenges, this paper introduces a novel interaction
paradigm-'Online Training using External Interactions'-that merges the benefits
of persistent, real-time model updates with the flexibility for individual
customization through external interactions such as AI agents or online/offline
knowledge bases.
