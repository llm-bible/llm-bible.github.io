---
layout: publication
title: 'Cumulative Reasoning With Large Language Models'
authors: Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-chih Yao
conference: "Arxiv"
year: 2023
bibkey: zhang2023cumulative
additional_links:
  - {name: "Paper", url: "http://arxiv.org/abs/2308.04371v6"}
  - {name: "Code", url: "https://github.com/iiis-ai/cumulative-reasoning"}
tags: ['Has Code', 'Pretraining Methods', 'RAG']
---
"Despite the recent advancements in language models (LMs), their ability to solve complex problems remains limited. This paper introduces Cumulative Reasoning (CR), a novel approach that utilizes LMs cumulatively and iteratively, mirroring human thought processes for problem-solving. CR decomposes tasks into smaller, manageable components and leverages previous propositions for effective composition, significantly enhancing problem-solving capabilities. We demonstrate CR's superiority through several complex reasoning tasks: it outperforms existing methods in logical inference tasks with up to a 9.3&#37; improvement, achieving 98.04&#37; accuracy on the curated FOLIO wiki dataset. In the Game of 24, it achieves 98&#37; accuracy, marking a 24&#37; improvement over the prior state-of-the-art. Additionally, CR sets new state-of-the-art on the MATH dataset, achieving a 4.2&#37; increase from previous methods and a 43&#37; relative improvement in the most challenging problems. By extending CR to incorporate a code environment without external aids like retrieval or web browsing, we further harness the computational and logical reasoning capabilities of LMs, achieving a remarkable 72.2&#37; accuracy on the MATH dataset and outperforming the PAL/PoT method by 38.8&#37;. Our work not only sets new state-of-the-art but also paves the way toward more sophisticated AI reasoning methods. The code is available at https://github.com/iiis-ai/cumulative-reasoning."
