---
layout: publication
title: 'Should I Share This Translation? Evaluating Quality Feedback For User Reliance On Machine Translation'
authors: Dayeon Ki, Kevin Duh, Marine Carpuat
conference: "Arxiv"
year: 2025
bibkey: ki2025should
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.24683"}
tags: ['Interpretability and Explainability', 'Applications', 'Reinforcement Learning']
---
As people increasingly use AI systems in work and daily life, feedback mechanisms that help them use AI responsibly are urgently needed, particularly in settings where users are not equipped to assess the quality of AI predictions. We study a realistic Machine Translation (MT) scenario where monolingual users decide whether to share an MT output, first without and then with quality feedback. We compare four types of quality feedback: explicit feedback that directly give users an assessment of translation quality using 1) error highlights and 2) LLM explanations, and implicit feedback that helps users compare MT inputs and outputs through 3) backtranslation and 4) question-answer (QA) tables. We find that all feedback types, except error highlights, significantly improve both decision accuracy and appropriate reliance. Notably, implicit feedback, especially QA tables, yields significantly greater gains than explicit feedback in terms of decision accuracy, appropriate reliance, and user perceptions, receiving the highest ratings for helpfulness and trust, and the lowest for mental burden.
