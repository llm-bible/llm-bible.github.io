---
layout: publication
title: All Thats human Is Not Gold Evaluating Human Evaluation Of Generated Text
authors: Clark Elizabeth, August Tal, Serrano Sofia, Haduong Nikita, Gururangan Suchin, Smith Noah A.
conference: "Arxiv"
year: 2021
bibkey: clark2021all
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2107.00061"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Training Techniques']
---
Human evaluations are typically considered the gold standard in natural language generation but as models fluency improves how well can evaluators detect and judge machine45;generated text We run a study assessing non45;experts ability to distinguish between human45; and machine45;authored text (GPT2 and GPT3) in three domains (stories news articles and recipes). We find that without training evaluators distinguished between GPT345; and human45;authored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT345;authored text (detailed instructions annotated examples and paired examples) and find that while evaluators accuracy improved up to 5537; it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state45;of45;the45;art models.
