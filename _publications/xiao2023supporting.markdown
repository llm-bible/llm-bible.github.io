---
layout: publication
title: Supporting Qualitative Analysis With Large Language Models Combining Codebook With GPT45;3 For Deductive Coding
authors: Xiao Ziang, Yuan Xingdi, Liao Q. Vera, Abdelghani Rania, Oudeyer Pierre-yves
conference: "Arxiv"
year: 2023
bibkey: xiao2023supporting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.10548"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Qualitative analysis of textual contents unpacks rich and valuable information by assigning labels to the data. However this process is often labor45;intensive particularly when working with large datasets. While recent AI45;based tools demonstrate utility researchers may not have readily available AI resources and expertise let alone be challenged by the limited generalizability of those task45;specific models. In this study we explored the use of large language models (LLMs) in supporting deductive coding a major category of qualitative analysis where researchers use pre45;determined codebooks to label the data into a fixed set of codes. Instead of training task45;specific models a pre45;trained LLM could be used directly for various tasks without fine45;tuning through prompt learning. Using a curiosity45;driven questions coding task as a case study we found by combining GPT45;3 with expert45;drafted codebooks our proposed approach achieved fair to substantial agreements with expert45;coded results. We lay out challenges and opportunities in using LLMs to support qualitative coding and beyond.
