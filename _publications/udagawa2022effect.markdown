---
layout: publication
title: Effect And Analysis Of Large45;scale Language Model Rescoring On Competitive ASR Systems
authors: Udagawa Takuma, Suzuki Masayuki, Kurata Gakuto, Itoh Nobuyasu, Saon George
conference: "Arxiv"
year: 2022
bibkey: udagawa2022effect
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.00212"}
tags: ['BERT', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
Large45;scale language models (LLMs) such as GPT45;2 BERT and RoBERTa have been successfully applied to ASR N45;best rescoring. However whether or how they can benefit competitive near state45;of45;the45;art ASR systems remains unexplored. In this study we incorporate LLM rescoring into one of the most competitive ASR baselines the Conformer45;Transducer model. We demonstrate that consistent improvement is achieved by the LLMs bidirectionality pretraining in45;domain finetuning and context augmentation. Furthermore our lexical analysis sheds light on how each of these components may be contributing to the ASR performance.
