---
layout: publication
title: 'A Survey On LLM Inference-time Self-improvement'
authors: Xiangjue Dong, Maria Teleki, James Caverlee
conference: "Arxiv"
year: 2024
bibkey: dong2024survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.14352"}
tags: ['RAG', 'Model Architecture', 'Attention Mechanism', 'Survey Paper']
---
Techniques that enhance inference through increased computation at test-time
have recently gained attention. In this survey, we investigate the current
state of LLM Inference-Time Self-Improvement from three different perspectives:
Independent Self-improvement, focusing on enhancements via decoding or sampling
methods; Context-Aware Self-Improvement, leveraging additional context or
datastore; and Model-Aided Self-Improvement, achieving improvement through
model collaboration. We provide a comprehensive review of recent relevant
studies, contribute an in-depth taxonomy, and discuss challenges and
limitations, offering insights for future research.
