---
layout: publication
title: Wikichat Stopping The Hallucination Of Large Language Model Chatbots By Few45;shot Grounding On Wikipedia
authors: Semnani Sina J., Yao Violet Z., Zhang Heidi C., Lam Monica S.
conference: "Arxiv"
year: 2023
bibkey: semnani2023stopping
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.14292"}
tags: ['Applications', 'GPT', 'Model Architecture']
---
This paper presents the first few45;shot LLM45;based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia the largest curated free45;text corpus. WikiChat generates a response from an LLM retains only the grounded facts and combines them with additional information it retrieves from the corpus to form factual and engaging responses. We distill WikiChat based on GPT45;4 into a 7B45;parameter LLaMA model with minimal loss of quality to significantly improve its latency cost and privacy and facilitate research and deployment. Using a novel hybrid human45;and45;LLM evaluation methodology we show that our best system achieves 97.337; factual accuracy in simulated conversations. It significantly outperforms all retrieval45;based and LLM45;based baselines and by 3.937; 38.637; and 51.037; on head tail and recent knowledge compared to GPT45;4. Compared to previous state45;of45;the45;art retrieval45;based chatbots WikiChat is also significantly more informative and engaging just like an LLM. WikiChat achieves 97.937; factual accuracy in conversations with human users about recent topics 55.037; better than GPT45;4 while receiving significantly higher user ratings and more favorable comments.
