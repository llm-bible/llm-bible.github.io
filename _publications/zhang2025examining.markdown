---
layout: publication
title: 'Examining The Robustness Of Large Language Models Across Language Complexity'
authors: Jiayi Zhang
conference: "Arxiv"
year: 2025
bibkey: zhang2025examining
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.18738"}
tags: ['RAG', 'Security', 'Reinforcement Learning']
---
With the advancement of large language models (LLMs), an increasing number of
student models have leveraged LLMs to analyze textual artifacts generated by
students to understand and evaluate their learning. These student models
typically employ pre-trained LLMs to vectorize text inputs into embeddings and
then use the embeddings to train models to detect the presence or absence of a
construct of interest. However, how reliable and robust are these models at
processing language with different levels of complexity? In the context of
learning where students may have different language backgrounds with various
levels of writing skills, it is critical to examine the robustness of such
models to ensure that these models work equally well for text with varying
levels of language complexity. Coincidentally, a few (but limited) research
studies show that the use of language can indeed impact the performance of
LLMs. As such, in the current study, we examined the robustness of several
LLM-based student models that detect student self-regulated learning (SRL) in
math problem-solving. Specifically, we compared how the performance of these
models vary using texts with high and low lexical, syntactic, and semantic
complexity measured by three linguistic measures.
