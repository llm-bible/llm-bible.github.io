---
layout: publication
title: Answer45;me Multi45;task Open45;vocabulary Visual Question Answering
authors: Piergiovanni Aj, Li Wei, Kuo Weicheng, Saffar Mohammad, Bertsch Fred, Angelova Anelia
conference: "Arxiv"
year: 2022
bibkey: piergiovanni2022answer
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2205.00949"}
tags: ['Applications', 'Model Architecture', 'Security', 'Tools', 'Training Techniques']
---
We present Answer45;Me a task45;aware multi45;task framework which unifies a variety of question answering tasks such as visual question answering visual entailment visual reasoning. In contrast to previous works using contrastive or generative captioning training we propose a novel and simple recipe to pre45;train a vision45;language joint model which is multi45;task as well. The pre45;training uses only noisy image captioning data and is formulated to use the entire architecture end45;to45;end with both a strong language encoder and decoder. Our results show state45;of45;the45;art performance zero45;shot generalization robustness to forgetting and competitive single45;task results across a variety of question answering tasks. Our multi45;task mixture training learns from tasks of various question intents and thus generalizes better including on zero45;shot vision45;language tasks. We conduct experiments in the challenging multi45;task and open45;vocabulary settings and across a variety of datasets and tasks such as VQA2.0 SNLI45;VE NLVR2 GQA. We observe that the proposed approach is able to generalize to unseen tasks and that more diverse mixtures lead to higher accuracy in both known and novel tasks.
