---
layout: publication
title: "Efficient LLM Context Distillation"
authors: Upadhayayaya Rajesh, Smith Zachary, Kottmyer Chritopher, Osti Manish Raj
conference: "Arxiv"
year: 2024
bibkey: upadhayayaya2024efficient
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.01930"}
tags: ['Distillation', 'Efficiency And Optimization']
---
This paper specifically investigates context distillation a method that extends the utility of task-specific examples by internalizing them thus augmenting the example set accessible for model inference.
