---
layout: publication
title: Efficient And Effective Vocabulary Expansion Towards Multilingual Large Language Models
authors: Kim Seungduk, Choi Seungtaek, Jeong Myeongho
conference: "Arxiv"
year: 2024
bibkey: kim2024efficient
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.14714"}
tags: ['Pretraining Methods', 'Training Techniques']
---
This report introduces texttt123;EEVE45;Korean45;v1.0125; a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English45;centric LLMs such as SOLAR45;10.7B and Phi45;2 where non45;English texts are inefficiently processed with English45;centric tokenizers we present an efficient and effective vocabulary expansion (EEVE) method which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens we show that our method can significantly boost non45;English proficiency within just 2 billion tokens. Surpassing most instruction45;tuned LLMs on the Open Ko45;LLM Leaderboard as of January 2024 our model texttt123;EEVE45;Korean45;10.8B45;v1.0125; ranks as the leading Korean pre45;trained model in the open45;source community according to Hugging Faces leaderboard. We open45;source our models on Huggingface to empower the open research community in various languages.
