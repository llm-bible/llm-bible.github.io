---
layout: publication
title: 'Mindmem: Multimodal For Predicting Advertisement Memorability Using Llms And Deep Learning'
authors: Sepehr Asgarian, Qayam Jetha, Jouhyun Jeon
conference: "Arxiv"
year: 2025
bibkey: asgarian2025multimodal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.18371"}
tags: ['RAG', 'Tools', 'Multimodal Models']
---
In the competitive landscape of advertising, success hinges on effectively
navigating and leveraging complex interactions among consumers, advertisers,
and advertisement platforms. These multifaceted interactions compel advertisers
to optimize strategies for modeling consumer behavior, enhancing brand recall,
and tailoring advertisement content. To address these challenges, we present
MindMem, a multimodal predictive model for advertisement memorability. By
integrating textual, visual, and auditory data, MindMem achieves
state-of-the-art performance, with a Spearman's correlation coefficient of
0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently
surpassing existing methods. Furthermore, our analysis identified key factors
influencing advertisement memorability, such as video pacing, scene complexity,
and emotional resonance. Expanding on this, we introduced MindMem-ReAd
(MindMem-Driven Re-generated Advertisement), which employs Large Language
Model-based simulations to optimize advertisement content and placement,
resulting in up to a 74.12% improvement in advertisement memorability. Our
results highlight the transformative potential of Artificial Intelligence in
advertising, offering advertisers a robust tool to drive engagement, enhance
competitiveness, and maximize impact in a rapidly evolving market.
