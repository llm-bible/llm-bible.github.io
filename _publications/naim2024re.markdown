---
layout: publication
title: 'Re-examining Learning Linear Functions In Context'
authors: Omar Naim, Guilhem Fouilh√©, Nicholas Asher
conference: "Arxiv"
year: 2024
bibkey: naim2024re
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.11465"}
tags: ['Transformer', 'GPT', 'Model Architecture', 'Training Techniques', 'Pretraining Methods', 'Prompting', 'In-Context Learning']
---
In-context learning (ICL) has emerged as a powerful paradigm for easily
adapting Large Language Models (LLMs) to various tasks. However, our
understanding of how ICL works remains limited. We explore a simple model of
ICL in a controlled setup with synthetic training data to investigate ICL of
univariate linear functions. We experiment with a range of GPT-2-like
transformer models trained from scratch. Our findings challenge the prevailing
narrative that transformers adopt algorithmic approaches like linear regression
to learn a linear function in-context. These models fail to generalize beyond
their training distribution, highlighting fundamental limitations in their
capacity to infer abstract task structures. Our experiments lead us to propose
a mathematically precise hypothesis of what the model might be learning.
