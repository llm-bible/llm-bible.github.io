---
layout: publication
title: Simul45;llm A Framework For Exploring High45;quality Simultaneous Translation With Large Language Models
authors: Agostinelli Victor, Wild Max, Raffel Matthew, Fuad Kazi Ahmed Asif, Chen Lizhong
conference: "Arxiv"
year: 2023
bibkey: agostinelli2023simul
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.04691"}
tags: ['Applications', 'Tools']
---
Large language models (LLMs) with billions of parameters and pretrained on massive amounts of data are now capable of near or better than state45;of45;the45;art performance in a variety of downstream natural language processing tasks. Neural machine translation (NMT) is one such task that LLMs have been applied to with great success. However little research has focused on applying LLMs to the more difficult subset of NMT called simultaneous translation (SimulMT) where translation begins before the entire source context is available to the model. In this paper we address key challenges facing LLMs fine45;tuned for SimulMT validate classical SimulMT concepts and practices in the context of LLMs explore adapting LLMs that are fine45;tuned for NMT to the task of SimulMT and introduce Simul45;LLM the first open45;source fine45;tuning and evaluation pipeline development framework for LLMs focused on SimulMT.
