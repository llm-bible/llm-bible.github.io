---
layout: publication
title: Self45;attentional Models Application In Task45;oriented Dialogue Generation Systems
authors: Mehrjardi Mansour Saffar, Trabelsi Amine, Zaiane Osmar R.
conference: "Arxiv"
year: 2019
bibkey: mehrjardi2019self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1909.05246"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Training Techniques', 'Transformer']
---
Self45;attentional models are a new paradigm for sequence modelling tasks which differ from common sequence modelling methods such as recurrence45;based and convolution45;based sequence learning in the way that their architecture is only based on the attention mechanism. Self45;attentional models have been used in the creation of the state45;of45;the45;art models in many NLP tasks such as neural machine translation but their usage has not been explored for the task of training end45;to45;end task45;oriented dialogue generation systems yet. In this study we apply these models on the three different datasets for training task45;oriented chatbots. Our finding shows that self45;attentional models can be exploited to create end45;to45;end task45;oriented chatbots which not only achieve higher evaluation scores compared to recurrence45;based models but also do so more efficiently.
