---
layout: publication
title: Deepspeed45;visualchat Multi45;round Multi45;image Interleave Chat Via Multi45;modal Causal Attention
authors: Yao Zhewei, Wu Xiaoxia, Li Conglong, Zhang Minjia, Qin Heyang, Ruwase Olatunji, Awan Ammar Ahmad, Rajbhandari Samyam, He Yuxiong
conference: "Arxiv"
year: 2023
bibkey: yao2023deepspeed
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.14327"}
tags: ['Attention Mechanism', 'Fine Tuning', 'Model Architecture', 'Reinforcement Learning', 'Tools', 'Training Techniques', 'Transformer']
---
Most of the existing multi45;modal models hindered by their incapacity to adeptly manage interleaved image45;and45;text inputs in multi45;image multi45;round dialogues face substantial constraints in resource allocation for training and data accessibility impacting their adaptability and scalability across varied interaction realms. To address this we present the DeepSpeed45;VisualChat framework designed to optimize Large Language Models (LLMs) by incorporating multi45;modal capabilities with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs. Our framework is notable for (1) its open45;source support for multi45;round and multi45;image dialogues (2) introducing an innovative multi45;modal causal attention mechanism and (3) utilizing data blending techniques on existing datasets to assure seamless interactions in multi45;round multi45;image conversations. Compared to existing frameworks DeepSpeed45;VisualChat shows superior scalability up to 70B parameter language model size representing a significant advancement in multi45;modal language models and setting a solid foundation for future explorations.
