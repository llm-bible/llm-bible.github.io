---
layout: publication
title: 'Navigating The Cultural Kaleidoscope: A Hitchhiker''s Guide To Sensitivity In Large Language Models'
authors: Somnath Banerjee, Sayan Layek, Hari Shrawgi, Rajarshi Mandal, Avik Halder, Shanu Kumar, Sagnik Basu, Parag Agrawal, Rima Hazra, Animesh Mukherjee
conference: "Arxiv"
year: 2024
bibkey: banerjee2024navigating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.12880"}
tags: ['Pretraining Methods', 'Training Techniques', 'Applications', 'Fine-Tuning']
---
As LLMs are increasingly deployed in global applications, the importance of
cultural sensitivity becomes paramount, ensuring that users from diverse
backgrounds feel respected and understood. Cultural harm can arise when these
models fail to align with specific cultural norms, resulting in
misrepresentations or violations of cultural values. This work addresses the
challenges of ensuring cultural sensitivity in LLMs, especially in
small-parameter models that often lack the extensive training data needed to
capture global cultural nuances. We present two key contributions: (1) A
cultural harm test dataset, created to assess model outputs across different
cultural contexts through scenarios that expose potential cultural
insensitivities, and (2) A culturally aligned preference dataset, aimed at
restoring cultural sensitivity through fine-tuning based on feedback from
diverse annotators. These datasets facilitate the evaluation and enhancement of
LLMs, ensuring their ethical and safe deployment across different cultural
landscapes. Our results show that integrating culturally aligned feedback leads
to a marked improvement in model behavior, significantly reducing the
likelihood of generating culturally insensitive or harmful content. Ultimately,
this work paves the way for more inclusive and respectful AI systems, fostering
a future where LLMs can safely and ethically navigate the complexities of
diverse cultural landscapes.
