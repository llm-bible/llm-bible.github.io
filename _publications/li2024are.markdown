---
layout: publication
title: Are Bigger Encoders Always Better In Vision Large Models?
authors: Li Bozhou, Liang Hao, Meng Zimo, Zhang Wentao
conference: "Arxiv"
year: 2024
bibkey: li2024are
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.00620"}
tags: ['Applications', 'Efficiency And Optimization', 'Large Scale Training', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Reinforcement Learning', 'Scaling Laws', 'Tools', 'Training Techniques']
---
In recent years multimodal large language models (MLLMs) have shown strong potential in real-world applications. They are developing rapidly due to their remarkable ability to comprehend multimodal information and their inherent powerful cognitive and reasoning capabilities. Among MLLMs vision language models (VLM) stand out for their ability to understand vision information. However the scaling trend of VLMs under the current mainstream paradigm has not been extensively studied. Whether we can achieve better performance by training even larger models is still unclear. To address this issue we conducted experiments on the pretraining stage of MLLMs. We conduct our experiment using different encoder sizes and large language model (LLM) sizes. Our findings indicate that merely increasing the size of encoders does not necessarily enhance the performance of VLMs. Moreover we analyzed the effects of LLM backbone parameter size and data quality on the pretraining outcomes. Additionally we explored the differences in scaling laws between LLMs and VLMs.
