---
layout: publication
title: 'Argumentation Computation With Large Language Models : A Benchmark Study'
authors: Zhaoqun Li, Xiaotong Fang, Chen Chen, Mengze Li, Beishui Liao
conference: "Arxiv"
year: 2024
bibkey: li2024argumentation
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.16725'}
tags: ['Interpretability and Explainability', 'RAG', 'Applications', 'Tools', 'Reinforcement Learning', 'Ethics and Bias', 'Interpretability']
---
In recent years, large language models (LLMs) have made significant
advancements in neuro-symbolic computing. However, the combination of LLM with
argumentation computation remains an underexplored domain, despite its
considerable potential for real-world applications requiring defeasible
reasoning. In this paper, we aim to investigate the capability of LLMs in
determining the extensions of various abstract argumentation semantics. To
achieve this, we develop and curate a benchmark comprising diverse abstract
argumentation frameworks, accompanied by detailed explanations of algorithms
for computing extensions. Subsequently, we fine-tune LLMs on the proposed
benchmark, focusing on two fundamental extension-solving tasks. As a
comparative baseline, LLMs are evaluated using a chain-of-thought approach,
where they struggle to accurately compute semantics. In the experiments, we
demonstrate that the process explanation plays a crucial role in semantics
computation learning. Models trained with explanations show superior
generalization accuracy compared to those trained solely with question-answer
pairs. Furthermore, by leveraging the self-explanation capabilities of LLMs,
our approach provides detailed illustrations that mitigate the lack of
transparency typically associated with neural networks. Our findings contribute
to the broader understanding of LLMs' potential in argumentation computation,
offering promising avenues for further research in this domain.
