---
layout: publication
title: Partially Randomizing Transformer Weights for Dialogue Response Diversity
authors: Lee Jing Yang, Lee Kong Aik, Gan Woon-seng
conference: "Arxiv"
year: 2023
bibkey: lee2023partially
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.10943"}
tags: ['Model Architecture', 'Transformer', 'Arxiv']
---
Despite recent progress in generative open-domain dialogue the issue of low response diversity persists. Prior works have addressed this issue via either novel objective functions alternative learning approaches such as variational frameworks or architectural extensions such as the Randomized Link (RL) Transformer. However these approaches typically entail either additional difficulties during training/inference or a significant increase in model size and complexity. Hence we propose the rtially ndomized trans (PaRaFormer) a simple extension of the transformer which involves freezing the weights of selected layers after random initialization. Experimental results reveal that the performance of the PaRaformer is comparable to that of the aforementioned approaches despite not entailing any additional training difficulty or increase in model complexity.
