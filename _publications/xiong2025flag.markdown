---
layout: publication
title: 'Flag-trader: Fusion Llm-agent With Gradient-based Reinforcement Learning For Financial Trading'
authors: Guojun Xiong, Zhiyang Deng, Keyi Wang, Yupeng Cao, Haohang Li, Yangyang Yu, Xueqing Peng, Mingquan Lin, Kaleb E Smith, Xiao-yang Liu, Jimin Huang, Sophia Ananiadou, Qianqian Xie
conference: "Arxiv"
year: 2025
bibkey: xiong2025flag
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.11433"}
tags: ['Fine-Tuning', 'Agentic', 'Efficiency and Optimization', 'Tools', 'RAG', 'Model Architecture', 'Reinforcement Learning', 'Merging', 'Training Techniques', 'Pretraining Methods', 'Multimodal Models']
---
Large language models (LLMs) fine-tuned on multimodal financial data have
demonstrated impressive reasoning capabilities in various financial tasks.
However, they often struggle with multi-step, goal-oriented scenarios in
interactive financial markets, such as trading, where complex agentic
approaches are required to improve decision-making. To address this, we propose
\textsc\{FLAG-Trader\}, a unified architecture integrating linguistic processing
(via LLMs) with gradient-driven reinforcement learning (RL) policy
optimization, in which a partially fine-tuned LLM acts as the policy network,
leveraging pre-trained knowledge while adapting to the financial domain through
parameter-efficient fine-tuning. Through policy gradient optimization driven by
trading rewards, our framework not only enhances LLM performance in trading but
also improves results on other financial-domain tasks. We present extensive
empirical evidence to validate these enhancements.
