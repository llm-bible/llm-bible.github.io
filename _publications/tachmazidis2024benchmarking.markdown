---
layout: publication
title: 'Benchmarking Defeasible Reasoning With Large Language Models -- Initial Experiments And Future Directions'
authors: Ilias Tachmazidis, Sotiris Batsakis, Grigoris Antoniou
conference: "Arxiv"
year: 2024
bibkey: tachmazidis2024benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.12509"}
tags: ['GPT', 'Model Architecture']
---
Large Language Models (LLMs) have gained prominence in the AI landscape due
to their exceptional performance. Thus, it is essential to gain a better
understanding of their capabilities and limitations, among others in terms of
nonmonotonic reasoning. This paper proposes a benchmark that corresponds to
various defeasible rule-based reasoning patterns. We modified an existing
benchmark for defeasible logic reasoners by translating defeasible rules into
text suitable for LLMs. We conducted preliminary experiments on nonmonotonic
rule-based reasoning using ChatGPT and compared it with reasoning patterns
defined by defeasible logic.
