---
layout: publication
title: From Bytes To Biases: Investigating The Cultural Self-perception Of Large Language Models
authors: Messner Wolfgang, Greene Tatum, Matalone Josephine
conference: "Arxiv"
year: 2023
bibkey: messner2023from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.17256"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
Large language models (LLMs) are able to engage in natural-sounding conversations with humans showcasing unprecedented capabilities for information retrieval and automated decision support. They have disrupted human-technology interaction and the way businesses operate. However technologies based on generative artificial intelligence (GenAI) are known to hallucinate misinform and display biases introduced by the massive datasets on which they are trained. Existing research indicates that humans may unconsciously internalize these biases which can persist even after they stop using the programs. This study explores the cultural self-perception of LLMs by prompting ChatGPT (OpenAI) and Bard (Google) with value questions derived from the GLOBE project. The findings reveal that their cultural self-perception is most closely aligned with the values of English-speaking countries and countries characterized by sustained economic competitiveness. Recognizing the cultural biases of LLMs and understanding how they work is crucial for all members of society because one does not want the black box of artificial intelligence to perpetuate bias in humans who might in turn inadvertently create and train even more biased algorithms.
