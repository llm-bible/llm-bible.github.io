---
layout: publication
title: From Bytes To Biases Investigating The Cultural Self45;perception Of Large Language Models
authors: Messner Wolfgang, Greene Tatum, Matalone Josephine
conference: "Arxiv"
year: 2023
bibkey: messner2023from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.17256"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
Large language models (LLMs) are able to engage in natural45;sounding conversations with humans showcasing unprecedented capabilities for information retrieval and automated decision support. They have disrupted human45;technology interaction and the way businesses operate. However technologies based on generative artificial intelligence (GenAI) are known to hallucinate misinform and display biases introduced by the massive datasets on which they are trained. Existing research indicates that humans may unconsciously internalize these biases which can persist even after they stop using the programs. This study explores the cultural self45;perception of LLMs by prompting ChatGPT (OpenAI) and Bard (Google) with value questions derived from the GLOBE project. The findings reveal that their cultural self45;perception is most closely aligned with the values of English45;speaking countries and countries characterized by sustained economic competitiveness. Recognizing the cultural biases of LLMs and understanding how they work is crucial for all members of society because one does not want the black box of artificial intelligence to perpetuate bias in humans who might in turn inadvertently create and train even more biased algorithms.
