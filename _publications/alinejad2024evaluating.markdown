---
layout: publication
title: Evaluating The Retrieval Component In Llm45;based Question Answering Systems
authors: Alinejad Ashkan, Kumar Krtin, Vahdat Ali
conference: "Arxiv"
year: 2024
bibkey: alinejad2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.06458"}
tags: ['Applications', 'RAG', 'Reinforcement Learning', 'Tools']
---
Question answering systems (QA) utilizing Large Language Models (LLMs) heavily depend on the retrieval component to provide them with domain45;specific information and reduce the risk of generating inaccurate responses or hallucinations. Although the evaluation of retrievers dates back to the early research in Information Retrieval assessing their performance within LLM45;based chatbots remains a challenge. This study proposes a straightforward baseline for evaluating retrievers in Retrieval45;Augmented Generation (RAG)45;based chatbots. Our findings demonstrate that this evaluation framework provides a better image of how the retriever performs and is more aligned with the overall performance of the QA system. Although conventional metrics such as precision recall and F1 score may not fully capture LLMs capabilities 45; as they can yield accurate responses despite imperfect retrievers 45; our method considers LLMs strengths to ignore irrelevant contexts as well as potential errors and hallucinations in their responses.
