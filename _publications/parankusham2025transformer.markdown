---
layout: publication
title: 'Lakotabert: A Transformer-based Model For Low Resource Lakota Language'
authors: Kanishka Parankusham, Rodrigue Rizk, Kc Santosh
conference: "Arxiv"
year: 2025
bibkey: parankusham2025transformer
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.18212"}
tags: ['Transformer', 'RAG', 'Language Modeling', 'Reinforcement Learning', 'Model Architecture', 'Masked Language Model', 'Pretraining Methods', 'BERT']
---
Lakota, a critically endangered language of the Sioux people in North
America, faces significant challenges due to declining fluency among younger
generations. This paper introduces LakotaBERT, the first large language model
(LLM) tailored for Lakota, aiming to support language revitalization efforts.
Our research has two primary objectives: (1) to create a comprehensive Lakota
language corpus and (2) to develop a customized LLM for Lakota. We compiled a
diverse corpus of 105K sentences in Lakota, English, and parallel texts from
various sources, such as books and websites, emphasizing the cultural
significance and historical context of the Lakota language. Utilizing the
RoBERTa architecture, we pre-trained our model and conducted comparative
evaluations against established models such as RoBERTa, BERT, and multilingual
BERT. Initial results demonstrate a masked language modeling accuracy of 51%
with a single ground truth assumption, showcasing performance comparable to
that of English-based models. We also evaluated the model using additional
metrics, such as precision and F1 score, to provide a comprehensive assessment
of its capabilities. By integrating AI and linguistic methodologies, we aspire
to enhance linguistic diversity and cultural resilience, setting a valuable
precedent for leveraging technology in the revitalization of other endangered
indigenous languages.
