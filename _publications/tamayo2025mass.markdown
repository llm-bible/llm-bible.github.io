---
layout: publication
title: 'Mass-editing Memory With Attention In Transformers: A Cross-lingual Exploration Of Knowledge'
authors: Daniel Tamayo, Aitor Gonzalez-agirre, Javier Hernando, Marta Villegas
conference: "Findings of the Association for Computational Linguistics ACL 2024. Pages 5831-5847"
year: 2025
bibkey: tamayo2025mass
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.02173"}
  - {name: "Code", url: "https://github.com/dtamayo-nlp/MEMAT"}
tags: ['Fine-Tuning', 'Transformer', 'Model Architecture', 'Training Techniques', 'Attention Mechanism', 'Has Code', 'Pretraining Methods']
---
Recent research has explored methods for updating and modifying factual
knowledge in large language models, often focusing on specific multi-layer
perceptron blocks. This study expands on this work by examining the
effectiveness of existing knowledge editing methods across languages and
delving into the role of attention mechanisms in this process. Drawing from the
insights gained, we propose Mass-Editing Memory with Attention in Transformers
(MEMAT), a method that achieves significant improvements in all metrics while
requiring minimal parameter modifications. MEMAT delivers a remarkable 10%
increase in magnitude metrics, benefits languages not included in the training
data and also demonstrates a high degree of portability. Our code and data are
at https://github.com/dtamayo-nlp/MEMAT.
