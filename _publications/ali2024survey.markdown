---
layout: publication
title: 'A Survey Of Large Language Models For European Languages'
authors: Wazir Ali, Sampo Pyysalo
conference: "Arxiv"
year: 2024
bibkey: ali2024survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.15040"}
tags: ['Training Techniques', 'Model Architecture', 'Survey Paper', 'Tools', 'GPT', 'Pretraining Methods', 'Attention Mechanism']
---
Large Language Models (LLMs) have gained significant attention due to their
high performance on a wide range of natural language tasks since the release of
ChatGPT. The LLMs learn to understand and generate language by training
billions of model parameters on vast volumes of text data. Despite being a
relatively new field, LLM research is rapidly advancing in various directions.
In this paper, we present an overview of LLM families, including LLaMA, PaLM,
GPT, and MoE, and the methods developed to create and enhance LLMs for official
European Union (EU) languages. We provide a comprehensive summary of common
monolingual and multilingual datasets used for pretraining large language
models.
