---
layout: publication
title: 'Llms Can Learn Self-restraint Through Iterative Self-reflection'
authors: Piché Alexandre, Milios Aristides, Bahdanau Dzmitry, Pal Chris
conference: "Arxiv"
year: 2024
bibkey: piché2024llms
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.13022"}
tags: ['Prompting', 'RAG', 'Uncategorized']
---
In order to be deployed safely, Large Language Models (LLMs) must be capable of dynamically adapting their behavior based on their level of knowledge and uncertainty associated with specific topics. This adaptive behavior, which we refer to as self-restraint, is non-trivial to teach since it depends on the internal knowledge of an LLM. By default, LLMs are trained to maximize the next token likelihood, which does not teach the model to modulate its answer based on its level of uncertainty. In order to learn self-restraint, we devise a utility function that can encourage the model to produce responses only when it is confident in them. This utility function can be used to score generation of different length and abstention. To optimize this function, we introduce ReSearch, a process of self-reflection consisting of iterative self-prompting and self-evaluation. We use the ReSearch algorithm to generate synthetic data on which we finetune our models. Compared to their original versions, our resulting models generate fewer \emph\{hallucinations\} overall at no additional inference cost, for both known and unknown topics, as the model learns to selectively restrain itself. In addition, our method elegantly incorporates the ability to abstain by augmenting the samples generated by the model during the search procedure with an answer expressing abstention.
