---
layout: publication
title: 'Preserving Generalization Of Language Models In Few-shot Continual Relation Extraction'
authors: Quyen Tran, Nguyen Xuan Thanh, Nguyen Hoang Anh, Nam Le Hai, Trung Le, Linh Van Ngo, Thien Huu Nguyen
conference: "Arxiv"
year: 2024
bibkey: tran2024preserving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.00334"}
tags: ['RAG', 'Few-Shot', 'Merging']
---
Few-shot Continual Relations Extraction (FCRE) is an emerging and dynamic
area of study where models can sequentially integrate knowledge from new
relations with limited labeled data while circumventing catastrophic forgetting
and preserving prior knowledge from pre-trained backbones. In this work, we
introduce a novel method that leverages often-discarded language model heads.
By employing these components via a mutual information maximization strategy,
our approach helps maintain prior knowledge from the pre-trained backbone and
strategically aligns the primary classification head, thereby enhancing model
performance. Furthermore, we explore the potential of Large Language Models
(LLMs), renowned for their wealth of knowledge, in addressing FCRE challenges.
Our comprehensive experimental results underscore the efficacy of the proposed
method and offer valuable insights for future work.
