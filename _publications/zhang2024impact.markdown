---
layout: publication
title: The Impact Of Demonstrations On Multilingual In45;context Learning A Multidimensional Analysis
authors: Zhang Miaoran, Gautam Vagrant, Wang Mingyang, Alabi Jesujoba O., Shen Xiaoyu, Klakow Dietrich, Mosbach Marius
conference: "Arxiv"
year: 2024
bibkey: zhang2024impact
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.12976"}
tags: ['GPT', 'Model Architecture', 'Prompting']
---
In45;context learning is a popular inference strategy where large language models solve a task using only a few labeled demonstrations without needing any parameter updates. Although there have been extensive studies on English in45;context learning multilingual in45;context learning remains under45;explored and we lack an in45;depth understanding of the role of demonstrations in this context. To address this gap we conduct a multidimensional analysis of multilingual in45;context learning experimenting with 5 models from different model families 9 datasets covering classification and generation tasks and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models tasks and languages. We also find that strong instruction45;following models including Llama 245;Chat GPT45;3.5 and GPT45;4 are largely insensitive to the quality of demonstrations. Instead a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in45;context learning.
