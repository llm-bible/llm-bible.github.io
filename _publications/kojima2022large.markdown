---
layout: publication
title: Large Language Models Are Zero45;shot Reasoners
authors: Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa
conference: "Arxiv"
year: 2022
bibkey: kojima2022large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2205.11916v4"}
tags: ['Efficiency And Optimization', 'GPT', 'Large Scale Training', 'Model Architecture', 'Prompting', 'Scaling Laws']
---
Pretrained large language models (LLMs) are widely used in many sub45;fields of natural language processing (NLP) and generally known as excellent few45;shot learners with task45;specific exemplars. Notably chain of thought (CoT) prompting a recent technique for eliciting complex multi45;step reasoning through step45;by45;step answer examples achieved the state45;of45;the45;art performances in arithmetics and symbolic reasoning difficult system45;2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs ability for few45;shot learning we show that LLMs are decent zero45;shot reasoners by simply adding Lets think step by step before each answer. Experimental results demonstrate that our Zero45;shot45;CoT using the same single prompt template significantly outperforms zero45;shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith GSM8K AQUA45;RAT SVAMP) symbolic reasoning (Last Letter Coin Flip) and other logical reasoning tasks (Date Understanding Tracking Shuffled Objects) without any hand45;crafted few45;shot examples e.g. increasing the accuracy on MultiArith from 17.737; to 78.737; and GSM8K from 10.437; to 40.737; with large InstructGPT model (text45;davinci45;002) as well as similar magnitudes of improvements with another off45;the45;shelf large model 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero45;shot capabilities of LLMs suggesting high45;level multi45;task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero45;shot baseline for the challenging reasoning benchmarks but also highlights the importance of carefully exploring and analyzing the enormous zero45;shot knowledge hidden inside LLMs before crafting finetuning datasets or few45;shot exemplars.
