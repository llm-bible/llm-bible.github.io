---
layout: publication
title: 'Enhancing Conversational Agents With Theory Of Mind: Aligning Beliefs, Desires, And Intentions For Human-like Interaction'
authors: Mehdi Jafari, Devin Yuncheng Hua, Hao Xue, Flora Salim
conference: "Arxiv"
year: 2025
bibkey: jafari2025enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.14171"}
tags: ['Agentic']
---
Natural language interaction with agentic Artificial Intelligence (AI), driven by Large Language Models (LLMs), is expected to remain a dominant paradigm in the near future. While humans instinctively align their communication with mental states -- an ability known as Theory of Mind (ToM), current LLM powered systems exhibit significant limitations in this regard. This study examines the extent to which open source language models (LLaMA) can capture and preserve ToM related information and how effectively it contributes to consistent ToM reasoning in generated responses. We further investigate whether explicit manipulation of ToM related components, such as beliefs, desires, and intentions, can enhance response alignment. Experiments on two LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves response quality, achieving win rates of 67 and 63 percent for the 3B and 8B models, respectively. These findings highlight the potential of ToM driven strategies to improve alignment in LLM based conversational agents.
