---
layout: publication
title: Emovit Revolutionizing Emotion Insights With Visual Instruction Tuning
authors: Xie Hongxia, Peng Chu-jun, Tseng Yu-wen, Chen Hung-jen, Hsu Chan-feng, Shuai Hong-han, Cheng Wen-huang
conference: "Arxiv"
year: 2024
bibkey: xie2024revolutionizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.16670"}
  - {name: "Code", url: "https://github.com/aimmemotion/EmoVIT&#125;"}
tags: ['Fine Tuning', 'GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning']
---
Visual Instruction Tuning represents a novel learning paradigm involving the fine45;tuning of pre45;trained language models using task45;specific instructions. This paradigm shows promising zero45;shot results in various natural language processing tasks but is still unexplored in vision emotion understanding. In this work we focus on enhancing the models proficiency in understanding and adhering to instructions related to emotional contexts. Initially we identify key visual clues critical to visual emotion recognition. Subsequently we introduce a novel GPT45;assisted pipeline for generating emotion visual instruction data effectively addressing the scarcity of annotated instruction data in this domain. Expanding on the groundwork established by InstructBLIP our proposed EmoVIT architecture incorporates emotion45;specific instruction data leveraging the powerful capabilities of Large Language Models to enhance performance. Through extensive experiments our model showcases its proficiency in emotion classification adeptness in affective reasoning and competence in comprehending humor. The comparative analysis provides a robust benchmark for Emotion Visual Instruction Tuning in the era of LLMs providing valuable insights and opening avenues for future exploration in this domain. Our code is available at url123;https://github.com/aimmemotion/EmoVIT&#125;.
