---
layout: publication
title: 'Lore: Personalizing Llms Via Low-rank Reward Modeling'
authors: Avinandan Bose, Zhihan Xiong, Yuejie Chi, Simon Shaolei Du, Lin Xiao, Maryam Fazel
conference: "Arxiv"
year: 2025
bibkey: bose2025personalizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.14439"}
tags: ['Agentic', 'Few-Shot', 'Tools', 'Reinforcement Learning', 'RAG']
---
Personalizing large language models (LLMs) to accommodate diverse user
preferences is essential for enhancing alignment and user satisfaction.
Traditional reinforcement learning from human feedback (RLHF) approaches often
rely on monolithic value representations, limiting their ability to adapt to
individual preferences. We introduce a novel framework that leverages low-rank
preference modeling to efficiently learn and generalize user-specific reward
functions. By representing reward functions in a low-dimensional subspace and
modeling individual preferences as weighted combinations of shared basis
functions, our approach avoids rigid user categorization while enabling
scalability and few-shot adaptation. We validate our method on multiple
preference datasets, demonstrating superior generalization to unseen users and
improved accuracy in preference prediction tasks.
