---
layout: publication
title: 'Walia-llm: Enhancing Amharic-llama By Integrating Task-specific And Generative Datasets'
authors: Israel Abebe Azime, Atnafu Lambebo Tonja, Tadesse Destaw Belay, Mitiku Yohannes Fuge, Aman Kassahun Wassie, Eyasu Shiferaw Jada, Yonas Chanie, Walelign Tewabe Sewunetie, Seid Muhie Yimam
conference: "Arxiv"
year: 2024
bibkey: azime2024walia
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2402.08015'}
tags: ['Attention Mechanism', 'Training Techniques', 'Model Architecture', 'Fine-Tuning', 'Pretraining Methods']
---
Large language models (LLMs) have received a lot of attention in natural
language processing (NLP) research because of their exceptional performance in
understanding and generating human languages. However, low-resource languages
are left behind due to the unavailability of resources. In this work, we focus
on enhancing the LLaMA-2-Amharic model by integrating task-specific and
generative datasets to improve language model performance for Amharic. We
compile an Amharic instruction fine-tuning dataset and fine-tuned
LLaMA-2-Amharic model. The fine-tuned model shows promising results in
different NLP tasks. We open-source our dataset creation pipeline, instruction
datasets, trained models, and evaluation outputs to promote language-specific
studies on these models.
