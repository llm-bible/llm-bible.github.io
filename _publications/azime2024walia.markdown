---
layout: publication
title: Walia-llm&#58; Enhancing Amharic-llama By Integrating Task-specific And Generative Datasets
authors: Azime Israel Abebe, Tonja Atnafu Lambebo, Belay Tadesse Destaw, Fuge Mitiku Yohannes, Wassie Aman Kassahun, Jada Eyasu Shiferaw, Chanie Yonas, Sewunetie Walelign Tewabe, Yimam Seid Muhie
conference: "Arxiv"
year: 2024
bibkey: azime2024walia
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.08015"}
tags: ['Attention Mechanism', 'Fine Tuning', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
Large language models (LLMs) have received a lot of attention in natural language processing (NLP) research because of their exceptional performance in understanding and generating human languages. However low-resource languages are left behind due to the unavailability of resources. In this work we focus on enhancing the LLaMA-2-Amharic model by integrating task-specific and generative datasets to improve language model performance for Amharic. We compile an Amharic instruction fine-tuning dataset and fine-tuned LLaMA-2-Amharic model. The fine-tuned model shows promising results in different NLP tasks. We open-source our dataset creation pipeline instruction datasets trained models and evaluation outputs to promote language-specific studies on these models.
