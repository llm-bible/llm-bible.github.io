---
layout: publication
title: Walia45;llm Enhancing Amharic45;llama By Integrating Task45;specific And Generative Datasets
authors: Azime Israel Abebe, Tonja Atnafu Lambebo, Belay Tadesse Destaw, Fuge Mitiku Yohannes, Wassie Aman Kassahun, Jada Eyasu Shiferaw, Chanie Yonas, Sewunetie Walelign Tewabe, Yimam Seid Muhie
conference: "Arxiv"
year: 2024
bibkey: azime2024walia
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.08015"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods']
---
Large language models (LLMs) have received a lot of attention in natural language processing (NLP) research because of their exceptional performance in understanding and generating human languages. However low45;resource languages are left behind due to the unavailability of resources. In this work we focus on enhancing the LLaMA45;245;Amharic model by integrating task45;specific and generative datasets to improve language model performance for Amharic. We compile an Amharic instruction fine45;tuning dataset and fine45;tuned LLaMA45;245;Amharic model. The fine45;tuned model shows promising results in different NLP tasks. We open45;source our dataset creation pipeline instruction datasets trained models and evaluation outputs to promote language45;specific studies on these models.
