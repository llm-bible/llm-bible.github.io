---
layout: publication
title: 'Chathome: Development And Evaluation Of A Domain-specific Language Model For Home Renovation'
authors: Cheng Wen, Xianghui Sun, Shuaijiang Zhao, Xiaoquan Fang, Liangyu Chen, Wei Zou
conference: "Arxiv"
year: 2023
bibkey: wen2023development
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2307.15290'}
tags: ['Training Techniques', 'GPT', 'Tools', 'Fine-Tuning', 'Model Architecture', 'Pretraining Methods']
---
This paper presents the development and evaluation of ChatHome, a
domain-specific language model (DSLM) designed for the intricate field of home
renovation. Considering the proven competencies of large language models (LLMs)
like GPT-4 and the escalating fascination with home renovation, this study
endeavors to reconcile these aspects by generating a dedicated model that can
yield high-fidelity, precise outputs relevant to the home renovation arena.
ChatHome's novelty rests on its methodology, fusing domain-adaptive pretraining
and instruction-tuning over an extensive dataset. This dataset includes
professional articles, standard documents, and web content pertinent to home
renovation. This dual-pronged strategy is designed to ensure that our model can
assimilate comprehensive domain knowledge and effectively address user
inquiries. Via thorough experimentation on diverse datasets, both universal and
domain-specific, including the freshly introduced "EvalHome" domain dataset, we
substantiate that ChatHome not only amplifies domain-specific functionalities
but also preserves its versatility.
