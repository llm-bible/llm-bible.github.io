---
layout: publication
title: Establishing Performance Baselines In Fine45;tuning Retrieval45;augmented Generation And Soft45;prompting For Non45;specialist LLM Users
authors: Dodgson Jennifer, Nanzheng Lin, Peh Julian, Pattirane Akira Rafhael Janson, Alhajir Alfath Daryl, Dinarto Eko Ridho, Lim Joseph, Ahmad Syed Danyal
conference: "Arxiv"
year: 2023
bibkey: dodgson2023establishing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.05903"}
tags: ['GPT', 'Model Architecture', 'Prompting', 'RAG', 'Tools', 'Training Techniques']
---
Research into methods for improving the performance of large language models (LLMs) through fine45;tuning retrieval45;augmented generation (RAG) and soft45;prompting has tended to focus on the use of highly technical or high45;cost techniques making many of the newly discovered approaches comparatively inaccessible to non45;technical users. In this paper we tested an unmodified version of GPT 3.5 a fine45;tuned version and the same unmodified model when given access to a vectorised RAG database both in isolation and in combination with a basic non45;algorithmic soft prompt. In each case we tested the models ability to answer a set of 100 questions relating primarily to events that occurred after September 2021 (the point at which GPT 3.5s training data set ends). We found that if commercial platforms are used and default settings are applied with no iteration in order to establish a baseline set of outputs a fine45;tuned model outperforms GPT 3.5 Turbo while the RAG approach out45;performed both. The application of a soft prompt significantly improved the performance of each approach.
