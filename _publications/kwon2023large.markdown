---
layout: publication
title: 'Large Language Models Are Clinical Reasoners: Reasoning-aware Diagnosis Framework With Prompt-generated Rationales'
authors: Taeyoon Kwon, Kai Tzu-iunn Ong, Dongjin Kang, Seungjun Moon, Jeong Ryong Lee, Dosik Hwang, Yongsik Sim, Beomseok Sohn, Dongha Lee, Jinyoung Yeo
conference: "Arxiv"
year: 2023
bibkey: kwon2023large
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2312.07399'}
tags: ['Reinforcement Learning', 'Prompting', 'Tools']
---
Machine reasoning has made great progress in recent years owing to large
language models (LLMs). In the clinical domain, however, most NLP-driven
projects mainly focus on clinical classification or reading comprehension, and
under-explore clinical reasoning for disease diagnosis due to the expensive
rationale annotation with clinicians. In this work, we present a
"reasoning-aware" diagnosis framework that rationalizes the diagnostic process
via prompt-based learning in a time- and labor-efficient manner, and learns to
reason over the prompt-generated rationales. Specifically, we address the
clinical reasoning for disease diagnosis, where the LLM generates diagnostic
rationales providing its insight on presented patient data and the reasoning
path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We
empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive
experiments and analyses on both rationale generation and disease diagnosis in
various settings. We further propose a novel set of criteria for evaluating
machine-generated rationales' potential for real-world clinical settings,
facilitating and benefiting future research in this area.
