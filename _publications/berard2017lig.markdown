---
layout: publication
title: Lig45;cristal System For The WMT17 Automatic Post45;editing Task
authors: Berard Alexandre, Pietquin Olivier, Besacier Laurent
conference: "Arxiv"
year: 2017
bibkey: berard2017lig
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1707.05118"}
tags: ['Attention Mechanism', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
This paper presents the LIG45;CRIStAL submission to the shared Automatic Post45; Editing task of WMT 2017. We propose two neural post45;editing models a monosource model with a task45;specific attention mechanism which performs particularly well in a low45;resource scenario; and a chained architecture which makes use of the source sentence to provide extra context. This latter architecture manages to slightly improve our results when more training data is available. We present and discuss our results on two datasets (en45;de and de45;en) that are made available for the task.
