---
layout: publication
title: 'Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods'
authors: Diana Bar-or Nirman, Ariel Weizman, Amos Azaria
conference: "Arxiv"
year: 2024
bibkey: nirman2024fool
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.11625'}
tags: ['RAG', 'Tools', 'Training Techniques']
---
While Large Language Models (LLMs) have become central tools in various
fields, they often provide inaccurate or false information. This study examines
user preferences regarding falsehood responses from LLMs. Specifically, we
evaluate preferences for LLM responses where false statements are explicitly
marked versus unmarked responses and preferences for confident falsehoods
compared to LLM disclaimers acknowledging a lack of knowledge. Additionally, we
investigate how requiring users to assess the truthfulness of statements
influences these preferences.
  Surprisingly, 61% of users prefer unmarked falsehood responses over marked
ones, and 69% prefer confident falsehoods over LLMs admitting lack of
knowledge. In all our experiments, a total of 300 users participated,
contributing valuable data to our analysis and conclusions. When users are
required to evaluate the truthfulness of statements, preferences for unmarked
and falsehood responses decrease slightly but remain high. These findings
suggest that user preferences, which influence LLM training via feedback
mechanisms, may inadvertently encourage the generation of falsehoods. Future
research should address the ethical and practical implications of aligning LLM
behavior with such preferences.
