---
layout: publication
title: Patent Claim Generation By Fine45;tuning Openai GPT45;2
authors: Lee Jieh-sheng, Hsiang Jieh
conference: "Arxiv"
year: 2019
bibkey: lee2019patent
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1907.02052"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'RAG', 'Reinforcement Learning']
---
In this work we focus on fine45;tuning an OpenAI GPT45;2 pre45;trained model for generating patent claims. GPT45;2 has demonstrated impressive efficacy of pre45;trained language models on various tasks particularly coherent text generation. Patent claim language itself has rarely been explored in the past and poses a unique challenge. We are motivated to generate coherent patent claims automatically so that augmented inventing might be viable someday. In our implementation we identified a unique language structure in patent claims and leveraged its implicit human annotations. We investigated the fine45;tuning process by probing the first 100 steps and observing the generated text at each step. Based on both conditional and unconditional random sampling we analyze the overall quality of generated patent claims. Our contributions include (1) being the first to generate patent claims by machines and being the first to apply GPT45;2 to patent claim generation (2) providing various experiment results for qualitative analysis and future research (3) proposing a new sampling approach for text generation and (4) building an e45;mail bot for future researchers to explore the fine45;tuned GPT45;2 model further.
