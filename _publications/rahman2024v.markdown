---
layout: publication
title: V45;zen Efficient GUI Understanding And Precise Grounding With A Novel Multimodal LLM
authors: Rahman Abdur, Chawla Rajat, Kumar Muskaan, Datta Arkajit, Jha Adarsh, Ns Mukunda, Bhola Ishaan
conference: "Arxiv"
year: 2024
bibkey: rahman2024v
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.15341"}
tags: ['Multimodal Models', 'Reinforcement Learning', 'Tools']
---
In the rapidly evolving landscape of AI research and application Multimodal Large Language Models (MLLMs) have emerged as a transformative force adept at interpreting and integrating information from diverse modalities such as text images and Graphical User Interfaces (GUIs). Despite these advancements the nuanced interaction and understanding of GUIs pose a significant challenge limiting the potential of existing models to enhance automation levels. To bridge this gap this paper presents V45;Zen an innovative Multimodal Large Language Model (MLLM) meticulously crafted to revolutionise the domain of GUI understanding and grounding. Equipped with dual45;resolution image encoders V45;Zen establishes new benchmarks in efficient grounding and next45;action prediction thereby laying the groundwork for self45;operating computer systems. Complementing V45;Zen is the GUIDE dataset an extensive collection of real45;world GUI elements and task45;based sequences serving as a catalyst for specialised fine45;tuning. The successful integration of V45;Zen and GUIDE marks the dawn of a new era in multimodal AI research opening the door to intelligent autonomous computing experiences. This paper extends an invitation to the research community to join this exciting journey shaping the future of GUI automation. In the spirit of open science our code data and model will be made publicly available paving the way for multimodal dialogue scenarios with intricate and precise interactions.
