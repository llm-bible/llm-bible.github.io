---
layout: publication
title: Egovlpv2 Egocentric Video45;language Pre45;training With Fusion In The Backbone
authors: Pramanick Shraman, Song Yale, Nag Sayan, Lin Kevin Qinghong, Shah Hardik, Shou Mike Zheng, Chellappa Rama, Zhang Pengchuan
conference: "Arxiv"
year: 2023
bibkey: pramanick2023egocentric
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.05463"}
  - {name: "Code", url: "https://shramanpramanick.github.io/EgoVLPv2/"}
tags: ['Applications', 'Attention Mechanism', 'Has Code', 'Merging', 'Model Architecture', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Video45;language pre45;training (VLP) has become increasingly important due to its ability to generalize to various vision and language tasks. However existing egocentric VLP frameworks utilize separate video and language encoders and learn task45;specific cross45;modal information only during fine45;tuning limiting the development of a unified system. In this work we introduce the second generation of egocentric video45;language pre45;training (EgoVLPv2) a significant improvement from the previous generation by incorporating cross45;modal fusion directly into the video and language backbones. EgoVLPv2 learns strong video45;text representation during pre45;training and reuses the cross45;modal attention modules to support different downstream tasks in a flexible and efficient manner reducing fine45;tuning costs. Moreover our proposed fusion in the backbone strategy is more lightweight and compute45;efficient than stacking additional fusion45;specific layers. Extensive experiments on a wide range of VL tasks demonstrate the effectiveness of EgoVLPv2 by achieving consistent state45;of45;the45;art performance over strong baselines across all downstream. Our project page can be found at https://shramanpramanick.github.io/EgoVLPv2/.
