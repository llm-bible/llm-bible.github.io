---
layout: publication
title: Tablevqa45;bench A Visual Question Answering Benchmark On Multiple Table Domains
authors: Kim Yoonsik, Yim Moonbin, Song Ka Yeon
conference: "Arxiv"
year: 2024
bibkey: kim2024tablevqa
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.19205"}
  - {name: "Code", url: "https://github.com/naver&#45;ai/tablevqabench&#125;&#123;https://github.com/naver&#45;ai/tablevqabench&#125;"}
tags: ['Applications', 'GPT', 'Has Code', 'Model Architecture']
---
In this paper we establish a benchmark for table visual question answering referred to as the TableVQA45;Bench derived from pre45;existing table question45;answering (QA) and table structure recognition datasets. It is important to note that existing datasets have not incorporated images or QA pairs which are two crucial components of TableVQA. As such the primary objective of this paper is to obtain these necessary components. Specifically images are sourced either through the application of a textit123;stylesheet125; or by employing the proposed table rendering system. QA pairs are generated by exploiting the large language model (LLM) where the input is a text45;formatted table. Ultimately the completed TableVQA45;Bench comprises 1500 QA pairs. We comprehensively compare the performance of various multi45;modal large language models (MLLMs) on TableVQA45;Bench. GPT45;4V achieves the highest accuracy among commercial and open45;sourced MLLMs from our experiments. Moreover we discover that the number of vision queries plays a significant role in TableVQA performance. To further analyze the capabilities of MLLMs in comparison to their LLM backbones we investigate by presenting image45;formatted tables to MLLMs and text45;formatted tables to LLMs respectively. Our findings suggest that processing visual inputs is more challenging than text inputs as evidenced by the lower performance of MLLMs despite generally requiring higher computational costs than LLMs. The proposed TableVQA45;Bench and evaluation codes are available at href123;https://github.com/naver&#45;ai/tablevqabench&#125;&#123;https://github.com/naver&#45;ai/tablevqabench&#125;.
