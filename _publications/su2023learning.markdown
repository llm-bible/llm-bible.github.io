---
layout: publication
title: Learning From Red Teaming Gender Bias Provocation And Mitigation In Large Language Models
authors: Su Hsuan, Cheng Cheng-chu, Farn Hua, Kumar Shachi H, Sahay Saurav, Chen Shang-tse, Lee Hung-yi
conference: "Arxiv"
year: 2023
bibkey: su2023learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.11079"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Model Architecture']
---
Recently researchers have made considerable improvements in dialogue systems with the progress of large language models (LLMs) such as ChatGPT and GPT45;4. These LLM45;based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. The traditional biases investigation methods often rely on human45;written test cases. However these test cases are usually expensive and limited. In this work we propose a first45;of45;its45;kind method that automatically generates test cases to detect LLMs potential gender bias. We apply our method to three well45;known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified we propose a mitigation strategy that uses the generated test cases as demonstrations for in45;context learning to circumvent the need for parameter fine45;tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.
