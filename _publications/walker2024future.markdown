---
layout: publication
title: 'Future Token Prediction -- Causal Language Modelling With Per-token Semantic State Vector For Multi-token Prediction'
authors: Nicholas Walker
conference: "Arxiv"
year: 2024
bibkey: walker2024future
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.18160"}
tags: ['Training Techniques', 'Model Architecture', 'Tools', 'Reinforcement Learning', 'GPT', 'Pretraining Methods', 'Transformer', 'Pre-Training']
---
Causal decoder-only transformer models used for generative language
modelling, such as Generative Pre-trained Transformers (GPT), are trained to
predict the next token in a sequence based only on its previous tokens. Despite
this simple training objective, they have proved to be powerful AI tools.
However, only predicting the next token results in top layer embedding vectors
that are highly token-focused. There may be benefits in generating embedding
vectors at each token position that better capture the overall meaning of
longer sequences of future text. Recent studies matching brain scans with deep
language models suggest that humans also predict upcoming words when listening
or reading but consider multiple future tokens rather than just one.
  This research investigates a new pretraining method called Future Token
Prediction (FTP). In FTP, a large transformer encoder generates top layer
embedding vectors for each token position, which, instead of being passed to a
language head, are linearly and expansively projected to a pseudo-sequence,
which is cross attended to by a small transformer decoder to predict the next N
tokens forward from that position in the sequence.
  The top layer embedding vectors from FTP models exhibit distinct properties
compared to those from standard GPT models, varying smoothly along a text
sequence as measured by cosine similarity between adjacent tokens. Text
generated by FTP models show improved topic coherence compared to standard
GPT-like models trained with the same prediction perplexity for the next single
token. The vectors are shown to better represent the topic of text based on the
results of text classification examples. On a toy, but complex, coding problem,
FTP networks produce significantly better results than GPT networks.
