---
layout: publication
title: Large Language Models On The Chessboard&#58; A Study On Chatgpt's Formal Language Comprehension And Complex Reasoning Skills
authors: Kuo Mu-tien, Hsueh Chih-chung, Tsai Richard Tzong-han
conference: "Arxiv"
year: 2023
bibkey: kuo2023large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.15118"}
tags: ['Attention Mechanism', 'Fine Tuning', 'GPT', 'Model Architecture', 'Transformer']
---
While large language models have made strides in natural language processing their proficiency in complex reasoning tasks requiring formal language comprehension such as chess remains less investigated. This paper probes the performance of ChatGPT a sophisticated language model by OpenAI in tackling such complex reasoning tasks using chess as a case study. Through robust metrics examining both the legality and quality of moves we assess ChatGPTs understanding of the chessboard adherence to chess rules and strategic decision-making abilities. Our evaluation identifies limitations within ChatGPTs attention mechanism that affect its formal language comprehension and uncovers the models underdeveloped self-regulation abilities. Our study also reveals ChatGPTs propensity for a coherent strategy in its gameplay and a noticeable uptick in decision-making assertiveness when the model is presented with a greater volume of natural language or possesses a more lucid understanding of the state of the chessboard. These findings contribute to the growing exploration of language models abilities beyond natural language processing providing valuable information for future research towards models demonstrating human-like cognitive abilities.
