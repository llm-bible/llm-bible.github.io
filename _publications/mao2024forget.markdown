---
layout: publication
title: Dont Forget Your Reward Values Language Model Alignment Via Value45;based Calibration
authors: Mao Xin, Li Feng-lin, Xu Huimin, Zhang Wei, Luu Anh Tuan
conference: "Arxiv"
year: 2024
bibkey: mao2024forget
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.16030"}
tags: ['Agentic', 'Applications', 'Efficiency And Optimization', 'Reinforcement Learning', 'Security']
---
While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs) recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm proposing a series of order45;based calibration methods as viable alternatives. This paper delves further into current order45;based methods examining their inefficiencies in utilizing reward values and addressing misalignment issues. Building upon these findings we propose a novel textbf123;V125;alue45;based textbf123;C125;alitextbf123;B125;ration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets providing impressive generalizability robustness and stability in diverse settings.
