---
layout: publication
title: Astrollama45;chat Scaling Astrollama With Conversational And Diverse Datasets
authors: Perkowski Ernest, Pan Rui, Nguyen Tuan Dung, Ting Yuan-sen, Kruk Sandor, Zhang Tong, O'neill Charlie, Jablonska Maja, Sun Zechang, Smith Michael J., Liu Huiling, Schawinski Kevin, Iyer Kartheik, Universetbd Ioana CiucÄƒ For
conference: "Arxiv"
year: 2024
bibkey: perkowski2024astrollama
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.01916"}
tags: ['GPT', 'Model Architecture', 'Training Techniques']
---
We explore the potential of enhancing LLM performance in astronomy45;focused question45;answering through targeted continual pre45;training. By employing a compact 7B45;parameter LLaMA45;2 model and focusing exclusively on a curated set of astronomy corpora 45;45; comprising abstracts introductions and conclusions 45;45; we achieve notable improvements in specialized topic comprehension. While general LLMs like GPT45;4 excel in broader question45;answering scenarios due to superior reasoning capabilities our findings suggest that continual pre45;training with limited resources can still enhance model performance on specialized topics. Additionally we present an extension of AstroLLaMA the fine45;tuning of the 7B LLaMA model on a domain45;specific conversational dataset culminating in the release of the chat45;enabled AstroLLaMA for community use. Comprehensive quantitative benchmarking is currently in progress and will be detailed in an upcoming full paper. The model AstroLLaMA45;Chat is now available at https://huggingface.co/universeTBD, providing the first open45;source conversational AI tool tailored for the astronomy community.
