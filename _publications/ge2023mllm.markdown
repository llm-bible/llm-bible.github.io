---
layout: publication
title: Mllm45;bench Evaluating Multimodal Llms With Per45;sample Criteria
authors: Ge Wentao, Chen Shunian, Chen Guiming Hardy, Chen Zhihong, Chen Junying, Yan Shuo, Zhu Chenghao, Lin Ziyue, Xie Wenya, Zhang Xinyi, Chai Yichen, Liu Xiaoyu, Song Dingjie, Wang Xidong, Gao Anningzhe, Zhang Zhiyi, Li Jianquan, Wan Xiang, Wang Benyou
conference: "Arxiv"
year: 2023
bibkey: ge2023mllm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.13951"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Multimodal Models', 'RAG', 'Reinforcement Learning', 'Tools']
---
Multimodal large language models (MLLMs) (e.g. GPT45;4V LLaVA and Claude45;3) have broadened the scope of AI applications. Yet evaluating their performance presents a significant challenge owing to the inherently subjective nature of tasks that do not yield clear45;cut solutions especially for those open45;ended queries. Existing automatic evaluation methodologies are mainly limited in evaluating objective queries without considering real45;world user experiences inadequately addressing the nuances of creative and associative multimodal tasks. In our paper we propose a new evaluation paradigm for MLLMs which is evaluating MLLMs with textit123;per45;sample criteria125; using potent MLLM as the judge. To validate the feasibility and effectiveness of this paradigm we design a benchmark dubbed textit123;MLLM45;Bench125; with the evaluation samples across six critical levels following the revised Blooms Taxonomy with the ethical consideration. We benchmark 21 popular MLLMs in a pairwise45;comparison fashion showing diverse performance across models. Moreover the validity of our benchmark manifests itself in reaching 88.0237; agreement with human evaluation. We contend that the proposed paradigm explores the potential of MLLMs as effective evaluation tools with the help of per45;sample criteria and that MLLM45;Bench will serve as a catalyst for encouraging the development of user45;centric MLLMs tailored to real45;world applications. Our benchmark data online leaderboard and submission entry are at https://mllm&#45;bench.llmzoo.com.
