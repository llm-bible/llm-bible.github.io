---
layout: publication
title: Visual Cot Advancing Multi45;modal Language Models With A Comprehensive Dataset And Benchmark For Chain45;of45;thought Reasoning
authors: Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li
conference: "Arxiv"
year: 2024
bibkey: shao2024visual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2403.16999v2"}
tags: ['Interpretability And Explainability', 'Tools']
---
Multi45;Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However they often lack interpretability and struggle with complex visual inputs especially when the resolution of the input image is high or when the interested region that could provide key information for answering the question is small. To address these challenges we collect and introduce the large45;scale Visual CoT dataset comprising 438k question45;answer pairs annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Additionally about 98k pairs of them are annotated with detailed reasoning steps. Importantly we propose a multi45;turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We also introduce the related benchmark to evaluate the MLLMs in scenarios requiring specific local region identification. Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset benchmark and pre45;trained models are released to foster further research in this direction.
