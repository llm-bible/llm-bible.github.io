---
layout: publication
title: Reinforcement Learning Fine45;tuning Of Language Models Is Biased Towards More Extractable Features
authors: Cruz Diogo, Pona Edoardo, Holness-tofts Alex, Schmied Elias, Alonso VÃ­ctor Abia, Griffin Charlie, Cirstea Bogdan-ionut
conference: "Arxiv"
year: 2023
bibkey: cruz2023reinforcement
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.04046"}
tags: ['Agentic', 'Ethics And Bias', 'Reinforcement Learning', 'Security', 'Training Techniques']
---
Many capable large language models (LLMs) are developed via self45;supervised pre45;training followed by a reinforcement45;learning fine45;tuning phase often based on human or AI feedback. During this stage models may be guided by their inductive biases to rely on simpler features which may be easier to extract at a cost to robustness and generalisation. We investigate whether principles governing inductive biases in the supervised fine45;tuning of LLMs also apply when the fine45;tuning process uses reinforcement learning. Following Lovering et al (2021) we test two hypotheses that features more textit123;extractable125; after pre45;training are more likely to be utilised by the final policy and that the evidence for/against a feature predicts whether it will be utilised. Through controlled experiments on synthetic and natural language tasks we find statistically significant correlations which constitute strong evidence for these hypotheses.
