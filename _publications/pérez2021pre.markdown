---
layout: publication
title: Robertuito A Pre45;trained Language Model For Social Media Text In Spanish
authors: Pérez Juan Manuel, Furman Damián A., Alemany Laura Alonso, Luque Franco
conference: "Arxiv"
year: 2021
bibkey: pérez2021pre
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2111.09453"}
tags: ['Applications', 'BERT', 'Fine Tuning', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Since BERT appeared Transformer language models and transfer learning have become state45;of45;the45;art for Natural Language Understanding tasks. Recently some works geared towards pre45;training specially45;crafted models for particular domains such as scientific papers medical documents user45;generated texts among others. These domain45;specific models have been shown to improve performance significantly in most tasks. However for languages other than English such models are not widely available. In this work we present RoBERTuito a pre45;trained language model for user45;generated text in Spanish trained on over 500 million tweets. Experiments on a benchmark of tasks involving user45;generated text showed that RoBERTuito outperformed other pre45;trained language models in Spanish. In addition to this our model achieves top results for some English45;Spanish tasks of the Linguistic Code45;Switching Evaluation benchmark (LinCE) and has also competitive performance against monolingual models in English tasks. To facilitate further research we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre45;train it.
