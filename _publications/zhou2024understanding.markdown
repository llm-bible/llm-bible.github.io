---
layout: publication
title: 'Understanding And Alleviating Memory Consumption In RLHF For Llms'
authors: Jin Jiaxun Zhou, Hanmei Jiaxun Yang, Jiaxun Steven, Tang, Mingcan Xiang, Hui Guan, Tongping Liu
conference: "Arxiv"
year: 2024
bibkey: zhou2024understanding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.15651"}
tags: ['Agentic', 'Training Techniques', 'Reinforcement Learning', 'Pretraining Methods', 'Fine-Tuning']
---
Fine-tuning with Reinforcement Learning with Human Feedback (RLHF) is
essential for aligning large language models (LLMs). However, RLHF often
encounters significant memory challenges. This study is the first to examine
memory usage in the RLHF context, exploring various memory management
strategies and unveiling the reasons behind excessive memory consumption.
Additionally, we introduce a simple yet effective approach that substantially
reduces the memory required for RLHF fine-tuning.
