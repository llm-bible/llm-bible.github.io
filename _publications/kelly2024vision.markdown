---
layout: publication
title: Visiongpt Vision45;language Understanding Agent Using Generalized Multimodal Framework
authors: Kelly Chris, Hu Luhui, Yang Bang, Tian Yu, Yang Deshun, Yang Cindy, Huang Zaoshan, Li Zihao, Hu Jiayin, Zou Yuexian
conference: "Arxiv"
year: 2024
bibkey: kelly2024vision
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.09027"}
tags: ['Agentic', 'Applications', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning', 'Tools']
---
With the emergence of large language models (LLMs) and vision foundation models how to combine the intelligence and capacity of these open45;sourced or API45;available models to achieve open45;world visual perception remains an open question. In this paper we introduce VisionGPT to consolidate and automate the integration of state45;of45;the45;art foundation models thereby facilitating vision45;language understanding and the development of vision45;oriented AI. VisionGPT builds upon a generalized multimodal framework that distinguishes itself through three key features (1) utilizing LLMs (e.g. LLaMA45;2) as the pivot to break down users requests into detailed action proposals to call suitable foundation models; (2) integrating multi45;source outputs from foundation models automatically and generating comprehensive responses for users; (3) adaptable to a wide range of applications such as text45;conditioned image understanding/generation/editing and visual question answering. This paper outlines the architecture and capabilities of VisionGPT demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency versatility and generalization and performance. Our code and models will be made publicly available. Keywords VisionGPT Open45;world visual perception Vision45;language understanding Large language model and Foundation model
