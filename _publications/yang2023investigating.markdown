---
layout: publication
title: Investigating Zero45;shot Generalizability On Mandarin45;english Code45;switched ASR And Speech45;to45;text Translation Of Recent Foundation Models With Self45;supervision And Weak Supervision
authors: Yang Chih-kai, Huang Kuan-po, Lu Ke-han, Kuan Chun-yi, Hsiao Chi-yuan, Lee Hung-yi
conference: "Arxiv"
year: 2023
bibkey: yang2023investigating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.00273"}
tags: ['Pretraining Methods', 'Training Techniques']
---
This work evaluated several cutting45;edge large45;scale foundation models based on self45;supervision or weak supervision including SeamlessM4T SeamlessM4T v2 and Whisper45;large45;v3 on three code45;switched corpora. We found that self45;supervised models can achieve performances close to the supervised model indicating the effectiveness of multilingual self45;supervised pre45;training. We also observed that these models still have room for improvement as they kept making similar mistakes and had unsatisfactory performances on modeling intra45;sentential code45;switching. In addition the validity of several variants of Whisper was explored and we concluded that they remained effective in a code45;switching scenario and similar techniques for self45;supervised models are worth studying to boost the performance of code45;switched tasks.
