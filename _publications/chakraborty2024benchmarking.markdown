---
layout: publication
title: "Detoxbench: Benchmarking Large Language Models For Multitask Fraud & Abuse Detection"
authors: Chakraborty Joymallya, Xia Wei, Majumder Anirban, Ma Dan, Chaabene Walid, Janvekar Naveed
conference: "Proceedings of the"
year: 2024
bibkey: chakraborty2024benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.06072"}
tags: ['Applications', 'Ethics And Bias', 'Fine Tuning', 'RAG', 'Reinforcement Learning']
---
Large language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks. However their practical application in high-stake domains such as fraud and abuse detection remains an area that requires further exploration. The existing applications often narrowly focus on specific tasks like toxicity or hate speech detection. In this paper we present a comprehensive benchmark suite designed to assess the performance of LLMs in identifying and mitigating fraudulent and abusive language across various real-world scenarios. Our benchmark encompasses a diverse set of tasks including detecting spam emails hate speech misogynistic language and more. We evaluated several state-of-the-art LLMs including models from Anthropic Mistral AI and the AI21 family to provide a comprehensive assessment of their capabilities in this critical domain. The results indicate that while LLMs exhibit proficient baseline performance in individual fraud and abuse detection tasks their performance varies considerably across tasks particularly struggling with tasks that demand nuanced pragmatic reasoning such as identifying diverse forms of misogynistic language. These findings have important implications for the responsible development and deployment of LLMs in high-risk applications. Our benchmark suite can serve as a tool for researchers and practitioners to systematically evaluate LLMs for multi-task fraud detection and drive the creation of more robust trustworthy and ethically-aligned systems for fraud and abuse detection.
