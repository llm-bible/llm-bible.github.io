---
layout: publication
title: 'Wisdombot: Tuning Large Language Models With Artificial Intelligence Knowledge'
authors: Jingyuan Chen, Tao Wu, Wei Ji, Fei Wu
conference: "Arxiv"
year: 2025
bibkey: chen2025tuning
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.12877'}
tags: ['Interpretability and Explainability', 'Training Techniques', 'Tools']
---
Large language models (LLMs) have emerged as powerful tools in natural
language processing (NLP), showing a promising future of artificial generated
intelligence (AGI). Despite their notable performance in the general domain,
LLMs have remained suboptimal in the field of education, owing to the unique
challenges presented by this domain, such as the need for more specialized
knowledge, the requirement for personalized learning experiences, and the
necessity for concise explanations of complex concepts. To address these
issues, this paper presents a novel LLM for education named WisdomBot, which
combines the power of LLMs with educational theories, enabling their seamless
integration into educational contexts. To be specific, we harness
self-instructed knowledge concepts and instructions under the guidance of
Bloom's Taxonomy as training data. To further enhance the accuracy and
professionalism of model's response on factual questions, we introduce two key
enhancements during inference, i.e., local knowledge base retrieval
augmentation and search engine retrieval augmentation during inference. We
substantiate the effectiveness of our approach by applying it to several
Chinese LLMs, thereby showcasing that the fine-tuned models can generate more
reliable and professional responses.
