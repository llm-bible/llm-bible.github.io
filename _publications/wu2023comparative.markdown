---
layout: publication
title: A Comparative Study Of Open45;source Large Language Models GPT45;4 And Claude 2 Multiple45;choice Test Taking In Nephrology
authors: Wu Sean, Koo Michael, Blum Lesley, Black Andy, Kao Liyo, Scalzo Fabien, Kurtz Ira
conference: "Arxiv"
year: 2023
bibkey: wu2023comparative
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.04709"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
In recent years there have been significant breakthroughs in the field of natural language processing particularly with the development of large language models (LLMs). These LLMs have showcased remarkable capabilities on various benchmarks. In the healthcare field the exact role LLMs and other future AI models will play remains unclear. There is a potential for these models in the future to be used as part of adaptive physician training medical co45;pilot applications and digital patient interaction scenarios. The ability of AI models to participate in medical training and patient care will depend in part on their mastery of the knowledge content of specific medical fields. This study investigated the medical knowledge capability of LLMs specifically in the context of internal medicine subspecialty multiple45;choice test45;taking ability. We compared the performance of several open45;source LLMs (Koala 7B Falcon 7B Stable45;Vicuna 13B and Orca Mini 13B) to GPT45;4 and Claude 2 on multiple45;choice questions in the field of Nephrology. Nephrology was chosen as an example of a particularly conceptually complex subspecialty field within internal medicine. The study was conducted to evaluate the ability of LLM models to provide correct answers to nephSAP (Nephrology Self45;Assessment Program) multiple45;choice questions. The overall success of open45;sourced LLMs in answering the 858 nephSAP multiple45;choice questions correctly was 17.137; 45; 25.537;. In contrast Claude 2 answered 54.437; of the questions correctly whereas GPT45;4 achieved a score of 73.337;. We show that current widely used open45;sourced LLMs do poorly in their ability for zero45;shot reasoning when compared to GPT45;4 and Claude 2. The findings of this study potentially have significant implications for the future of subspecialty medical training and patient care.
