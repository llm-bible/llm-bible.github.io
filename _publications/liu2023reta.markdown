---
layout: publication
title: RETA45;LLM A Retrieval45;augmented Large Language Model Toolkit
authors: Liu Jiongnan, Jin Jiajie, Wang Zihan, Cheng Jiehan, Dou Zhicheng, Wen Ji-rong
conference: "Arxiv"
year: 2023
bibkey: liu2023reta
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.05212"}
  - {name: "Code", url: "https://github.com/RUC&#45;GSAI/YuLan&#45;IR/tree/main/RETA&#45;LLM"}
tags: ['Applications', 'Has Code', 'RAG', 'Reinforcement Learning']
---
Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval45;augmented LLMs). Applying this strategy LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition by incorporating external knowledge retrieval45;augmented LLMs can answer in45;domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval45;augmented LLM systems we develop RETA45;LLM a 123;RET125;reival45;123;A125;ugmented LLM toolkit. In RETA45;LLM we create a complete pipeline to help researchers and users build their customized in45;domain LLM45;based systems. Compared with previous retrieval45;augmented LLM systems RETA45;LLM provides more plug45;and45;play modules to support better interaction between IR systems and LLMs including 123;request rewriting document retrieval passage extraction answer generation and fact checking125; modules. Our toolkit is publicly available at https://github.com/RUC&#45;GSAI/YuLan&#45;IR/tree/main/RETA&#45;LLM.
