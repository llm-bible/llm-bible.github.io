---
layout: publication
title: MMHQA45;ICL Multimodal In45;context Learning For Hybrid Question Answering Over Text Tables And Images
authors: Liu Weihao, Lei Fangyu, Luo Tongxu, Lei Jiahe, He Shizhu, Zhao Jun, Liu Kang
conference: "Arxiv"
year: 2023
bibkey: liu2023mmhqa
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.04790"}
tags: ['Applications', 'Multimodal Models', 'Prompting', 'RAG', 'Reinforcement Learning', 'Tools']
---
In the real world knowledge often exists in a multimodal and heterogeneous form. Addressing the task of question answering with hybrid data types including text tables and images is a challenging task (MMHQA). Recently with the rise of large language models (LLM) in45;context learning (ICL) has become the most popular way to solve QA problems. We propose MMHQA45;ICL framework for addressing this problems which includes stronger heterogeneous data retriever and an image caption module. Most importantly we propose a Type45;specific In45;context Learning Strategy for MMHQA enabling LLMs to leverage their powerful performance in this task. We are the first to use end45;to45;end LLM prompting method for this task. Experimental results demonstrate that our framework outperforms all baselines and methods trained on the full dataset achieving state45;of45;the45;art results under the few45;shot setting on the MultimodalQA dataset.
