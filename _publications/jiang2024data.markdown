---
layout: publication
title: 'Data Augmentation Of Multi-turn Psychological Dialogue Via Knowledge-driven Progressive Thought Prompting'
authors: Jiyue Jiang, Liheng Chen, Sheng Wang, Lingpeng Kong, Yu Li, Chuan Wu
conference: "Arxiv"
year: 2024
bibkey: jiang2024data
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.16567"}
tags: ['Training Techniques', 'Prompting']
---
Existing dialogue data augmentation (DA) techniques predominantly focus on
augmenting utterance-level dialogues, which makes it difficult to take dialogue
contextual information into account. The advent of large language models (LLMs)
has simplified the implementation of multi-turn dialogues. Due to absence of
professional understanding and knowledge, it remains challenging to deliver
satisfactory performance in low-resource domain, like psychological dialogue
dialogue. DA involves creating new training or prompting data based on the
existing data, which help the model better understand and generate
psychology-related responses. In this paper, we aim to address the issue of
multi-turn dialogue data augmentation for boosted performance in the psychology
domain. We propose a knowledge-driven progressive thought prompting method to
guide LLM to generate multi-turn psychology-related dialogue. This method
integrates a progressive thought generator, a psychology knowledge generator,
and a multi-turn dialogue generator. The thought generated by the progressive
thought generator serves as a prompt to prevent the generated dialogue from
having significant semantic deviations, while the psychology knowledge
generator produces psychological knowledge to serve as the dialogue history for
the LLM, guiding the dialogue generator to create multi-turn psychological
dialogue. To ensure the precision of multi-turn psychological dialogue
generation by LLM, a meticulous professional evaluation is required. Extensive
experiments conducted on three datasets related to psychological dialogue
verify the effectiveness of the proposed method.
