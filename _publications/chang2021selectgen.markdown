---
layout: publication
title: 'The Selectgen Challenge: Finding The Best Training Samples For Few-shot Neural Text Generation'
authors: Ernie Chang, Xiaoyu Shen, Alex Marin, Vera Demberg
conference: "Arxiv"
year: 2021
bibkey: chang2021selectgen
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2108.06614'}
tags: ['Attention Mechanism', 'Language Modeling', 'Few-Shot', 'Training Techniques', 'Model Architecture', 'Applications']
---
We propose a shared task on training instance selection for few-shot neural
text generation. Large-scale pretrained language models have led to dramatic
improvements in few-shot text generation. Nonetheless, almost all previous work
simply applies random sampling to select the few-shot training instances.
Little to no attention has been paid to the selection strategies and how they
would affect model performance. The study of the selection strategy can help us
to (1) make the most use of our annotation budget in downstream tasks and (2)
better benchmark few-shot text generative models. We welcome submissions that
present their selection strategies and the effects on the generation quality.
