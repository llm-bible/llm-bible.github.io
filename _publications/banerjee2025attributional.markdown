---
layout: publication
title: 'Attributional Safety Failures In Large Language Models Under Code-mixed Perturbations'
authors: Somnath Banerjee, Pratyush Chatterjee, Shanu Kumar, Sayan Layek, Parag Agrawal, Rima Hazra, Animesh Mukherjee
conference: "Arxiv"
year: 2025
bibkey: banerjee2025attributional
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.14469"}
tags: ['Responsible AI', 'Reinforcement Learning', 'Interpretability', 'Interpretability and Explainability', 'Prompting']
---
Recent advancements in LLMs have raised significant safety concerns, particularly when dealing with code-mixed inputs and outputs. Our study systematically investigates the increased susceptibility of LLMs to produce unsafe outputs from code-mixed prompts compared to monolingual English prompts. Utilizing explainability methods, we dissect the internal attribution shifts causing model's harmful behaviors. In addition, we explore cultural dimensions by distinguishing between universally unsafe and culturally-specific unsafe queries. This paper presents novel experimental insights, clarifying the mechanisms driving this phenomenon.
