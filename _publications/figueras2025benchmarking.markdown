---
layout: publication
title: 'Benchmarking Critical Questions Generation: A Challenging Reasoning Task For Large Language Models'
authors: Banca Calvo Figueras, Rodrigo Agerri
conference: "Arxiv"
year: 2025
bibkey: figueras2025benchmarking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.11341'}
tags: ['Reinforcement Learning', 'RAG']
---
The task of Critical Questions Generation (CQs-Gen) aims to foster critical thinking by enabling systems to generate questions that expose underlying assumptions and challenge the validity of argumentative reasoning structures. Despite growing interest in this area, progress has been hindered by the lack of suitable datasets and automatic evaluation standards. This paper presents a comprehensive approach to support the development and benchmarking of systems for this task. We construct the first large-scale dataset including \\(~\\)5K manually annotated questions. We also investigate automatic evaluation methods and propose a reference-based technique using large language models (LLMs) as the strategy that best correlates with human judgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline while showcasing the difficulty of the task. Data and code plus a public leaderboard are provided to encourage further research not only in terms of model performance, but also to explore the practical benefits of CQs-Gen for both automated reasoning and human critical thinking.
