---
layout: publication
title: Leveraging Biases In Large Language Models bias45;knn For Effective Few45;shot Learning
authors: Zhang Yong, Li Hanzhang, Li Zhitao, Cheng Ning, Li Ming, Xiao Jing, Wang Jianzong
conference: "Arxiv"
year: 2024
bibkey: zhang2024leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.09783"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Model Architecture', 'RAG', 'Security', 'Tools']
---
Large Language Models (LLMs) have shown significant promise in various applications including zero45;shot and few45;shot learning. However their performance can be hampered by inherent biases. Instead of traditionally sought methods that aim to minimize or correct these biases this study introduces a novel methodology named bias45;kNN. This approach capitalizes on the biased outputs harnessing them as primary features for kNN and supplementing with gold labels. Our comprehensive evaluations spanning diverse domain text classification datasets and different GPT45;2 model sizes indicate the adaptability and efficacy of the bias45;kNN method. Remarkably this approach not only outperforms conventional in45;context learning in few45;shot scenarios but also demonstrates robustness across a spectrum of samples templates and verbalizers. This study therefore presents a unique perspective on harnessing biases transforming them into assets for enhanced model performance.
