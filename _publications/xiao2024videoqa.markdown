---
layout: publication
title: Videoqa In The Era Of Llms An Empirical Study
authors: Xiao Junbin, Huang Nanxin, Qin Hangyu, Li Dongyang, Li Yicong, Zhu Fengbin, Tao Zhulin, Yu Jianxing, Lin Liang, Chua Tat-seng, Yao Angela
conference: "Arxiv"
year: 2024
bibkey: xiao2024videoqa
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.04223"}
tags: ['Applications', 'Interpretability And Explainability', 'Security', 'Survey Paper']
---
Video Large Language Models (Video45;LLMs) are flourishing and has advanced many video45;language tasks. As a golden testbed Video Question Answering (VideoQA) plays pivotal role in Video45;LLM developing. This work conducts a timely and comprehensive study of Video45;LLMs behavior in VideoQA aiming to elucidate their success and failure modes and provide insights towards more human45;like video understanding and question answering. Our analyses demonstrate that Video45;LLMs excel in VideoQA; they can correlate contextual cues and generate plausible responses to questions about varied video contents. However models falter in handling video temporality both in reasoning about temporal content ordering and grounding QA45;relevant temporal moments. Moreover the models behave unintuitively 45; they are unresponsive to adversarial video perturbations while being sensitive to simple variations of candidate answers and questions. Also they do not necessarily generalize better. The findings demonstrate Video45;LLMs QA capability in standard condition yet highlight their severe deficiency in robustness and interpretability suggesting the urgent need on rationales in Video45;LLM developing.
