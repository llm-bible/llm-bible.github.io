---
layout: publication
title: 'Robeczech: Czech Roberta, A Monolingual Contextualized Language Representation
  Model'
authors: "Milan Straka, Jakub N\xE1plava, Jana Strakov\xE1, David Samuel"
conference: Arxiv
year: 2021
citations: 19
bibkey: straka2021czech
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2105.11314'}]
tags: [Transformer, BERT]
---
We present RobeCzech, a monolingual RoBERTa language representation model
trained on Czech data. RoBERTa is a robustly optimized Transformer-based
pretraining approach. We show that RobeCzech considerably outperforms
equally-sized multilingual and Czech-trained contextualized language
representation models, surpasses current state of the art in all five evaluated
NLP tasks and reaches state-of-the-art results in four of them. The RobeCzech
model is released publicly at https://hdl.handle.net/11234/1-3691 and
https://huggingface.co/ufal/robeczech-base.