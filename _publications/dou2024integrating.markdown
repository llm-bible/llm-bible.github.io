---
layout: publication
title: Integrating Physician Diagnostic Logic Into Large Language Models Preference Learning From Process Feedback
authors: Dou Chengfeng, Jin Zhi, Jiao Wenpin, Zhao Haiyan, Zhao Yongqiang, Tao Zhenwei
conference: "Arxiv"
year: 2024
bibkey: dou2024integrating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.05695"}
tags: ['Agentic', 'Attention Mechanism', 'Model Architecture', 'Reinforcement Learning']
---
The use of large language models in medical dialogue generation has garnered significant attention with a focus on improving response quality and fluency. While previous studies have made progress in optimizing model performance for single45;round medical Qamp;A tasks there is a need to enhance the models capability for multi45;round conversations to avoid logical inconsistencies. To address this we propose an approach called preference learning from process feedback~(PLPF) which integrates the doctors diagnostic logic into LLMs. PLPF involves rule modeling preference data generation and preference alignment to train the model to adhere to the diagnostic process. Experimental results using Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.637; outperforming traditional reinforcement learning from human feedback. Additionally PLPF demonstrates effectiveness in both multi45;round and single45;round dialogue tasks showcasing its potential for improving medical dialogue generation.
