---
layout: publication
title: 'MUTAN: Multimodal Tucker Fusion For Visual Question Answering'
authors: "Hedi Ben-younes, R\xE9mi Cadene, Matthieu Cord, Nicolas Thome"
conference: Arxiv
year: 2017
citations: 422
bibkey: benyounes2017multimodal
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1705.06676'}]
tags: [Multimodal Models]
---
Bilinear models provide an appealing framework for mixing and merging
information in Visual Question Answering (VQA) tasks. They help to learn high
level associations between question meaning and visual concepts in the image,
but they suffer from huge dimensionality issues. We introduce MUTAN, a
multimodal tensor-based Tucker decomposition to efficiently parametrize
bilinear interactions between visual and textual representations. Additionally
to the Tucker framework, we design a low-rank matrix-based decomposition to
explicitly constrain the interaction rank. With MUTAN, we control the
complexity of the merging scheme while keeping nice interpretable fusion
relations. We show how our MUTAN model generalizes some of the latest VQA
architectures, providing state-of-the-art results.