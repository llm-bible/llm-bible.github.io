---
layout: publication
title: 'Structural Perturbation In Large Language Model Representations Through Recursive Symbolic Regeneration'
authors: Kathlyn Eaglewood, Tobias Featherington, Dorian Mayfair, Sylvester Grimshaw, James Pettigrew
conference: "Arxiv"
year: 2025
bibkey: eaglewood2025structural
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.05794'}
tags: ['Attention Mechanism', 'Language Modeling', 'Training Techniques', 'Applications', 'Model Architecture', 'Fine-Tuning', 'Reinforcement Learning', 'Pretraining Methods']
---
Symbolic perturbations offer a novel approach for influencing neural
representations without requiring direct modification of model parameters. The
recursive regeneration of symbolic structures introduces structured variations
in latent embeddings, leading to controlled shifts in attention dynamics and
lexical diversity across sequential generations. A comparative analysis with
conventional fine-tuning techniques reveals that structural modifications at
the symbolic level induce distinct variations in contextual sensitivity while
maintaining overall model fluency and coherence. Shifts in attention weight
distributions highlight the role of symbolic modifications in adjusting token
dependencies, influencing response variability, and refining long-form text
generation. Experimental findings suggest that symbolic perturbations can
enhance adaptability in domain-specific applications, allowing modifications in
model behavior without retraining. Evaluations of semantic drift indicate that
recursive regeneration alters long-range token dependencies, affecting topic
coherence across extended text sequences. Results from lexical variability
assessments further support the conclusion that symbolic-level modifications
introduce interpretable variations in generated responses, potentially enabling
more controlled stylistic adjustments in automated text generation.
