---
layout: publication
title: 'Explainable And Interpretable Multimodal Large Language Models: A Comprehensive Survey'
authors: Yunkai Dang, Kaichen Huang, Jiahao Huo, Yibo Yan, Sirui Huang, Dongrui Liu, Mengxi Gao, Jie Zhang, Chen Qian, Kun Wang, Yong Liu, Jing Shao, Hui Xiong, Xuming Hu
conference: "Arxiv"
year: 2024
bibkey: dang2024explainable
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.02104"}
tags: ['Multimodal Models', 'Training Techniques', 'Model Architecture', 'Survey Paper', 'Tools', 'Language Modeling', 'Ethics and Bias', 'Interpretability', 'Interpretability and Explainability', 'Applications']
---
The rapid development of Artificial Intelligence (AI) has revolutionized
numerous fields, with large language models (LLMs) and computer vision (CV)
systems driving advancements in natural language understanding and visual
processing, respectively. The convergence of these technologies has catalyzed
the rise of multimodal AI, enabling richer, cross-modal understanding that
spans text, vision, audio, and video modalities. Multimodal large language
models (MLLMs), in particular, have emerged as a powerful framework,
demonstrating impressive capabilities in tasks like image-text generation,
visual question answering, and cross-modal retrieval. Despite these
advancements, the complexity and scale of MLLMs introduce significant
challenges in interpretability and explainability, essential for establishing
transparency, trustworthiness, and reliability in high-stakes applications.
This paper provides a comprehensive survey on the interpretability and
explainability of MLLMs, proposing a novel framework that categorizes existing
research across three perspectives: (I) Data, (II) Model, (III) Training \&
Inference. We systematically analyze interpretability from token-level to
embedding-level representations, assess approaches related to both architecture
analysis and design, and explore training and inference strategies that enhance
transparency. By comparing various methodologies, we identify their strengths
and limitations and propose future research directions to address unresolved
challenges in multimodal explainability. This survey offers a foundational
resource for advancing interpretability and transparency in MLLMs, guiding
researchers and practitioners toward developing more accountable and robust
multimodal AI systems.
