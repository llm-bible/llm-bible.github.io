---
layout: publication
title: Unique Security And Privacy Threats Of Large Language Model\: A Comprehensive Survey
authors: Wang Shang, Zhu Tianqing, Liu Bo, Ding Ming, Guo Xu, Ye Dayong, Zhou Wanlei, Yu Philip S.
conference: "Arxiv"
year: 2024
bibkey: wang2024unique
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.07973"}
tags: ['Agentic', 'Applications', 'Attention Mechanism', 'Fine Tuning', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Security', 'Survey Paper', 'Tools', 'Training Techniques']
---
With the rapid development of artificial intelligence large language models (LLMs) have made remarkable advancements in natural language processing. These models are trained on vast datasets to exhibit powerful language understanding and generation capabilities across various applications including machine translation chatbots and agents. However LLMs have revealed a variety of privacy and security issues throughout their life cycle drawing significant academic and industrial attention. Moreover the risks faced by LLMs differ significantly from those encountered by traditional language models. Given that current surveys lack a clear taxonomy of unique threat models across diverse scenarios we emphasize the unique privacy and security threats associated with five specific scenarios pre-training fine-tuning retrieval-augmented generation systems deployment and LLM-based agents. Addressing the characteristics of each risk this survey outlines potential threats and countermeasures. Research on attack and defense situations can offer feasible research directions enabling more areas to benefit from LLMs.
