---
layout: publication
title: Large Language Models Meet Open-world Intent Discovery And Recognition An Evaluation Of Chatgpt
authors: Song Xiaoshuai, He Keqing, Wang Pei, Dong Guanting, Mou Yutao, Wang Jingang, Xian Yunsen, Cai Xunliang, Xu Weiran
conference: "Arxiv"
year: 2023
bibkey: song2023large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.10176"}
tags: ['Fine Tuning', 'GPT', 'In Context Learning', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Training Techniques']
---
The tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently although some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks it is still unclear for the ability of ChatGPT to discover and incrementally extent OOD intents. In this paper we comprehensively evaluate ChatGPT on OOD intent discovery and GID and then outline the strengths and weaknesses of ChatGPT. Overall ChatGPT exhibits consistent advantages under zero-shot settings but is still at a disadvantage compared to fine-tuned models. More deeply through a series of analytical experiments we summarize and discuss the challenges faced by LLMs including clustering domain-specific understanding and cross-domain in-context learning scenarios. Finally we provide empirical guidance for future directions to address these challenges.
