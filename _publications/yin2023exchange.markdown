---
layout: publication
title: Exchange45;of45;thought Enhancing Large Language Model Capabilities Through Cross45;model Communication
authors: Yin Zhangyue, Sun Qiushi, Chang Cheng, Guo Qipeng, Dai Junqi, Huang Xuanjing, Qiu Xipeng
conference: "Arxiv"
year: 2023
bibkey: yin2023exchange
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.01823"}
tags: ['Ethics And Bias', 'Tools']
---
Large Language Models (LLMs) have recently made significant strides in complex reasoning tasks through the Chain45;of45;Thought technique. Despite this progress their reasoning is often constrained by their intrinsic understanding lacking external insights. To address this we propose Exchange45;of45;Thought (EoT) a novel framework that enables cross45;model communication during problem45;solving. Drawing inspiration from network topology EoT integrates four unique communication paradigms Memory Report Relay and Debate. This paper delves into the communication dynamics and volume associated with each paradigm. To counterbalance the risks of incorrect reasoning chains we implement a robust confidence evaluation mechanism within these communications. Our experiments across diverse complex reasoning tasks demonstrate that EoT significantly surpasses established baselines underscoring the value of external insights in enhancing LLM performance. Furthermore we show that EoT achieves these superior results in a cost45;effective manner marking a promising advancement for efficient and collaborative AI problem45;solving.
