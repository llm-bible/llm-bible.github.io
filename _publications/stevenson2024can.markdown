---
layout: publication
title: 'Can Large Language Models Generalize Analogy Solving Like People Can?'
authors: Claire E. Stevenson, Alexandra Pafford, Han L. J. Van Der Maas, Melanie Mitchell
conference: "Arxiv"
year: 2024
bibkey: stevenson2024can
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.02348'}
tags: ['Uncategorized']
---
When we solve an analogy we transfer information from a known context to a
new one through abstract rules and relational similarity. In people, the
ability to solve analogies such as "body : feet :: table : ?" emerges in
childhood, and appears to transfer easily to other domains, such as the visual
domain "( : ) :: < : ?". Recent research shows that large language models
(LLMs) can solve various forms of analogies. However, can LLMs generalize
analogy solving to new domains like people can? To investigate this, we had
children, adults, and LLMs solve a series of letter-string analogies (e.g., a b
: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek
alphabet), and a far transfer domain (list of symbols). As expected, children
and adults easily generalized their knowledge to unfamiliar domains, whereas
LLMs did not. This key difference between human and AI performance is evidence
that these LLMs still struggle with robust human-like analogical transfer.
