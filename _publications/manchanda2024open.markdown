---
layout: publication
title: 'The Open Source Advantage In Large Language Models (llms)'
authors: Jiya Manchanda, Laura Boettcher, Matheus Westphalen, Jasser Jasser
conference: "Arxiv"
year: 2024
bibkey: manchanda2024open
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.12004'}
tags: ['Language Modeling', 'Model Architecture', 'Applications', 'Tools', 'Fine-Tuning', 'GPT', 'Bias Mitigation', 'Reinforcement Learning', 'Ethics and Bias', 'Interpretability']
---
Large language models (LLMs) have rapidly advanced natural language
processing, driving significant breakthroughs in tasks such as text generation,
machine translation, and domain-specific reasoning. The field now faces a
critical dilemma in its approach: closed-source models like GPT-4 deliver
state-of-the-art performance but restrict reproducibility, accessibility, and
external oversight, while open-source frameworks like LLaMA and Mixtral
democratize access, foster collaboration, and support diverse applications,
achieving competitive results through techniques like instruction tuning and
LoRA. Hybrid approaches address challenges like bias mitigation and resource
accessibility by combining the scalability of closed-source systems with the
transparency and inclusivity of open-source framework. However, in this
position paper, we argue that open-source remains the most robust path for
advancing LLM research and ethical deployment.
