---
layout: publication
title: 'Stackeval: Benchmarking Llms In Coding Assistance'
authors: Nidhish Shah, Zulkuf Genc, Dogu Araci
conference: "Arxiv"
year: 2024
bibkey: shah2024benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.05288"}
  - {name: "Code", url: "https://github.com/ProsusAI/stack-eval"}
tags: ['Survey Paper', 'Reinforcement Learning', 'Merging', 'Ethics and Bias', 'Has Code']
---
We present two comprehensive benchmarks to evaluate the performance of
language models in coding assistance tasks, covering code writing, debugging,
code review, and conceptual understanding. Our main contribution includes two
curated datasets: StackEval, a large-scale benchmark derived from Stack
Overflow questions, and StackUnseen, a dynamic benchmark featuring the most
recent Stack Overflow content. These benchmarks offer novel insights into the
capabilities and limitations of LLMs, particularly in handling new and emerging
content. Additionally, we assess LLMs' proficiency as judges for coding tasks
using a curated, human-annotated dataset, exploring their evaluation
capabilities and potential biases, including whether they favor their own
generated solutions. Our findings underscore the potential of these benchmarks
to advance LLM development and application in coding assistance. To ensure
reproducibility, we publicly share our datasets and evaluation code at
https://github.com/ProsusAI/stack-eval .
