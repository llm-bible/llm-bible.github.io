---
layout: publication
title: Shepherd A Critic For Language Model Generation
authors: Wang Tianlu, Yu Ping, Tan Xiaoqing Ellen, O'brien Sean, Pasunuru Ramakanth, Dwivedi-yu Jane, Golovneva Olga, Zettlemoyer Luke, Fazel-zarandi Maryam, Celikyilmaz Asli
conference: "Arxiv"
year: 2023
bibkey: wang2023critic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.04592"}
tags: ['Applications', 'GPT', 'Model Architecture', 'RAG']
---
As large language models improve there is increasing interest in techniques that leverage these models capabilities to refine their own outputs. In this work we introduce Shepherd a language model specifically tuned to critique responses and suggest refinements extending beyond the capabilities of an untuned model to identify diverse errors and provide suggestions to remedy them. At the core of our approach is a high quality feedback dataset which we curate from community feedback and human annotations. Even though Shepherd is small (7B parameters) its critiques are either equivalent or preferred to those from established models including ChatGPT. Using GPT45;4 for evaluation Shepherd reaches an average win45;rate of 5345;8737; compared to competitive alternatives. In human evaluation Shepherd strictly outperforms other models and on average closely ties with ChatGPT.
