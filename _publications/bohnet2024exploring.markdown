---
layout: publication
title: 'Exploring And Benchmarking The Planning Capabilities Of Large Language Models'
authors: Bernd Bohnet, Azade Nova, Aaron T Parisi, Kevin Swersky, Katayoon Goshvadi, Hanjun Dai, Dale Schuurmans, Noah Fiedel, Hanie Sedghi
conference: "Arxiv"
year: 2024
bibkey: bohnet2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.13094"}
tags: ['Training Techniques', 'Pretraining Methods', 'Fine-Tuning', 'Prompting', 'In-Context Learning']
---
Classical and natural language planning tasks remain a difficult domain for
modern large language models (LLMs). In this work, we lay the foundations for
improving planning capabilities of LLMs. First, we construct a comprehensive
benchmark suite encompassing both classical planning benchmarks and natural
language scenarios. This suite includes algorithms to methodically generate
instances of tasks with varying levels of difficulty, allowing for rigorous and
systematic evaluation of LLM performance. Next, we investigate the use of
many-shot in-context learning to enhance LLM planning, exploring the
relationship between increased context length and improved planning
performance. In addition, we demonstrate the positive impact of fine-tuning
LLMs on optimal planning paths. We also probe the efficacy of chain-of-thought
reasoning methods to improve LLM planning performance. Moreover, we probe the
performance of the proposed methods in out-of-distribution scenarios, assessing
the ability to generalize to novel and unseen planning challenges. Finally, we
investigate model's failure modes and reveal insights that hold true across
different benchmarks.
