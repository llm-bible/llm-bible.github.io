---
layout: publication
title: Bridging the Gap Dynamic Learning Strategies for Improving Multilingual Performance in LLMs
authors: Kumar Somnath, Balloli Vaibhav, Ranjit Mercy, Ahuja Kabir, Ganu Tanuja, Sitaram Sunayana, Bali Kalika, Nambi Akshay
conference: "Arxiv"
year: 2024
bibkey: kumar2024bridging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.18359"}
tags: ['ARXIV', 'Fine Tuning', 'LLM', 'Pretraining Methods', 'Prompting', 'RAG', 'Tools', 'Training Techniques']
---
Large language models (LLMs) are at the forefront of transforming numerous domains globally. However their inclusivity and effectiveness remain limited for non-Latin scripts and low-resource languages. This paper tackles the imperative challenge of enhancing the multilingual performance of LLMs without extensive training or fine-tuning. Through systematic investigation and evaluation of diverse languages using popular question-answering (QA) datasets we present novel techniques that unlock the true potential of LLMs in a polyglot landscape. Our approach encompasses three key strategies that yield significant improvements in multilingual proficiency. First by meticulously optimizing prompts tailored for polyglot LLMs we unlock their latent capabilities resulting in substantial performance boosts across languages. Second we introduce a new hybrid approach that synergizes LLM Retrieval Augmented Generation (RAG) with multilingual embeddings and achieves improved multilingual task performance. Finally we introduce a novel learning approach that dynamically selects the optimal prompt strategy LLM model and embedding model per query at run-time. This dynamic adaptation maximizes the efficacy of LLMs across languages outperforming best static and random strategies. Additionally our approach adapts configurations in both offline and online settings and can seamlessly adapt to new languages and datasets leading to substantial advancements in multilingual understanding and generation across diverse languages.
