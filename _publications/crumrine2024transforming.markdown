---
layout: publication
title: 'Transforming Computer Security And Public Trust Through The Exploration Of Fine-tuning Large Language Models'
authors: Garrett Crumrine, Izzat Alsmadi, Jesus Guerrero, Yuvaraj Munian
conference: "Arxiv"
year: 2024
bibkey: crumrine2024transforming
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.00628"}
tags: ['Fine-Tuning', 'Efficiency and Optimization', 'Applications', 'Security', 'Training Techniques', 'Pretraining Methods']
---
Large language models (LLMs) have revolutionized how we interact with
machines. However, this technological advancement has been paralleled by the
emergence of "Mallas," malicious services operating underground that exploit
LLMs for nefarious purposes. Such services create malware, phishing attacks,
and deceptive websites, escalating the cyber security threats landscape. This
paper delves into the proliferation of Mallas by examining the use of various
pre-trained language models and their efficiency and vulnerabilities when
misused. Building on a dataset from the Common Vulnerabilities and Exposures
(CVE) program, it explores fine-tuning methodologies to generate code and
explanatory text related to identified vulnerabilities. This research aims to
shed light on the operational strategies and exploitation techniques of Mallas,
leading to the development of more secure and trustworthy AI applications. The
paper concludes by emphasizing the need for further research, enhanced
safeguards, and ethical guidelines to mitigate the risks associated with the
malicious application of LLMs.
