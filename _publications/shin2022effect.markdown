---
layout: publication
title: On The Effect Of Pretraining Corpora On In45;context Learning By A Large45;scale Language Model
authors: Seongjin Shin, Sang-woo Lee, Hwijeen Ahn, Sungdong Kim, Hyoungseok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, Woomyoung Park, Jung-woo Ha, Nako Sung
conference: "Arxiv"
year: 2022
bibkey: shin2022effect
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2204.13509v2"}
tags: ['GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
Many recent studies on large45;scale language models have reported successful in45;context zero45; and few45;shot learning ability. However the in45;depth analysis of when in45;context learning occurs is still lacking. For example it is unknown how in45;context learning performance changes as the training corpus varies. Here we investigate the effects of the source and size of the pretraining corpus on in45;context learning in HyperCLOVA a Korean45;centric GPT45;3 model. From our in45;depth investigation we introduce the following observations (1) in45;context learning performance heavily depends on the corpus domain source and the size of the pretraining corpus does not necessarily determine the emergence of in45;context learning (2) in45;context learning ability can emerge when a language model is trained on a combination of multiple corpora even when each corpus does not result in in45;context learning on its own (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in45;context learning performance of the downstream task especially in the few45;shot setting and (4) the relationship between language modeling (measured in perplexity) and in45;context learning does not always correlate e.g. low perplexity does not always imply high in45;context few45;shot learning performance.
