---
layout: publication
title: 'Evaluating Large Language Models: A Comprehensive Survey'
authors: Guo Zishan, Jin Renren, Liu Chuang, Huang Yufei, Shi Dan, Supryadi, Yu Linhao, Liu Yan, Li Jiaxuan, Xiong Bojian, Xiong Deyi
conference: "Arxiv"
year: 2023
bibkey: guo2023evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.19736"}
  - {name: "Code", url: "https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers"}
tags: ['Applications', 'Attention Mechanism', 'Ethics And Bias', 'Has Code', 'Model Architecture', 'Responsible AI', 'Survey Paper', 'Tools']
---
'Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs. This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs'' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability. We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.'
