---
layout: publication
title: 'RLDBF: Enhancing Llms Via Reinforcement Learning With Database Feedback'
authors: Weichen Dai, Zijie Dai, Zhijie Huang, Yixuan Pan, Xinhe Li, Xi Li, Yi Zhou, Ji Qi, Wu Jiang
conference: "Arxiv"
year: 2025
bibkey: dai2025enhancing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.03713'}
tags: ['Agentic', 'RAG', 'Training Techniques', 'Fine-Tuning', 'Reinforcement Learning', 'Pre-Training', 'Pretraining Methods']
---
While current large language models (LLMs) demonstrate remarkable linguistic
capabilities through training on massive unstructured text corpora, they remain
inadequate in leveraging structured scientific data (e.g., chemical molecular
properties in databases) that encapsulate centuries of accumulated scientific
expertise. These structured datasets hold strategic significance for advancing
AI for Science yet current approaches merely treat them as auxiliary
supplements to unstructured text. This study pioneers a systematic
investigation into enhancing LLMs with structured scientific data, using
chemical molecular science as a testbed. We investigate the impact of
incorporating molecular property data on LLM across distinct training phases,
including continual pre-training, supervised fine-tuning, and reinforcement
learning. Notably, to address the inherent limitation of numerical
insensitivity in large models, we propose an innovative methodology termed
"Reinforcement Learning with Database Feedback" (RLDBF). Experimental
evaluations demonstrate the efficacy of the proposed approach, with the model
exhibiting remarkable generalization capabilities on previously unseen data and
other chemical tasks. The results substantiate the potential of our method in
advancing the field of structured scientific data processing within LLMs.
