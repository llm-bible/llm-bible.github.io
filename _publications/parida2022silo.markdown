---
layout: publication
title: Silo Nlps Participation At WAT2022
authors: Parida Shantipriya, Panda Subhadarshi, Gr√∂nroos Stig-arne, Granroth-wilding Mark, Koistinen Mika
conference: "Arxiv"
year: 2022
bibkey: parida2022silo
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2208.01296"}
tags: ['Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Transformer']
---
This paper provides the system description of Silo NLPs submission to the Workshop on Asian Translation (WAT2022). We have participated in the Indic Multimodal tasks (English45;Hindi English45;Malayalam and English45;Bengali Multimodal Translation). For text45;only translation we trained Transformers from scratch and fine45;tuned mBART45;50 models. For multimodal translation we used the same mBART architecture and extracted object tags from the images to use as visual features concatenated with the text sequence. Our submission tops many tasks including English45;Hindi multimodal translation (evaluation test) English45;Malayalam text45;only and multimodal translation (evaluation test) English45;Bengali multimodal translation (challenge test) and English45;Bengali text45;only translation (evaluation test).
