---
layout: publication
title: Silo Nlp's Participation At WAT2022
authors: Parida Shantipriya, Panda Subhadarshi, Gr√∂nroos Stig-arne, Granroth-wilding Mark, Koistinen Mika
conference: "Arxiv"
year: 2022
bibkey: parida2022silo
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2208.01296"}
tags: ['Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Transformer']
---
This paper provides the system description of Silo NLPs submission to the Workshop on Asian Translation (WAT2022). We have participated in the Indic Multimodal tasks (English-Hindi English-Malayalam and English-Bengali Multimodal Translation). For text-only translation we trained Transformers from scratch and fine-tuned mBART-50 models. For multimodal translation we used the same mBART architecture and extracted object tags from the images to use as visual features concatenated with the text sequence. Our submission tops many tasks including English-Hindi multimodal translation (evaluation test) English-Malayalam text-only and multimodal translation (evaluation test) English-Bengali multimodal translation (challenge test) and English-Bengali text-only translation (evaluation test).
