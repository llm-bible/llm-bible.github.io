---
layout: publication
title: 'What Prompts Don''t Say: Understanding And Managing Underspecification In LLM Prompts'
authors: Chenyang Yang, Yike Shi, Qianou Ma, Michael Xieyang Liu, Christian KÃ¤stner, Tongshuang Wu
conference: "Arxiv"
year: 2025
bibkey: yang2025what
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.13360"}
tags: ['RAG', 'Efficiency and Optimization', 'Prompting']
---
Building LLM-powered software requires developers to communicate their requirements through natural language, but developer prompts are frequently underspecified, failing to fully capture many user-important requirements. In this paper, we present an in-depth analysis of prompt underspecification, showing that while LLMs can often (41.1%) guess unspecified requirements by default, such behavior is less robust: Underspecified prompts are 2x more likely to regress over model or prompt changes, sometimes with accuracy drops by more than 20%. We then demonstrate that simply adding more requirements to a prompt does not reliably improve performance, due to LLMs' limited instruction-following capabilities and competing constraints, and standard prompt optimizers do not offer much help. To address this, we introduce novel requirements-aware prompt optimization mechanisms that can improve performance by 4.8% on average over baselines that naively specify everything in the prompt. Beyond prompt optimization, we envision that effectively managing prompt underspecification requires a broader process, including proactive requirements discovery, evaluation, and monitoring.
