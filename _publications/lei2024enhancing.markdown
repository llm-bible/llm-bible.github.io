---
layout: publication
title: Autocoder Enhancing Code Large Language Model With textsc123;aiev45;instruct125;
authors: Lei Bin, Li Yuchen, Chen Qiuwu
conference: "Arxiv"
year: 2024
bibkey: lei2024enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.14906"}
  - {name: "Code", url: "https://github.com/bin123apple/AutoCoder&#125;"}
tags: ['Agentic', 'Applications', 'GPT', 'Has Code', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
We introduce AutoCoder the first Large Language Model to surpass GPT45;4 Turbo (April 2024) and GPT45;4o in pass35;64;1 on the Human Eval benchmark test (mathbf123;90.937;125; vs. mathbf123;90.237;125;). In addition AutoCoder offers a more versatile code interpreter compared to GPT45;4 Turbo and GPT45;4o. Its code interpreter can install external packages instead of limiting to built45;in packages. AutoCoders training data is a multi45;turn dialogue dataset created by a system combining agent interaction and external code execution verification a method we term textbf123;textsc123;AIEV45;Instruct125;125; (Instruction Tuning with Agent45;Interaction and Execution45;Verified). Compared to previous large45;scale code dataset generation methods textsc123;AIEV45;Instruct125; reduces dependence on proprietary large models and provides execution45;validated code dataset. The code and the demo video is available in url123;https://github.com/bin123apple/AutoCoder&#125;.
