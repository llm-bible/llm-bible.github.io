---
layout: publication
title: 'Poison As Cure: Visual Noise For Mitigating Object Hallucinations In Lvms'
authors: Kejia Zhang, Keda Tao, Jiasheng Tang, Huan Wang
conference: "Arxiv"
year: 2025
bibkey: zhang2025poison
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.19164"}
tags: ['Security', 'Efficiency and Optimization', 'Multimodal Models', 'RAG', 'Ethics and Bias']
---
Large vision-language models (LVMs) extend large language models (LLMs) with
visual perception capabilities, enabling them to process and interpret visual
information. A major challenge compromising their reliability is object
hallucination that LVMs may generate plausible but factually inaccurate
information. We propose a novel visual adversarial perturbation (VAP) method to
mitigate this hallucination issue. VAP alleviates LVM hallucination by applying
strategically optimized visual noise without altering the base model. Our
approach formulates hallucination suppression as an optimization problem,
leveraging adversarial strategies to generate beneficial visual perturbations
that enhance the model's factual grounding and reduce parametric knowledge
bias. Extensive experimental results demonstrate that our method consistently
reduces object hallucinations across 8 state-of-the-art LVMs, validating its
efficacy across diverse evaluations.
