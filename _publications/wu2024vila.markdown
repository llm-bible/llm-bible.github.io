---
layout: publication
title: VILA45;U A Unified Foundation Model Integrating Visual Understanding And Generation
authors: Wu Yecheng, Zhang Zhuoyang, Chen Junyu, Tang Haotian, Li Dacheng, Fang Yunhao, Zhu Ligeng, Xie Enze, Yin Hongxu, Yi Li, Han Song, Lu Yao
conference: "Arxiv"
year: 2024
bibkey: wu2024vila
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.04429"}
tags: ['GPT', 'Merging', 'Pretraining Methods', 'Tools', 'Training Techniques']
---
VILA45;U is a Unified foundation model that integrates Video Image Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content which can lead to misalignment and increased complexity. In contrast VILA45;U employs a single autoregressive next45;token prediction framework for both tasks eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state45;of45;the45;art performance in visual language understanding and generation. The success of VILA45;U is attributed to two main factors the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining which enhances visual perception and autoregressive image generation can achieve similar quality as diffusion models with high45;quality dataset. This allows VILA45;U to perform comparably to more complex models using a fully token45;based autoregressive framework.
