---
layout: publication
title: Measuring And Improving Chain45;of45;thought Reasoning In Vision45;language Models
authors: Chen Yangyi, Sikka Karan, Cogswell Michael, Ji Heng, Divakaran Ajay
conference: "Arxiv"
year: 2023
bibkey: chen2023measuring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.04461"}
tags: ['Applications', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Vision45;language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human45;like outputs. In this work we explore the ability of these models to demonstrate human45;like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded we also measure the reasoning consistency of these models. We achieve this by proposing a chain45;of45;thought (CoT) based consistency measure. However such an evaluation requires a benchmark that encompasses both high45;level inference and detailed reasoning chains which is costly. We tackle this challenge by proposing a LLM45;Human45;in45;the45;Loop pipeline which notably reduces cost while simultaneously ensuring the generation of a high45;quality dataset. Based on this pipeline and the existing coarse45;grained annotated dataset we build the CURE benchmark to measure both the zero45;shot reasoning performance and consistency of VLMs. We evaluate existing state45;of45;the45;art VLMs and find that even the best45;performing model is unable to demonstrate strong visual reasoning capabilities and consistency indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step we propose a two45;stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine45;tuning of VLMs using step45;by45;step reasoning samples automatically generated by LLMs. In the second stage we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.
