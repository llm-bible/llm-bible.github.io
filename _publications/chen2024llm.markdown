---
layout: publication
title: Llm45;based Multi45;hop Question Answering With Knowledge Graph Integration In Evolving Environments
authors: Chen Ruirui, Jiang Weifeng, Qin Chengwei, Rawal Ishaan Singh, Tan Cheston, Choi Dongkyu, Xiong Bo, Ai Bo
conference: "Arxiv"
year: 2024
bibkey: chen2024llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.15903"}
tags: ['Applications', 'RAG', 'Reinforcement Learning', 'Tools']
---
The rapid obsolescence of information in Large Language Models (LLMs) has driven the development of various techniques to incorporate new facts. However existing methods for knowledge editing still face difficulties with multi45;hop questions that require accurate fact identification and sequential logical reasoning particularly among numerous fact updates. To tackle these challenges this paper introduces Graph Memory45;based Editing for Large Language Models (GMeLLo) a straitforward and effective method that merges the explicit knowledge representation of Knowledge Graphs (KGs) with the linguistic flexibility of LLMs. Beyond merely leveraging LLMs for question answering GMeLLo employs these models to convert free45;form language into structured queries and fact triples facilitating seamless interaction with KGs for rapid updates and precise multi45;hop reasoning. Our results show that GMeLLo significantly surpasses current state45;of45;the45;art knowledge editing methods in the multi45;hop question answering benchmark MQuAKE especially in scenarios with extensive knowledge edits.
