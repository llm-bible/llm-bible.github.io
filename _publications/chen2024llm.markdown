---
layout: publication
title: 'Llm-based Translation Inference With Iterative Bilingual Understanding'
authors: Andong Chen, Kehai Chen, Yang Xiang, Xuefeng Bai, Muyun Yang, Yang Feng, Tiejun Zhao, Min Zhang
conference: "Arxiv"
year: 2024
bibkey: chen2024llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.12543"}
tags: ['Uncategorized']
---
The remarkable understanding and generation capabilities of large language
models (LLMs) have greatly improved translation performance. However, incorrect
understanding of the sentence to be translated can degrade translation quality.
To address this issue, we proposed a novel Iterative Bilingual Understanding
Translation (IBUT) method based on the cross-lingual capabilities of LLMs and
the dual characteristics of translation tasks. The cross-lingual capability of
LLMs enables the generation of contextual understanding for both the source and
target languages separately. Furthermore, the dual characteristics allow IBUT
to generate effective cross-lingual feedback, iteratively refining contextual
understanding, thereby reducing errors and improving translation performance.
Experimental results showed that the proposed IBUT outperforms several strong
comparison methods, especially being generalized to multiple domains (e.g.,
news, commonsense, and cultural translation benchmarks).
