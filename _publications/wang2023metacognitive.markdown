---
layout: publication
title: Metacognitive Prompting Improves Understanding In Large Language Models
authors: Wang Yuqing, Zhao Yun
conference: "Arxiv"
year: 2023
bibkey: wang2023metacognitive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.05342"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Prompting']
---
In Large Language Models (LLMs) there have been consistent advancements in task45;specific performance largely influenced by effective prompt design. Recent advancements in prompting have enhanced reasoning in logic45;intensive tasks for LLMs yet the nuanced understanding abilities of these models crucial for processing and interpreting complex information remain underexplored. In this study we introduce Metacognitive Prompting (MP) a strategy inspired by human introspective reasoning processes. Using MP LLMs undergo a systematic series of structured self45;aware evaluations drawing on both their vast inherent knowledge and new insights. We conduct extensive experiments on four prevalent LLMs Llama2 PaLM2 GPT45;3.5 and GPT45;4 across ten natural language understanding (NLU) datasets from GLUE SuperGLUE BLUE and LexGLUE benchmarks. Additionally we compare our method with chain45;of45;thought prompting and its advanced versions. The results show that GPT45;4 consistently excels across all tasks while other models have shown significant progress in some tasks when used in conjunction with MP. Furthermore MP consistently outperforms existing prompting methods in both general and domain45;specific NLU tasks. This study underscores the potential to amplify the understanding abilities of LLMs and highlights the benefits of mirroring human introspective reasoning in NLU tasks.
