---
layout: publication
title: Learning45;from45;mistakes Prompting For Indigenous Language Translation
authors: Liao You-cheng, Yu Chen-jui, Lin Chi-yi, Yun He-feng, Wang Yen-hsiang, Li Hsiao-min, Fan Yao-chung
conference: "Arxiv"
year: 2024
bibkey: liao2024learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.13343"}
tags: ['GPT', 'Model Architecture', 'Prompting']
---
Using large language models this paper presents techniques to improve extremely low45;resourced indigenous language translations. Our approaches are grounded in the use of (1) the presence of a datastore consisting of a limited number of parallel translation examples (2) the inherent capabilities of LLMs like GPT45;3.5 and (3) a word45;level translation dictionary. We harness the potential of LLMs and in45;context learning techniques in such a setting for using LLMs as universal translators for extremely low45;resourced languages. Our methodology hinges on utilizing LLMs as language compilers for selected language pairs hypothesizing that they could internalize syntactic structures to facilitate accurate translation. We introduce three techniques KNNPrompting with Retrieved Prompting Context Chain45;of45;Thought Prompting and Learningfrom45;Mistakes Prompting with the last method addressing past errors. The evaluation results suggest that even with limited corpora LLMs can effectively translate extremely low45;resource languages when paired with proper prompting.
