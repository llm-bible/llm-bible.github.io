---
layout: publication
title: 'Anchored Alignment For Self-explanations Enhancement'
authors: Luis Felipe Villa-arenas, Ata Nizamoglu, Qianli Wang, Sebastian MÃ¶ller, Vera Schmitt
conference: "Arxiv"
year: 2024
bibkey: villaarenas2024anchored
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.13216"}
tags: ['Fine-Tuning', 'Efficiency and Optimization', 'Interpretability and Explainability', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods']
---
In this work, we introduce a methodology for alignment designed to enhance
the ability of large language models (LLMs) to articulate their reasoning
(self-explanation) even in the absence of annotated rationale explanations. Our
alignment methodology comprises three key components: explanation quality
assessment, self-instruction dataset generation, and model alignment.
Additionally, we present a novel technique called Alignment with Anchor
Preference Pairs, which improves the selection of preference pairs by
categorizing model outputs into three groups: consistently correct,
consistently incorrect, and variable. By applying tailored strategies to each
category, we enhance the effectiveness of Direct Preference Optimization (DPO).
Our experimental results demonstrate that this approach significantly improves
explanation quality while maintaining accuracy compared to other fine-tuning
strategies.
