---
layout: publication
title: 'IDEAL: Leveraging Infinite And Dynamic Characterizations Of Large Language Models For Query-focused Summarization'
authors: Jie Cao, Dian Jiao, Qiang Yan, Wenqiao Zhang, Siliang Tang, Yueting Zhuang
conference: "Arxiv"
year: 2024
bibkey: cao2024leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.10486"}
  - {name: "Code", url: "https://github.com/DCDmllm/IDEAL_Summary"}
tags: ['Applications', 'RAG', 'Model Architecture', 'Training Techniques', 'Attention Mechanism', 'Has Code', 'Pretraining Methods']
---
Query-focused summarization (QFS) aims to produce summaries that answer
particular questions of interest, enabling greater user control and
personalization. With the advent of large language models (LLMs), shows their
impressive capability of textual understanding through large-scale pretraining,
which implies the great potential of extractive snippet generation. In this
paper, we systematically investigated two indispensable characteristics that
the LLMs-based QFS models should be harnessed, Lengthy Document Summarization
and Efficiently Fine-grained Query-LLM Alignment, respectively.
Correspondingly, we propose two modules called Query-aware HyperExpert and
Query-focused Infini-attention to access the aforementioned characteristics.
These innovations pave the way for broader application and accessibility in the
field of QFS technology. Extensive experiments conducted on existing QFS
benchmarks indicate the effectiveness and generalizability of the proposed
approach. Our code is publicly available at
https://github.com/DCDmllm/IDEAL_Summary.
