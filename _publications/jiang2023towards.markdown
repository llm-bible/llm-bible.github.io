---
layout: publication
title: Tigerscore Towards Building Explainable Metric For All Text Generation Tasks
authors: Jiang Dongfu, Li Yishan, Zhang Ge, Huang Wenhao, Lin Bill Yuchen, Chen Wenhu
conference: "Arxiv"
year: 2023
bibkey: jiang2023towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.00752"}
tags: ['Applications', 'GPT', 'Interpretability And Explainability', 'Language Modeling', 'Model Architecture', 'Reinforcement Learning']
---
We present TIGERScore a textbf123;T125;rained metric that follows textbf123;I125;nstruction textbf123;G125;uidance to perform textbf123;E125;xplainable and textbf123;R125;eference45;free evaluation over a wide spectrum of text generation tasks. Different from other automatic evaluation methods that only provide arcane scores TIGERScore is guided by natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA45;2 trained on our meticulously curated instruction45;tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 42K quadruple in the form of (instruction input system output â†’ error analysis). We collected the system outputs through from a large variety of models to cover different types of errors. To quantitatively assess our metric we evaluate its correlation with human ratings on 5 held45;in datasets 2 held45;out datasets and show that TIGERScore can achieve the open45;source SoTA correlation with human ratings across these datasets and almost approaches GPT45;4 evaluator. As a reference45;free metric its correlation can even surpass the best existing reference45;based metrics. To further qualitatively assess the rationale generated by our metric we conduct human evaluation on the generated explanations and found that the explanations are 70.837; accurate. Through these experimental results we believe TIGERScore demonstrates the possibility of building universal explainable metrics to evaluate any text generation task. All the resourced are released in our project website url123;https://tiger&#45;ai&#45;lab.github.io/TIGERScore/&#125;.
