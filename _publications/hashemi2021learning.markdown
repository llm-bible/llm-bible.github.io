---
layout: publication
title: 'Laviter: Learning Aligned Visual And Textual Representations Assisted By Image And Caption Generation'
authors: Mohammad Abuzar Hashemi, Zhanghexuan Li, Mihir Chauhan, Yan Shen, Abhishek Satbhai, Mir Basheer Ali, Mingchen Gao, Sargur Srihari
conference: "Arxiv"
year: 2021
bibkey: hashemi2021learning
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2109.04993'}
tags: ['Attention Mechanism', 'Transformer', 'Training Techniques', 'Model Architecture', 'Multimodal Models', 'Pre-Training', 'Pretraining Methods']
---
Pre-training visual and textual representations from large-scale image-text
pairs is becoming a standard approach for many downstream vision-language
tasks. The transformer-based models learn inter and intra-modal attention
through a list of self-supervised learning tasks. This paper proposes LAViTeR,
a novel architecture for visual and textual representation learning. The main
module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,
GAN-based image synthesis and Image Captioning. We also propose a new
evaluation metric measuring the similarity between the learnt visual and
textual embedding. The experimental results on two public datasets, CUB and
MS-COCO, demonstrate superior visual and textual representation alignment in
the joint feature embedding space
