---
layout: publication
title: Tri45;attention Explicit Context45;aware Attention Mechanism For Natural Language Processing
authors: Yu Rui, Li Yifeng, Lu Wenpeng, Cao Longbing
conference: "Arxiv"
year: 2022
bibkey: yu2022tri
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.02899"}
tags: ['Attention Mechanism', 'Model Architecture', 'Tools', 'Transformer']
---
In natural language processing (NLP) the context of a word or sentence plays an essential role. Contextual information such as the semantic representation of a passage or historical dialogue forms an essential part of a conversation and a precise understanding of the present phrase or sentence. However the standard attention mechanisms typically generate weights using query and key but ignore context forming a Bi45;Attention framework despite their great success in modeling sequence alignment. This Bi45;Attention mechanism does not explicitly model the interactions between the contexts queries and keys of target sequences missing important contextual information and resulting in poor attention performance. Accordingly a novel and general triple45;attention (Tri45;Attention) framework expands the standard Bi45;Attention mechanism and explicitly interacts query key and context by incorporating context as the third dimension in calculating relevance scores. Four variants of Tri45;Attention are generated by expanding the two45;dimensional vector45;based additive dot45;product scaled dot45;product and bilinear operations in Bi45;Attention to the tensor operations for Tri45;Attention. Extensive experiments on three NLP tasks demonstrate that Tri45;Attention outperforms about 30 state45;of45;the45;art non45;attention standard Bi45;Attention contextual Bi45;Attention approaches and pretrained neural language models1.
