---
layout: publication
title: 'Tri-attention: Explicit Context-aware Attention Mechanism For Natural Language Processing'
authors: Rui Yu, Yifeng Li, Wenpeng Lu, Longbing Cao
conference: "Arxiv"
year: 2022
bibkey: yu2022tri
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2211.02899'}
tags: ['Attention Mechanism', 'Transformer', 'Model Architecture', 'Tools']
---
In natural language processing (NLP), the context of a word or sentence plays
an essential role. Contextual information such as the semantic representation
of a passage or historical dialogue forms an essential part of a conversation
and a precise understanding of the present phrase or sentence. However, the
standard attention mechanisms typically generate weights using query and key
but ignore context, forming a Bi-Attention framework, despite their great
success in modeling sequence alignment. This Bi-Attention mechanism does not
explicitly model the interactions between the contexts, queries and keys of
target sequences, missing important contextual information and resulting in
poor attention performance. Accordingly, a novel and general triple-attention
(Tri-Attention) framework expands the standard Bi-Attention mechanism and
explicitly interacts query, key, and context by incorporating context as the
third dimension in calculating relevance scores. Four variants of Tri-Attention
are generated by expanding the two-dimensional vector-based additive,
dot-product, scaled dot-product, and bilinear operations in Bi-Attention to the
tensor operations for Tri-Attention. Extensive experiments on three NLP tasks
demonstrate that Tri-Attention outperforms about 30 state-of-the-art
non-attention, standard Bi-Attention, contextual Bi-Attention approaches and
pretrained neural language models1.
