---
layout: publication
title: KV Cache Compression But What Must We Give In Return A Comprehensive Benchmark Of Long Context Capable Approaches
authors: Yuan Jiayi Henry, Liu Hongyi Henry, Shaochen Henry, Zhong, Chuang Yu-neng, Li Songchen, Wang Guanchu, Le Duy, Jin Hongye, Chaudhary Vipin, Xu Zhaozhuo, Liu Zirui, Hu Xia
conference: "Arxiv"
year: 2024
bibkey: yuan2024kv
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.01527"}
  - {name: "Code", url: "https://github.com/henryzhongsc/longctx&#95;bench"}
tags: ['Applications', 'Efficiency And Optimization', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Quantization', 'Transformer']
---
Long context capability is a crucial competency for large language models (LLMs) as it mitigates the human struggle to digest long45;form texts. This capability enables complex task45;solving scenarios such as book summarization code assistance and many more tasks that are traditionally manpower45;intensive. However transformer45;based LLMs face significant challenges with long context input due to the growing size of the KV cache and the intrinsic complexity of attending to extended inputs; where multiple schools of efficiency45;driven approaches 45;45; such as KV cache quantization token dropping prompt compression linear45;time sequence models and hybrid architectures 45;45; have been proposed to produce efficient yet long context45;capable models. Despite these advancements no existing work has comprehensively benchmarked these methods in a reasonably aligned environment. In this work we fill this gap by providing a taxonomy of current methods and evaluating 10+ state45;of45;the45;art approaches across seven categories of long context tasks. Our work reveals numerous previously unknown phenomena and offers insights 45;45; as well as a friendly workbench 45;45; for the future development of long context45;capable LLMs. The source code will be available at https://github.com/henryzhongsc/longctx&#95;bench
