---
layout: publication
title: Attentive Fine45;tuning Of Transformers For Translation Of Low45;resourced Languages 35;64;loresmt 2021
authors: Puranik Karthik, Hande Adeep, Priyadharshini Ruba, Durairaj Thenmozhi, Sampath Anbukkarasi, Thamburaj Kingston Pal, Chakravarthi Bharathi Raja
conference: "Arxiv"
year: 2021
bibkey: puranik2021attentive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2108.08556"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
This paper reports the Machine Translation (MT) systems submitted by the IIITT team for the English45;Marathi and English45;Irish language pairs LoResMT 2021 shared task. The task focuses on getting exceptional translations for rather low45;resourced languages like Irish and Marathi. We fine45;tune IndicTrans a pretrained multilingual NMT model for English45;Marathi using external parallel corpus as input for additional training. We have used a pretrained Helsinki45;NLP Opus MT English45;Irish model for the latter language pair. Our approaches yield relatively promising results on the BLEU metrics. Under the team name IIITT our systems ranked 1 1 and 2 in English45;Marathi Irish45;English and English45;Irish respectively.
