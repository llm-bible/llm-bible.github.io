---
layout: publication
title: Exploring The Factual Consistency In Dialogue Comprehension Of Large Language Models
authors: She Shuaijie, Huang Shujian, Wang Xingyun, Zhou Yanke, Chen Jiajun
conference: "Arxiv"
year: 2023
bibkey: she2023exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.07194"}
tags: ['Applications', 'GPT', 'Model Architecture', 'RAG']
---
LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions which naturally require dialogue comprehension abilities. However dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC45;Sum) of different LLMs we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC45;QA). Our evaluation shows that on average 26.837; of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT the strongest model evaluated has such errors in 1637; of its summaries. For answering the factual questions which is more challenging the average error rate of all evaluated LLMs is 36.137;. Both results indicate serious deficiencies. Detailed analysis shows that the understanding of subject/object of the conversation is still challenging for LLMs. Furthermore to stimulate and enhance the dialogue comprehension ability of LLMs we propose a fine45;tuning paradigm with auto45;constructed multi45;task data which achieved a relative error rate reduction of 1137; on DIAC45;QA.
