---
layout: publication
title: 'Kallini Et Al. (2024) Do Not Compare Impossible Languages With Constituency-based Ones'
authors: Tim Hunter
conference: "Arxiv"
year: 2024
bibkey: hunter2024kallini
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.12271"}
tags: ['Training Techniques', 'Model Architecture', 'Reinforcement Learning', 'GPT', 'Ethics and Bias', 'ACL', 'Applications']
---
A central goal of linguistic theory is to find a precise characterization of
the notion "possible human language", in the form of a computational device
that is capable of describing all and only the languages that can be acquired
by a typically developing human child. The success of recent large language
models (LLMs) in NLP applications arguably raises the possibility that LLMs
might be computational devices that meet this goal. This would only be the case
if, in addition to succeeding in learning human languages, LLMs struggle to
learn "impossible" human languages. Kallini et al. (2024; "Mission: Impossible
Language Models", Proc. ACL) conducted experiments aiming to test this by
training GPT-2 on a variety of synthetic languages, and found that it learns
some more successfully than others. They present these asymmetries as support
for the idea that LLMs' inductive biases align with what is regarded as
"possible" for human languages, but the most significant comparison has a
confound that makes this conclusion unwarranted. In this paper I explain the
confound and suggest some ways forward towards constructing a comparison that
appropriately tests the underlying issue.
