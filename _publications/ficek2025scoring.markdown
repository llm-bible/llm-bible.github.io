---
layout: publication
title: 'Scoring Verifiers: Evaluating Synthetic Verification For Code And Reasoning'
authors: Aleksander Ficek, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg
conference: "Arxiv"
year: 2025
bibkey: ficek2025scoring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.13820"}
tags: ['Agentic', 'Reinforcement Learning']
---
Synthetic verification techniques such as generating test cases and reward
modelling are common ways to enhance the coding capabilities of large language
models (LLM) beyond predefined tests. Additionally, code verification has
recently found great success as a critical component in improving reasoning
capability of LLMs via reinforcement learning. In this paper, we propose a an
approach which can transform existing coding benchmarks into scoring and
ranking datasets to evaluate the effectiveness of synthetic verifiers. We also
propose multiple metrics to measure different aspects of the synthetic
verifiers with the proposed benchmarks. By employing the proposed approach, we
release four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed
synthetic verification methods with standard, reasoning-based, and reward-based
LLMs. Our experiments show that reasoning can significantly improve test case
generation and that scaling the number of test cases enhances the verification
accuracy.
