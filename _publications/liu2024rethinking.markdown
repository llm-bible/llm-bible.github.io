---
layout: publication
title: Rethinking Machine Unlearning For Large Language Models
authors: Liu Sijia, Yao Yuanshun, Jia Jinghan, Casper Stephen, Baracaldo Nathalie, Hase Peter, Yao Yuguang, Liu Chris Yuhao, Xu Xiaojun, Li Hang, Varshney Kush R., Bansal Mohit, Koyejo Sanmi, Liu Yang
conference: "Arxiv"
year: 2024
bibkey: liu2024rethinking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.08787"}
tags: ['Agentic', 'Applications', 'Interpretability And Explainability', 'Reinforcement Learning', 'Security', 'Tools', 'Training Techniques']
---
We explore machine unlearning (MU) in the domain of large language models (LLMs) referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g. sensitive or illegal information) and the associated model capabilities while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs potentially standing as an essential foundation for developing generative AI that is not only safe secure and trustworthy but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation methodologies metrics and applications. In particular we highlight the often-overlooked aspects of existing LLM unlearning research e.g. unlearning scope data-model interaction and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing influence functions model explanation adversarial training and reinforcement learning. Furthermore we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.
