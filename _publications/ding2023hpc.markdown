---
layout: publication
title: HPC45;GPT Integrating Large Language Model For High45;performance Computing
authors: Ding Xianzhong, Chen Le, Emani Murali, Liao Chunhua, Lin Pei-hung, Vanderbruggen Tristan, Xie Zhen, Cerpa Alberto E., Du Wan
conference: "Arxiv"
year: 2023
bibkey: ding2023hpc
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.12833"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Pretraining Methods']
---
Large Language Models (LLMs) including the LLaMA model have exhibited their efficacy across various general45;domain natural language processing (NLP) tasks. However their performance in high45;performance computing (HPC) domain tasks has been less than optimal due to the specialized expertise required to interpret the model responses. In response to this challenge we propose HPC45;GPT a novel LLaMA45;based model that has been supervised fine45;tuning using generated QA (Question45;Answer) instances for the HPC domain. To evaluate its effectiveness we concentrate on two HPC tasks managing AI models and datasets for HPC and data race detection. By employing HPC45;GPT we demonstrate comparable performance with existing methods on both tasks exemplifying its excellence in HPC45;related scenarios. Our experiments on open45;source benchmarks yield extensive results underscoring HPC45;GPTs potential to bridge the performance gap between LLMs and HPC45;specific tasks. With HPC45;GPT we aim to pave the way for LLMs to excel in HPC domains simplifying the utilization of language models in complex computing applications.
