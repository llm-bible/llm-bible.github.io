---
layout: publication
title: 'In Dialogue With Intelligence: Rethinking Large Language Models As Collective Knowledge'
authors: Eleni Vasilaki
conference: "Arxiv"
year: 2025
bibkey: vasilaki2025dialogue
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.22767"}
tags: ['Training Techniques', 'Model Architecture', 'GPT', 'Pretraining Methods', 'Fine-Tuning']
---
Large Language Models (LLMs) are typically analysed through architectural, behavioural, or training-data lenses. This article offers a theoretical and experiential re-framing: LLMs as dynamic instantiations of Collective human Knowledge (CK), where intelligence is evoked through dialogue rather than stored statically. Drawing on concepts from neuroscience and AI, and grounded in sustained interaction with ChatGPT-4, I examine emergent dialogue patterns, the implications of fine-tuning, and the notion of co-augmentation: mutual enhancement between human and machine cognition. This perspective offers a new lens for understanding interaction, representation, and agency in contemporary AI systems.
