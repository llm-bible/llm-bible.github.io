---
layout: publication
title: 'Trusting CHATGPT: How Minor Tweaks In The Prompts Lead To Major Differences In Sentiment Classification'
authors: Jaime E. Cuellar, Oscar Moreno-martinez, Paula Sofia Torres-rodriguez, Jaime Andres Pavlich-mariscal, Andres Felipe Mican-castiblanco, Juan Guillermo Torres-hurtado
conference: "Arxiv"
year: 2025
bibkey: cuellar2025trusting
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.12180'}
tags: ['Interpretability and Explainability', 'Security', 'GPT', 'Model Architecture', 'Fine-Tuning', 'Prompting']
---
One fundamental question for the social sciences today is: how much can we
trust highly complex predictive models like ChatGPT? This study tests the
hypothesis that subtle changes in the structure of prompts do not produce
significant variations in the classification results of sentiment polarity
analysis generated by the Large Language Model GPT-4o mini. Using a dataset of
100.000 comments in Spanish on four Latin American presidents, the model
classified the comments as positive, negative, or neutral on 10 occasions,
varying the prompts slightly each time. The experimental methodology included
exploratory and confirmatory analyses to identify significant discrepancies
among classifications.
  The results reveal that even minor modifications to prompts such as lexical,
syntactic, or modal changes, or even their lack of structure impact the
classifications. In certain cases, the model produced inconsistent responses,
such as mixing categories, providing unsolicited explanations, or using
languages other than Spanish. Statistical analysis using Chi-square tests
confirmed significant differences in most comparisons between prompts, except
in one case where linguistic structures were highly similar.
  These findings challenge the robustness and trust of Large Language Models
for classification tasks, highlighting their vulnerability to variations in
instructions. Moreover, it was evident that the lack of structured grammar in
prompts increases the frequency of hallucinations. The discussion underscores
that trust in Large Language Models is based not only on technical performance
but also on the social and institutional relationships underpinning their use.
