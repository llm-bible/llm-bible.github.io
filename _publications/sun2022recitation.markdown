---
layout: publication
title: Recitation45;augmented Language Models
authors: Sun Zhiqing, Wang Xuezhi, Tay Yi, Yang Yiming, Zhou Denny
conference: "Arxiv"
year: 2022
bibkey: sun2022recitation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2210.01296"}
  - {name: "Code", url: "https://github.com/Edward&#45;Sun/RECITE"}
tags: ['Applications', 'Has Code']
---
We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus called RECITation45;augmented gEneration (RECITE). Different from retrieval45;augmented language models that retrieve relevant documents before generating the outputs given an input RECITE first recites one or several relevant passages from LLMs own memory via sampling and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge45;intensive NLP tasks. Specifically we show that by utilizing recitation as the intermediate step a recite45;and45;answer scheme can achieve new state45;of45;the45;art performance in various closed45;book question answering (CBQA) tasks. In experiments we verify the effectiveness of method~on four pre45;trained models (PaLM UL2 OPT and Codex) and three CBQA tasks (Natural Questions TriviaQA and HotpotQA). Our code is available at https://github.com/Edward&#45;Sun/RECITE.
