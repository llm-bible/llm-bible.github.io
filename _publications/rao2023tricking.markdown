---
layout: publication
title: Tricking Llms Into Disobedience\: Formalizing, Analyzing, And Detecting Jailbreaks
authors: Rao Abhinav, Vashistha Sachin, Naik Atharva, Aditya Somak, Choudhury Monojit
conference: "Arxiv"
year: 2023
bibkey: rao2023tricking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.14965"}
tags: ['Ethics And Bias', 'Fine Tuning', 'GPT', 'Model Architecture', 'Prompting', 'Security', 'Survey Paper']
---
Recent explorations with commercial Large Language Models (LLMs) have shown that non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior privacy and security breaches offensive outputs and violations of content regulator policies. Limited studies have been conducted to formalize and analyze these attacks and their mitigations. We bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. We survey existing jailbreak methods and their effectiveness on open-source and commercial LLMs (such as GPT-based models OPT BLOOM and FLAN-T5-XXL). We further discuss the challenges of jailbreak detection in terms of their effectiveness against known attacks. For further analysis we release a dataset of model outputs across 3700 jailbreak prompts over 4 tasks.
