---
layout: publication
title: 'Assessing Code Understanding In Llms'
authors: Cosimo Laneve, Alvise Span√≤, Dalila Ressi, Sabina Rossi, Michele Bugliesi
conference: "Arxiv"
year: 2025
bibkey: laneve2025assessing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.00065'}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Tools']
---
We present an empirical evaluation of Large Language Models in code
understanding associated with non-trivial, semantic-preserving program
transformations such as copy propagation or constant folding. Our findings show
that LLMs fail to judge semantic equivalence in approximately 41% of cases
when no context is provided and in 29% when given a simple generic context. To
improve accuracy, we advocate integrating LLMs with code-optimization tools to
enhance training and facilitate more robust program understanding.
