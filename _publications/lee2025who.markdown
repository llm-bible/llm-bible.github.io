---
layout: publication
title: 'Who Relies More On World Knowledge And Bias For Syntactic Ambiguity Resolution: Humans Or Llms?'
authors: So Young Lee, Russell Scheinberg, Amber Shore, Ameeta Agrawal
conference: "Arxiv"
year: 2025
bibkey: lee2025who
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.10838"}
tags: ['RAG', 'Training Techniques', 'Ethics and Bias', 'Reinforcement Learning']
---
This study explores how recent large language models (LLMs) navigate relative
clause attachment \{ambiguity\} and use world knowledge biases for disambiguation
in six typologically diverse languages: English, Chinese, Japanese, Korean,
Russian, and Spanish. We describe the process of creating a novel dataset --
MultiWho -- for fine-grained evaluation of relative clause attachment
preferences in ambiguous and unambiguous contexts. Our experiments with three
LLMs indicate that, contrary to humans, LLMs consistently exhibit a preference
for local attachment, displaying limited responsiveness to syntactic variations
or language-specific attachment patterns. Although LLMs performed well in
unambiguous cases, they rigidly prioritized world knowledge biases, lacking the
flexibility of human language processing. These findings highlight the need for
more diverse, pragmatically nuanced multilingual training to improve LLMs'
handling of complex structures and human-like comprehension.
