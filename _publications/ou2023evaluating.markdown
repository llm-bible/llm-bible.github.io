---
layout: publication
title: Dialogbench Evaluating Llms As Human45;like Dialogue Systems
authors: Ou Jiao, Lu Junda, Liu Che, Tang Yihong, Zhang Fuzheng, Zhang Di, Gai Kun
conference: "Arxiv"
year: 2023
bibkey: ou2023evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.01677"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'RAG']
---
Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities by leveraging instruction tuning which refreshes human impressions of dialogue systems. The long45;standing goal of dialogue systems is to be human45;like enough to establish long45;term connections with users. Therefore there has been an urgent need to evaluate LLMs as human45;like dialogue systems. In this paper we propose DialogBench a dialogue evaluation benchmark that contains 12 dialogue tasks to probe the capabilities of LLMs as human45;like dialogue systems should have. Specifically we prompt GPT45;4 to generate evaluation instances for each task. We first design the basic prompt based on widely used design principles and further mitigate the existing biases to generate higher45;quality evaluation instances. Our extensive tests on English and Chinese DialogBench of 26 LLMs show that instruction tuning improves the human likeness of LLMs to a certain extent but most LLMs still have much room for improvement as human45;like dialogue systems. Interestingly results also show that the positioning of assistant AI can make instruction tuning weaken the human emotional perception of LLMs and their mastery of information about human daily life.
