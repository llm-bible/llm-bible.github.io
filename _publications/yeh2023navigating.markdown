---
layout: publication
title: 'Navigating Text-to-image Customization: From Lycoris Fine-tuning To Model Evaluation'
authors: Yeh Shih-ying, Hsieh Yu-guan, Gao Zhidong, Yang Bernard B W, Oh Giyeong, Gong Yanmin
conference: "Arxiv"
year: 2023
bibkey: yeh2023navigating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.14859"}
  - {name: "Code", url: "https://github.com/KohakuBlueleaf/LyCORIS],"}
tags: ['Attention Mechanism', 'Fine Tuning', 'Has Code', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Tools', 'Training Techniques']
---
Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.
