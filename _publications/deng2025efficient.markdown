---
layout: publication
title: 'PLM: Efficient Peripheral Language Models Hardware-co-designed For Ubiquitous Computing'
authors: Cheng Deng, Luoyang Sun, Jiwen Jiang, Yongcheng Zeng, Xinjian Wu, Wenxin Zhao, Qingfa Xiao, Jiachuan Wang, Haoyang Li, Lei Chen, Lionel M. Ni, Haifeng Zhang, Jun Wang
conference: "Arxiv"
year: 2025
bibkey: deng2025efficient
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.12167"}
  - {name: "Code", url: "https://github.com/plm-team/PLM"}
tags: ['Agentic', 'Efficiency and Optimization', 'Model Architecture', 'Training Techniques', 'Scaling Laws', 'Reinforcement Learning', 'RAG', 'Large-Scale Training', 'Fine-Tuning', 'Transformer', 'Has Code', 'Dataset', 'Pre-Training', 'Applications', 'Attention Mechanism']
---
While scaling laws have been continuously validated in large language models
(LLMs) with increasing model parameters, the inherent tension between the
inference demands of LLMs and the limited resources of edge devices poses a
critical challenge to the development of edge intelligence. Recently, numerous
small language models have emerged, aiming to distill the capabilities of LLMs
into smaller footprints. However, these models often retain the fundamental
architectural principles of their larger counterparts, still imposing
considerable strain on the storage and bandwidth capacities of edge devices. In
this paper, we introduce the PLM, a Peripheral Language Model, developed
through a co-design process that jointly optimizes model architecture and edge
system constraints. The PLM utilizes a Multi-head Latent Attention mechanism
and employs the squared ReLU activation function to encourage sparsity, thereby
reducing peak memory footprint during inference. During training, we collect
and reorganize open-source datasets, implement a multi-phase training strategy,
and empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning
rate scheduler. Additionally, we incorporate Reinforcement Learning from Human
Feedback (RLHF) by adopting the ARIES preference learning approach. Following a
two-phase SFT process, this method yields performance gains of 2% in general
tasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel
architecture, evaluation results demonstrate that PLM outperforms existing
small language models trained on publicly available data while maintaining the
lowest number of activated parameters. Furthermore, deployment across various
edge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis,
validates PLM's suitability for peripheral applications. The PLM series models
are publicly available at https://github.com/plm-team/PLM.
