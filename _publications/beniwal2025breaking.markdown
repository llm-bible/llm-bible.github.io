---
layout: publication
title: 'Breaking Mbad! Supervised Fine-tuning For Cross-lingual Detoxification'
authors: Himanshu Beniwal, Youngwoo Kim, Maarten Sap, Soham Dan, Thomas Hartvigsen
conference: "Arxiv"
year: 2025
bibkey: beniwal2025breaking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.16722'}
  - {name: "Code", url: 'https://github.com/himanshubeniwal/Breaking-mBad'}
tags: ['Has Code', 'Training Techniques', 'Applications', 'Fine-Tuning', 'Responsible AI', 'Pretraining Methods']
---
As large language models (LLMs) become increasingly prevalent in global applications, ensuring that they are toxicity-free across diverse linguistic contexts remains a critical challenge. We explore "Cross-lingual Detoxification", a cross-lingual paradigm that mitigates toxicity, enabling detoxification capabilities to transfer between high and low-resource languages across different script families. We analyze cross-lingual detoxification's effectiveness through 504 extensive settings to evaluate toxicity reduction in cross-distribution settings with limited data and investigate how mitigation impacts model performance on non-toxic tasks, revealing trade-offs between safety and knowledge preservation. Our code and dataset are publicly available at https://github.com/himanshubeniwal/Breaking-mBad.
