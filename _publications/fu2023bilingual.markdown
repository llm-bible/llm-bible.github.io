---
layout: publication
title: 'Codeapex: A Bilingual Programming Evaluation Benchmark For Large Language Models'
authors: Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, Yong Yu
conference: "Arxiv"
year: 2023
bibkey: fu2023bilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.01940"}
tags: ['Model Architecture', 'Reinforcement Learning', 'GPT', 'Applications', 'Attention Mechanism']
---
With the emergence of Large Language Models (LLMs), there has been a
significant improvement in the programming capabilities of models, attracting
growing attention from researchers. Evaluating the programming capabilities of
LLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has
numerous downstream applications. In this paper, we propose CodeApex, a
bilingual benchmark dataset focusing on the programming comprehension, code
generation, and code correction abilities of LLMs. Programming comprehension
task tests LLMs on multiple-choice exam questions covering conceptual
understanding, commonsense reasoning, and multi-hop reasoning. The code
generation task evaluates LLMs through completing C++ functions based on
provided descriptions and prototypes. The code correction task asks LLMs to fix
real-world erroneous code segments with different error messages. We evaluate
12 widely used LLMs, including both general-purpose and specialized models.
GPT-4 exhibits the best programming capabilities, achieving approximate
accuracy of 69%, 54%, and 66% on the three tasks, respectively. Compared to
human performance, there is still significant room for improvement in LLM
programming. We hope that CodeApex can serve as a reference for evaluating the
coding capabilities of LLMs, further promoting their development and growth.
