---
layout: publication
title: 'A Survey On Memory-efficient Large-scale Model Training In AI For Science'
authors: Kaiyuan Tian, Linbo Qiao, Baihui Liu, Gongqingjian Jiang, Dongsheng Li
conference: "Arxiv"
year: 2025
bibkey: tian2025survey
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.11847'}
tags: ['Large-Scale Training', 'Transformer', 'RAG', 'Efficiency and Optimization', 'Training Techniques', 'Applications', 'Model Architecture', 'Survey Paper', 'Pretraining Methods']
---
Scientific research faces high costs and inefficiencies with traditional
methods, but the rise of deep learning and large language models (LLMs) offers
innovative solutions. This survey reviews LLM applications across scientific
fields such as biology, medicine, chemistry, and meteorology, underscoring
their role in advancing research. However, the continuous expansion of model
size has led to significant memory demands, hindering further development and
application of LLMs for science. To address this, we review memory-efficient
training techniques for LLMs based on the transformer architecture, including
distributed training, mixed precision training, and gradient checkpointing.
Using AlphaFold 2 as an example, we demonstrate how tailored memory
optimization methods can reduce storage needs while preserving prediction
accuracy. We also discuss the challenges of memory optimization in practice and
potential future directions, hoping to provide valuable insights for
researchers and engineers.
