---
layout: publication
title: Verigen A Large Language Model For Verilog Code Generation
authors: Thakur Shailja, Ahmad Baleegh, Pearce Hammond, Tan Benjamin, Dolan-gavitt Brendan, Karri Ramesh, Garg Siddharth
conference: "Arxiv"
year: 2023
bibkey: thakur2023large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.00708"}
tags: ['Applications', 'GPT', 'Model Architecture']
---
In this study we explore the capability of Large Language Models (LLMs) to automate hardware design by generating high45;quality Verilog code a common language for designing and modeling digital systems. We fine45;tune pre45;existing LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We evaluate the functional correctness of the generated Verilog code using a specially designed test suite featuring a custom problem set and testing benches. Here our fine45;tuned open45;source CodeGen45;16B model outperforms the commercial state45;of45;the45;art GPT45;3.545;turbo model with a 1.137; overall increase. Upon testing with a more diverse and complex problem set we find that the fine45;tuned model shows competitive performance against state45;of45;the45;art gpt45;3.545;turbo excelling in certain scenarios. Notably it demonstrates a 4137; improvement in generating syntactically correct Verilog code across various problem categories compared to its pre45;trained counterpart highlighting the potential of smaller in45;house LLMs in hardware design automation.
