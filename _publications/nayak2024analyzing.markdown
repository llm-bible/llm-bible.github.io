---
layout: publication
title: 'Analyzing Persuasive Strategies In Meme Texts: A Fusion Of Language Models With Paraphrase Enrichment'
authors: Kota Shamanth Ramanath Nayak, Leila Kosseim
conference: "Computer Science Information Technology (CS IT) ISSN 2231 - 5403 Volume 14 Number 11 June 2024"
year: 2024
bibkey: nayak2024analyzing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.01784"}
tags: ['Fine-Tuning', 'GPT', 'RAG', 'Model Architecture', 'Merging', 'Training Techniques', 'Pretraining Methods', 'BERT']
---
This paper describes our approach to hierarchical multi-label detection of
persuasion techniques in meme texts. Our model, developed as a part of the
recent SemEval task, is based on fine-tuning individual language models (BERT,
XLM-RoBERTa, and mBERT) and leveraging a mean-based ensemble model in addition
to dataset augmentation through paraphrase generation from ChatGPT. The scope
of the study encompasses enhancing model performance through innovative
training techniques and data augmentation strategies. The problem addressed is
the effective identification and classification of multiple persuasive
techniques in meme texts, a task complicated by the diversity and complexity of
such content. The objective of the paper is to improve detection accuracy by
refining model training methods and examining the impact of balanced versus
unbalanced training datasets. Novelty in the results and discussion lies in the
finding that training with paraphrases enhances model performance, yet a
balanced training set proves more advantageous than a larger unbalanced one.
Additionally, the analysis reveals the potential pitfalls of indiscriminate
incorporation of paraphrases from diverse distributions, which can introduce
substantial noise. Results with the SemEval 2024 data confirm these insights,
demonstrating improved model efficacy with the proposed methods.
