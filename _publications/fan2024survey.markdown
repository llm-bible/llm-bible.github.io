---
layout: publication
title: A Survey On RAG Meeting Llms Towards Retrieval45;augmented Large Language Models
authors: Fan Wenqi, Ding Yujuan, Ning Liangbo, Wang Shijie, Li Hengyun, Yin Dawei, Chua Tat-seng, Li Qing
conference: "Arxiv"
year: 2024
bibkey: fan2024survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.06211"}
tags: ['Applications', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
As one of the most advanced techniques in AI Retrieval45;Augmented Generation (RAG) can offer reliable and up45;to45;date external knowledge providing huge convenience for numerous tasks. Particularly in the era of AI45;Generated Content (AIGC) the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high45;quality outputs. Recently Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation while still facing inherent limitations such as hallucinations and out45;of45;date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information Retrieval45;Augmented Large Language Models (RA45;LLMs) have emerged to harness external and authoritative knowledge bases rather than solely relying on the models internal knowledge to augment the generation quality of LLMs. In this survey we comprehensively review existing research studies in RA45;LLMs covering three primary technical perspectives architectures training strategies and applications. As the preliminary knowledge we briefly introduce the foundations and recent advances of LLMs. Then to illustrate the practical significance of RAG for LLMs we systematically review mainstream relevant work by their architectures training strategies and application areas detailing specifically the challenges of each and the corresponding capabilities of RA45;LLMs. Finally to deliver deeper insights we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at https://advanced&#45;recommender&#45;systems.github.io/RAG&#45;Meets&#45;LLMs/
