---
layout: publication
title: Visual Instruction Tuning
authors: Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee
conference: "Arxiv"
year: 2023
bibkey: liu2023visual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2304.08485v2"}
tags: ['GPT', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning']
---
Instruction tuning large language models (LLMs) using machine45;generated instruction45;following data has improved zero45;shot capabilities on new tasks but the idea is less explored in the multimodal field. In this paper we present the first attempt to use language45;only GPT45;4 to generate multimodal language45;image instruction45;following data. By instruction tuning on such generated data we introduce LLaVA Large Language and Vision Assistant an end45;to45;end trained large multimodal model that connects a vision encoder and LLM for general45;purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities sometimes exhibiting the behaviors of multimodal GPT45;4 on unseen images/instructions and yields a 85.137; relative score compared with GPT45;4 on a synthetic multimodal instruction45;following dataset. When fine45;tuned on Science QA the synergy of LLaVA and GPT45;4 achieves a new state45;of45;the45;art accuracy of 92.5337;. We make GPT45;4 generated visual instruction tuning data our model and code base publicly available.
