---
layout: publication
title: A Zero45;shot And Few45;shot Study Of Instruction45;finetuned Large Language Models Applied To Clinical And Biomedical Tasks
authors: Labrak Yanis, Rouvier Mickael, Dufour Richard
conference: "Proceedings of the"
year: 2023
bibkey: labrak2023zero
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.12114"}
tags: ['Applications', 'BERT', 'GPT', 'Model Architecture', 'Reinforcement Learning']
---
We evaluate four state45;of45;the45;art instruction45;tuned large language models (LLMs) 45;45; ChatGPT Flan45;T5 UL2 Tk45;Instruct and Alpaca 45;45; on a set of 13 real45;world clinical and biomedical natural language processing (NLP) tasks in English such as named45;entity recognition (NER) question45;answering (QA) relation extraction (RE) etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state45;of45;the45;art models in zero45; and few45;shot scenarios for most tasks and particularly well for the QA task even though they have never seen examples from these tasks before. However we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field such as PubMedBERT. Finally we noted that no LLM outperforms all the others on all the studied tasks with some models being better suited for certain tasks than others.
