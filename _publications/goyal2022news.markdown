---
layout: publication
title: News Summarization And Evaluation In The Era Of GPT45;3
authors: Tanya Goyal, Junyi Jessy Li, Greg Durrett
conference: "Arxiv"
year: 2022
bibkey: goyal2022news
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2209.12356v2"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
The recent success of prompting large language models like GPT45;3 has led to a paradigm shift in NLP research. In this paper we study its impact on text summarization focusing on the classic benchmark domain of news summarization. First we investigate how GPT45;3 compares against fine45;tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT45;3 summaries prompted using only a task description but these also do not suffer from common dataset45;specific issues such as poor factuality. Next we study what this means for evaluation particularly the role of gold standard test sets. Our experiments show that both reference45;based and reference45;free automatic metrics cannot reliably evaluate GPT45;3 summaries. Finally we evaluate models on a setting beyond generic summarization specifically keyword45;based summarization and show how dominant fine45;tuning approaches compare to prompting. To support further research we release (a) a corpus of 10K generated summaries from fine45;tuned and prompt45;based models across 4 standard summarization benchmarks (b) 1K human preference judgments comparing different systems for generic45; and keyword45;based summarization.
