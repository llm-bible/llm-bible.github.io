---
layout: publication
title: Tablebench A Comprehensive And Complex Benchmark For Table Question Answering
authors: Wu Xianjie, Yang Jian, Chai Linzheng, Zhang Ge, Liu Jiaheng, Du Xinrun, Liang Di, Shu Daixin, Cheng Xianfu, Sun Tianzhen, Niu Guanglin, Li Tongliang, Li Zhoujun
conference: "Arxiv"
year: 2024
bibkey: wu2024comprehensive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.09174"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
Recent advancements in Large Language Models (LLMs) have markedly enhanced the interpretation and processing of tabular data introducing previously unimaginable capabilities. Despite these achievements LLMs still encounter significant challenges when applied in industrial scenarios particularly due to the increased complexity of reasoning required with real45;world tabular data underscoring a notable disparity between academic benchmarks and practical applications. To address this discrepancy we conduct a detailed investigation into the application of tabular data in industrial scenarios and propose a comprehensive and complex benchmark TableBench including 18 fields within four major categories of table question answering (TableQA) capabilities. Furthermore we introduce TableLLM trained on our meticulously constructed training set TableInstruct achieving comparable performance with GPT45;3.5. Massive experiments conducted on TableBench indicate that both open45;source and proprietary LLMs still have significant room for improvement to meet real45;world demands where the most advanced model GPT45;4 achieves only a modest score compared to humans.
