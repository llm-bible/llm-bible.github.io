---
layout: publication
title: Lumina45;t2x Transforming Text Into Any Modality Resolution And Duration Via Flow45;based Large Diffusion Transformers
authors: Gao Peng, Zhuo Le, Liu Dongyang, Du Ruoyi, Luo Xu, Qiu Longtian, Zhang Yuhang, Lin Chen, Huang Rongjie, Geng Shijie, Zhang Renrui, Xi Junlin, Shao Wenqi, Jiang Zhengkai, Yang Tianshuo, Ye Weicai, Tong He, He Jingwen, Qiao Yu, Li Hongsheng
conference: "Arxiv"
year: 2024
bibkey: gao2024lumina
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.05945"}
tags: ['Attention Mechanism', 'Ethics And Bias', 'Merging', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Training Techniques', 'Transformer']
---
Sora unveils the potential of scaling Diffusion Transformer for generating photorealistic images and videos at arbitrary resolutions aspect ratios and durations yet it still lacks sufficient implementation details. In this technical report we introduce the Lumina45;T2X family 45; a series of Flow45;based Large Diffusion Transformers (Flag45;DiT) equipped with zero45;initialized attention as a unified framework designed to transform noise into images videos multi45;view 3D objects and audio clips conditioned on text instructions. By tokenizing the latent spatial45;temporal space and incorporating learnable placeholders such as nextline and nextframe tokens Lumina45;T2X seamlessly unifies the representations of different modalities across various spatial45;temporal resolutions. This unified approach enables training within a single framework for different modalities and allows for flexible generation of multimodal data at any resolution aspect ratio and length during inference. Advanced techniques like RoPE RMSNorm and flow matching enhance the stability flexibility and scalability of Flag45;DiT enabling models of Lumina45;T2X to scale up to 7 billion parameters and extend the context window to 128K tokens. This is particularly beneficial for creating ultra45;high45;definition images with our Lumina45;T2I model and long 720p videos with our Lumina45;T2V model. Remarkably Lumina45;T2I powered by a 545;billion45;parameter Flag45;DiT requires only 3537; of the training computational costs of a 60045;million45;parameter naive DiT. Our further comprehensive analysis underscores Lumina45;T2Xs preliminary capability in resolution extrapolation high45;resolution editing generating consistent 3D views and synthesizing videos with seamless transitions. We expect that the open45;sourcing of Lumina45;T2X will further foster creativity transparency and diversity in the generative AI community.
