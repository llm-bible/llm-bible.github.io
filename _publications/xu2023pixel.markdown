---
layout: publication
title: Pixel Aligned Language Models
authors: Xu Jiarui, Zhou Xingyi, Yan Shen, Gu Xiuye, Arnab Anurag, Sun Chen, Wang Xiaolong, Schmid Cordelia
conference: "Arxiv"
year: 2023
bibkey: xu2023pixel
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.09237"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods']
---
Large language models have achieved great success in recent years so as their variants in vision. Existing vision45;language models can describe images in natural languages answer visual45;related questions or perform complex reasoning about the image. However it is yet unclear how localization tasks such as word grounding or referring localization can be performed using large language models. In this work we aim to develop a vision45;language model that can take locations for example a set of points or boxes as either inputs or outputs. When taking locations as inputs the model performs location45;conditioned captioning which generates captions for the indicated object or region. When generating locations as outputs our model regresses pixel coordinates for each output word generated by the language model and thus performs dense word grounding. Our model is pre45;trained on the Localized Narrative dataset which contains pixel45;word45;aligned captioning from human attention. We show our model can be applied to various location45;aware vision45;language tasks including referring localization location45;conditioned captioning and dense object captioning archiving state45;of45;the45;art performance on RefCOCO and Visual Genome. Project page https://jerryxu.net/PixelLLM .
