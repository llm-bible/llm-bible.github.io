---
layout: publication
title: 'Vl-cache: Sparsity And Modality-aware KV Cache Compression For Vision-language Model Inference Acceleration'
authors: Dezhan Tu, Danylo Vashchilenko, Yuzhe Lu, Panpan Xu
conference: "Arxiv"
year: 2024
bibkey: tu2024vl
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.23317"}
tags: ['Attention Mechanism', 'Multimodal Models', 'Model Architecture']
---
Vision-Language Models (VLMs) have demonstrated impressive performance across
a versatile set of tasks. A key challenge in accelerating VLMs is storing and
accessing the large Key-Value (KV) cache that encodes long visual contexts,
such as images or videos. While existing KV cache compression methods are
effective for Large Language Models (LLMs), directly migrating them to VLMs
yields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,
a novel KV cache compression recipe tailored for accelerating VLM inference. In
this paper, we first investigate the unique sparsity pattern of VLM attention
by distinguishing visual and text tokens in prefill and decoding phases. Based
on these observations, we introduce a layer-adaptive sparsity-aware cache
budget allocation method that effectively distributes the limited cache budget
across different layers, further reducing KV cache size without compromising
accuracy. Additionally, we develop a modality-aware token scoring policy to
better evaluate the token importance. Empirical results on multiple benchmark
datasets demonstrate that retaining only 10% of KV cache achieves accuracy
comparable to that with full cache. In a speed benchmark, our method
accelerates end-to-end latency of generating 100 tokens by up to 2.33x and
speeds up decoding by up to 7.08x, while reducing the memory footprint of KV
cache in GPU by 90%.
