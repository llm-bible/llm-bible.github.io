---
layout: publication
title: Internlm45;math Open Math Large Language Models Toward Verifiable Reasoning
authors: Ying Huaiyuan, Zhang Shuo, Li Linyang, Zhou Zhejian, Shao Yunfan, Fei Zhaoye, Ma Yichuan, Hong Jiawei, Liu Kuikun, Wang Ziyi, Wang Yudong, Wu Zijian, Li Shuaibin, Zhou Fengzhe, Liu Hongwei, Zhang Songyang, Zhang Wenwei, Yan Hang, Qiu Xipeng, Wang Jiayu, Chen Kai, Lin Dahua
conference: "Arxiv"
year: 2024
bibkey: ying2024internlm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.06332"}
  - {name: "Code", url: "https://github.com/InternLM/InternLM&#45;Math&#125;"}
tags: ['Has Code', 'Reinforcement Learning', 'Tools']
---
The math abilities of large language models can represent their abstract reasoning ability. In this paper we introduce and open45;source our math reasoning LLMs InternLM45;Math which is continue pre45;trained from InternLM2. We unify chain45;of45;thought reasoning reward modeling formal reasoning data augmentation and code interpreter in a unified seq2seq format and supervise our model to be a versatile math reasoner verifier prover and augmenter. These abilities can be used to develop the next math LLMs or self45;iteration. InternLM45;Math obtains open45;sourced state45;of45;the45;art performance under the setting of in45;context learning supervised fine45;tuning and code45;assisted reasoning in various informal and formal benchmarks including GSM8K MATH Hungary math exam MathBench45;ZH and MiniF2F. Our pre45;trained model achieves 30.3 on the MiniF2F test set without fine45;tuning. We further explore how to use LEAN to solve math problems and study its performance under the setting of multi45;task learning which shows the possibility of using LEAN as a unified platform for solving and proving in math. Our models codes and data are released at url123;https://github.com/InternLM/InternLM&#45;Math&#125;.
