---
layout: publication
title: Adaptive Activation Steering A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories
authors: Wang Tianlong, Jiao Xianfeng, He Yifan, Chen Zhongzhi, Zhu Yinghao, Chu Xu, Gao Junyi, Wang Yasha, Ma Liantao
conference: "Arxiv"
year: 2024
bibkey: wang2024adaptive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.00034"}
tags: ['ARXIV', 'LLM', 'Pretraining Methods']
---
Recent studies have indicated that Large Language Models (LLMs) harbor an inherent understanding of truthfulness yet often fail to express fully and generate false statements. This gap between knowing and telling poses a challenge for ensuring the truthfulness of generated content. To address this we introduce Adaptive Activation Steering (ACT) a tuning-free method that adaptively shift LLMs activations in truthful direction during inference. ACT addresses diverse categories of hallucinations by utilizing diverse steering vectors and adjusting the steering intensity adaptively. Applied as an add-on across various models ACT significantly improves truthfulness in LLaMA (uparrow 142) LLaMA2 (uparrow 24) Alpaca (uparrow 36) Vicuna (uparrow 28) and LLaMA2-Chat (uparrow 19). Furthermore we verify ACTs scalability across larger models (13B 33B 65B) underscoring the adaptability of ACT to large-scale language models.
