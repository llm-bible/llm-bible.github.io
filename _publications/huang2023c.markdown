---
layout: publication
title: C45;eval A Multi45;level Multi45;discipline Chinese Evaluation Suite For Foundation Models
authors: Huang Yuzhen, Bai Yuzhuo, Zhu Zhihao, Zhang Junlei, Zhang Jinghan, Su Tangjun, Liu Junteng, Lv Chuancheng, Zhang Yikai, Lei Jiayi, Fu Yao, Sun Maosong, He Junxian
conference: "Arxiv"
year: 2023
bibkey: huang2023c
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.08322"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Tools']
---
New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C45;Eval the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C45;Eval comprises multiple45;choice questions across four difficulty levels middle school high school college and professional. The questions span 52 diverse disciplines ranging from humanities to science and engineering. C45;Eval is accompanied by C45;Eval Hard a subset of very challenging subjects in C45;Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C45;Eval including both English45; and Chinese45;oriented models. Results indicate that only GPT45;4 could achieve an average accuracy of over 6037; suggesting that there is still significant room for improvement for current LLMs. We anticipate C45;Eval will help analyze important strengths and shortcomings of foundation models and foster their development and growth for Chinese users.
