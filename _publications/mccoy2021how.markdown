---
layout: publication
title: How Much Do Language Models Copy From Their Training Data Evaluating Linguistic Novelty In Text Generation Using RAVEN
authors: Mccoy R. Thomas, Smolensky Paul, Linzen Tal, Gao Jianfeng, Celikyilmaz Asli
conference: "Arxiv"
year: 2021
bibkey: mccoy2021how
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2111.09509"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Current language models can generate high45;quality text. Are they simply copying text they have seen before or have they learned generalizable linguistic abstractions To tease apart these possibilities we introduce RAVEN a suite of analyses for assessing the novelty of generated text focusing on sequential structure (n45;grams) and syntactic structure. We apply these analyses to four neural language models (an LSTM a Transformer Transformer45;XL and GPT45;2). For local structure 45; e.g. individual dependencies 45; model45;generated text is substantially less novel than our baseline of human45;generated text from each models test set. For larger45;scale structure 45; e.g. overall sentence structure 45; model45;generated text is as novel or even more novel than the human45;generated baseline but models still sometimes copy substantially in some cases duplicating passages over 1000 words long from the training set. We also perform extensive manual analysis showing that GPT45;2s novel text is usually well45;formed morphologically and syntactically but has reasonably frequent semantic issues (e.g. being self45;contradictory).
