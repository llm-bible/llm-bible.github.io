---
layout: publication
title: Ocassionally Secure&#58; A Comparative Analysis Of Code Generation Assistants
authors: Elgedawy Ran, Sadik John, Dutta Senjuti, Gautam Anuj, Georgiou Konstantinos, Gholamrezae Farzin, Ji Fujiao, Lim Kyungchan, Liu Qian, Ruoti Scott
conference: "Arxiv"
year: 2024
bibkey: elgedawy2024ocassionally
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.00689"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Security']
---
Large Language Models (LLMs) are being increasingly utilized in various applications with code generations being a notable example. While previous research has shown that LLMs have the capability to generate both secure and insecure code the literature does not take into account what factors help generate secure and effective code. Therefore in this paper we focus on identifying and understanding the conditions and contexts in which LLMs can be effectively and safely deployed in real-world scenarios to generate quality code. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to assess each models code generation capabilities. We contextualized our study to represent the typical use cases of a real-life developer employing LLMs for everyday tasks as work. Additionally we place an emphasis on security awareness which is represented through the use of two distinct versions of our developer persona. In total we collected 61 code outputs and analyzed them across several aspects functionality security performance complexity and reliability. These insights are crucial for understanding the models capabilities and limitations guiding future development and practical applications in the field of automated code generation.
