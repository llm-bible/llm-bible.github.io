---
layout: publication
title: 'How Good Is Google Bard''s Visual Understanding? An Empirical Study On Open Challenges'
authors: Haotong Qin, Ge-peng Ji, Salman Khan, Deng-ping Fan, Fahad Shahbaz Khan, Luc Van Gool
conference: "Machine Intelligence Research. 20(5) October 2023 605-613"
year: 2023
bibkey: qin2023how
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.15016"}
  - {name: "Code", url: "https://github.com/htqin/GoogleBard-VisUnderstand"}
tags: ['Fine-Tuning', 'GPT', 'Model Architecture', 'Has Code', 'Prompting']
---
Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in
the field of conversational AI. Notably, Bard has recently been updated to
handle visual inputs alongside text prompts during conversations. Given Bard's
impressive track record in handling textual inputs, we explore its capabilities
in understanding and interpreting visual data (images) conditioned by text
questions. This exploration holds the potential to unveil new insights and
challenges for Bard and other forthcoming multi-modal Generative models,
especially in addressing complex computer vision problems that demand accurate
visual and language understanding. Specifically, in this study, we focus on 15
diverse task scenarios encompassing regular, camouflaged, medical, under-water
and remote sensing data to comprehensively evaluate Bard's performance. Our
primary finding indicates that Bard still struggles in these vision scenarios,
highlighting the significant gap in vision-based understanding that needs to be
bridged in future developments. We expect that this empirical study will prove
valuable in advancing future models, leading to enhanced capabilities in
comprehending and interpreting fine-grained visual data. Our project is
released on https://github.com/htqin/GoogleBard-VisUnderstand
