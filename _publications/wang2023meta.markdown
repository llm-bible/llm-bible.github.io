---
layout: publication
title: Meta45;reasoning Semantics45;symbol Deconstruction For Large Language Models
authors: Wang Yiming, Zhang Zhuosheng, Zhang Pei, Yang Baosong, Wang Rui
conference: "Arxiv"
year: 2023
bibkey: wang2023meta
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.17820"}
  - {name: "Code", url: "https://github.com/Alsace08/Meta&#45;Reasoning&#125;"}
tags: ['Efficiency And Optimization', 'Has Code', 'Pretraining Methods', 'Reinforcement Learning']
---
Neural45;symbolic methods have demonstrated efficiency in enhancing the reasoning abilities of large language models (LLMs). However existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs which cater to the computer execution mindset and deviate from human reasoning habits. To broaden symbolic methods applicability and adaptability in the real world we propose the Meta45;Reasoning from a linguistic perspective. This method empowers LLMs to deconstruct reasoning45;independent semantic information into generic symbolic representations thereby efficiently capturing more generalized reasoning knowledge. We conduct extensive experiments on more than ten datasets encompassing conventional reasoning tasks like arithmetic symbolic and logical reasoning and the more complex interactive reasoning tasks like theory45;of45;mind reasoning. Experimental results demonstrate that Meta45;Reasoning significantly enhances in45;context reasoning accuracy learning efficiency out45;of45;domain generalization and output stability compared to the Chain45;of45;Thought technique. Code and data are publicly available at url123;https://github.com/Alsace08/Meta&#45;Reasoning&#125;.
