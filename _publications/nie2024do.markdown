---
layout: publication
title: 'Do Multilingual Large Language Models Mitigate Stereotype Bias?'
authors: Shangrui Nie, Michael Fromm, Charles Welch, Rebekka GÃ¶rge, Akbar Karimi, Joan Plepi, Nazia Afsan Mowmita, Nicolas Flores-herr, Mehdi Ali, Lucie Flek
conference: "Arxiv"
year: 2024
bibkey: nie2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.05740"}
tags: ['Training Techniques', 'Model Architecture', 'Bias Mitigation', 'Ethics and Bias', 'Dataset']
---
While preliminary findings indicate that multilingual LLMs exhibit reduced
bias compared to monolingual ones, a comprehensive understanding of the effect
of multilingual training on bias mitigation, is lacking. This study addresses
this gap by systematically training six LLMs of identical size (2.6B
parameters) and architecture: five monolingual models (English, German, French,
Italian, and Spanish) and one multilingual model trained on an equal
distribution of data across these languages, all using publicly available data.
To ensure robust evaluation, standard bias benchmarks were automatically
translated into the five target languages and verified for both translation
quality and bias preservation by human annotators. Our results consistently
demonstrate that multilingual training effectively mitigates bias. Moreover, we
observe that multilingual models achieve not only lower bias but also superior
prediction accuracy when compared to monolingual models with the same amount of
training data, model architecture, and size.
