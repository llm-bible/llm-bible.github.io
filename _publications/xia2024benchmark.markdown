---
layout: publication
title: FOFO A Benchmark To Evaluate Llms Format45;following Capability
authors: Xia Congying, Xing Chen, Du Jiangshu, Yang Xinyi, Feng Yihao, Xu Ran, Yin Wenpeng, Xiong Caiming
conference: "Arxiv"
year: 2024
bibkey: xia2024benchmark
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.18667"}
  - {name: "Code", url: "https://github.com/SalesforceAIResearch/FoFo"}
tags: ['Agentic', 'Applications', 'GPT', 'Has Code', 'Model Architecture', 'Reinforcement Learning']
---
This paper presents FoFo a pioneering benchmark for evaluating large language models (LLMs) ability to follow complex domain45;specific formats a crucial yet underexamined capability for their application as AI agents. Despite LLMs advancements existing benchmarks fail to assess their format45;following proficiency adequately. FoFo fills this gap with a diverse range of real45;world formats and instructions developed through an AI45;Human collaborative method. Our evaluation across both open45;source (e.g. Llama 2 WizardLM) and closed45;source (e.g. GPT45;4 PALM2 Gemini) LLMs highlights three key findings open45;source models significantly lag behind closed45;source ones in format adherence; LLMs format45;following performance is independent of their content generation quality; and LLMs format proficiency varies across different domains. These insights suggest the need for specialized tuning for format45;following skills and highlight FoFos role in guiding the selection of domain45;specific AI agents. FoFo is released here at https://github.com/SalesforceAIResearch/FoFo.
