---
layout: publication
title: POUF Prompt45;oriented Unsupervised Fine45;tuning For Large Pre45;trained Models
authors: Tanwisuth Korawat, Zhang Shujian, Zheng Huangjie, He Pengcheng, Zhou Mingyuan
conference: "Arxiv"
year: 2023
bibkey: tanwisuth2023prompt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.00350"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Tools']
---
Through prompting large45;scale pre45;trained models have become more expressive and powerful gaining significant attention in recent years. Though these big models have zero45;shot capabilities in general labeled data are still required to adapt them to downstream tasks. To overcome this critical limitation we propose an unsupervised fine45;tuning framework to directly fine45;tune the model or prompt on the unlabeled target data. We demonstrate how to apply our method to both language45;augmented vision and masked45;language models by aligning the discrete distributions extracted from the prompts and target data. To verify our approachs applicability we conduct extensive experiments on image classification sentiment analysis and natural language inference tasks. Across 13 image45;related tasks and 15 language45;related ones the proposed approach achieves consistent improvements over the baselines.
