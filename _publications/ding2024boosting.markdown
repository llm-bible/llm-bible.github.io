---
layout: publication
title: 'Boosting Large Language Models With Socratic Method For Conversational Mathematics Teaching'
authors: Ding Yuyang, Hu Hanglei, Zhou Jie, Chen Qin, Jiang Bo, He Liang
conference: "Arxiv"
year: 2024
bibkey: ding2024boosting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.17349"}
  - {name: "Code", url: "https://github.com/ECNU-ICALK/SocraticMath"}
tags: ['Applications', 'Has Code', 'Reinforcement Learning', 'Survey Paper']
---
With the introduction of large language models (LLMs) automatic math reasoning has seen tremendous success. However current methods primarily focus on providing solutions or using techniques like Chain-of-Thought to enhance problem-solving accuracy. In this paper we focus on improving the capability of mathematics teaching via a Socratic teaching-based LLM (textttSocraticLLM) which guides learners toward profound thinking with clarity and self-discovery via conversation. We collect and release a high-quality mathematical teaching dataset named textttSocraticMATH which provides Socratic-style conversations of problems with extra knowledge. Also we propose a knowledge-enhanced LLM as a strong baseline to generate reliable responses with review guidance/heuristic rectification and summarization. Experimental results show the great advantages of textttSocraticLLM by comparing it with several strong generative models. The codes and datasets are available on urlhttps://github.com/ECNU-ICALK/SocraticMath\}."
