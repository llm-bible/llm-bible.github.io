---
layout: publication
title: Boosting Large Language Models With Socratic Method For Conversational Mathematics Teaching
authors: Ding Yuyang, Hu Hanglei, Zhou Jie, Chen Qin, Jiang Bo, He Liang
conference: "Arxiv"
year: 2024
bibkey: ding2024boosting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.17349"}
  - {name: "Code", url: "https://github.com/ECNU&#45;ICALK/SocraticMath&#125;"}
tags: ['Applications', 'Has Code', 'Reinforcement Learning', 'Survey Paper']
---
With the introduction of large language models (LLMs) automatic math reasoning has seen tremendous success. However current methods primarily focus on providing solutions or using techniques like Chain45;of45;Thought to enhance problem45;solving accuracy. In this paper we focus on improving the capability of mathematics teaching via a Socratic teaching45;based LLM (texttt123;SocraticLLM125;) which guides learners toward profound thinking with clarity and self45;discovery via conversation. We collect and release a high45;quality mathematical teaching dataset named texttt123;SocraticMATH125; which provides Socratic45;style conversations of problems with extra knowledge. Also we propose a knowledge45;enhanced LLM as a strong baseline to generate reliable responses with review guidance/heuristic rectification and summarization. Experimental results show the great advantages of texttt123;SocraticLLM125; by comparing it with several strong generative models. The codes and datasets are available on url123;https://github.com/ECNU&#45;ICALK/SocraticMath&#125;.
