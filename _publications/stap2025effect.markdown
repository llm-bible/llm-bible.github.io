---
layout: publication
title: 'The Effect Of Language Diversity When Fine-tuning Large Language Models For Translation'
authors: David Stap, Christof Monz
conference: "Arxiv"
year: 2025
bibkey: stap2025effect
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.13090'}
tags: ['Fine-Tuning', 'Training Techniques', 'Pretraining Methods']
---
Prior research diverges on language diversity in LLM fine-tuning: Some studies report benefits while others find no advantages. Through controlled fine-tuning experiments across 132 translation directions, we systematically resolve these disparities. We find that expanding language diversity during fine-tuning improves translation quality for both unsupervised and -- surprisingly -- supervised pairs, despite less diverse models being fine-tuned exclusively on these supervised pairs. However, benefits plateau or decrease beyond a certain diversity threshold. We show that increased language diversity creates more language-agnostic representations. These representational adaptations help explain the improved performance in models fine-tuned with greater diversity.
