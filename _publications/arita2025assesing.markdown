---
layout: publication
title: 'Assesing Llms In Art Contexts: Critique Generation And Theory Of Mind Evaluation'
authors: Takaya Arita, Wenxian Zheng, Reiji Suzuki, Fuminori Akiba
conference: "Arxiv"
year: 2025
bibkey: arita2025assesing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.12805"}
tags: ['Prompting', 'Tools']
---
This study explored how large language models (LLMs) perform in two areas
related to art: writing critiques of artworks and reasoning about mental states
(Theory of Mind, or ToM) in art-related situations. For the critique generation
part, we built a system that combines Noel Carroll's evaluative framework with
a broad selection of art criticism theories. The model was prompted to first
write a full-length critique and then shorter, more coherent versions using a
step-by-step prompting process. These AI-generated critiques were then compared
with those written by human experts in a Turing test-style evaluation. In many
cases, human subjects had difficulty telling which was which, and the results
suggest that LLMs can produce critiques that are not only plausible in style
but also rich in interpretation, as long as they are carefully guided. In the
second part, we introduced new simple ToM tasks based on situations involving
interpretation, emotion, and moral tension, which can appear in the context of
art. These go beyond standard false-belief tests and allow for more complex,
socially embedded forms of reasoning. We tested 41 recent LLMs and found that
their performance varied across tasks and models. In particular, tasks that
involved affective or ambiguous situations tended to reveal clearer
differences. Taken together, these results help clarify how LLMs respond to
complex interpretative challenges, revealing both their cognitive limitations
and potential. While our findings do not directly contradict the so-called
Generative AI Paradox--the idea that LLMs can produce expert-like output
without genuine understanding--they suggest that, depending on how LLMs are
instructed, such as through carefully designed prompts, these models may begin
to show behaviors that resemble understanding more closely than we might
assume.
