---
layout: publication
title: 'Enhancing Visual Question Answering Through Ranking-based Hybrid Training And Multimodal Fusion'
authors: Chen Peiyuan, Zhang Zecheng, Dong Yiping, Zhou Li, Wang Han
conference: "Arxiv"
year: 2024
bibkey: chen2024enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.07303"}
tags: ['Applications', 'Attention Mechanism', 'BERT', 'Merging', 'Model Architecture', 'Multimodal Models', 'RAG', 'Security', 'Training Techniques', 'Transformer']
---
Visual Question Answering (VQA) is a challenging task that requires systems
to provide accurate answers to questions based on image content. Current VQA
models struggle with complex questions due to limitations in capturing and
integrating multimodal information effectively. To address these challenges, we
propose the Rank VQA model, which leverages a ranking-inspired hybrid training
strategy to enhance VQA performance. The Rank VQA model integrates high-quality
visual features extracted using the Faster R-CNN model and rich semantic text
features obtained from a pre-trained BERT model. These features are fused
through a sophisticated multimodal fusion technique employing multi-head
self-attention mechanisms. Additionally, a ranking learning module is
incorporated to optimize the relative ranking of answers, thus improving answer
accuracy. The hybrid training strategy combines classification and ranking
losses, enhancing the model's generalization ability and robustness across
diverse datasets. Experimental results demonstrate the effectiveness of the
Rank VQA model. Our model significantly outperforms existing state-of-the-art
models on standard VQA datasets, including VQA v2.0 and COCO-QA, in terms of
both accuracy and Mean Reciprocal Rank (MRR). The superior performance of Rank
VQA is evident in its ability to handle complex questions that require
understanding nuanced details and making sophisticated inferences from the
image and text. This work highlights the effectiveness of a ranking-based
hybrid training strategy in improving VQA performance and lays the groundwork
for further research in multimodal learning methods.
