---
layout: publication
title: 'Enhancing Item Tokenization For Generative Recommendation Through Self-improvement'
authors: Runjin Chen, Mingxuan Ju, Ngoc Bui, Dimosthenis Antypas, Stanley Cai, Xiaopeng Wu, Leonardo Neves, Zhangyang Wang, Neil Shah, Tong Zhao
conference: "Arxiv"
year: 2024
bibkey: chen2024enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.17171"}
tags: ['Tokenization', 'RAG', 'Training Techniques']
---
Generative recommendation systems, driven by large language models (LLMs),
present an innovative approach to predicting user preferences by modeling items
as token sequences and generating recommendations in a generative manner. A
critical challenge in this approach is the effective tokenization of items,
ensuring that they are represented in a form compatible with LLMs. Current item
tokenization methods include using text descriptions, numerical strings, or
sequences of discrete tokens. While text-based representations integrate
seamlessly with LLM tokenization, they are often too lengthy, leading to
inefficiencies and complicating accurate generation. Numerical strings, while
concise, lack semantic depth and fail to capture meaningful item relationships.
Tokenizing items as sequences of newly defined tokens has gained traction, but
it often requires external models or algorithms for token assignment. These
external processes may not align with the LLM's internal pretrained
tokenization schema, leading to inconsistencies and reduced model performance.
To address these limitations, we propose a self-improving item tokenization
method that allows the LLM to refine its own item tokenizations during training
process. Our approach starts with item tokenizations generated by any external
model and periodically adjusts these tokenizations based on the LLM's learned
patterns. Such alignment process ensures consistency between the tokenization
and the LLM's internal understanding of the items, leading to more accurate
recommendations. Furthermore, our method is simple to implement and can be
integrated as a plug-and-play enhancement into existing generative
recommendation systems. Experimental results on multiple datasets and using
various initial tokenization strategies demonstrate the effectiveness of our
method, with an average improvement of 8% in recommendation performance.
