---
layout: publication
title: 'Large-scale Study Of Human Memory For Meaningful Narratives'
authors: Antonios Georgiou, Tankut Can, Mikhail Katkov, Misha Tsodyks
conference: "Arxiv"
year: 2023
bibkey: georgiou2023large
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2311.04742'}
tags: ['Reinforcement Learning']
---
The statistical study of human memory requires large-scale experiments,
involving many stimuli conditions and test subjects. While this approach has
proven to be quite fruitful for meaningless material such as random lists of
words, naturalistic stimuli, like narratives, have until now resisted such a
large-scale study, due to the quantity of manual labor required to design and
analyze such experiments. In this work, we develop a pipeline that uses large
language models (LLMs) both to design naturalistic narrative stimuli for
large-scale recall and recognition memory experiments, as well as to analyze
the results. We performed online memory experiments with a large number of
participants and collected recognition and recall data for narratives of
different sizes. We found that both recall and recognition performance scale
linearly with narrative length; however, for longer narratives people tend to
summarize the content rather than recalling precise details. To investigate the
role of narrative comprehension in memory, we repeated these experiments using
scrambled versions of the narratives. Although recall performance declined
significantly, recognition remained largely unaffected. Recalls in this
condition seem to follow the original narrative order rather than the actual
scrambled presentation, pointing to a contextual reconstruction of the story in
memory. Finally, using LLM text embeddings, we construct a simple measure for
each clause based on semantic similarity to the whole narrative, that shows a
strong correlation with recall probability. Overall, our work demonstrates the
power of LLMs in accessing new regimes in the study of human memory, as well as
suggesting novel psychologically informed benchmarks for LLM performance.
