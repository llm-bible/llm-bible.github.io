---
layout: publication
title: Cotever Chain Of Thought Prompting Annotation Toolkit For Explanation Verification
authors: Kim Seungone, Joo Se June, Jang Yul, Chae Hyungjoo, Yeo Jinyoung
conference: "Arxiv"
year: 2023
bibkey: kim2023chain
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2303.03628"}
  - {name: "Code", url: "https://github.com/SeungoneKim/CoTEVer"}
tags: ['Applications', 'Has Code', 'Interpretability And Explainability', 'Pretraining Methods', 'Prompting']
---
Chain45;of45;thought (CoT) prompting enables large language models (LLMs) to solve complex reasoning tasks by generating an explanation before the final prediction. Despite its promising ability a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation. To improve the correctness of the explanations fine45;tuning language models with explanation data is needed. However there exists only a few datasets that can be used for such approaches and no data collection tool for building them. Thus we introduce CoTEVer a tool45;kit for annotating the factual correctness of generated explanations and collecting revision data of wrong explanations. Furthermore we suggest several use cases where the data collected with CoTEVer can be utilized for enhancing the faithfulness of explanations. Our toolkit is publicly available at https://github.com/SeungoneKim/CoTEVer.
