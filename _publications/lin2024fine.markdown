---
layout: publication
title: Fine45;tuned Network Relies On Generic Representation To Solve Unseen Cognitive Task
authors: Lin Dongyan
conference: "Arxiv"
year: 2024
bibkey: lin2024fine
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.18926"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
Fine45;tuning pretrained language models has shown promising results on a wide range of tasks but when encountering a novel task do they rely more on generic pretrained representation or develop brand new task45;specific solutions Here we fine45;tuned GPT45;2 on a context45;dependent decision45;making task novel to the model but adapted from neuroscience literature. We compared its performance and internal mechanisms to a version of GPT45;2 trained from scratch on the same task. Our results show that fine45;tuned models depend heavily on pretrained representations particularly in later layers while models trained from scratch develop different more task45;specific mechanisms. These findings highlight the advantages and limitations of pretraining for task generalization and underscore the need for further investigation into the mechanisms underpinning task45;specific fine45;tuning in LLMs.
