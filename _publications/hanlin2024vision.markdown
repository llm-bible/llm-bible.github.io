---
layout: publication
title: Vision45;and45;language Navigation Generative Pretrained Transformer
authors: Hanlin Wen
conference: "Arxiv"
year: 2024
bibkey: hanlin2024vision
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.16994"}
tags: ['Agentic', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
In the Vision45;and45;Language Navigation (VLN) field agents are tasked with navigating real45;world scenes guided by linguistic instructions. Enabling the agent to adhere to instructions throughout the process of navigation represents a significant challenge within the domain of VLN. To address this challenge common approaches often rely on encoders to explicitly record past locations and actions increasing model complexity and resource consumption. Our proposal the Vision45;and45;Language Navigation Generative Pretrained Transformer (VLN45;GPT) adopts a transformer decoder model (GPT2) to model trajectory sequence dependencies bypassing the need for historical encoding modules. This method allows for direct historical information access through trajectory sequence enhancing efficiency. Furthermore our model separates the training process into offline pre45;training with imitation learning and online fine45;tuning with reinforcement learning. This distinction allows for more focused training objectives and improved performance. Performance assessments on the VLN dataset reveal that VLN45;GPT surpasses complex state45;of45;the45;art encoder45;based models.
