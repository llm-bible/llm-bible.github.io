---
layout: publication
title: CLIP Models Are Few45;shot Learners Empirical Studies On VQA And Visual Entailment
authors: Song Haoyu, Dong Li, Zhang Wei-nan, Liu Ting, Wei Furu
conference: "Arxiv"
year: 2022
bibkey: song2022clip
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2203.07190"}
tags: ['Applications', 'RAG', 'Training Techniques']
---
CLIP has shown a remarkable zero45;shot capability on a wide range of vision tasks. Previously CLIP is only regarded as a powerful visual encoder. However after being pre45;trained by language supervision from a large amount of image45;caption pairs CLIP itself should also have acquired some few45;shot abilities for vision45;language tasks. In this work we empirically show that CLIP can be a strong vision45;language few45;shot learner by leveraging the power of language. We first evaluate CLIPs zero45;shot performance on a typical visual question answering task and demonstrate a zero45;shot cross45;modality transfer capability of CLIP on the visual entailment task. Then we propose a parameter45;efficient fine45;tuning strategy to boost the few45;shot performance on the vqa task. We achieve competitive zero/few45;shot results on the visual question answering and visual entailment tasks without introducing any additional pre45;training procedure.
