---
layout: publication
title: Empower Nested Boolean Logic Via Self45;supervised Curriculum Learning
authors: Wu Hongqiu, Liu Linfeng, Zhao Hai, Zhang Min
conference: "Arxiv"
year: 2023
bibkey: wu2023empower
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.05450"}
tags: ['Reinforcement Learning', 'Training Techniques']
---
Beyond the great cognitive powers showcased by language models it is crucial to scrutinize whether their reasoning capabilities stem from strong generalization or merely exposure to relevant data. As opposed to constructing increasingly complex logic this paper probes into the boolean logic the root capability of a logical reasoner. We find that any pre45;trained language models even including large language models only behave like a random selector in the face of multi45;nested boolean logic a task that humans can handle with ease. To empower language models with this fundamental capability this paper proposes a new self45;supervised learning method textit123;Curriculum Logical Reasoning125; (textsc123;Clr125;) where we augment the training data with nested boolean logic chain step45;by45;step and program the training from simpler logical patterns gradually to harder ones. This new training paradigm allows language models to effectively generalize to much harder and longer45;hop logic which can hardly be learned through naive training. Furthermore we show that boolean logic is a great foundation for improving the subsequent general logical tasks.
