---
layout: publication
title: Can Large Language Models Generate High45;quality Patent Claims
authors: Jiang Lekang, Zhang Caiqi, Scherz Pascal A, Goetz Stephan
conference: "Arxiv"
year: 2024
bibkey: jiang2024can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.19465"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'RAG', 'Security']
---
Large language models (LLMs) have shown exceptional performance across various text generation tasks but remain under45;explored in the patent domain which offers highly structured and precise language. This paper constructs a dataset to investigate the performance of current LLMs in patent claim generation. Our results demonstrate that generating claims based on patent descriptions outperforms previous research relying on abstracts. Interestingly current patent45;specific LLMs perform much worse than state45;of45;the45;art general LLMs highlighting the necessity for future research on in45;domain LLMs. We also find that LLMs can produce high45;quality first independent claims but their performances markedly decrease for subsequent dependent claims. Moreover fine45;tuning can enhance the completeness of inventions features conceptual clarity and feature linkage. Among the tested LLMs GPT45;4 demonstrates the best performance in comprehensive human evaluations by patent experts with better feature coverage conceptual clarity and technical coherence. Despite these capabilities comprehensive revision and modification are still necessary to pass rigorous patent scrutiny and ensure legal robustness.
