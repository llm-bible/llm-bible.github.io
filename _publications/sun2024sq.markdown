---
layout: publication
title: Sq45;llava Self45;questioning For Large Vision45;language Assistant
authors: Sun Guohao, Qin Can, Wang Jiamian, Chen Zeyuan, Xu Ran, Tao Zhiqiang
conference: "Arxiv"
year: 2024
bibkey: sun2024sq
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.11299"}
tags: ['Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Recent advances in vision45;language models have shown notable generalization in broad tasks through visual instruction tuning. However bridging the gap between the pre45;trained vision encoder and the large language models (LLMs) becomes the whole networks bottleneck. To improve cross45;modality alignment existing works usually consider more visual instruction data covering a broader range of vision tasks to fine45;tune the model for question45;answering which however is costly to obtain and has not thoroughly explored the rich contextual information contained in images. This paper first attempts to harness the overlooked context within visual instruction data training the model to self45;supervised learning how to ask high45;quality questions. In this way we introduce a novel framework named SQ45;LLaVA Self45;Questioning for Large Vision45;Language Assistant. SQ45;LLaVA exhibits proficiency in generating flexible and meaningful image45;related questions while analyzing the visual clue and prior language knowledge signifying an advanced level of generalized visual understanding. Moreover fine45;tuning SQ45;LLaVA on higher45;quality instruction data shows a performance improvement compared with traditional visual45;instruction tuning methods. This improvement highlights the efficacy of self45;questioning techniques in achieving a deeper and more nuanced comprehension of visual content across various contexts.
