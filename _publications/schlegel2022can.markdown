---
layout: publication
title: 'Can Transformers Reason In Fragments Of Natural Language?'
authors: Viktor Schlegel, Kamen V. Pavlov, Ian Pratt-hartmann
conference: "Arxiv"
year: 2022
bibkey: schlegel2022can
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2211.05417'}
tags: ['RAG', 'Transformer', 'Model Architecture', 'Pretraining Methods']
---
State-of-the-art deep-learning-based approaches to Natural Language
Processing (NLP) are credited with various capabilities that involve reasoning
with natural language texts. In this paper we carry out a large-scale empirical
study investigating the detection of formally valid inferences in controlled
fragments of natural language for which the satisfiability problem becomes
increasingly complex. We find that, while transformer-based language models
perform surprisingly well in these scenarios, a deeper analysis re-veals that
they appear to overfit to superficial patterns in the data rather than
acquiring the logical principles governing the reasoning in these fragments.
