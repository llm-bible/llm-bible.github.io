---
layout: publication
title: Moral Moe Augmented Lora For Llms Lifelong Learning
authors: Yang Shu, Ali Muhammad Asif, Wang Cheng-long, Hu Lijie, Wang Di
conference: "Arxiv"
year: 2024
bibkey: yang2024moe
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.11260"}
tags: ['Fine Tuning', 'Reinforcement Learning']
---
Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper we propose MoRAL i.e. Mixture45;of45;Experts augmented Low45;Rank Adaptation for Lifelong Learning. MoRAL combines the multi45;tasking abilities of MoE with the fine45;tuning abilities of LoRA for effective life45;long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question45;answer pairs which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings we introduce a new evaluation benchmark namely Life Long Learning of LLM (5L45;bench) encompassing a newly curated dataset of question45;answer pairs and a set of evaluation metrics for rigorous evaluation of MoRAL in open45;book and closed45;book settings. Experimental evaluation shows (i) LLMs learn fast in open45;book settings with up to 30.1537; improvement in RA for Phi45;245;2.7B compared to closed45;book (for models fine45;tuned with MoRAL); (ii) MoRAL shows higher performance improvement for models with a greater number of parameters; (iii) MoRAL is robust to catastrophic forgetting offering better knowledge retention compared to baselines.
