---
layout: publication
title: 'A Neural Conversation Generation Model Via Equivalent Shared Memory Investigation'
authors: Ji Changzhen, Zhang Yating, Liu Xiaozhong, Jatowt Adam, Sun Changlong, Zhu Conghui, Zhao Tiejun
conference: "Arxiv"
year: 2021
bibkey: ji2021neural
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2108.09164"}
tags: ['Attention Mechanism', 'Model Architecture', 'Tools']
---
Conversation generation as a challenging task in Natural Language Generation
(NLG) has been increasingly attracting attention over the last years. A number
of recent works adopted sequence-to-sequence structures along with external
knowledge, which successfully enhanced the quality of generated conversations.
Nevertheless, few works utilized the knowledge extracted from similar
conversations for utterance generation. Taking conversations in customer
service and court debate domains as examples, it is evident that essential
entities/phrases, as well as their associated logic and inter-relationships can
be extracted and borrowed from similar conversation instances. Such information
could provide useful signals for improving conversation generation. In this
paper, we propose a novel reading and memory framework called Deep Reading
Memory Network (DRMN) which is capable of remembering useful information of
similar conversations for improving utterance generation. We apply our model to
two large-scale conversation datasets of justice and e-commerce fields.
Experiments prove that the proposed model outperforms the state-of-the-art
approaches.
