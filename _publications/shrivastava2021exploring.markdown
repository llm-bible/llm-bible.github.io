---
layout: publication
title: Exploring Low45;cost Transformer Model Compression For Large45;scale Commercial Reply Suggestions
authors: Shrivastava Vaishnavi, Gaonkar Radhika, Gupta Shashank, Jha Abhishek
conference: "Arxiv"
year: 2021
bibkey: shrivastava2021exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2111.13999"}
tags: ['Applications', 'Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Quantization', 'Security', 'Training Techniques', 'Transformer']
---
Fine45;tuning pre45;trained language models improves the quality of commercial reply suggestion systems but at the cost of unsustainable training times. Popular training time reduction approaches are resource intensive thus we explore low45;cost model compression techniques like Layer Dropping and Layer Freezing. We demonstrate the efficacy of these techniques in large45;data scenarios enabling the training time reduction for a commercial email reply suggestion system by 4237; without affecting the model relevance or user engagement. We further study the robustness of these techniques to pre45;trained model and dataset size ablation and share several insights and recommendations for commercial applications.
