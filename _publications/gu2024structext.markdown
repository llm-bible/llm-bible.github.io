---
layout: publication
title: Structext45;eval An Autogenerated Benchmark For Evaluating Large Language Models Ability In Structure45;rich Text Understanding
authors: Gu Zhouhong, Ye Haoning, Zhou Zeyang, Feng Hongwei, Xiao Yanghua
conference: "Arxiv"
year: 2024
bibkey: gu2024structext
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.10621"}
  - {name: "Code", url: "https://github.com/MikeGu721/StrucText&#45;Eval"}
tags: ['Has Code', 'Pretraining Methods']
---
Given the substantial volumes of structured data held by many companies enabling Large Language Models (LLMs) to directly understand structured text in non45;structured forms could significantly enhance their capabilities across various business scenarios. To this end we propose evaluation data generation method for assessing LLMs ability in understanding the structure45;rich text which generates structured data of controllable complexity based on manually crafted question templates and generation rules. Building on this generation method we introduce StrucText45;Eval a benchmark comprising 6032 questions across 8 different structured languages and 29 specific tasks. Furthermore considering human proficiency in rule45;based tasks we also present StrucText45;Eval45;Hard which includes 3016 questions designed to further examine the gap between LLMs and human performance. Results indicate that the best45;performing LLM currently achieve an accuracy of 65.037; on StrucText45;Eval45;Hard while human accuracy reaches up to 95.737;. Moreover while fine45;tuning using StrucText45;Eval can enhance existing LLMs understanding of all structured languages it does not necessarily improve performance across all task types. The benchmark and generation codes are open sourced in https://github.com/MikeGu721/StrucText&#45;Eval
