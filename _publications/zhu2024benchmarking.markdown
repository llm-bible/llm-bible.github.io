---
layout: publication
title: 'Benchmarking And Improving Large Vision-language Models For Fundamental Visual Graph Understanding And Reasoning'
authors: Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, Min Zhang
conference: "Arxiv"
year: 2024
bibkey: zhu2024benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.13540"}
tags: ['Fine-Tuning', 'Tools', 'Reinforcement Learning', 'Security', 'Training Techniques', 'Pretraining Methods', 'Multimodal Models']
---
Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across diverse tasks. Despite great success, recent studies show that LVLMs
encounter substantial limitations when engaging with visual graphs. To study
the reason behind these limitations, we propose VGCure, a comprehensive
benchmark covering 22 tasks for examining the fundamental graph understanding
and reasoning capacities of LVLMs. Extensive evaluations conducted on 14 LVLMs
reveal that LVLMs are weak in basic graph understanding and reasoning tasks,
particularly those concerning relational or structurally complex information.
Based on this observation, we propose a structure-aware fine-tuning framework
to enhance LVLMs with structure learning abilities through three
self-supervised learning tasks. Experiments validate the effectiveness of our
method in improving LVLMs' performance on fundamental and downstream graph
learning tasks, as well as enhancing their robustness against complex visual
graphs.
