---
layout: publication
title: An Empirical Study Of Instruction45;tuning Large Language Models In Chinese
authors: Si Qingyi, Wang Tong, Lin Zheng, Zhang Xu, Cao Yanan, Wang Weiping
conference: "Arxiv"
year: 2023
bibkey: si2023empirical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.07328"}
  - {name: "Code", url: "https://github.com/PhoebusSi/Alpaca&#45;CoT"}
tags: ['Efficiency And Optimization', 'GPT', 'Has Code', 'Model Architecture', 'Reinforcement Learning']
---
The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently the release of LLMs has sparked the open45;source communitys interest in instruction45;tuning which is deemed to accelerate ChatGPTs replication process. However research on instruction45;tuning LLMs in Chinese the worlds most spoken language is still in its early stages. Therefore this paper makes an in45;depth empirical study of instruction45;tuning LLMs in Chinese which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically we systematically explore the impact of LLM bases parameter45;efficient methods instruction data types which are the three most important elements for instruction45;tuning. Besides we also conduct experiment to study the impact of other factors e.g. chain45;of45;thought data and human45;value alignment. We hope that this empirical study can make a modest contribution to the open Chinese version of ChatGPT. This paper will release a powerful Chinese LLMs that is comparable to ChatGLM. The code and data are available at https://github.com/PhoebusSi/Alpaca&#45;CoT.
