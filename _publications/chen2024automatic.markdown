---
layout: publication
title: 'An Automatic And Cost-efficient Peer-review Framework For Language Generation Evaluation'
authors: Junjie Chen, Weihang Su, Zhumin Chu, Haitao Li, Qinyao Ai, Yiqun Liu, Min Zhang, Shaoping Ma
conference: "Arxiv"
year: 2024
bibkey: chen2024automatic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.12265"}
tags: ['Efficiency and Optimization', 'Survey Paper', 'Tools', 'Ethics and Bias', 'Prompting']
---
With the rapid development of large language models (LLMs), how to
efficiently evaluate them has become an important research question. Existing
evaluation methods often suffer from high costs, limited test formats, the need
of human references, and systematic evaluation biases. To address these
limitations, our study introduces the Auto-PRE, an automatic LLM evaluation
framework based on peer review. In contrast to previous studies that rely on
human annotations, Auto-PRE selects evaluator LLMs automatically based on their
inherent traits including consistency, self-confidence, and pertinence. We
conduct extensive experiments on three tasks: summary generation, non-factoid
question-answering, and dialogue generation. Experimental results indicate our
Auto-PRE achieves state-of-the-art performance at a lower cost. Moreover, our
study highlights the impact of prompt strategies and evaluation formats on
evaluation performance, offering guidance for method optimization in the
future.
