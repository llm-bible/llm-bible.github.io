---
layout: publication
title: H2o45;danube3 Technical Report
authors: Pfeiffer Pascal, Singer Philipp, Babakhin Yauhen, Fodor Gabor, Dhankhar Nischay, Ambati Sri Satish
conference: "Arxiv"
year: 2024
bibkey: pfeiffer2024technical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.09276"}
tags: ['Model Architecture', 'Pretraining Methods', 'Tools']
---
We present H2O45;Danube3 a series of small language models consisting of H2O45;Danube345;4B trained on 6T tokens and H2O45;Danube345;500M trained on 4T tokens. Our models are pre45;trained on high quality Web data consisting of primarily English tokens in three stages with different data mixes before final supervised tuning for chat version. The models exhibit highly competitive metrics across a multitude of academic chat and fine45;tuning benchmarks. Thanks to its compact architecture H2O45;Danube3 can be efficiently run on a modern smartphone enabling local inference and rapid processing capabilities even on mobile devices. We make all models openly available under Apache 2.0 license further democratizing LLMs to a wider audience economically.
