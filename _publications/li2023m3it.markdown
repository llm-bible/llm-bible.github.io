---
layout: publication
title: M^3IT A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning
authors: Li Lei, Yin Yuwei, Li Shicheng, Chen Liang, Wang Peiyi, Ren Shuhuai, Li Mukai, Yang Yazheng, Xu Jingjing, Sun Xu, Kong Lingpeng, Liu Qi
conference: "Arxiv"
year: 2023
bibkey: li2023m3it
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.04387"}
tags: ['GPT', 'Model Architecture', 'Multimodal Models', 'RAG', 'Reinforcement Learning']
---
Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT enabling them to align with human instructions across diverse tasks. However progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field we introduce the Multi-Modal Multilingual Instruction Tuning (M^3IT) dataset designed to optimize VLM alignment with human instructions. Our M^3IT dataset comprises 40 carefully curated datasets including 2.4 million instances and 400 manually written task instructions reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system ensuring broader accessibility. M^3IT surpasses previous datasets regarding task coverage instruction number and instance scale. Moreover we develop Ying-VLM a VLM model trained on our M^3IT dataset showcasing its potential to answer complex questions requiring world knowledge generalize to unseen video tasks and comprehend unseen instructions in Chinese. We have open-sourced the dataset to encourage further research.
