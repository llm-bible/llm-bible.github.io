---
layout: publication
title: Integrating Summarization And Retrieval For Enhanced Personalization Via Large Language Models
authors: Richardson Chris, Zhang Yao, Gillespie Kellen, Kar Sudipta, Singh Arshdeep, Raeesy Zeynab, Khan Omar Zia, Sethy Abhinav
conference: "Arxiv"
year: 2023
bibkey: richardson2023integrating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.20081"}
tags: ['Applications', 'Prompting', 'RAG', 'Reinforcement Learning']
---
Personalization the ability to tailor a system to individual users is an essential factor in user experience with natural language processing (NLP) systems. With the emergence of Large Language Models (LLMs) a key question is how to leverage these models to better personalize user experiences. To personalize a language models output a straightforward approach is to incorporate past user data into the language model prompt but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. Existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. However retrieval45;based methods are limited by potential information loss lack of more profound user understanding and cold45;start challenges. To overcome these limitations we propose a novel summary45;augmented approach by extending retrieval45;augmented personalization with task45;aware user summaries generated by LLMs. The summaries can be generated and stored offline enabling real45;world systems with runtime constraints like voice assistants to leverage the power of LLMs. Experiments show our method with 7537; less of retrieved user data is on45;par or outperforms retrieval augmentation on most tasks in the LaMP personalization benchmark. We demonstrate that offline summarization via LLMs and runtime retrieval enables better performance for personalization on a range of tasks under practical constraints.
