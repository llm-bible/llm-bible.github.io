---
layout: publication
title: 'Can Llms Detect Intrinsic Hallucinations In Paraphrasing And Machine Translation?'
authors: Evangelia Gogoulou, Shorouq Zahra, Liane Guillou, Luise DÃ¼rlich, Joakim Nivre
conference: "Arxiv"
year: 2025
bibkey: gogoulou2025can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.20699"}
tags: ['Prompting', 'Applications']
---
A frequently observed problem with LLMs is their tendency to generate output
that is nonsensical, illogical, or factually incorrect, often referred to
broadly as hallucination. Building on the recently proposed HalluciGen task for
hallucination detection and generation, we evaluate a suite of open-access LLMs
on their ability to detect intrinsic hallucinations in two conditional
generation tasks: translation and paraphrasing. We study how model performance
varies across tasks and language and we investigate the impact of model size,
instruction tuning, and prompt choice. We find that performance varies across
models but is consistent across prompts. Finally, we find that NLI models
perform comparably well, suggesting that LLM-based detectors are not the only
viable option for this specific task.
