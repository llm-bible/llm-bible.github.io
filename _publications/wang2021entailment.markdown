---
layout: publication
title: Entailment As Few45;shot Learner
authors: Wang Sinong, Fang Han, Khabsa Madian, Mao Hanzi, Ma Hao
conference: "Arxiv"
year: 2021
bibkey: wang2021entailment
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2104.14690"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
Large pre45;trained language models (LMs) have demonstrated remarkable ability as few45;shot learners. However their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper we propose a new approach named as EFL that can turn small LMs into better few45;shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one and then fine45;tune the model with as little as 8 examples. We further demonstrate our proposed method can be (i) naturally combined with an unsupervised contrastive learning45;based data augmentation method; (ii) easily extended to multilingual few45;shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few45;shot learning methods by 1237; and yields competitive few45;shot performance with 500 times larger models such as GPT45;3.
