---
layout: publication
title: 'Assessing The Performance Of Human-capable Llms -- Are Llms Coming For Your Job?'
authors: John Mavi, Nathan Summers, Sergio Coronado
conference: "Arxiv"
year: 2024
bibkey: mavi2024assessing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.16285'}
tags: ['Agentic', 'RAG', 'Reinforcement Learning', 'Ethics and Bias', 'Interpretability']
---
The current paper presents the development and validation of SelfScore, a
novel benchmark designed to assess the performance of automated Large Language
Model (LLM) agents on help desk and professional consultation tasks. Given the
increasing integration of AI in industries, particularly within customer
service, SelfScore fills a crucial gap by enabling the comparison of automated
agents and human workers. The benchmark evaluates agents on problem complexity
and response helpfulness, ensuring transparency and simplicity in its scoring
system. The study also develops automated LLM agents to assess SelfScore and
explores the benefits of Retrieval-Augmented Generation (RAG) for
domain-specific tasks, demonstrating that automated LLM agents incorporating
RAG outperform those without. All automated LLM agents were observed to perform
better than the human control group. Given these results, the study raises
concerns about the potential displacement of human workers, especially in areas
where AI technologies excel. Ultimately, SelfScore provides a foundational tool
for understanding the impact of AI in help desk environments while advocating
for ethical considerations in the ongoing transition towards automation.
