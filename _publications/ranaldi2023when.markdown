---
layout: publication
title: 'When Large Language Models Contradict Humans? Large Language Models'' Sycophantic Behaviour'
authors: Leonardo Ranaldi, Giulia Pucci
conference: "Arxiv"
year: 2023
bibkey: ranaldi2023when
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2311.09410'}
tags: ['Reinforcement Learning', 'Ethics and Bias', 'Security', 'Prompting']
---
Large Language Models have been demonstrating the ability to solve complex
tasks by delivering answers that are positively evaluated by humans due in part
to the intensive use of human feedback that refines responses. However, the
suggestibility transmitted through human feedback increases the inclination to
produce responses that correspond to the users' beliefs or misleading prompts
as opposed to true facts, a behaviour known as sycophancy. This phenomenon
decreases the bias, robustness, and, consequently, their reliability. In this
paper, we shed light on the suggestibility of Large Language Models (LLMs) to
sycophantic behaviour, demonstrating these tendencies via human-influenced
prompts over different tasks. Our investigation reveals that LLMs show
sycophantic tendencies when responding to queries involving subjective opinions
and statements that should elicit a contrary response based on facts. In
contrast, when confronted with mathematical tasks or queries that have an
objective answer, these models at various scales seem not to follow the users'
hints by demonstrating confidence in delivering the correct answers.
