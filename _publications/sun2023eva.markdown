---
layout: publication
title: EVA45;CLIP Improved Training Techniques For CLIP At Scale
authors: Sun Quan, Fang Yuxin, Wu Ledell, Wang Xinlong, Cao Yue
conference: "Arxiv"
year: 2023
bibkey: sun2023eva
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2303.15389"}
  - {name: "Code", url: "https://github.com/baaivision/EVA/tree/master/EVA&#45;CLIP"}
tags: ['Attention Mechanism', 'Efficiency And Optimization', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
Contrastive language45;image pre45;training CLIP for short has gained increasing attention for its potential in various scenarios. In this paper we propose EVA45;CLIP a series of models that significantly improve the efficiency and effectiveness of CLIP training. Our approach incorporates new techniques for representation learning optimization and augmentation enabling EVA45;CLIP to achieve superior performance compared to previous CLIP models with the same number of parameters but significantly smaller training costs. Notably our largest 5.0B45;parameter EVA45;0245;CLIP45;E/14+ with only 9 billion seen samples achieves 82.0 zero45;shot top45;1 accuracy on ImageNet45;1K val. A smaller EVA45;0245;CLIP45;L/14+ with only 430 million parameters and 6 billion seen samples achieves 80.4 zero45;shot top45;1 accuracy on ImageNet45;1K val. To facilitate open access and open research we release the complete suite of EVA45;CLIP to the community at https://github.com/baaivision/EVA/tree/master/EVA&#45;CLIP.
