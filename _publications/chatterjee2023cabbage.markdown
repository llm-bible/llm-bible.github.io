---
layout: publication
title: 'Cabbage Sweeter Than Cake? Analysing The Potential Of Large Language Models For Learning Conceptual Spaces'
authors: Usashi Chatterjee, Amit Gajbhiye, Steven Schockaert
conference: "Arxiv"
year: 2023
bibkey: chatterjee2023cabbage
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.05481"}
tags: ['Model Architecture', 'Tools', 'RAG', 'GPT', 'BERT', 'Applications']
---
The theory of Conceptual Spaces is an influential cognitive-linguistic
framework for representing the meaning of concepts. Conceptual spaces are
constructed from a set of quality dimensions, which essentially correspond to
primitive perceptual features (e.g. hue or size). These quality dimensions are
usually learned from human judgements, which means that applications of
conceptual spaces tend to be limited to narrow domains (e.g. modelling colour
or taste). Encouraged by recent findings about the ability of Large Language
Models (LLMs) to learn perceptually grounded representations, we explore the
potential of such models for learning conceptual spaces. Our experiments show
that LLMs can indeed be used for learning meaningful representations to some
extent. However, we also find that fine-tuned models of the BERT family are
able to match or even outperform the largest GPT-3 model, despite being 2 to 3
orders of magnitude smaller.
