---
layout: publication
title: 'Assessing Large Language Models For Automated Feedback Generation In Learning Programming Problem Solving'
authors: Priscylla Silva, Evandro Costa
conference: "Arxiv"
year: 2025
bibkey: silva2025assessing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.14630"}
tags: ['Tools', 'GPT', 'Applications', 'Interpretability and Explainability', 'Model Architecture', 'Reinforcement Learning']
---
Providing effective feedback is important for student learning in programming
problem-solving. In this sense, Large Language Models (LLMs) have emerged as
potential tools to automate feedback generation. However, their reliability and
ability to identify reasoning errors in student code remain not well
understood. This study evaluates the performance of four LLMs (GPT-4o, GPT-4o
mini, GPT-4-Turbo, and Gemini-1.5-pro) on a benchmark dataset of 45 student
solutions. We assessed the models' capacity to provide accurate and insightful
feedback, particularly in identifying reasoning mistakes. Our analysis reveals
that 63% of feedback hints were accurate and complete, while 37% contained
mistakes, including incorrect line identification, flawed explanations, or
hallucinated issues. These findings highlight the potential and limitations of
LLMs in programming education and underscore the need for improvements to
enhance reliability and minimize risks in educational applications.
