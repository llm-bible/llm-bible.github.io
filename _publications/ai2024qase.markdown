---
layout: publication
title: QASE Enhanced PLMs Improved Control in Text Generation for MRC
authors: Ai Lin, Hui Zheng, Liu Zizhou, Hirschberg Julia
conference: "Arxiv"
year: 2024
bibkey: ai2024qase
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.04771"}
tags: ['Applications', 'Fine Tuning', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
To address the challenges of out-of-control generation in generative models for machine reading comprehension (MRC) we introduce the Question-Attended Span Extraction (QASE) module. Integrated during the fine-tuning of pre-trained generative language models (PLMs) QASE enables these PLMs to match SOTA extractive methods and outperform leading LLMs like GPT-4 in MRC tasks without significant increases in computational costs.
