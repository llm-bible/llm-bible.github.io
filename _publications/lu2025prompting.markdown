---
layout: publication
title: 'Prompting Is Not All You Need! Evaluating LLM Agent Simulation Methodologies With Real-world Online Customer Behavior Data'
authors: Yuxuan Jessie Lu, Jing Jessie Huang, Yan Jessie Han, Bingsheng Jessie Yao, Sisong Jessie Bei, Jiri Jessie Gesi, Yaochen Jessie Xie, Jessie Zheshen, Wang, Qi He, Dakuo Wang
conference: "Arxiv"
year: 2025
bibkey: lu2025prompting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.20749"}
tags: ['Fine-Tuning', 'Agentic', 'RAG', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods', 'Prompting']
---
Recent research shows that LLMs can simulate ``believable'' human behaviors to power LLM agents via prompt-only methods. In this work, we focus on evaluating LLM's objective ``accuracy'' rather than the subjective ``believability'' in simulating human behavior, leveraging a large-scale, real-world dataset collected from customers' online shopping actions. We present the first comprehensive evaluation of state-of-the-art LLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web shopping action generation. Our results show that out-of-the-box LLM-generated actions are often misaligned with actual human behavior, whereas fine-tuning LLMs on real-world behavioral data substantially improves their ability to generate accurate actions compared to prompt-only methods. Furthermore, incorporating synthesized reasonings into model training leads to additional performance gains, demonstrating the value of explicit rationale in behavior modeling. This work evaluates state-of-the-art LLMs in behavior simulation and provides actionable insights into how real-world action data can enhance the fidelity of LLM agents.
