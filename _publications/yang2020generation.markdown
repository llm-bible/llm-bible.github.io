---
layout: publication
title: "On The Generation Of Medical Dialogues For COVID-19"
authors: Yang Wenmian, Zeng Guangtao, Tan Bowen, Ju Zeqian, Chakravorty Subrato, He Xuehai, Chen Shu, Yang Xingyi, Wu Qingyang, Yu Zhou, Xing Eric, Xie Pengtao
conference: "Arxiv"
year: 2020
bibkey: yang2020generation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2005.05442"}
  - {name: "Code", url: "https://github.com/UCSD-AI4H/COVID-Dialogue"}
tags: ['BERT', 'Fine Tuning', 'GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Transformer']
---
Under the pandemic of COVID-19 people experiencing COVID19-related symptoms or exposed to risk factors have a pressing need to consult doctors. Due to hospital closure a lot of consulting services have been moved online. Because of the shortage of medical professionals many people cannot receive online consultations timely. To address this problem we aim to develop a medical dialogue system that can provide COVID19-related consultations. We collected two dialogue datasets -- CovidDialog -- (in English and Chinese respectively) containing conversations between doctors and patients about COVID-19. On these two datasets we train several dialogue generation models based on Transformer GPT and BERT-GPT. Since the two COVID-19 dialogue datasets are small in size which bear high risk of overfitting we leverage transfer learning to mitigate data deficiency. Specifically we take the pretrained models of Transformer GPT and BERT-GPT on dialog datasets and other large-scale texts then finetune them on our CovidDialog tasks. We perform both automatic and human evaluation of responses generated by these models. The results show that the generated responses are promising in being doctor-like relevant to the conversation history and clinically informative. The data and code are available at https://github.com/UCSD-AI4H/COVID-Dialogue."
