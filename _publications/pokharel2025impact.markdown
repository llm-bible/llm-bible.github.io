---
layout: publication
title: 'The Impact Of Model Scaling On Seen And Unseen Language Performance'
authors: Rhitabrat Pokharel, Sina Bagheri Nezhad, Ameeta Agrawal, Suresh Singh
conference: "Arxiv"
year: 2025
bibkey: pokharel2025impact
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.05629"}
tags: ['Tools', 'Applications', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods', 'Scaling Laws', 'Few-Shot']
---
The rapid advancement of Large Language Models (LLMs), particularly those
trained on multilingual corpora, has intensified the need for a deeper
understanding of their performance across a diverse range of languages and
model sizes. Our research addresses this critical need by studying the
performance and scaling behavior of multilingual LLMs in text classification
and machine translation tasks across 204 languages. We systematically examine
both seen and unseen languages across three model families of varying sizes in
zero-shot and few-shot settings. Our findings show significant differences in
scaling behavior between zero-shot and two-shot scenarios, with striking
disparities in performance between seen and unseen languages. Model scale has
little effect on zero-shot performance, which remains mostly flat. However, in
two-shot settings, larger models show clear linear improvements in multilingual
text classification. For translation tasks, however, only the instruction-tuned
model showed clear benefits from scaling. Our analysis also suggests that
overall resource levels, not just the proportions of pretraining languages, are
better predictors of model performance, shedding light on what drives
multilingual LLM effectiveness.
