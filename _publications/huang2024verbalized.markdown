---
layout: publication
title: 'Verbalized Probabilistic Graphical Modeling'
authors: Hengguan Huang, Xing Shen, Songtao Wang, Lingfa Meng, Dianbo Liu, Hao Wang, Samir Bhatt
conference: "Arxiv"
year: 2024
bibkey: huang2024verbalized
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.05516"}
tags: ['Training Techniques', 'Tools', 'Reinforcement Learning', 'Language Modeling', 'Prompting', 'Applications']
---
Human cognition excels at transcending sensory input and forming latent
representations that structure our understanding of the world. Although Large
Language Models (LLMs) can produce chain-of-thought reasoning, they lack a
principled framework to capture latent structures and model uncertainty,
especially in compositional reasoning tasks. We propose Verbalized
Probabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that
guides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)
in natural language. Unlike many traditional probabilistic methods requiring
substantial domain expertise or specialized training, vPGM bypasses
expert-driven model design, making it well-suited for scenarios with limited
assumptions or scarce data. We evaluated our model on several compositional
reasoning tasks, both close-ended and open-ended. Our results indicate that the
model effectively enhances confidence calibration and text generation quality.
