---
layout: publication
title: Mt45;bench45;101 A Fine45;grained Benchmark For Evaluating Large Language Models In Multi45;turn Dialogues
authors: Bai Ge, Liu Jie, Bu Xingyuan, He Yancheng, Liu Jiaheng, Zhou Zhanhui, Lin Zhuoran, Su Wenbo, Ge Tiezheng, Zheng Bo, Ouyang Wanli
conference: "Arxiv"
year: 2024
bibkey: bai2024mt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.14762"}
  - {name: "Code", url: "https://github.com/mtbench101/mt&#45;bench&#45;101&#125;"}
tags: ['Applications', 'Has Code', 'Reinforcement Learning']
---
The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single45;turn dialogues or provided coarse45;grained and incomplete assessments of multi45;turn dialogues overlooking the complexity and fine45;grained nuances of real45;life dialogues. To address this issue we introduce MT45;Bench45;101 specifically designed to evaluate the fine45;grained abilities of LLMs in multi45;turn dialogues. By conducting a detailed analysis of real multi45;turn dialogue data we construct a three45;tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi45;turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT45;Bench45;101 conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat45;specific designs has led to obvious enhancements in the multi45;turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi45;turn abilities. The data and code are available at url123;https://github.com/mtbench101/mt&#45;bench&#45;101&#125;.
