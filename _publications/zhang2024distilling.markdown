---
layout: publication
title: Distilling Implicit Multimodal Knowledge Into Llms For Zero45;resource Dialogue Generation
authors: Zhang Bo, Ma Hui, Ding Jian, Wang Jian, Xu Bo, Lin Hongfei
conference: "Arxiv"
year: 2024
bibkey: zhang2024distilling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.10121"}
tags: ['Distillation', 'Efficiency And Optimization', 'Merging', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'RAG', 'Tools', 'Transformer']
---
Integrating multimodal knowledge into large language models (LLMs) represents a significant advancement in dialogue generation capabilities. However the effective incorporation of such knowledge in zero45;resource scenarios remains a substantial challenge due to the scarcity of diverse high45;quality dialogue datasets. To address this we propose the Visual Implicit Knowledge Distillation Framework (VIKDF) an innovative approach aimed at enhancing LLMs for enriched dialogue generation in zero45;resource contexts by leveraging implicit multimodal knowledge. VIKDF comprises two main stages knowledge distillation using an Implicit Query Transformer to extract and encode visual implicit knowledge from image45;text pairs into knowledge vectors; and knowledge integration employing a novel Bidirectional Variational Information Fusion technique to seamlessly integrate these distilled vectors into LLMs. This enables the LLMs to generate dialogues that are not only coherent and engaging but also exhibit a deep understanding of the context through implicit multimodal cues effectively overcoming the limitations of zero45;resource scenarios. Our extensive experimentation across two dialogue datasets shows that VIKDF outperforms existing state45;of45;the45;art models in generating high45;quality dialogues. The code will be publicly available following acceptance.
