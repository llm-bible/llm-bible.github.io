---
layout: publication
title: 'Omniscience: A Domain-specialized LLM For Scientific Reasoning And Discovery'
authors: Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu
conference: "Arxiv"
year: 2025
bibkey: prabhakar2025domain
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.17604"}
tags: ['Agentic', 'Efficiency and Optimization', 'Training Techniques', 'Distillation', 'Pretraining Methods', 'Fine-Tuning']
---
Large Language Models (LLMs) have demonstrated remarkable potential in
advancing scientific knowledge and addressing complex challenges. In this work,
we introduce OmniScience, a specialized large reasoning model for general
science, developed through three key components: (1) domain adaptive
pretraining on a carefully curated corpus of scientific literature, (2)
instruction tuning on a specialized dataset to guide the model in following
domain-specific tasks, and (3) reasoning-based knowledge distillation through
fine-tuning to significantly enhance its ability to generate contextually
relevant and logically sound responses. We demonstrate the versatility of
OmniScience by developing a battery agent that efficiently ranks molecules as
potential electrolyte solvents or additives. Comprehensive evaluations reveal
that OmniScience is competitive with state-of-the-art large reasoning models on
the GPQA Diamond and domain-specific battery benchmarks, while outperforming
all public reasoning and non-reasoning models with similar parameter counts. We
further demonstrate via ablation experiments that domain adaptive pretraining
and reasoning-based knowledge distillation are critical to attain our
performance levels, across benchmarks.
