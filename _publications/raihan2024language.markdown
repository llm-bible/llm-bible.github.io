---
layout: publication
title: 'Mojobench: Language Modeling And Benchmarks For Mojo'
authors: Nishat Raihan, Joanna C. S. Santos, Marcos Zampieri
conference: "Arxiv"
year: 2024
bibkey: raihan2024language
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.17736'}
tags: ['Attention Mechanism', 'Language Modeling', 'GPT', 'Tools', 'Model Architecture', 'Merging', 'Applications', 'Reinforcement Learning']
---
The recently introduced Mojo programming language (PL) by Modular, has
received significant attention in the scientific community due to its claimed
significant speed boost over Python. Despite advancements in code Large
Language Models (LLMs) across various PLs, Mojo remains unexplored in this
context. To address this gap, we introduce MojoBench, the first framework for
Mojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset
designed for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM
pretrained and finetuned for Mojo code generation, which supports instructions
in 5 natural languages (NLs). Our results show that Mojo-Coder achieves a
30-35% performance improvement over leading models like GPT-4o and
Claude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with
underrepresented and unseen PLs, offering potential strategies for enhancing
model adaptability. MojoBench contributes to our understanding of LLM
capabilities and limitations in emerging programming paradigms fostering more
robust code generation systems.
