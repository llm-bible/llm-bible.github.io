---
layout: publication
title: 'Comparing Large Language Models And Traditional Machine Translation Tools For Translating Medical Consultation Summaries: A Pilot Study'
authors: Andy Li, Wei Zhou, Rashina Hoda, Chris Bain, Peter Poon
conference: "Arxiv"
year: 2025
bibkey: li2025comparing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.16601"}
tags: ['Training Techniques', 'Applications', 'Tools', 'Reinforcement Learning']
---
This study evaluates how well large language models (LLMs) and traditional
machine translation (MT) tools translate medical consultation summaries from
English into Arabic, Chinese, and Vietnamese. It assesses both patient,
friendly and clinician, focused texts using standard automated metrics. Results
showed that traditional MT tools generally performed better, especially for
complex texts, while LLMs showed promise, particularly in Vietnamese and
Chinese, when translating simpler summaries. Arabic translations improved with
complexity due to the language's morphology. Overall, while LLMs offer
contextual flexibility, they remain inconsistent, and current evaluation
metrics fail to capture clinical relevance. The study highlights the need for
domain-specific training, improved evaluation methods, and human oversight in
medical translation.
