---
layout: publication
title: "Transformer Explainer: Interactive Learning Of Text-generative Models"
authors: Cho Aeree, Kim Grace C., Karpekov Alexander, Helbling Alec, Wang Zijie J., Lee Seongmin, Hoover Benjamin, Chau Duen Horng
conference: "Arxiv"
year: 2024
bibkey: cho2024transformer
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.04619"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Transformers have revolutionized machine learning yet their inner workings remain opaque to many. We present Transformer Explainer an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and enabling smooth transitions across abstraction levels of mathematical operations and model structures. It runs a live GPT-2 instance locally in the users browser empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware broadening the publics education access to modern generative AI techniques. Our open-sourced tool is available at https://poloclub.github.io/transformer-explainer/. A video demo is available at https://youtu.be/ECR4oAwocjs."
