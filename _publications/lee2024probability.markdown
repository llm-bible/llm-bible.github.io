---
layout: publication
title: 'Probability Of Differentiation Reveals Brittleness Of Homogeneity Bias In Large Language Models'
authors: Lee Messi H. J., Lai Calvin K.
conference: "Arxiv"
year: 2024
bibkey: lee2024probability
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.07329"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Language Modeling', 'Model Architecture', 'Prompting']
---
Homogeneity bias in Large Language Models (LLMs) refers to their tendency to
homogenize the representations of some groups compared to others. Previous
studies documenting this bias have predominantly used encoder models, which may
have inadvertently introduced biases. To address this limitation, we prompted
GPT-4 to generate single word/expression completions associated with 18
situation cues - specific, measurable elements of environments that influence
how individuals perceive situations and compared the variability of these
completions using probability of differentiation. This approach directly
assessed homogeneity bias from the model's outputs, bypassing encoder models.
Across five studies, we find that homogeneity bias is highly volatile across
situation cues and writing prompts, suggesting that the bias observed in past
work may reflect those within encoder models rather than LLMs. Furthermore,
these results suggest that homogeneity bias in LLMs is brittle, as even minor
and arbitrary changes in prompts can significantly alter the expression of
biases. Future work should further explore how variations in syntactic features
and topic choices in longer text generations influence homogeneity bias in
LLMs.
