---
layout: publication
title: 'Probability Of Differentiation Reveals Brittleness Of Homogeneity Bias In GPT-4'
authors: Messi H. J. Lee, Calvin K. Lai
conference: "Arxiv"
year: 2024
bibkey: lee2024probability
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.07329"}
tags: ['GPT', 'Applications', 'Ethics and Bias', 'Language Modeling', 'Model Architecture', 'Prompting']
---
Homogeneity bias in Large Language Models (LLMs) refers to their tendency to
homogenize the representations of some groups compared to others. Previous
studies documenting this bias have predominantly used encoder models, which may
have inadvertently introduced biases. To address this limitation, we prompted
GPT-4 to generate single word/expression completions associated with 18
situation cues-specific, measurable elements of environments that influence how
individuals perceive situations and compared the variability of these
completions using probability of differentiation. This approach directly
assessed homogeneity bias from the model's outputs, bypassing encoder models.
Across five studies, we find that homogeneity bias is highly volatile across
situation cues and writing prompts, suggesting that the bias observed in past
work may reflect those within encoder models rather than LLMs. Furthermore, we
find that homogeneity bias in LLMs is brittle, as even minor and arbitrary
changes in prompts can significantly alter the expression of biases. Future
work should further explore how variations in syntactic features and topic
choices in longer text generations influence homogeneity bias in LLMs.
