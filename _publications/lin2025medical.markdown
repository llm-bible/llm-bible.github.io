---
layout: publication
title: 'Healthgpt: A Medical Large Vision-language Model For Unifying Comprehension And Generation Via Heterogeneous Knowledge Adaptation'
authors: Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, Beng Chin Ooi
conference: "Arxiv"
year: 2025
bibkey: lin2025medical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.09838"}
  - {name: "Code", url: "https://github.com/DCDmllm/HealthGPT"}
tags: ['Fine-Tuning', 'GPT', 'Model Architecture', 'Has Code', 'Pretraining Methods', 'Multimodal Models']
---
We present HealthGPT, a powerful Medical Large Vision-Language Model
(Med-LVLM) that integrates medical visual comprehension and generation
capabilities within a unified autoregressive paradigm. Our bootstrapping
philosophy is to progressively adapt heterogeneous comprehension and generation
knowledge to pre-trained large language models (LLMs). This is achieved through
a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is
complemented by a tailored hierarchical visual perception approach and a
three-stage learning strategy. To effectively learn the HealthGPT, we devise a
comprehensive medical domain-specific comprehension and generation dataset
called VL-Health. Experimental results demonstrate exceptional performance and
scalability of HealthGPT in medical visual unified tasks. Our project can be
accessed at https://github.com/DCDmllm/HealthGPT.
