---
layout: publication
title: Multimodal Question Answering For Unified Information Extraction
authors: Sun Yuxuan, Zhang Kai, Su Yu
conference: "Arxiv"
year: 2023
bibkey: sun2023multimodal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.03017"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Multimodal Models', 'Prompting', 'Reinforcement Learning', 'Tools']
---
Multimodal information extraction (MIE) aims to extract structured information from unstructured multimedia content. Due to the diversity of tasks and settings most current MIE models are task45;specific and data45;intensive which limits their generalization to real45;world scenarios with diverse task requirements and limited labeled data. To address these issues we propose a novel multimodal question answering (MQA) framework to unify three MIE tasks by reformulating them into a unified span extraction and multi45;choice QA pipeline. Extensive experiments on six datasets show that 1) Our MQA framework consistently and significantly improves the performances of various off45;the45;shelf large multimodal models (LMM) on MIE tasks compared to vanilla prompting. 2) In the zero45;shot setting MQA outperforms previous state45;of45;the45;art baselines by a large margin. In addition the effectiveness of our framework can successfully transfer to the few45;shot setting enhancing LMMs on a scale of 10B parameters to be competitive or outperform much larger language models such as ChatGPT and GPT45;4. Our MQA framework can serve as a general principle of utilizing LMMs to better solve MIE and potentially other downstream multimodal tasks.
