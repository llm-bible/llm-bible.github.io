---
layout: publication
title: 'LTNER: Large Language Model Tagging For Named Entity Recognition With Contextualized Entity Marking'
authors: Yan Faren, Yu Peng, Chen Xin
conference: "Arxiv"
year: 2024
bibkey: yan2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.05624"}
tags: ['Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Tools', 'Training Techniques']
---
The use of LLMs for natural language processing has become a popular trend in the past two years, driven by their formidable capacity for context comprehension and learning, which has inspired a wave of research from academics and industry professionals. However, for certain NLP tasks, such as NER, the performance of LLMs still falls short when compared to supervised learning methods. In our research, we developed a NER processing framework called LTNER that incorporates a revolutionary Contextualized Entity Marking Gen Method. By leveraging the cost-effective GPT-3.5 coupled with context learning that does not require additional training, we significantly improved the accuracy of LLMs in handling NER tasks. The F1 score on the CoNLL03 dataset increased from the initial 85.9&#37; to 91.9&#37;, approaching the performance of supervised fine-tuning. This outcome has led to a deeper understanding of the potential of LLMs.
