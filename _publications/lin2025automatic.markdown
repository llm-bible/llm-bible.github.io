---
layout: publication
title: 'ADO: Automatic Data Optimization For Inputs In LLM Prompts'
authors: Sam Lin, Wenyue Hua, Lingyao Li, Zhenting Wang, Yongfeng Zhang
conference: "Arxiv"
year: 2025
bibkey: lin2025automatic
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.11436'}
  - {name: "Code", url: 'https://anonymous.4open.science/r/ADO-6BC5/'}
tags: ['Prompting', 'Has Code', 'Efficiency and Optimization']
---
This study explores a novel approach to enhance the performance of Large
Language Models (LLMs) through the optimization of input data within prompts.
While previous research has primarily focused on refining instruction
components and augmenting input data with in-context examples, our work
investigates the potential benefits of optimizing the input data itself. We
introduce a two-pronged strategy for input data optimization: content
engineering and structural reformulation. Content engineering involves imputing
missing values, removing irrelevant attributes, and enriching profiles by
generating additional information inferred from existing attributes. Subsequent
to content engineering, structural reformulation is applied to optimize the
presentation of the modified content to LLMs, given their sensitivity to input
format. Our findings suggest that these optimizations can significantly improve
the performance of LLMs in various tasks, offering a promising avenue for
future research in prompt engineering. The source code is available at
https://anonymous.4open.science/r/ADO-6BC5/
