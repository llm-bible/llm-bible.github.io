---
layout: publication
title: Anymal An Efficient And Scalable Any45;modality Augmented Language Model
authors: Moon Seungwhan, Madotto Andrea, Lin Zhaojiang, Nagarajan Tushar, Smith Matt, Jain Shashank, Yeh Chun-fu, Murugesan Prakash, Heidari Peyman, Liu Yue, Srinet Kavya, Damavandi Babak, Kumar Anuj
conference: "Arxiv"
year: 2023
bibkey: moon2023efficient
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.16058"}
tags: ['Multimodal Models']
---
We present Any45;Modality Augmented Language Model (AnyMAL) a unified model that reasons over diverse input modality signals (i.e. text image video audio IMU motion sensor) and generates textual responses. AnyMAL inherits the powerful text45;based reasoning abilities of the state45;of45;the45;art LLMs including LLaMA45;2 (70B) and converts modality45;specific signals to the joint textual space through a pre45;trained aligner module. To further strengthen the multimodal LLMs capabilities we fine45;tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs. We conduct comprehensive empirical analysis comprising both human and automatic evaluations and demonstrate state45;of45;the45;art performance on various multimodal tasks.
