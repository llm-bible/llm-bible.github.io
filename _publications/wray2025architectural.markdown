---
layout: publication
title: 'Architectural Precedents For General Agents Using Large Language Models'
authors: Robert E. Wray, James R. Kirk, John E. Laird
conference: "Arxiv"
year: 2025
bibkey: wray2025architectural
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.07087'}
tags: ['Agentic', 'Transformer', 'Applications', 'Model Architecture', 'Pretraining Methods']
---
One goal of AI (and AGI) is to identify and understand specific mechanisms and representations sufficient for general intelligence. Often, this work manifests in research focused on architectures and many cognitive architectures have been explored in AI/AGI. However, different research groups and even different research traditions have somewhat independently identified similar/common patterns of processes and representations or cognitive design patterns that are manifest in existing architectures. Today, AI systems exploiting large language models (LLMs) offer a relatively new combination of mechanism and representation available for exploring the possibilities of general intelligence. In this paper, we summarize a few recurring cognitive design patterns that have appeared in various pre-transformer AI architectures. We then explore how these patterns are evident in systems using LLMs, especially for reasoning and interactive ("agentic") use cases. By examining and applying these recurring patterns, we can also predict gaps or deficiencies in today's Agentic LLM Systems and identify likely subjects of future research towards general intelligence using LLMs and other generative foundation models.
