---
layout: publication
title: 'Occ-mllm-alpha:empowering Multi-modal Large Language Model For The Understanding Of Occluded Objects With Self-supervised Test-time Learning'
authors: Shuxin Yang, Xinhan Di
conference: "Arxiv"
year: 2024
bibkey: yang2024occ
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.01861"}
tags: ['Pretraining Methods', 'Training Techniques', 'Tools', 'Reinforcement Learning']
---
There is a gap in the understanding of occluded objects in existing
large-scale visual language multi-modal models. Current state-of-the-art
multi-modal models fail to provide satisfactory results in describing occluded
objects through universal visual encoders and supervised learning strategies.
Therefore, we introduce a multi-modal large language framework and
corresponding self-supervised learning strategy with support of 3D generation.
We start our experiments comparing with the state-of-the-art models in the
evaluation of a large-scale dataset SOMVideo [18]. The initial results
demonstrate the improvement of 16.92% in comparison with the state-of-the-art
VLM models.
