---
layout: publication
title: Non-autoregressive Transformer By Position Learning
authors: Yu Bao et al.
conference: Arxiv
year: 2019
citations: 29
bibkey: bao2019non
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1911.10677'}]
tags: [Language Modeling, Transformer]
---
Non-autoregressive models are promising on various text generation tasks.
Previous work hardly considers to explicitly model the positions of generated
words. However, position modeling is an essential problem in non-autoregressive
text generation. In this study, we propose PNAT, which incorporates positions
as a latent variable into the text generative process. Experimental results
show that PNAT achieves top results on machine translation and paraphrase
generation tasks, outperforming several strong baselines.