---
layout: publication
title: 'Flaming-hot Initiation With Regular Execution Sampling For Large Language Models'
authors: Weizhe Chen, Zhicheng Zhang, Guanlin Liu, Renjie Zheng, Wenlei Shi, Chen Dun, Zheng Wu, Xing Jin, Lin Yan
conference: "Arxiv"
year: 2024
bibkey: chen2024flaming
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.21236'}
tags: ['Training Techniques', 'Model Architecture', 'GPT']
---
Since the release of ChatGPT, large language models (LLMs) have demonstrated
remarkable capabilities across various domains. A key challenge in developing
these general capabilities is efficiently sourcing diverse, high-quality data.
This becomes especially critical in reasoning-related tasks with sandbox
checkers, such as math or code, where the goal is to generate correct solutions
to specific problems with higher probability. In this work, we introduce
Flaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet
highly effective method to efficiently find good responses. Our empirical
findings show that FIRE sampling enhances inference-time generation quality and
also benefits training in the alignment stage. Furthermore, we explore how FIRE
sampling improves performance by promoting diversity and analyze the impact of
employing FIRE at different positions within a response.
