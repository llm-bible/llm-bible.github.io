---
layout: publication
title: 'From Reward Shaping To Q-shaping: Achieving Unbiased Learning With Llm-guided Knowledge'
authors: Xiefeng Wu
conference: "Arxiv"
year: 2024
bibkey: wu2024from
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.01458'}
tags: ['Agentic', 'Efficiency and Optimization', 'Training Techniques', 'Tools', 'Reinforcement Learning', 'Ethics and Bias']
---
Q-shaping is an extension of Q-value initialization and serves as an
alternative to reward shaping for incorporating domain knowledge to accelerate
agent training, thereby improving sample efficiency by directly shaping
Q-values. This approach is both general and robust across diverse tasks,
allowing for immediate impact assessment while guaranteeing optimality. We
evaluated Q-shaping across 20 different environments using a large language
model (LLM) as the heuristic provider. The results demonstrate that Q-shaping
significantly enhances sample efficiency, achieving a \textbf\{16.87%\}
improvement over the best baseline in each environment and a \textbf\{253.80%\}
improvement compared to LLM-based reward shaping methods. These findings
establish Q-shaping as a superior and unbiased alternative to conventional
reward shaping in reinforcement learning.
