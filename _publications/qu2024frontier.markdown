---
layout: publication
title: The Frontier Of Data Erasure Machine Unlearning For Large Language Models
authors: Qu Youyang, Ding Ming, Sun Nan, Thilakarathna Kanchana, Zhu Tianqing, Niyato Dusit
conference: "Arxiv"
year: 2024
bibkey: qu2024frontier
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.15779"}
tags: ['Applications', 'Ethics And Bias', 'Language Modeling', 'Reinforcement Learning', 'Training Techniques']
---
Large Language Models (LLMs) are foundational to AI advancements facilitating applications like predictive text generation. Nonetheless they pose risks by potentially memorizing and disseminating sensitive biased or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs introducing methods for the targeted forgetting of information to address privacy ethical and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning this analysis also points out the hurdles in preserving model integrity avoiding excessive or insufficient data removal and ensuring consistent outputs underlining the role of machine unlearning in advancing responsible ethical AI.
