---
layout: publication
title: 'Mechanistic Unveiling Of Transformer Circuits: Self-influence As A Key To Model Reasoning'
authors: Lin Zhang, Lijie Hu, Di Wang
conference: "Arxiv"
year: 2025
bibkey: zhang2025mechanistic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.09022"}
tags: ['Model Architecture', 'Reinforcement Learning', 'GPT', 'Pretraining Methods', 'Transformer', 'Interpretability and Explainability']
---
Transformer-based language models have achieved significant success; however,
their internal mechanisms remain largely opaque due to the complexity of
non-linear interactions and high-dimensional operations. While previous studies
have demonstrated that these models implicitly embed reasoning trees, humans
typically employ various distinct logical reasoning mechanisms to complete the
same task. It is still unclear which multi-step reasoning mechanisms are used
by language models to solve such tasks. In this paper, we aim to address this
question by investigating the mechanistic interpretability of language models,
particularly in the context of multi-step reasoning tasks. Specifically, we
employ circuit analysis and self-influence functions to evaluate the changing
importance of each token throughout the reasoning process, allowing us to map
the reasoning paths adopted by the model. We apply this methodology to the
GPT-2 model on a prediction task (IOI) and demonstrate that the underlying
circuits reveal a human-interpretable reasoning process used by the model.
