---
layout: publication
title: 'Efficient Language Modeling For Low-resource Settings With Hybrid Rnn-transformer Architectures'
authors: Gabriel Lindenmaier, Sean Papay, Sebastian Pad√≥
conference: "Arxiv"
year: 2025
bibkey: lindenmaier2025efficient
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.00617'}
tags: ['Attention Mechanism', 'Language Modeling', 'Transformer', 'Training Techniques', 'Model Architecture', 'Applications', 'Pretraining Methods']
---
Transformer-based language models have recently been at the forefront of
active research in text generation. However, these models' advances come at the
price of prohibitive training costs, with parameter counts in the billions and
compute requirements measured in petaflop/s-decades. In this paper, we
investigate transformer-based architectures for improving model performance in
a low-data regime by selectively replacing attention layers with feed-forward
and quasi-recurrent neural network layers. We test these architectures on the
standard Enwik8 and Wikitext-103 corpora. Our results show that our reduced
architectures outperform existing models with a comparable number of
parameters, and obtain comparable performance to larger models while
significantly reducing the number of parameters.
