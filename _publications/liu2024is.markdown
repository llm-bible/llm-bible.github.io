---
layout: publication
title: Is Translation All You Need A Study On Solving Multilingual Tasks With Large Language Models
authors: Liu Chaoqun, Zhang Wenxuan, Zhao Yiran, Luu Anh Tuan, Bing Lidong
conference: "Arxiv"
year: 2024
bibkey: liu2024is
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.10258"}
tags: ['Pretraining Methods', 'Prompting', 'RAG', 'Training Techniques']
---
Large language models (LLMs) have demonstrated multilingual capabilities; yet they are mostly English45;centric due to the imbalanced training corpora. Existing works leverage this phenomenon to improve their multilingual performances through translation primarily on natural language processing (NLP) tasks. This work extends the evaluation from NLP tasks to real user queries and from English45;centric LLMs to non45;English45;centric LLMs. While translation into English can help improve the performance of multilingual NLP tasks for English45;centric LLMs it may not be optimal for all scenarios. For culture45;related tasks that need deep language understanding prompting in the native language tends to be more promising as it better captures the nuances of culture and language. Our experiments reveal varied behaviors among different LLMs and tasks in the multilingual context. Therefore we advocate for more comprehensive multilingual evaluation and more efforts toward developing multilingual LLMs beyond English45;centric ones.
