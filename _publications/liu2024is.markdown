---
layout: publication
title: 'Is Translation All You Need? A Study On Solving Multilingual Tasks With Large Language Models'
authors: Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan Luu, Lidong Bing
conference: "Arxiv"
year: 2024
bibkey: liu2024is
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.10258"}
tags: ['Training Techniques', 'Reinforcement Learning', 'RAG', 'Ethics and Bias', 'Prompting']
---
Large language models (LLMs) have demonstrated multilingual capabilities, yet
they are mostly English-centric due to the imbalanced training corpora. While
prior works have leveraged this bias to enhance multilingual performance
through translation, they have been largely limited to natural language
processing (NLP) tasks. In this work, we extend the evaluation to real-world
user queries and non-English-centric LLMs, offering a broader examination of
multilingual performance. Our key contribution lies in demonstrating that while
translation into English can boost the performance of English-centric LLMs on
NLP tasks, it is not universally optimal. For culture-related tasks that need
deep language understanding, prompting in the native language proves more
effective as it better captures the nuances of culture and language. Our
experiments expose varied behaviors across LLMs and tasks in the multilingual
context, underscoring the need for a more comprehensive approach to
multilingual evaluation. Therefore, we call for greater efforts in developing
and evaluating LLMs that go beyond English-centric paradigms.
