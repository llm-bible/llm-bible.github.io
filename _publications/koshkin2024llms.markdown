---
layout: publication
title: LLMs Are Zero-Shot Context-Aware Simultaneous Translators
authors: Koshkin Roman, Sudoh Katsuhito, Nakamura Satoshi
conference: "Arxiv"
year: 2024
bibkey: koshkin2024llms
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.13476"}
tags: ['ARXIV', 'Fine Tuning', 'LLM', 'Pretraining Methods', 'Tools', 'Transformer']
---
The advent of transformers has fueled progress in machine translation. More recently large language models (LLMs) have come to the spotlight thanks to their generality and strong performance in a wide range of language tasks including translation. Here we show that open-source LLMs perform on par with or better than some state-of-the-art baselines in simultaneous machine translation (SiMT) tasks zero-shot. We also demonstrate that injection of minimal background information which is easy with an LLM brings further performance gains especially on challenging technical subject-matter. This highlights LLMs potential for building next generation of massively multilingual context-aware and terminologically accurate SiMT systems that require no resource-intensive training or fine-tuning.
