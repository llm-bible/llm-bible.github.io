---
layout: publication
title: Llms Are Zero45;shot Context45;aware Simultaneous Translators
authors: Koshkin Roman, Sudoh Katsuhito, Nakamura Satoshi
conference: "Arxiv"
year: 2024
bibkey: koshkin2024llms
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.13476"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
The advent of transformers has fueled progress in machine translation. More recently large language models (LLMs) have come to the spotlight thanks to their generality and strong performance in a wide range of language tasks including translation. Here we show that open45;source LLMs perform on par with or better than some state45;of45;the45;art baselines in simultaneous machine translation (SiMT) tasks zero45;shot. We also demonstrate that injection of minimal background information which is easy with an LLM brings further performance gains especially on challenging technical subject45;matter. This highlights LLMs potential for building next generation of massively multilingual context45;aware and terminologically accurate SiMT systems that require no resource45;intensive training or fine45;tuning.
