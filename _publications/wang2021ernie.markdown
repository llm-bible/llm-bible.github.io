---
layout: publication
title: ERNIE 3.0 Titan Exploring Larger45;scale Knowledge Enhanced Pre45;training For Language Understanding And Generation
authors: Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, Jiaxiang Liu, Xuyi Chen, Yuxiang Lu, Weixin Liu, Xi Wang, Yangfan Bai, Qiuliang Chen, Li Zhao, Shiyong Li, Peng Sun, Dianhai Yu, Yanjun Ma, Hao Tian, Hua Wu, Tian Wu, Wei Zeng, Ge Li, Wen Gao, Haifeng Wang
conference: "Arxiv"
year: 2021
bibkey: wang2021ernie
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2112.12731v1"}
tags: ['Distillation', 'Efficiency And Optimization', 'GPT', 'Language Modeling', 'Model Architecture', 'Security', 'Tools', 'Training Techniques']
---
Pre45;trained language models have achieved state45;of45;the45;art results in various Natural Language Processing (NLP) tasks. GPT45;3 has shown that scaling up pre45;trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre45;training large45;scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state45;of45;the45;art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0 we train a hundred45;billion45;parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore we design a self45;supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission we propose an online distillation framework for ERNIE 3.0 Titan where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre45;trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state45;of45;the45;art models on 68 NLP datasets.
