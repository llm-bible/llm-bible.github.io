---
layout: publication
title: What Is The Best Model Application45;driven Evaluation For Large Language Models
authors: Lian Shiguo, Zhao Kaikai, Liu Xinhui, Lei Xuejiao, Yang Bikun, Zhang Wenjing, Wang Kai, Liu Zhaoxiang
conference: "Arxiv"
year: 2024
bibkey: lian2024what
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.10307"}
  - {name: "Code", url: "https://github.com/UnicomAI/DataSet/tree/main/TestData/GeneralAbility"}
tags: ['Agentic', 'Has Code', 'Prompting', 'Reinforcement Learning']
---
General large language models enhanced with supervised fine45;tuning and reinforcement learning from human feedback are increasingly popular in academia and industry as they generalize foundation models to various practical tasks in a prompt manner. To assist users in selecting the best model in practical application scenarios i.e. choosing the model that meets the application requirements while minimizing cost we introduce A45;Eval an application45;driven LLMs evaluation benchmark for general large language models. First we categorize evaluation tasks into five main categories and 27 sub45;categories from a practical application perspective. Next we construct a dataset comprising 678 question45;and45;answer pairs through a process of collecting annotating and reviewing. Then we design an objective and effective evaluation method and evaluate a series of LLMs of different scales on A45;Eval. Finally we reveal interesting laws regarding model scale and task difficulty level and propose a feasible method for selecting the best model. Through A45;Eval we provide clear empirical and engineer guidance for selecting the best model reducing barriers to selecting and using LLMs and promoting their application and development. Our benchmark is publicly available at https://github.com/UnicomAI/DataSet/tree/main/TestData/GeneralAbility.
