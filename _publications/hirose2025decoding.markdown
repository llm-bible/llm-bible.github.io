---
layout: publication
title: 'Decoding The Mind Of Large Language Models: A Quantitative Evaluation Of Ideology And Biases'
authors: Manari Hirose, Masato Uchida
conference: "2025 International Joint Conference on Neural Networks (IJCNN 2025)"
year: 2025
bibkey: hirose2025decoding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.12183"}
tags: ['Tools', 'GPT', 'Ethics and Bias', 'Model Architecture']
---
The widespread integration of Large Language Models (LLMs) across various sectors has highlighted the need for empirical research to understand their biases, thought patterns, and societal implications to ensure ethical and effective use. In this study, we propose a novel framework for evaluating LLMs, focusing on uncovering their ideological biases through a quantitative analysis of 436 binary-choice questions, many of which have no definitive answer. By applying our framework to ChatGPT and Gemini, findings revealed that while LLMs generally maintain consistent opinions on many topics, their ideologies differ across models and languages. Notably, ChatGPT exhibits a tendency to change their opinion to match the questioner's opinion. Both models also exhibited problematic biases, unethical or unfair claims, which might have negative societal impacts. These results underscore the importance of addressing both ideological and ethical considerations when evaluating LLMs. The proposed framework offers a flexible, quantitative method for assessing LLM behavior, providing valuable insights for the development of more socially aligned AI systems.
