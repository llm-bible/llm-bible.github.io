---
layout: publication
title: 'MHPP: Exploring The Capabilities And Limitations Of Language Models Beyond Basic Code Generation'
authors: Dai Jianbo, Lu Jianqiao, Feng Yunlong, Ruan Rongju, Cheng Ming, Tan Haochen, Guo Zhijiang
conference: "Arxiv"
year: 2024
bibkey: dai2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.11430"}
  - {name: "Code", url: "https://github.com/SparksofAGI/MHPP"}
tags: ['Applications', 'GPT', 'Has Code', 'Model Architecture']
---
"Recent advancements in large language models (LLMs) have greatly improved code generation, specifically at the function level. For instance, GPT-4 has achieved an 88.4&#37; pass rate on HumanEval. However, this draws into question the adequacy of existing benchmarks in thoroughly assessing function-level code generation capabilities. Our study analyzed two common benchmarks, HumanEval and MBPP, and found that these might not thoroughly evaluate LLMs' code generation capacities due to limitations in quality, difficulty, and granularity. To resolve this, we introduce the Mostly Hard Python Problems (MHPP) dataset, consisting of 140 unique human-curated problems. By focusing on the combination of natural language and code reasoning, MHPP gauges LLMs' abilities to comprehend specifications and restrictions, engage in multi-step reasoning, and apply coding knowledge effectively. Initial evaluations of 22 LLMs using MHPP showed many high-performing models on HumanEval failed to achieve similar success on MHPP. Moreover, MHPP highlighted various previously undiscovered limitations within various LLMs, leading us to believe that it could pave the way for a better understanding of LLMs' capabilities and limitations. Dataset and code are available at https://github.com/SparksofAGI/MHPP."
