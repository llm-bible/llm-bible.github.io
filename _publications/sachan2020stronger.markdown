---
layout: publication
title: Stronger Transformers For Neural Multi45;hop Question Generation
authors: Sachan Devendra Singh, Wu Lingfei, Sachan Mrinmaya, Hamilton William
conference: "Arxiv"
year: 2020
bibkey: sachan2020stronger
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2010.11374"}
tags: ['Model Architecture', 'Pretraining Methods', 'RAG', 'Transformer']
---
Prior work on automated question generation has almost exclusively focused on generating simple questions whose answers can be extracted from a single document. However there is an increasing interest in developing systems that are capable of more complex multi45;hop question generation where answering the questions requires reasoning over multiple documents. In this work we introduce a series of strong transformer models for multi45;hop question generation including a graph45;augmented transformer that leverages relations between entities in the text. While prior work has emphasized the importance of graph45;based models we show that we can substantially outperform the state45;of45;the45;art by 5 BLEU points using a standard transformer architecture. We further demonstrate that graph45;based augmentations can provide complimentary improvements on top of this foundation. Interestingly we find that several important factors45;45;such as the inclusion of an auxiliary contrastive objective and data filtering could have larger impacts on performance. We hope that our stronger baselines and analysis provide a constructive foundation for future work in this area.
