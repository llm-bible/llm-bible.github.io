---
layout: publication
title: Investigating Pretrained Language Models For Graph45;to45;text Generation
authors: Ribeiro Leonardo F. R., Schmitt Martin, Sch√ºtze Hinrich, Gurevych Iryna
conference: "Arxiv"
year: 2020
bibkey: ribeiro2020investigating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2007.08426"}
tags: ['Applications', 'Language Modeling', 'Pretraining Methods', 'Training Techniques']
---
Graph45;to45;text generation aims to generate fluent texts from graph45;based data. In this paper we investigate two recently proposed pretrained language models (PLMs) and analyze the impact of different task45;adaptive pretraining strategies for PLMs in graph45;to45;text generation. We present a study across three graph domains meaning representations Wikipedia knowledge graphs (KGs) and scientific KGs. We show that the PLMs BART and T5 achieve new state45;of45;the45;art results and that task45;adaptive pretraining strategies improve their performance even further. In particular we report new state45;of45;the45;art BLEU scores of 49.72 on LDC2017T10 59.70 on WebNLG and 25.66 on AGENDA datasets 45; a relative improvement of 31.837; 4.537; and 42.437; respectively. In an extensive analysis we identify possible reasons for the PLMs success on graph45;to45;text tasks. We find evidence that their knowledge about true facts helps them perform well even when the input graph representation is reduced to a simple bag of node and edge labels.
