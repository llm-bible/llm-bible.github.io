---
layout: publication
title: Sirens Song In The AI Ocean A Survey On Hallucination In Large Language Models
authors: Zhang Yue, Li Yafu, Cui Leyang, Cai Deng, Liu Lemao, Fu Tingchen, Huang Xinting, Zhao Enbo, Zhang Yu, Chen Yulong, Wang Longyue, Luu Anh Tuan, Bi Wei, Shi Freda, Shi Shuming
conference: "Arxiv"
year: 2023
bibkey: zhang2023song
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.01219"}
tags: ['Interpretability And Explainability', 'Pretraining Methods', 'Reinforcement Learning', 'Survey Paper']
---
While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks a significant concern revolves around their propensity to exhibit hallucinations LLMs occasionally generate content that diverges from the user input contradicts previously generated context or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real45;world scenarios. In this paper we survey recent efforts on the detection explanation and mitigation of hallucination with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks analyze existing approaches aiming at mitigating LLM hallucination and discuss potential directions for future research.
