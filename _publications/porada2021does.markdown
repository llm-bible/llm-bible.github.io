---
layout: publication
title: Does Pre45;training Induce Systematic Inference How Masked Language Models Acquire Commonsense Knowledge
authors: Porada Ian, Sordoni Alessandro, Cheung Jackie Chi Kit
conference: "Arxiv"
year: 2021
bibkey: porada2021does
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2112.08583"}
tags: ['BERT', 'Masked Language Model', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
Transformer models pre45;trained with a masked45;language45;modeling objective (e.g. BERT) encode commonsense knowledge as evidenced by behavioral probes; however the extent to which this knowledge is acquired by systematic inference over the semantics of the pre45;training corpora is an open question. To answer this question we selectively inject verbalized knowledge into the minibatches of a BERT model during pre45;training and evaluate how well the model generalizes to supported inferences. We find generalization does not improve over the course of pre45;training suggesting that commonsense knowledge is acquired from surface45;level co45;occurrence patterns rather than induced systematic reasoning.
