---
layout: publication
title: Does Pre-training Induce Systematic Inference How Masked Language Models Acquire Commonsense Knowledge
authors: Porada Ian, Sordoni Alessandro, Cheung Jackie Chi Kit
conference: "Arxiv"
year: 2021
bibkey: porada2021does
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2112.08583"}
tags: ['BERT', 'Masked Language Model', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
Transformer models pre-trained with a masked-language-modeling objective (e.g. BERT) encode commonsense knowledge as evidenced by behavioral probes; however the extent to which this knowledge is acquired by systematic inference over the semantics of the pre-training corpora is an open question. To answer this question we selectively inject verbalized knowledge into the minibatches of a BERT model during pre-training and evaluate how well the model generalizes to supported inferences. We find generalization does not improve over the course of pre-training suggesting that commonsense knowledge is acquired from surface-level co-occurrence patterns rather than induced systematic reasoning.
