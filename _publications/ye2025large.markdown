---
layout: publication
title: 'Large Language Model Psychometrics: A Systematic Review Of Evaluation, Validation, And Enhancement'
authors: Haoran Ye, Jing Jin, Yuhang Xie, Xin Zhang, Guojie Song
conference: "Arxiv"
year: 2025
bibkey: ye2025large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.08245"}
  - {name: "Code", url: "https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics"}
tags: ['Tools', 'Survey Paper', 'RAG', 'Merging', 'Has Code']
---
The rapid advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. It presents novel challenges, such as measuring human-like psychological constructs, navigating beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with Psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This survey introduces and synthesizes an emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. We systematically explore the role of Psychometrics in shaping benchmarking principles, broadening evaluation scopes, refining methodologies, validating results, and advancing LLM capabilities. This paper integrates diverse perspectives to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, we aim to provide actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.
