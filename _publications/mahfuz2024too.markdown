---
layout: publication
title: Too Late To Train Too Early To Use A Study On Necessity And Viability Of Low45;resource Bengali Llms
authors: Mahfuz Tamzeed, Dey Satak Kumar, Naswan Ruwad, Adil Hasnaen, Sayeed Khondker Salman, Shahgir Haz Sameen
conference: "Arxiv"
year: 2024
bibkey: mahfuz2024too
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.00416"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Tokenization', 'Training Techniques']
---
Each new generation of English45;oriented Large Language Models (LLMs) exhibits enhanced cross45;lingual transfer capabilities and significantly outperforms older LLMs on low45;resource languages. This prompts the question Is there a need for LLMs dedicated to a particular low45;resource language We aim to explore this question for Bengali a low45;to45;moderate resource Indo45;Aryan language native to the Bengal region of South Asia. We compare the performance of open45;weight and closed45;source LLMs such as LLaMA45;3 and GPT45;4 against fine45;tuned encoder45;decoder models across a diverse set of Bengali downstream tasks including translation summarization paraphrasing question45;answering and natural language inference. Our findings reveal that while LLMs generally excel in reasoning tasks their performance in tasks requiring Bengali script generation is inconsistent. Key challenges include inefficient tokenization of Bengali script by existing LLMs leading to increased computational costs and potential performance degradation. Additionally we highlight biases in machine45;translated datasets commonly used for Bengali NLP tasks. We conclude that there is a significant need for a Bengali45;oriented LLM but the field currently lacks the high45;quality pretraining and instruction45;tuning datasets necessary to develop a highly effective model.
