---
layout: publication
title: Attention45;informed Mixed45;language Training For Zero45;shot Cross45;lingual Task45;oriented Dialogue Systems
authors: Liu Zihan, Winata Genta Indra, Lin Zhaojiang, Xu Peng, Fung Pascale
conference: "Arxiv"
year: 2019
bibkey: liu2019attention
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1911.09273"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
Recently data45;driven task45;oriented dialogue systems have achieved promising performance in English. However developing dialogue systems that support low45;resource languages remains a long45;standing challenge due to the absence of high45;quality data. In order to circumvent the expensive and time45;consuming data collection we introduce Attention45;Informed Mixed45;Language Training (MLT) a novel zero45;shot adaptation method for cross45;lingual task45;oriented dialogue systems. It leverages very few task45;related parallel word pairs to generate code45;switching sentences for learning the inter45;lingual semantics across languages. Instead of manually selecting the word pairs we propose to extract source words based on the scores computed by the attention layer of a trained English task45;related model and then generate word pairs using existing bilingual dictionaries. Furthermore intensive experiments with different cross45;lingual embeddings demonstrate the effectiveness of our approach. Finally with very few word pairs our model achieves significant zero45;shot adaptation performance improvements in both cross45;lingual dialogue state tracking and natural language understanding (i.e. intent detection and slot filling) tasks compared to the current state45;of45;the45;art approaches which utilize a much larger amount of bilingual data.
