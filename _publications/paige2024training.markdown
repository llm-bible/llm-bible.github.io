---
layout: publication
title: Training Llms To Recognize Hedges In Spontaneous Narratives
authors: Paige Amie J., Soubki Adil, Murzaku John, Rambow Owen, Brennan Susan E.
conference: "SIGDIAL"
year: 2024
bibkey: paige2024training
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.03319"}
tags: ['BERT', 'GPT', 'Model Architecture', 'Prompting', 'Training Techniques']
---
Hedges allow speakers to mark utterances as provisional whether to signal non45;prototypicality or fuzziness to indicate a lack of commitment to an utterance to attribute responsibility for a statement to someone else to invite input from a partner or to soften critical feedback in the service of face45;management needs. Here we focus on hedges in an experimentally parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced from memory by 21 speakers for co45;present addressees transcribed to text (Galati and Brennan 2010). We created a gold standard of hedges annotated by human coders (the Roadrunner45;Hedge corpus) and compared three LLM45;based approaches for hedge detection fine45;tuning BERT and zero and few45;shot prompting with GPT45;4o and LLaMA45;3. The best45;performing approach was a fine45;tuned BERT model followed by few45;shot GPT45;4o. After an error analysis on the top performing approaches we used an LLM45;in45;the45;Loop approach to improve the gold standard coding as well as to highlight cases in which hedges are ambiguous in linguistically interesting ways that will guide future research. This is the first step in our research program to train LLMs to interpret and generate collateral signals appropriately and meaningfully in conversation.
