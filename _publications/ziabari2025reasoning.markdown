---
layout: publication
title: 'Reasoning On A Spectrum: Aligning Llms To System 1 And System 2 Thinking'
authors: Alireza S. Ziabari, Nona Ghazizadeh, Zhivar Sourati, Farzan Karimi-malekabadi, Payam Piray, Morteza Dehghani
conference: "Arxiv"
year: 2025
bibkey: ziabari2025reasoning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.12470"}
tags: ['Efficiency and Optimization']
---
Large Language Models (LLMs) exhibit impressive reasoning abilities, yet
their reliance on structured step-by-step processing reveals a critical
limitation. While human cognition fluidly adapts between intuitive, heuristic
(System 1) and analytical, deliberative (System 2) reasoning depending on the
context, LLMs lack this dynamic flexibility. This rigidity can lead to brittle
and unreliable performance when faced with tasks that deviate from their
trained patterns. To address this, we create a dataset of 2,000 samples with
valid System 1 and System 2 answers, explicitly align LLMs with these reasoning
styles, and evaluate their performance across reasoning benchmarks. Our results
reveal an accuracy-efficiency trade-off: System 2-aligned models excel in
arithmetic and symbolic reasoning, while System 1-aligned models perform better
in commonsense tasks. A mechanistic analysis of model responses shows that
System 1 models employ more definitive answers, whereas System 2 models
demonstrate greater uncertainty. Interpolating between these extremes produces
a monotonic transition in reasoning accuracy, preserving coherence. This work
challenges the assumption that step-by-step reasoning is always optimal and
highlights the need for adapting reasoning strategies based on task demands.
