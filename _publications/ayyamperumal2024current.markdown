---
layout: publication
title: 'Current State Of LLM Risks And AI Guardrails'
authors: Ayyamperumal Suriya Ganesh, Ge Limin
conference: "Arxiv"
year: 2024
bibkey: ayyamperumal2024current
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.12934"}
tags: ['Agentic', 'Applications', 'Bias Mitigation', 'Ethics And Bias', 'Fairness', 'Interpretability And Explainability', 'Model Architecture', 'Prompting', 'RAG', 'Reinforcement Learning', 'Responsible AI']
---
Large language models (LLMs) have become increasingly sophisticated, leading
to widespread deployment in sensitive applications where safety and reliability
are paramount. However, LLMs have inherent risks accompanying them, including
bias, potential for unsafe actions, dataset poisoning, lack of explainability,
hallucinations, and non-reproducibility. These risks necessitate the
development of "guardrails" to align LLMs with desired behaviors and mitigate
potential harm.
  This work explores the risks associated with deploying LLMs and evaluates
current approaches to implementing guardrails and model alignment techniques.
We examine intrinsic and extrinsic bias evaluation methods and discuss the
importance of fairness metrics for responsible AI development. The safety and
reliability of agentic LLMs (those capable of real-world actions) are explored,
emphasizing the need for testability, fail-safes, and situational awareness.
  Technical strategies for securing LLMs are presented, including a layered
protection model operating at external, secondary, and internal levels. System
prompts, Retrieval-Augmented Generation (RAG) architectures, and techniques to
minimize bias and protect privacy are highlighted.
  Effective guardrail design requires a deep understanding of the LLM's
intended use case, relevant regulations, and ethical considerations. Striking a
balance between competing requirements, such as accuracy and privacy, remains
an ongoing challenge. This work underscores the importance of continuous
research and development to ensure the safe and responsible use of LLMs in
real-world applications.
