---
layout: publication
title: Synthetic Data Generation In Low45;resource Settings Via Fine45;tuning Of Large Language Models
authors: Kaddour Jean, Liu Qi
conference: "Arxiv"
year: 2023
bibkey: kaddour2023synthetic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.01119"}
tags: ['Applications', 'Language Modeling', 'Reinforcement Learning', 'Training Techniques']
---
The in45;context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However they require enormous computational resources to be deployed. Alternatively smaller models can solve specific tasks if fine45;tuned with enough labeled examples. These examples however are expensive to obtain. In pursuit of the best of both worlds we study synthetic data generation of fine45;tuning training data via fine45;tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks we find that both data generation and annotation dramatically improve the respective downstream models performance occasionally necessitating only a minor fraction of the original training dataset.
