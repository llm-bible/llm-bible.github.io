---
layout: publication
title: Synthetic Data Generation In Low-resource Settings Via Fine-tuning Of Large Language Models
authors: Kaddour Jean, Liu Qi
conference: "Arxiv"
year: 2023
bibkey: kaddour2023synthetic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.01119"}
tags: ['Applications', 'Fine Tuning', 'In Context Learning', 'Language Modeling', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Training Techniques']
---
The in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However they require enormous computational resources to be deployed. Alternatively smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples however are expensive to obtain. In pursuit of the best of both worlds we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks we find that both data generation and annotation dramatically improve the respective downstream models performance occasionally necessitating only a minor fraction of the original training dataset.
