---
layout: publication
title: 'Verila: A Human-centered Evaluation Framework For Interpretable Verification Of LLM Agent Failures'
authors: Yoo Yeon Sung, Hannah Kim, Dan Zhang
conference: "Arxiv"
year: 2025
bibkey: sung2025human
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.12651'}
tags: ['Agentic', 'Responsible AI', 'Tools']
---
AI practitioners increasingly use large language model (LLM) agents in
compound AI systems to solve complex reasoning tasks, these agent executions
often fail to meet human standards, leading to errors that compromise the
system's overall performance. Addressing these failures through human
intervention is challenging due to the agents' opaque reasoning processes,
misalignment with human expectations, the complexity of agent dependencies, and
the high cost of manual inspection. This paper thus introduces a human-centered
evaluation framework for Verifying LLM Agent failures (VeriLA), which
systematically assesses agent failures to reduce human effort and make these
agent failures interpretable to humans. The framework first defines clear
expectations of each agent by curating human-designed agent criteria. Then, it
develops a human-aligned agent verifier module, trained with human gold
standards, to assess each agent's execution output. This approach enables
granular evaluation of each agent's performance by revealing failures from a
human standard, offering clear guidelines for revision, and reducing human
cognitive load. Our case study results show that VeriLA is both interpretable
and efficient in helping practitioners interact more effectively with the
system. By upholding accountability in human-agent collaboration, VeriLA paves
the way for more trustworthy and human-aligned compound AI systems.
