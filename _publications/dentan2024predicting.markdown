---
layout: publication
title: 'Predicting Memorization Within Large Language Models Fine-tuned For Classification'
authors: Jérémie Dentan, Davide Buscaldi, Aymen Shabou, Sonia Vanier
conference: "Arxiv"
year: 2024
bibkey: dentan2024predicting
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2409.18858'}
tags: ['Attention Mechanism', 'Interpretability and Explainability', 'Training Techniques', 'Model Architecture', 'Reinforcement Learning']
---
Large Language Models have received significant attention due to their abilities to solve a wide range of complex tasks. However these models memorize a significant proportion of their training data, posing a serious threat when disclosed at inference time. To mitigate this unintended memorization, it is crucial to understand what elements are memorized and why. This area of research is largely unexplored, with most existing works providing a posteriori explanations. To address this gap, we propose a new approach to detect memorized samples a priori in LLMs fine-tuned for classification tasks. This method is effective from the early stages of training and readily adaptable to other classification settings, such as training vision models from scratch. Our method is supported by new theoretical results, and requires a low computational budget. We achieve strong empirical results, paving the way for the systematic identification and protection of vulnerable samples before they are memorized.
