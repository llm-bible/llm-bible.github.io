---
layout: publication
title: 'RWKVTTS: Yet Another TTS Based On RWKV-7'
authors: Lin Yueyu, Liu Xiao
conference: "Arxiv"
year: 2025
bibkey: yueyu2025yet
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.03289"}
  - {name: "Code", url: "https://github.com/yynil/RWKVTTS,"}
  - {name: "Code", url: "https://huggingface.co/spaces/RWKV-Red-Team"}
tags: ['Transformer', 'Efficiency and Optimization', 'Applications', 'RAG', 'Model Architecture', 'Reinforcement Learning', 'Has Code', 'Pretraining Methods']
---
Human-AI interaction thrives on intuitive and efficient interfaces, among
which voice stands out as a particularly natural and accessible modality.
Recent advancements in transformer-based text-to-speech (TTS) systems, such as
Fish-Speech, CosyVoice, and MegaTTS 3, have delivered remarkable improvements
in quality and realism, driving a significant evolution in the TTS domain. In
this paper, we introduce RWKV-7 \cite\{peng2025rwkv\}, a cutting-edge RNN-based
architecture tailored for TTS applications. Unlike traditional transformer
models, RWKV-7 leverages the strengths of recurrent neural networks to achieve
greater computational efficiency and scalability, while maintaining
high-quality output. Our comprehensive benchmarks demonstrate that RWKV-7
outperforms transformer-based models across multiple key metrics, including
synthesis speed, naturalness of speech, and resource efficiency. Furthermore,
we explore its adaptability to diverse linguistic contexts and low-resource
environments, showcasing its potential to democratize TTS technology. These
findings position RWKV-7 as a powerful and innovative alternative, paving the
way for more accessible and versatile voice synthesis solutions in real-world
applications.Our code and weights are https://github.com/yynil/RWKVTTS,
https://huggingface.co/spaces/RWKV-Red-Team
