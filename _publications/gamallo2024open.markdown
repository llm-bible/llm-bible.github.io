---
layout: publication
title: 'Open Generative Large Language Models For Galician'
authors: Pablo Gamallo, Pablo Rodríguez, Iria De-dios-flores, Susana Sotelo, Silvia Paniagua, Daniel Bardanca, José Ramom Pichel, Marcos Garcia
conference: "Arxiv"
year: 2024
bibkey: gamallo2024open
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2406.13893'}
tags: ['RAG', 'Training Techniques', 'GPT', 'Model Architecture', 'Ethics and Bias', 'Pretraining Methods']
---
Large language models (LLMs) have transformed natural language processing.
Yet, their predominantly English-centric training has led to biases and
performance disparities across languages. This imbalance marginalizes
minoritized languages, making equitable access to NLP technologies more
difficult for languages with lower resources, such as Galician. We present the
first two generative LLMs focused on Galician to bridge this gap. These models,
freely available as open-source resources, were trained using a GPT
architecture with 1.3B parameters on a corpus of 2.1B words. Leveraging
continual pretraining, we adapt to Galician two existing LLMs trained on larger
corpora, thus mitigating the data constraints that would arise if the training
were performed from scratch. The models were evaluated using human judgments
and task-based datasets from standardized benchmarks. These evaluations reveal
a promising performance, underscoring the importance of linguistic diversity in
generative models.
