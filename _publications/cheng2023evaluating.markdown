---
layout: publication
title: Egothink Evaluating First45;person Perspective Thinking Capability Of Vision45;language Models
authors: Cheng Sijie, Guo Zhicheng, Wu Jingwen, Fang Kechen, Li Peng, Liu Huaping, Liu Yang
conference: "Arxiv"
year: 2023
bibkey: cheng2023evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.15596"}
tags: ['Agent', 'Agentic', 'GPT', 'Model Architecture']
---
Vision45;language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities with the majority focusing on the third45;person perspective and only a few addressing specific tasks from the first45;person perspective. However the capability of VLMs to think from a first45;person perspective a crucial attribute for advancing autonomous agents and robotics remains largely unexplored. To bridge this research gap we introduce EgoThink a novel visual question45;answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos with manually annotated question45;answer pairs containing first45;person information. To comprehensively assess VLMs we evaluate eighteen popular VLMs on EgoThink. Moreover given the open45;ended format of the answers we use GPT45;4 as the automatic judge to compute single45;answer grading. Experimental results indicate that although GPT45;4V leads in numerous dimensions all evaluated VLMs still possess considerable potential for improvement in first45;person perspective tasks. Meanwhile enlarging the number of trainable parameters has the most significant impact on model performance on EgoThink. In conclusion EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs providing an indispensable resource for future research in the realm of embodied artificial intelligence and robotics.
