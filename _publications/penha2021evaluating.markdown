---
layout: publication
title: Evaluating The Robustness Of Retrieval Pipelines With Query Variation Generators
authors: "Gustavo Penha, Arthur C\xE2mara, Claudia Hauff"
conference: Arxiv
year: 2021
citations: 17
bibkey: penha2021evaluating
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2111.13057'}, {name: Code,
    url: 'https://github.com/Guzpenha/query_variation_generators'}]
tags: [RAG, BERT, Security]
---
Heavily pre-trained transformers for language modelling, such as BERT, have
shown to be remarkably effective for Information Retrieval (IR) tasks,
typically applied to re-rank the results of a first-stage retrieval model. IR
benchmarks evaluate the effectiveness of retrieval pipelines based on the
premise that a single query is used to instantiate the underlying information
need. However, previous research has shown that (I) queries generated by users
for a fixed information need are extremely variable and, in particular, (II)
neural models are brittle and often make mistakes when tested with modified
inputs. Motivated by those observations we aim to answer the following
question: how robust are retrieval pipelines with respect to different
variations in queries that do not change the queries' semantics? In order to
obtain queries that are representative of users' querying variability, we first
created a taxonomy based on the manual annotation of transformations occurring
in a dataset (UQV100) of user-created query variations. For each
syntax-changing category of our taxonomy, we employed different automatic
methods that when applied to a query generate a query variation. Our
experimental results across two datasets for two IR tasks reveal that retrieval
pipelines are not robust to these query variations, with effectiveness drops of
\\(\approx20%\\) on average. The code and datasets are available at
https://github.com/Guzpenha/query_variation_generators.