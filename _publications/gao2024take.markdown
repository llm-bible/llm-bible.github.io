---
layout: publication
title: 'Take Caution In Using Llms As Human Surrogates: Scylla Ex Machina'
authors: Yuan Gao, Dokyun Lee, Gordon Burtch, Sina Fazelpour
conference: "Arxiv"
year: 2024
bibkey: gao2024take
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.19599'}
tags: ['Reinforcement Learning', 'Survey Paper']
---
Recent studies suggest large language models (LLMs) can exhibit human-like
reasoning, aligning with human behavior in economic experiments, surveys, and
political discourse. This has led many to propose that LLMs can be used as
surrogates or simulations for humans in social science research. However, LLMs
differ fundamentally from humans, relying on probabilistic patterns, absent the
embodied experiences or survival objectives that shape human cognition. We
assess the reasoning depth of LLMs using the 11-20 money request game. Nearly
all advanced approaches fail to replicate human behavior distributions across
many models. Causes of failure are diverse and unpredictable, relating to input
language, roles, and safeguarding. These results advise caution when using LLMs
to study human behavior or as surrogates or simulations.
