---
layout: publication
title: KIT45;19 A Comprehensive Korean Instruction Toolkit On 19 Tasks For Fine45;tuning Korean Large Language Models
authors: Jang Dongjun, Byun Sungjoo, Jo Hyemi, Shin Hyopil
conference: "Arxiv"
year: 2024
bibkey: jang2024kit
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.16444"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
Instruction Tuning on Large Language Models is an essential process for model to function well and achieve high performance in specific tasks. Accordingly in mainstream languages such as English instruction45;based datasets are being constructed and made publicly available. In the case of Korean publicly available models and datasets all rely on using the output of ChatGPT or translating datasets built in English. In this paper We introduce textit123;KIT45;19125; as an instruction dataset for the development of LLM in Korean. textit123;KIT45;19125; is a dataset created in an instruction format comprising 19 existing open45;source datasets for Korean NLP tasks. In this paper we train a Korean Pretrained LLM using textit123;KIT45;19125; to demonstrate its effectiveness. The experimental results show that the model trained on textit123;KIT45;19125; significantly outperforms existing Korean LLMs. Based on the its quality and empirical results this paper proposes that textit123;KIT45;19125; has the potential to make a substantial contribution to the future improvement of Korean LLMs performance.
