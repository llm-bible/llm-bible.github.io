---
layout: publication
title: 'Small Vision-language Models: A Survey On Compact Architectures And Techniques'
authors: Nitesh Patnaik, Navdeep Nayak, Himani Bansal Agrawal, Moinak Chinmoy Khamaru, Gourav Bal, Saishree Smaranika Panda, Rishi Raj, Vishal Meena, Kartheek Vadlamani
conference: "Arxiv"
year: 2025
bibkey: patnaik2025small
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.10665"}
tags: ['Efficiency and Optimization', 'Model Architecture', 'Multimodal Models', 'Survey Paper', 'Distillation', 'GPT', 'Merging', 'Ethics and Bias', 'Pretraining Methods', 'Fine-Tuning', 'Transformer', 'Attention Mechanism']
---
The emergence of small vision-language models (sVLMs) marks a critical
advancement in multimodal AI, enabling efficient processing of visual and
textual data in resource-constrained environments. This survey offers a
comprehensive exploration of sVLM development, presenting a taxonomy of
architectures - transformer-based, mamba-based, and hybrid - that highlight
innovations in compact design and computational efficiency. Techniques such as
knowledge distillation, lightweight attention mechanisms, and modality
pre-fusion are discussed as enablers of high performance with reduced resource
requirements. Through an in-depth analysis of models like TinyGPT-V, MiniGPT-4,
and VL-Mamba, we identify trade-offs between accuracy, efficiency, and
scalability. Persistent challenges, including data biases and generalization to
complex tasks, are critically examined, with proposed pathways for addressing
them. By consolidating advancements in sVLMs, this work underscores their
transformative potential for accessible AI, setting a foundation for future
research into efficient multimodal systems.
