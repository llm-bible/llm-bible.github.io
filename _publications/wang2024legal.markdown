---
layout: publication
title: 'Legal Evalutions And Challenges Of Large Language Models'
authors: Jiaqi Wang, Huan Zhao, Zhenyuan Yang, Peng Shu, Junhao Chen, Haobo Sun, Ruixi Liang, Shixin Li, Pengcheng Shi, Longjun Ma, Zongjia Liu, Zhengliang Liu, Tianyang Zhong, Yutong Zhang, Chong Ma, Xin Zhang, Tuo Zhang, Tianli Ding, Yudan Ren, Tianming Liu, Xi Jiang, Shu Zhang
conference: "Arxiv"
year: 2024
bibkey: wang2024legal
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.10137'}
tags: ['Reinforcement Learning', 'Applications', 'Survey Paper']
---
In this paper, we review legal testing methods based on Large Language Models
(LLMs), using the OPENAI o1 model as a case study to evaluate the performance
of large models in applying legal provisions. We compare current
state-of-the-art LLMs, including open-source, closed-source, and legal-specific
models trained specifically for the legal domain. Systematic tests are
conducted on English and Chinese legal cases, and the results are analyzed in
depth. Through systematic testing of legal cases from common law systems and
China, this paper explores the strengths and weaknesses of LLMs in
understanding and applying legal texts, reasoning through legal issues, and
predicting judgments. The experimental results highlight both the potential and
limitations of LLMs in legal applications, particularly in terms of challenges
related to the interpretation of legal language and the accuracy of legal
reasoning. Finally, the paper provides a comprehensive analysis of the
advantages and disadvantages of various types of models, offering valuable
insights and references for the future application of AI in the legal field.
