---
layout: publication
title: 'Scaling Laws For Economic Productivity: Experimental Evidence In Llm-assisted Translation'
authors: Ali Merali
conference: "Arxiv"
year: 2024
bibkey: merali2024scaling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.02391"}
tags: ['Pre-Training', 'Efficiency and Optimization', 'Model Architecture', 'Large-Scale Training', 'Training Techniques', 'Scaling Laws']
---
This paper derives "scaling laws"--empirical relationships between the
training compute of Large Language Models (LLMs) and their performance--for
economic outcomes. In a preregistered online experiment, 300 professional
translators completed 1,800 tasks using one of 13 LLMs (or a control). A
tenfold increase in model compute improved task completion speed by 12.3%,
grades by 0.18 standard deviations, and earnings per minute by 16.1%. Gains
were four times larger for lower-skilled workers. These findings suggest
continued model scaling could boost U.S. productivity by at least 6.9% over the
next decade.
