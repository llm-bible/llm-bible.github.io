---
layout: publication
title: 'Are Large Language Model-based Evaluators The Solution To Scaling Up Multilingual Evaluation?'
authors: Rishav Hada, Varun Gumma, Adrian De Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram
conference: "Arxiv"
year: 2023
bibkey: hada2023are
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.07462"}
tags: ['Ethics and Bias', 'Model Architecture', 'GPT', 'Reinforcement Learning']
---
Large Language Models (LLMs) excel in various Natural Language Processing
(NLP) tasks, yet their evaluation, particularly in languages beyond the top
\\(20\\), remains inadequate due to existing benchmarks and metrics limitations.
Employing LLMs as evaluators to rank or score other models' outputs emerges as
a viable solution, addressing the constraints tied to human annotators and
established benchmarks. In this study, we explore the potential of LLM-based
evaluators, specifically GPT-4 in enhancing multilingual evaluation by
calibrating them against \\(20\\)K human judgments across three text-generation
tasks, five metrics, and eight languages. Our analysis reveals a bias in
GPT4-based evaluators towards higher scores, underscoring the necessity of
calibration with native speaker judgments, especially in low-resource and
non-Latin script languages, to ensure accurate evaluation of LLM performance
across diverse languages.
