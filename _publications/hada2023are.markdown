---
layout: publication
title: Are Large Language Model45;based Evaluators The Solution To Scaling Up Multilingual Evaluation
authors: Hada Rishav, Gumma Varun, De Wynter Adrian, Diddee Harshita, Ahmed Mohamed, Choudhury Monojit, Bali Kalika, Sitaram Sunayana
conference: "Arxiv"
year: 2023
bibkey: hada2023are
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.07462"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Reinforcement Learning']
---
Large Language Models (LLMs) excel in various Natural Language Processing (NLP) tasks yet their evaluation particularly in languages beyond the top 20 remains inadequate due to existing benchmarks and metrics limitations. Employing LLMs as evaluators to rank or score other models outputs emerges as a viable solution addressing the constraints tied to human annotators and established benchmarks. In this study we explore the potential of LLM45;based evaluators specifically GPT45;4 in enhancing multilingual evaluation by calibrating them against 20K human judgments across three text45;generation tasks five metrics and eight languages. Our analysis reveals a bias in GPT445;based evaluators towards higher scores underscoring the necessity of calibration with native speaker judgments especially in low45;resource and non45;Latin script languages to ensure accurate evaluation of LLM performance across diverse languages.
