---
layout: publication
title: 'A Linguistically-motivated Evaluation Methodology For Unraveling Model''s Abilities In Reading Comprehension Tasks'
authors: Elie Lis, Talep Antoine, Frédéric Lis, Talep Béchet, Géraldine Diro Damnati, Philippe Diro Langlais
conference: "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing Association for Computational Linguistics Nov 2024 Miami United States. pp.18376-18392"
year: 2025
bibkey: antoine2025linguistically
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.17569'}
tags: ['GPT', 'Tools', 'Model Architecture']
---
We introduce an evaluation methodology for reading comprehension tasks based
on the intuition that certain examples, by the virtue of their linguistic
complexity, consistently yield lower scores regardless of model size or
architecture. We capitalize on semantic frame annotation for characterizing
this complexity, and study seven complexity factors that may account for
model's difficulty. We first deploy this methodology on a carefully annotated
French reading comprehension benchmark showing that two of those complexity
factors are indeed good predictors of models' failure, while others are less
so. We further deploy our methodology on a well studied English benchmark by
using Chat-GPT as a proxy for semantic annotation. Our study reveals that
fine-grained linguisticallymotivated automatic evaluation of a reading
comprehension task is not only possible, but helps understand models' abilities
to handle specific linguistic characteristics of input examples. It also shows
that current state-of-the-art models fail with some for those characteristics
which suggests that adequately handling them requires more than merely
increasing model size.
