---
layout: publication
title: P3 A Policy45;driven Pace45;adaptive And Diversity45;promoted Framework For Optimizing LLM Training
authors: Yang Yingxuan, Wang Huayi, Wen Muning, Zhang Weinan
conference: "Arxiv"
year: 2024
bibkey: yang2024policy
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.05541"}
tags: ['Efficiency And Optimization', 'Pruning', 'Tools', 'Training Techniques']
---
In the rapidly evolving field of Large Language Models (LLMs) selecting high45;quality data for fine45;tuning is essential. This paper focuses on task45;specific data pruning and selection to enhance fine45;tuning. We introduce an innovative framework termed P3 which improves LLM performance through a dynamic adaptive training strategy. Specifically P3 comprises the following components (1) Policy45;driven Difficulty Measurement we begin by measuring the difficulty of data based on the models real45;time performance transitioning from static predefined metrics to more dynamic and adaptable ones. (2) Pace45;adaptive Selection we employ self45;paced learning (SPL) to gradually select increasingly challenging data thereby progressively enhancing the models performance. (3) Diversity Promotion we integrate Determinantal Point Process (DPP) into the selection process to promote the diversity within and between samples enriching the learning process. We have validated our method on two well45;known LLM datasets APPS and MATH designed for logical reasoning scenarios. The results show that our P3 framework significantly improves training outcomes compared to traditional methods. By fundamentally refining data selection and utilization strategies P3 not only advances theoretical understanding of dynamic training approaches but also provides a versatile framework that can revolutionize model training in natural language processing.
