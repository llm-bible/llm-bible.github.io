---
layout: publication
title: 'Testing Uncertainty Of Large Language Models For Physics Knowledge And Reasoning'
authors: Elizaveta Reganova, Peter Steinbach
conference: "Arxiv"
year: 2024
bibkey: reganova2024testing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.14465"}
tags: ['Agentic', 'RAG', 'Model Architecture', 'GPT']
---
Large Language Models (LLMs) have gained significant popularity in recent
years for their ability to answer questions in various fields. However, these
models have a tendency to "hallucinate" their responses, making it challenging
to evaluate their performance. A major challenge is determining how to assess
the certainty of a model's predictions and how it correlates with accuracy. In
this work, we introduce an analysis for evaluating the performance of popular
open-source LLMs, as well as gpt-3.5 Turbo, on multiple choice physics
questionnaires. We focus on the relationship between answer accuracy and
variability in topics related to physics. Our findings suggest that most models
provide accurate replies in cases where they are certain, but this is by far
not a general behavior. The relationship between accuracy and uncertainty
exposes a broad horizontal bell-shaped distribution. We report how the
asymmetry between accuracy and uncertainty intensifies as the questions demand
more logical reasoning of the LLM agent, while the same relationship remains
sharp for knowledge retrieval tasks.
