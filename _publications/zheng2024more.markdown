---
layout: publication
title: More45;3smultimodal45;based Offline Reinforcement Learning With Shared Semantic Spaces
authors: Zheng Tianyu, Zhang Ge, Qu Xingwei, Kuang Ming, Huang Stephen W., He Zhaofeng
conference: "Arxiv"
year: 2024
bibkey: zheng2024more
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.12845"}
  - {name: "Code", url: "https://github.com/Zheng0428/MORE&#95;"}
tags: ['Agentic', 'Efficiency And Optimization', 'Has Code', 'Multimodal Models', 'Reinforcement Learning', 'Training Techniques']
---
Drawing upon the intuition that aligning different modalities to the same semantic embedding space would allow models to understand states and actions more easily we propose a new perspective to the offline reinforcement learning (RL) challenge. More concretely we transform it into a supervised learning task by integrating multimodal and pre45;trained language models. Our approach incorporates state information derived from images and action45;related data obtained from text thereby bolstering RL training performance and promoting long45;term strategic thinking. We emphasize the contextual understanding of language and demonstrate how decision45;making in RL can benefit from aligning states and actions representation with languages representation. Our method significantly outperforms current baselines as evidenced by evaluations conducted on Atari and OpenAI Gym environments. This contributes to advancing offline RL performance and efficiency while providing a novel perspective on offline RL.Our code and data are available at https://github.com/Zheng0428/MORE&#95;.
