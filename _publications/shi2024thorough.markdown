---
layout: publication
title: A Thorough Examination Of Decoding Methods In The Era Of Llms
authors: Shi Chufan, Yang Haoran, Cai Deng, Zhang Zhisong, Wang Yifan, Yang Yujiu, Lam Wai
conference: "Arxiv"
year: 2024
bibkey: shi2024thorough
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.06925"}
tags: ['Efficiency And Optimization', 'Quantization', 'Security']
---
Decoding methods play an indispensable role in converting language models from next45;token predictors into practical task solvers. Prior research on decoding methods primarily focusing on task45;specific models may not extend to the current era of general45;purpose large language models (LLMs). Moreover the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs evaluating their performance robustness to hyperparameter changes and decoding speeds across a wide range of tasks models and deployment environments. Our findings reveal that decoding method performance is notably task45;dependent and influenced by factors such as alignment model size and quantization. Intriguingly sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning highlighting the trade45;off between attaining optimal results and the practicality of implementation in varying contexts.
