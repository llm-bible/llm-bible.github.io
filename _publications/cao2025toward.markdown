---
layout: publication
title: 'Toward Generalizable Evaluation In The LLM Era: A Survey Beyond Benchmarks'
authors: Yixin Cao, Shibo Hong, Xinze Li, Jiahao Ying, Yubo Ma, Haiyuan Liang, Yantao Liu, Zijun Yao, Xiaozhi Wang, Dan Huang, Wenxuan Zhang, Lifu Huang, Muhao Chen, Lei Hou, Qianru Sun, Xingjun Ma, Zuxuan Wu, Min-yen Kan, David Lo, Qi Zhang, Heng Ji, Jing Jiang, Juanzi Li, Aixin Sun, Xuanjing Huang, Tat-seng Chua, Yu-gang Jiang
conference: "Arxiv"
year: 2025
bibkey: cao2025toward
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.18838"}
tags: ['Responsible AI', 'Survey Paper', 'Applications', 'TACL', 'ACL']
---
Large Language Models (LLMs) are advancing at an amazing speed and have
become indispensable across academia, industry, and daily applications. To keep
pace with the status quo, this survey probes the core challenges that the rise
of LLMs poses for evaluation. We identify and analyze two pivotal transitions:
(i) from task-specific to capability-based evaluation, which reorganizes
benchmarks around core competencies such as knowledge, reasoning, instruction
following, multi-modal understanding, and safety; and (ii) from manual to
automated evaluation, encompassing dynamic dataset curation and
"LLM-as-a-judge" scoring.
  Yet, even with these transitions, a crucial obstacle persists: the evaluation
generalization issue. Bounded test sets cannot scale alongside models whose
abilities grow seemingly without limit. We will dissect this issue, along with
the core challenges of the above two transitions, from the perspectives of
methods, datasets, evaluators, and metrics. Due to the fast evolving of this
field, we will maintain a living GitHub repository (links are in each section)
to crowd-source updates and corrections, and warmly invite contributors and
collaborators.
