---
layout: publication
title: Viclevr A Visual Reasoning Dataset And Hybrid Multimodal Fusion Model For Visual Question Answering In Vietnamese
authors: Tran Khiem Vinh, Phan Hao Phu, Van Nguyen Kiet, Nguyen Ngan Luu Thuy
conference: "Arxiv"
year: 2023
bibkey: tran2023visual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.18046"}
  - {name: "Code", url: "https://github.com/kvt0012/ViCLEVR&#125;"}
tags: ['Applications', 'Attention Mechanism', 'Ethics And Bias', 'Has Code', 'Merging', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Transformer']
---
In recent years Visual Question Answering (VQA) has gained significant attention for its diverse applications including intelligent car assistance aiding visually impaired individuals and document image information retrieval using natural language queries. VQA requires effective integration of information from questions and images to generate accurate answers. Neural models for VQA have made remarkable progress on large45;scale datasets with a primary focus on resource45;rich languages like English. To address this we introduce the ViCLEVR dataset a pioneering collection for evaluating various visual reasoning capabilities in Vietnamese while mitigating biases. The dataset comprises over 26000 images and 30000 question45;answer pairs (QAs) each question annotated to specify the type of reasoning involved. Leveraging this dataset we conduct a comprehensive analysis of contemporary visual reasoning systems offering valuable insights into their strengths and limitations. Furthermore we present PhoVIT a comprehensive multimodal fusion that identifies objects in images based on questions. The architecture effectively employs transformers to enable simultaneous reasoning over textual and visual data merging both modalities at an early model stage. The experimental findings demonstrate that our proposed model achieves state45;of45;the45;art performance across four evaluation metrics. The accompanying code and dataset have been made publicly accessible at url123;https://github.com/kvt0012/ViCLEVR&#125;. This provision seeks to stimulate advancements within the research community fostering the development of more multimodal fusion algorithms specifically tailored to address the nuances of low45;resource languages exemplified by Vietnamese.
