---
layout: publication
title: Large Language Models For Generative Information Extraction&#58; A Survey
authors: Xu Derong, Chen Wei, Peng Wenjun, Zhang Chao, Xu Tong, Zhao Xiangyu, Wu Xian, Zheng Yefeng, Wang Yang, Chen Enhong
conference: "Arxiv"
year: 2023
bibkey: xu2023large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.17617"}
  - {name: "Code", url: "https://github.com/quqxui/Awesome-LLM4IE-Papers"}
tags: ['Applications', 'Fine Tuning', 'Has Code', 'Merging', 'Reinforcement Learning', 'Survey Paper']
---
Information extraction (IE) aims to extract structural knowledge (such as entities relations and events) from plain natural language texts. Recently generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation allowing for generalization across various domains and tasks. As a result numerous works have been proposed to harness abilities of LLMs and offer viable solutions for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks in this study we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and learning paradigms then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on thorough review conducted we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related resources at (url)https://github.com/quqxui/Awesome-LLM4IE-Papers\}."
