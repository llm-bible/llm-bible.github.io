---
layout: publication
title: Pretrained Language Models For Document45;level Neural Machine Translation
authors: Li Liangyou, Jiang Xin, Liu Qun
conference: "Arxiv"
year: 2019
bibkey: li2019pretrained
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1911.03110"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Training Techniques']
---
Previous work on document45;level NMT usually focuses on limited contexts because of degraded performance on larger contexts. In this paper we investigate on using large contexts with three main contributions (1) Different from previous work which pertrained models on large45;scale sentence45;level parallel corpora we use pretrained language models specifically BERT which are trained on monolingual documents; (2) We propose context manipulation methods to control the influence of large contexts which lead to comparable results on systems using small and large contexts; (3) We introduce a multi45;task training for regularization to avoid models overfitting our training corpora which further improves our systems together with a deeper encoder. Experiments are conducted on the widely used IWSLT data sets with three language pairs i.e. Chinese45;45;English French45;45;English and Spanish45;45;English. Results show that our systems are significantly better than three previously reported document45;level systems.
