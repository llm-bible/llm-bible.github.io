---
layout: publication
title: VAR45;CLIP Text45;to45;image Generator With Visual Auto45;regressive Modeling
authors: Zhang Qian, Dai Xiangzi, Yang Ninghua, An Xiang, Feng Ziyong, Ren Xingyu
conference: "Arxiv"
year: 2024
bibkey: zhang2024var
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.01181"}
  - {name: "Code", url: "https://github.com/daixiangzi/VAR&#45;CLIP"}
tags: ['Has Code', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques', 'Transformer']
---
VAR is a new generation paradigm that employs next45;scale prediction as opposed to next45;token prediction. This innovative transformation enables auto45;regressive (AR) transformers to rapidly learn visual distributions and achieve robust generalization. However the original VAR model is constrained to class45;conditioned synthesis relying solely on textual captions for guidance. In this paper we introduce VAR45;CLIP a novel text45;to45;image model that integrates Visual Auto45;Regressive techniques with the capabilities of CLIP. The VAR45;CLIP framework encodes captions into text embeddings which are then utilized as textual conditions for image generation. To facilitate training on extensive datasets such as ImageNet we have constructed a substantial image45;text dataset leveraging BLIP2. Furthermore we delve into the significance of word positioning within CLIP for the purpose of caption guidance. Extensive experiments confirm VAR45;CLIPs proficiency in generating fantasy images with high fidelity textual congruence and aesthetic excellence. Our project page are https://github.com/daixiangzi/VAR&#45;CLIP
