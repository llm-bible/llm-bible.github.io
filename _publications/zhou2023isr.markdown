---
layout: publication
title: ISR45;LLM Iterative Self45;refined Large Language Model For Long45;horizon Sequential Task Planning
authors: Zhou Zhehua, Song Jiayang, Yao Kunpeng, Shu Zhan, Ma Lei
conference: "Arxiv"
year: 2023
bibkey: zhou2023isr
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.13724"}
tags: ['Pretraining Methods', 'Tools']
---
Motivated by the substantial achievements observed in Large Language Models (LLMs) in the field of natural language processing recent research has commenced investigations into the application of LLMs for complex long45;horizon sequential task planning challenges in robotics. LLMs are advantageous in offering the potential to enhance the generalizability as task45;agnostic planners and facilitate flexible interaction between human instructors and planning systems. However task plans generated by LLMs often lack feasibility and correctness. To address this challenge we introduce ISR45;LLM a novel framework that improves LLM45;based planning through an iterative self45;refinement process. The framework operates through three sequential steps preprocessing planning and iterative self45;refinement. During preprocessing an LLM translator is employed to convert natural language input into a Planning Domain Definition Language (PDDL) formulation. In the planning phase an LLM planner formulates an initial plan which is then assessed and refined in the iterative self45;refinement step by using a validator. We examine the performance of ISR45;LLM across three distinct planning domains. The results show that ISR45;LLM is able to achieve markedly higher success rates in task accomplishments compared to state45;of45;the45;art LLM45;based planners. Moreover it also preserves the broad applicability and generalizability of working with natural language instructions.
