---
layout: publication
title: 'Position Engineering: Boosting Large Language Models Through Positional Information Manipulation'
authors: Zhiyuan He, Huiqiang Jiang, Zilong Wang, Yuqing Yang, Luna Qiu, Lili Qiu
conference: "Arxiv"
year: 2024
bibkey: he2024position
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2404.11216'}
tags: ['RAG', 'Prompting', 'In-Context Learning']
---
The performance of large language models (LLMs) is significantly influenced
by the quality of the prompts provided. In response, researchers have developed
enormous prompt engineering strategies aimed at modifying the prompt text to
enhance task performance. In this paper, we introduce a novel technique termed
position engineering, which offers a more efficient way to guide large language
models. Unlike prompt engineering, which requires substantial effort to modify
the text provided to LLMs, position engineering merely involves altering the
positional information in the prompt without modifying the text itself. We have
evaluated position engineering in two widely-used LLM scenarios:
retrieval-augmented generation (RAG) and in-context learning (ICL). Our
findings show that position engineering substantially improves upon the
baseline in both cases. Position engineering thus represents a promising new
strategy for exploiting the capabilities of large language models.
