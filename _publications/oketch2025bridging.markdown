---
layout: publication
title: 'Bridging The LLM Accessibility Divide? Performance, Fairness, And Cost Of Closed Versus Open Llms For Automated Essay Scoring'
authors: Kezia Oketch, John P. Lalor, Yi Yang, Ahmed Abbasi
conference: "Arxiv"
year: 2025
bibkey: oketch2025bridging
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.11827'}
tags: ['Few-Shot', 'Fairness', 'GPT', 'Model Architecture', 'Bias Mitigation', 'Ethics and Bias', 'Interpretability']
---
Closed large language models (LLMs) such as GPT-4 have set state-of-the-art
results across a number of NLP tasks and have become central to NLP and machine
learning (ML)-driven solutions. Closed LLMs' performance and wide adoption has
sparked considerable debate about their accessibility in terms of availability,
cost, and transparency. In this study, we perform a rigorous comparative
analysis of nine leading LLMs, spanning closed, open, and open-source LLM
ecosystems, across text assessment and generation tasks related to automated
essay scoring. Our findings reveal that for few-shot learning-based assessment
of human generated essays, open LLMs such as Llama 3 and Qwen2.5 perform
comparably to GPT-4 in terms of predictive performance, with no significant
differences in disparate impact scores when considering age- or race-related
fairness. Moreover, Llama 3 offers a substantial cost advantage, being up to 37
times more cost-efficient than GPT-4. For generative tasks, we find that essays
generated by top open LLMs are comparable to closed LLMs in terms of their
semantic composition/embeddings and ML assessed scores. Our findings challenge
the dominance of closed LLMs and highlight the democratizing potential of open
LLMs, suggesting they can effectively bridge accessibility divides while
maintaining competitive performance and fairness.
