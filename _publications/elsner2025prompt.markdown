---
layout: publication
title: 'Prompt And Circumstance: A Word-by-word LLM Prompting Approach To Interlinear Glossing For Low-resource Languages'
authors: Micha Elsner, David Liu
conference: "Arxiv"
year: 2025
bibkey: elsner2025prompt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.09778"}
tags: ['Model Architecture', 'Reinforcement Learning', 'ACL', 'BERT', 'Prompting']
---
Partly automated creation of interlinear glossed text (IGT) has the potential
to assist in linguistic documentation. We argue that LLMs can make this process
more accessible to linguists because of their capacity to follow
natural-language instructions. We investigate the effectiveness of a
retrieval-based LLM prompting approach to glossing, applied to the seven
languages from the SIGMORPHON 2023 shared task. Our system beats the BERT-based
shared task baseline for every language in the morpheme-level score category,
and we show that a simple 3-best oracle has higher word-level scores than the
challenge winner (a tuned sequence model) in five languages. In a case study on
Tsez, we ask the LLM to automatically create and follow linguistic
instructions, reducing errors on a confusing grammatical feature. Our results
thus demonstrate the potential contributions which LLMs can make in interactive
systems for glossing, both in making suggestions to human annotators and
following directions.
