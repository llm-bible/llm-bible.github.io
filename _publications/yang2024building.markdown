---
layout: publication
title: 'Building A Taiwanese Mandarin Spoken Language Model: A First Attempt'
authors: Chih-kai Yang, Yu-kuan Fu, Chen-an Li, Yi-cheng Lin, Yu-xiang Lin, Wei-chih Chen, Ho Lam Chung, Chun-yi Kuan, Wei-ping Huang, Ke-han Lu, Tzu-quan Lin, Hsiu-hsuan Wang, En-pei Hu, Chan-jan Hsu, Liang-hsuan Tseng, I-hsiang Chiu, Ulin Sanga, Xuanjun Chen, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee
conference: "Arxiv"
year: 2024
bibkey: yang2024building
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.07111'}
tags: ['Transformer', 'Training Techniques', 'Model Architecture', 'Tools', 'Pretraining Methods']
---
This technical report presents our initial attempt to build a spoken large
language model (LLM) for Taiwanese Mandarin, specifically tailored to enable
real-time, speech-to-speech interaction in multi-turn conversations. Our
end-to-end model incorporates a decoder-only transformer architecture and aims
to achieve seamless interaction while preserving the conversational flow,
including full-duplex capabilities allowing simultaneous speaking and
listening. The paper also details the training process, including data
preparation with synthesized dialogues and adjustments for real-time
interaction. We also developed a platform to evaluate conversational fluency
and response coherence in multi-turn dialogues. We hope the release of the
report can contribute to the future development of spoken LLMs in Taiwanese
Mandarin.
