---
layout: publication
title: 'Mudaf: Long-context Multi-document Attention Focusing Through Contrastive Learning On Attention Heads'
authors: Weihao Liu, Ning Wu, Shiping Yang, Wenbiao Ding, Shining Liang, Ming Gong, Dongmei Zhang
conference: "Arxiv"
year: 2025
bibkey: liu2025long
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.13963"}
tags: ['Model Architecture', 'Pretraining Methods', 'Interpretability', 'Applications', 'Attention Mechanism']
---
Large Language Models (LLMs) frequently show distracted attention due to
irrelevant information in the input, which severely impairs their long-context
capabilities. Inspired by recent studies on the effectiveness of retrieval
heads in long-context factutality, we aim at addressing this distraction issue
through improving such retrieval heads directly. We propose Multi-Document
Attention Focusing (MuDAF), a novel method that explicitly optimizes the
attention distribution at the head level through contrastive learning.
According to the experimental results, MuDAF can significantly improve the
long-context question answering performance of LLMs, especially in
multi-document question answering. Extensive evaluations on retrieval scores
and attention visualizations show that MuDAF possesses great potential in
making attention heads more focused on relevant information and reducing
attention distractions.
