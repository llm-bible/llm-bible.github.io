---
layout: publication
title: 'Adversarial Reasoning At Jailbreaking Time'
authors: Mahdi Sabbaghi, Paul Kassianik, George Pappas, Yaron Singer, Amin Karbasi, Hamed Hassani
conference: "Arxiv"
year: 2025
bibkey: sabbaghi2025adversarial
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.01633"}
tags: ['Security']
---
As large language models (LLMs) are becoming more capable and widespread, the
study of their failure cases is becoming increasingly important. Recent
advances in standardizing, measuring, and scaling test-time compute suggest new
methodologies for optimizing models to achieve high performance on hard tasks.
In this paper, we apply these advances to the task of model jailbreaking:
eliciting harmful responses from aligned LLMs. We develop an adversarial
reasoning approach to automatic jailbreaking via test-time computation that
achieves SOTA attack success rates (ASR) against many aligned LLMs, even the
ones that aim to trade inference-time compute for adversarial robustness. Our
approach introduces a new paradigm in understanding LLM vulnerabilities, laying
the foundation for the development of more robust and trustworthy AI systems.
