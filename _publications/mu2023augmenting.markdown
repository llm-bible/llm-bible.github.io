---
layout: publication
title: Augmenting Large Language Model Translators Via Translation Memories
authors: Mu Yongyu, Reheman Abudurexiti, Cao Zhiquan, Fan Yuchun, Li Bei, Li Yinqiao, Xiao Tong, Zhang Chunliang, Zhu Jingbo
conference: "Arxiv"
year: 2023
bibkey: mu2023augmenting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.17367"}
tags: ['Applications', 'Prompting']
---
Using translation memories (TMs) as prompts is a promising approach to in45;context learning of machine translation models. In this work we take a step towards prompting large language models (LLMs) with TMs and making them better translators. We find that the ability of LLMs to understand prompts is indeed helpful for making better use of TMs. Experiments show that the results of a pre45;trained LLM translator can be greatly improved by using high45;quality TM45;based prompts. These results are even comparable to those of the state45;of45;the45;art NMT systems which have access to large45;scale in45;domain bilingual data and are well tuned on the downstream tasks.
