---
layout: publication
title: 'Generalization Of Medical Large Language Models Through Cross-domain Weak Supervision'
authors: Robert Long, Eric Gonzalez, Harrison Fuller
conference: "Arxiv"
year: 2025
bibkey: long2025generalization
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.00832'}
tags: ['Efficiency and Optimization', 'Applications', 'Training Techniques', 'Tools', 'Fine-Tuning', 'Reinforcement Learning', 'Pretraining Methods']
---
The advancement of large language models (LLMs) has opened new frontiers in
natural language processing, particularly in specialized domains like
healthcare. In this paper, we propose the Incremental Curriculum-Based
Fine-Tuning (ICFT) framework to enhance the generative capabilities of medical
large language models (MLLMs). ICFT combines curriculum-based learning,
dual-stage memory coordination, and parameter-efficient fine-tuning to enable a
progressive transition from general linguistic knowledge to strong
domain-specific expertise. Experimental results across diverse medical NLP
tasks, including question answering, preference classification, and response
generation, demonstrate that ICFT consistently outperforms state-of-the-art
baselines, achieving improvements in both accuracy and efficiency. Further
analysis reveals the framework's ability to generalize to unseen data, reduce
errors, and deliver diverse, contextually relevant medical responses. These
findings establish ICFT as a robust and scalable solution for adapting LLMs to
the medical domain, offering practical benefits for real-world healthcare
applications.
