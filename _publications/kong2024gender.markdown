---
layout: publication
title: 'Gender Bias In Llm-generated Interview Responses'
authors: Haein Kong, Yongsu Ahn, Sangyub Lee, Yunho Maeng
conference: "Arxiv"
year: 2024
bibkey: kong2024gender
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.20739"}
tags: ['Ethics and Bias', 'Model Architecture', 'Applications', 'GPT']
---
LLMs have emerged as a promising tool for assisting individuals in diverse
text-generation tasks, including job-related texts. However, LLM-generated
answers have been increasingly found to exhibit gender bias. This study
evaluates three LLMs (GPT-3.5, GPT-4, Claude) to conduct a multifaceted audit
of LLM-generated interview responses across models, question types, and jobs,
and their alignment with two gender stereotypes. Our findings reveal that
gender bias is consistent, and closely aligned with gender stereotypes and the
dominance of jobs. Overall, this study contributes to the systematic
examination of gender bias in LLM-generated interview responses, highlighting
the need for a mindful approach to mitigate such biases in related
applications.
