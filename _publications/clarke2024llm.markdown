---
layout: publication
title: 'Llm-based Relevance Assessment Still Can''t Replace Human Relevance Assessment'
authors: Charles L. A. Clarke, Laura Dietz
conference: "Arxiv"
year: 2024
bibkey: clarke2024llm
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.17156'}
tags: ['Reinforcement Learning', 'Attention Mechanism', 'Applications', 'Model Architecture']
---
The use of large language models (LLMs) for relevance assessment in
information retrieval has gained significant attention, with recent studies
suggesting that LLM-based judgments provide comparable evaluations to human
judgments. Notably, based on TREC 2024 data, Upadhyay et al. make a bold claim
that LLM-based relevance assessments, such as those generated by the UMBRELA
system, can fully replace traditional human relevance assessments in TREC-style
evaluations. This paper critically examines this claim, highlighting practical
and theoretical limitations that undermine the validity of this conclusion.
First, we question whether the evidence provided by Upadhyay et al. really
supports their claim, particularly if a test collection is used asa benchmark
for future improvements. Second, through a submission deliberately intended to
do so, we demonstrate the ease with which automatic evaluation metrics can be
subverted, showing that systems designed to exploit these evaluations can
achieve artificially high scores. Theoretical challenges -- such as the
inherent narcissism of LLMs, the risk of overfitting to LLM-based metrics, and
the potential degradation of future LLM performance -- must be addressed before
LLM-based relevance assessments can be considered a viable replacement for
human judgments.
