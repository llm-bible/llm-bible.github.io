---
layout: publication
title: 'Harry Potter Is Still Here! Probing Knowledge Leakage In Targeted Unlearned Large Language Models Via Automated Adversarial Prompting'
authors: Bang Trinh Tran To, Thai Le
conference: "Arxiv"
year: 2025
bibkey: to2025harry
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.17160"}
tags: ['Prompting', 'Security', 'Tools']
---
This work presents LURK (Latent UnleaRned Knowledge), a novel framework that probes for hidden retained knowledge in unlearned LLMs through adversarial suffix prompting. LURK automatically generates adversarial prompt suffixes designed to elicit residual knowledge about the Harry Potter domain, a commonly used benchmark for unlearning. Our experiments reveal that even models deemed successfully unlearned can leak idiosyncratic information under targeted adversarial conditions, highlighting critical limitations of current unlearning evaluation standards. By uncovering latent knowledge through indirect probing, LURK offers a more rigorous and diagnostic tool for assessing the robustness of unlearning algorithms. All code will be publicly available.
