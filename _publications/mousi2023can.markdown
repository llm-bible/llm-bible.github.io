---
layout: publication
title: Can Llms Facilitate Interpretation Of Pre45;trained Language Models
authors: Mousi Basel, Durrani Nadir, Dalvi Fahim
conference: "Arxiv"
year: 2023
bibkey: mousi2023can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.13386"}
tags: ['Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Tools']
---
Work done to uncover the knowledge encoded within pre45;trained language models rely on annotated corpora or human45;in45;the45;loop methods. However these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model ChatGPT as an annotator to enable fine45;grained interpretation analysis of pre45;trained language models. We discover latent concepts within pre45;trained language models by applying agglomerative hierarchical clustering over contextualized representations and then annotate these concepts using ChatGPT. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human45;annotated concepts. Additionally we showcase how GPT45;based annotations empower interpretation analysis methodologies of which we demonstrate two probing frameworks and neuron interpretation. To facilitate further exploration and experimentation in the field we make available a substantial ConceptNet dataset (TCN) comprising 39000 annotated concepts.
