---
layout: publication
title: Robustsentembed Robust Sentence Embeddings Using Adversarial Self-supervised Contrastive Learning
authors: Asl Javad Rafiei, Panzade Prajwal, Blanco Eduardo, Takabi Daniel, Cai Zhipeng
conference: "Arxiv"
year: 2024
bibkey: asl2024robust
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.11082"}
tags: ['BERT', 'Model Architecture', 'Pretraining Methods', 'Security', 'Tools', 'Training Techniques']
---
Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless despite their success with unseen data current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper we introduce RobustSentEmbed a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically Our framework achieves a significant reduction in the success rate of various adversarial attacks notably reducing the BERTAttack success rate by almost half (from 75.5137; to 38.8137;). The framework also yields improvements of 1.5937; and 0.2337; in semantic textual similarity tasks and various transfer tasks respectively.
