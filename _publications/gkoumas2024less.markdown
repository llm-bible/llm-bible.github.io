---
layout: publication
title: 'Less For More: Enhanced Feedback-aligned Mixed Llms For Molecule Caption Generation And Fine-grained NLI Evaluation'
authors: Dimitris Gkoumas, Maria Liakata
conference: "Arxiv"
year: 2024
bibkey: gkoumas2024less
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2405.13984'}
tags: ['RAG', 'Training Techniques', 'Merging', 'Fine-Tuning', 'Multimodal Models', 'Pretraining Methods']
---
Scientific language models drive research innovation but require extensive fine-tuning on large datasets. This work enhances such models by improving their inference and evaluation capabilities with minimal or no additional training. Focusing on molecule caption generation, we explore post-training synergies between alignment fine-tuning and model merging in a cross-modal setup. We reveal intriguing insights into the behaviour and suitability of such methods while significantly surpassing state-of-the-art models. Moreover, we propose a novel atomic-level evaluation method leveraging off-the-shelf Natural Language Inference (NLI) models for use in the unseen chemical domain. Our experiments demonstrate that our evaluation operates at the right level of granularity, effectively handling multiple content units and subsentence reasoning, while widely adopted NLI methods consistently misalign with assessment criteria.
