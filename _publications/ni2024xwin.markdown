---
layout: publication
title: Xwin45;lm Strong And Scalable Alignment Practice For Llms
authors: Ni Bolin, Hu Jingcheng, Wei Yixuan, Peng Houwen, Zhang Zheng, Meng Gaofeng, Hu Han
conference: "Arxiv"
year: 2024
bibkey: ni2024xwin
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.20335"}
  - {name: "Code", url: "https://github.com/Xwin&#45;LM/Xwin&#45;LM"}
tags: ['Efficiency And Optimization', 'Fine Tuning', 'GPT', 'Has Code', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
In this work we present Xwin45;LM a comprehensive suite of alignment methodologies for large language models (LLMs). This suite encompasses several key techniques including supervised finetuning (SFT) reward modeling (RM) rejection sampling finetuning (RS) and direct preference optimization (DPO). The key components are as follows (1) Xwin45;LM45;SFT models initially finetuned with high45;quality instruction data; (2) Xwin45;Pair a large45;scale multi45;turn preference dataset meticulously annotated using GPT45;4; (3) Xwin45;RM reward models trained on Xwin45;Pair developed at scales of 7B 13B and 70B parameters; (4) Xwin45;Set a multiwise preference dataset in which each prompt is linked to 64 unique responses generated by Xwin45;LM45;SFT and scored by Xwin45;RM; (5) Xwin45;LM45;RS models finetuned with the highest45;scoring responses from Xwin45;Set; (6) Xwin45;LM45;DPO models further optimized on Xwin45;Set using the DPO algorithm. Our evaluations on AlpacaEval and MT45;bench demonstrate consistent and significant improvements across the pipeline demonstrating the strength and scalability of Xwin45;LM. The repository https://github.com/Xwin&#45;LM/Xwin&#45;LM will be continually updated to foster community research.
