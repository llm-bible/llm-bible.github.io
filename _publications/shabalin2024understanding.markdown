---
layout: publication
title: Tencdm Understanding The Properties Of Diffusion Model In The Space Of Language Model Encodings
authors: Shabalin Alexander, Meshchaninov Viacheslav, Chimbulatov Egor, Lapikov Vladislav, Kim Roman, Bartosh Grigory, Molchanov Dmitry, Markov Sergey, Vetrov Dmitry
conference: "Arxiv"
year: 2024
bibkey: shabalin2024understanding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.19097"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
This paper presents the Text Encoding Diffusion Model (TEncDM) a novel approach to diffusion modeling that operates in the space of pre45;trained language model encodings. In contrast to traditionally used embeddings encodings integrate contextual information. In our approach we also employ a transformer45;based decoder specifically designed to incorporate context in the token prediction process. We conduct a comprehensive examination of the influence of the encoder decoder noise scheduler and self45;conditioning on zero45;shot generation. Furthermore we compare TEncDM with previous approaches on three conditional text generation tasks QQP XSum and Wiki45;Auto. The results show that TEncDM exhibits superior performance compared to existing non45;autoregressive diffusion models.
