---
layout: publication
title: 'Agentspec: Customizable Runtime Enforcement For Safe And Reliable LLM Agents'
authors: Haoyu Wang, Christopher M. Poskitt, Jun Sun
conference: "Arxiv"
year: 2025
bibkey: wang2025customizable
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.18666"}
tags: ['Responsible AI', 'Agentic', 'Security', 'Efficiency and Optimization', 'Reinforcement Learning', 'Interpretability and Explainability', 'Applications']
---
Agents built on LLMs are increasingly deployed across diverse domains,
automating complex decision-making and task execution. However, their autonomy
introduces safety risks, including security vulnerabilities, legal violations,
and unintended harmful actions. Existing mitigation methods, such as
model-based safeguards and early enforcement strategies, fall short in
robustness, interpretability, and adaptability. To address these challenges, we
propose AgentSpec, a lightweight domain-specific language for specifying and
enforcing runtime constraints on LLM agents. With AgentSpec, users define
structured rules that incorporate triggers, predicates, and enforcement
mechanisms, ensuring agents operate within predefined safety boundaries. We
implement AgentSpec across multiple domains, including code execution, embodied
agents, and autonomous driving, demonstrating its adaptability and
effectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe
executions in over 90% of code agent cases, eliminates all hazardous actions in
embodied agent tasks, and enforces 100% compliance by autonomous vehicles
(AVs). Despite its strong safety guarantees, AgentSpec remains computationally
lightweight, with overheads in milliseconds. By combining interpretability,
modularity, and efficiency, AgentSpec provides a practical and scalable
solution for enforcing LLM agent safety across diverse applications. We also
automate the generation of rules using LLMs and assess their effectiveness. Our
evaluation shows that the rules generated by OpenAI o1 achieve a precision of
95.56% and recall of 70.96% for embodied agents, successfully identifying
87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8
scenarios.
