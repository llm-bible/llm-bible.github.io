---
layout: publication
title: 'Understanding Knowledge Hijack Mechanism In In-context Learning Through Associative Memory'
authors: Shuo Wang, Issei Sato
conference: "Arxiv"
year: 2024
bibkey: wang2024understanding
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.11459'}
tags: ['Transformer', 'RAG', 'Training Techniques', 'Model Architecture', 'Fine-Tuning', 'Prompting', 'In-Context Learning', 'Pretraining Methods']
---
In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks without fine-tuning by leveraging contextual information provided
within a prompt. However, ICL relies not only on contextual clues but also on
the global knowledge acquired during pretraining for the next token prediction.
Analyzing this process has been challenging due to the complex computational
circuitry of LLMs. This paper investigates the balance between in-context
information and pretrained bigram knowledge in token prediction, focusing on
the induction head mechanism, a key component in ICL. Leveraging the fact that
a two-layer transformer can implement the induction head mechanism with
associative memories, we theoretically analyze the logits when a two-layer
transformer is given prompts generated by a bigram model. In the experiments,
we design specific prompts to evaluate whether the outputs of a two-layer
transformer align with the theoretical results.
