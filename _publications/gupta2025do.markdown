---
layout: publication
title: 'Do Gflownets Transfer? Case Study On The Game Of 24/42'
authors: Adesh Gupta, Abhinav Kumar, Mansi Gupta, Paras Chopra
conference: "Arxiv"
year: 2025
bibkey: gupta2025do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.01819"}
tags: ['Fine-Tuning', 'Training Techniques', 'GPT', 'Pretraining Methods']
---
Generating diverse solutions is key to human-like reasoning, yet
autoregressive language models focus on single accurate responses, limiting
creativity. GFlowNets optimize solution generation as a flow network, promising
greater diversity. Our case study shows their limited zero-shot transferability
by fine-tuning small and medium-sized large language models on the Game of 24
and testing them on the Game of 42 datasets. Results revealed that GFlowNets
struggle to maintain solution diversity and accuracy, highlighting key
limitations in their cross-task generalization and the need for future research
in improved transfer learning capabilities.
