---
layout: publication
title: 'Vision-language Models For Medical Report Generation And Visual Question Answering:
  A Review'
authors: Iryna Hartsock, Ghulam Rasool
conference: Arxiv
year: 2024
citations: 32
bibkey: hartsock2024vision
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2403.02469'}]
tags: [Multimodal Models, Survey Paper, Applications, Pre-Training]
---
Medical vision-language models (VLMs) combine computer vision (CV) and
natural language processing (NLP) to analyze visual and textual medical data.
Our paper reviews recent advancements in developing VLMs specialized for
healthcare, focusing on models designed for medical report generation and
visual question answering (VQA). We provide background on NLP and CV,
explaining how techniques from both fields are integrated into VLMs to enable
learning from multimodal data. Key areas we address include the exploration of
medical vision-language datasets, in-depth analyses of architectures and
pre-training strategies employed in recent noteworthy medical VLMs, and
comprehensive discussion on evaluation metrics for assessing VLMs' performance
in medical report generation and VQA. We also highlight current challenges and
propose future directions, including enhancing clinical validity and addressing
patient privacy concerns. Overall, our review summarizes recent progress in
developing VLMs to harness multimodal medical data for improved healthcare
applications.