---
layout: publication
title: 'Language Model Behavior: A Comprehensive Survey'
authors: Tyler A. Chang, Benjamin K. Bergen
conference: "Arxiv"
year: 2023
bibkey: chang2023language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2303.11504"}
tags: ['Fine-Tuning', 'Transformer', 'Survey Paper', 'Ethics and Bias', 'RAG', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques', 'Attention Mechanism', 'Pretraining Methods']
---
Transformer language models have received widespread public attention, yet
their generated text is often surprising even to NLP researchers. In this
survey, we discuss over 250 recent studies of English language model behavior
before task-specific fine-tuning. Language models possess basic capabilities in
syntax, semantics, pragmatics, world knowledge, and reasoning, but these
capabilities are sensitive to specific inputs and surface features. Despite
dramatic increases in generated text quality as models scale to hundreds of
billions of parameters, the models are still prone to unfactual responses,
commonsense errors, memorized text, and social biases. Many of these weaknesses
can be framed as over-generalizations or under-generalizations of learned
patterns in text. We synthesize recent results to highlight what is currently
known about large language model capabilities, thus providing a resource for
applied work and for research in adjacent fields that use language models.
