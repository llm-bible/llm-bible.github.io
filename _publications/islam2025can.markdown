---
layout: publication
title: 'Can Llms Assist Annotators In Identifying Morality Frames? -- Case Study On Vaccination Debate On Social Media'
authors: Tunazzina Islam, Dan Goldwasser
conference: "Arxiv"
year: 2025
bibkey: islam2025can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.01991"}
tags: ['RAG', 'Few-Shot', 'Tools', 'Interpretability and Explainability']
---
Nowadays, social media is pivotal in shaping public discourse, especially on
polarizing issues like vaccination, where diverse moral perspectives influence
individual opinions. In NLP, data scarcity and complexity of psycholinguistic
tasks, such as identifying morality frames, make relying solely on human
annotators costly, time-consuming, and prone to inconsistency due to cognitive
load. To address these issues, we leverage large language models (LLMs), which
are adept at adapting new tasks through few-shot learning, utilizing a handful
of in-context examples coupled with explanations that connect examples to task
principles. Our research explores LLMs' potential to assist human annotators in
identifying morality frames within vaccination debates on social media. We
employ a two-step process: generating concepts and explanations with LLMs,
followed by human evaluation using a "think-aloud" tool. Our study shows that
integrating LLMs into the annotation process enhances accuracy, reduces task
difficulty, lowers cognitive load, suggesting a promising avenue for human-AI
collaboration in complex psycholinguistic tasks.
