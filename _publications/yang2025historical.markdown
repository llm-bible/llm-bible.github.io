---
layout: publication
title: 'Kongzi: A Historical Large Language Model With Fact Enhancement'
authors: Jiashu Yang, Ningning Wang, Yian Zhao, Chaoran Feng, Junjia Du, Hao Pang, Zhirui Fang, Xuxin Cheng
conference: "Arxiv"
year: 2025
bibkey: yang2025historical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.09488"}
tags: ['Agentic', 'RAG', 'Applications', 'Reinforcement Learning']
---
The capabilities of the latest large language models (LLMs) have been
extended from pure natural language understanding to complex reasoning tasks.
However, current reasoning models often exhibit factual inaccuracies in longer
reasoning chains, which poses challenges for historical reasoning and limits
the potential of LLMs in complex, knowledge-intensive tasks. Historical studies
require not only the accurate presentation of factual information but also the
ability to establish cross-temporal correlations and derive coherent
conclusions from fragmentary and often ambiguous sources. To address these
challenges, we propose Kongzi, a large language model specifically designed for
historical analysis. Through the integration of curated, high-quality
historical data and a novel fact-reinforcement learning strategy, Kongzi
demonstrates strong factual alignment and sophisticated reasoning depth.
Extensive experiments on tasks such as historical question answering and
narrative generation demonstrate that Kongzi outperforms existing models in
both factual accuracy and reasoning depth. By effectively addressing the unique
challenges inherent in historical texts, Kongzi sets a new standard for the
development of accurate and reliable LLMs in professional domains.
