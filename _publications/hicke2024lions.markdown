---
layout: publication
title: Lions 1 and Tigers 2 and Bears 3 Oh My! Literary Coreference Annotation with LLMs
authors: Hicke Rebecca M. M., Mimno David
conference: "Arxiv"
year: 2024
bibkey: hicke2024lions
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.17922"}
tags: ['ARXIV', 'LLM', 'Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Coreference annotation and resolution is a vital component of computational literary studies. However it has previously been difficult to build high quality systems for fiction. Coreference requires complicated structured outputs and literary text involves subtle inferences and highly varied language. New language-model-based seq2seq systems present the opportunity to solve both these problems by learning to directly generate a copy of an input sentence with markdown-like annotations. We create evaluate and release several trained models for coreference as well as a workflow for training new models.
