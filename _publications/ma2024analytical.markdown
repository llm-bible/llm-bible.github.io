---
layout: publication
title: 'Agentboard: An Analytical Evaluation Board Of Multi-turn LLM Agents'
authors: Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He
conference: "Arxiv"
year: 2024
bibkey: ma2024analytical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.13178"}
tags: ['Agentic', 'Tools', 'Applications', 'Interpretability and Explainability', 'TACL', 'ACL']
---
Evaluating Large Language Models (LLMs) as general-purpose agents is
essential for understanding their capabilities and facilitating their
integration into practical applications. However, the evaluation process
presents substantial challenges. A primary obstacle is the benchmarking of
agent performance across diverse scenarios within a unified framework,
especially in maintaining partially-observable environments and ensuring
multi-round interactions. Moreover, current evaluation frameworks mostly focus
on the final success rate, revealing few insights during the process and
failing to provide a deep understanding of the model abilities. To address
these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark
and accompanied open-source evaluation framework tailored to analytical
evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric
that captures incremental advancements as well as a comprehensive evaluation
toolkit that features easy assessment of agents for multi-faceted analysis.
This not only sheds light on the capabilities and limitations of LLM agents but
also propels the interpretability of their performance to the forefront.
Ultimately, AgentBoard serves as a step towards demystifying agent behaviors
and accelerating the development of stronger LLM agents.
