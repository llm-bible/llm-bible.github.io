---
layout: publication
title: In45;context Time Series Predictor
authors: Lu Jiecheng, Sun Yan, Yang Shihao
conference: "Arxiv"
year: 2024
bibkey: lu2024time
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.14982"}
tags: ['Model Architecture', 'Pretraining Methods', 'Transformer']
---
Recent Transformer45;based large language models (LLMs) demonstrate in45;context learning ability to perform various functions based solely on the provided context without updating model parameters. To fully utilize the in45;context capabilities in time series forecasting (TSF) problems unlike previous Transformer45;based or LLM45;based time series forecasting methods we reformulate time series forecasting tasks as input tokens by constructing a series of (lookback future) pairs within the tokens. This method aligns more closely with the inherent in45;context mechanisms and is more parameter45;efficient without the need of using pre45;trained LLM parameters. Furthermore it addresses issues such as overfitting in existing Transformer45;based TSF models consistently achieving better performance across full45;data few45;shot and zero45;shot settings compared to previous architectures.
