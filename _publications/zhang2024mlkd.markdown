---
layout: publication
title: MLKD45;BERT Multi45;level Knowledge Distillation For Pre45;trained Language Models
authors: Zhang Ying, Yang Ziheng, Ji Shufan
conference: "Arxiv"
year: 2024
bibkey: zhang2024mlkd
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.02775"}
tags: ['Applications', 'Attention Mechanism', 'BERT', 'Distillation', 'Efficiency And Optimization', 'Model Architecture', 'Quantization', 'Tools']
---
Knowledge distillation is an effective technique for pre45;trained language model compression. Although existing knowledge distillation methods perform well for the most typical model BERT they could be further improved in two aspects the relation45;level knowledge could be further explored to improve model performance; and the setting of student attention head number could be more flexible to decrease inference time. Therefore we are motivated to propose a novel knowledge distillation method MLKD45;BERT to distill multi45;level knowledge in teacher45;student framework. Extensive experiments on GLUE benchmark and extractive question answering tasks demonstrate that our method outperforms state45;of45;the45;art knowledge distillation methods on BERT. In addition MLKD45;BERT can flexibly set student attention head number allowing for substantial inference time decrease with little performance drop.
