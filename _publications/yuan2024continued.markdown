---
layout: publication
title: 'A Continued Pretrained LLM Approach For Automatic Medical Note Generation'
authors: Dong Yuan, Eti Rastogi, Gautam Naik, Sree Prasanna Rajagopal, Sagar Goyal, Fen Zhao, Bharath Chintagunta, Jeff Ward
conference: "Arxiv"
year: 2024
bibkey: yuan2024continued
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2403.09057'}
tags: ['GPT', 'Model Architecture']
---
LLMs are revolutionizing NLP tasks. However, the use of the most advanced
LLMs, such as GPT-4, is often prohibitively expensive for most specialized
fields. We introduce HEAL, the first continuously trained 13B LLaMA2-based LLM
that is purpose-built for medical conversations and measured on automated
scribing. Our results demonstrate that HEAL outperforms GPT-4 and PMC-LLaMA in
PubMedQA, with an accuracy of 78.4%. It also achieves parity with GPT-4 in
generating medical notes. Remarkably, HEAL surpasses GPT-4 and Med-PaLM 2 in
identifying more correct medical concepts and exceeds the performance of human
scribes and other comparable models in correctness and completeness.
