---
layout: publication
title: 'The Life Cycle Of Knowledge In Big Language Models: A Survey'
authors: Cao Boxi, Lin Hongyu, Han Xianpei, Sun Le
conference: "Machine Intelligence Research. vol."
year: 2023
bibkey: cao2023life
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2303.07616"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Survey Paper']
---
Knowledge plays a critical role in artificial intelligence. Recently, the extensive success of pre-trained language models (PLMs) has raised significant attention about how knowledge can be acquired, maintained, updated and used by language models. Despite the enormous amount of related studies, there still lacks a unified view of how knowledge circulates within language models throughout the learning, tuning, and application processes, which may prevent us from further understanding the connections between current progress or realizing existing limitations. In this survey, we revisit PLMs as knowledge-based systems by dividing the life circle of knowledge in PLMs into five critical periods, and investigating how knowledge circulates when it is built, maintained and used. To this end, we systematically review existing studies of each period of the knowledge life cycle, summarize the main challenges and current limitations, and discuss future directions.
