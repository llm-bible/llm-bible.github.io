---
layout: publication
title: Towards Llm45;driven Dialogue State Tracking
authors: Feng Yujie, Lu Zexin, Liu Bo, Zhan Liming, Wu Xiao-ming
conference: "Arxiv"
year: 2023
bibkey: feng2023towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.14970"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Tools']
---
Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within task45;oriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across diverse applications. In this study we conduct an initial examination of ChatGPTs capabilities in DST. Our evaluation uncovers the exceptional performance of ChatGPT in this task offering valuable insights to researchers regarding its capabilities and providing useful directions for designing and enhancing dialogue systems. Despite its impressive performance ChatGPT has significant limitations including its closed45;source nature request restrictions raising data privacy concerns and lacking local deployment capabilities. To address these concerns we present LDST an LLM45;driven DST framework based on smaller open45;source foundation models. By utilizing a novel domain45;slot instruction tuning method LDST achieves performance on par with ChatGPT. Comprehensive evaluations across three distinct experimental settings we find that LDST exhibits remarkable performance improvements in both zero45;shot and few45;shot setting compared to previous SOTA methods. The source code is provided for reproducibility.
