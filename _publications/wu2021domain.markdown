---
layout: publication
title: Domain45;adaptive Pretraining Methods For Dialogue Understanding
authors: Wu Han, Xu Kun, Song Linfeng, Jin Lifeng, Zhang Haisong, Song Linqi
conference: "Arxiv"
year: 2021
bibkey: wu2021domain
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2105.13665"}
tags: ['BERT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
Language models like BERT and SpanBERT pretrained on open45;domain data have obtained impressive gains on various NLP tasks. In this paper we probe the effectiveness of domain45;adaptive pretraining objectives on downstream tasks. In particular three objectives including a novel objective focusing on modeling predicate45;argument relations are evaluated on two challenging dialogue understanding tasks. Experimental results demonstrate that domain45;adaptive pretraining with proper objectives can significantly improve the performance of a strong baseline on these tasks achieving the new state45;of45;the45;art performances.
