---
layout: publication
title: 'Domain-adaptive Pretraining Methods For Dialogue Understanding'
authors: Han Wu, Kun Xu, Linfeng Song, Lifeng Jin, Haisong Zhang, Linqi Song
conference: "Arxiv"
year: 2021
bibkey: wu2021domain
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2105.13665'}
tags: ['Training Techniques', 'BERT', 'Model Architecture', 'Pretraining Methods']
---
Language models like BERT and SpanBERT pretrained on open-domain data have
obtained impressive gains on various NLP tasks. In this paper, we probe the
effectiveness of domain-adaptive pretraining objectives on downstream tasks. In
particular, three objectives, including a novel objective focusing on modeling
predicate-argument relations, are evaluated on two challenging dialogue
understanding tasks. Experimental results demonstrate that domain-adaptive
pretraining with proper objectives can significantly improve the performance of
a strong baseline on these tasks, achieving the new state-of-the-art
performances.
