---
layout: publication
title: A Theoretical Understanding Of Self45;correction Through In45;context Alignment
authors: Wang Yifei, Wu Yuyang, Wei Zeming, Jegelka Stefanie, Wang Yisen
conference: "Arxiv"
year: 2024
bibkey: wang2024theoretical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.18634"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
Going beyond mimicking limited human experiences recent studies show initial evidence that like humans large language models (LLMs) are capable of improving their abilities purely by self45;correction i.e. correcting previous responses through self45;examination in certain circumstances. Nevertheless little is known about how such capabilities arise. In this work based on a simplified setup akin to an alignment task we theoretically analyze self45;correction from an in45;context learning perspective showing that when LLMs give relatively accurate self45;examinations as rewards they are capable of refining responses in an in45;context way. Notably going beyond previous theories on over45;simplified linear transformers our theoretical construction underpins the roles of several key designs of realistic transformers for self45;correction softmax attention multi45;head attention and the MLP block. We validate these findings extensively on synthetic datasets. Inspired by these findings we also illustrate novel applications of self45;correction such as defending against LLM jailbreaks where a simple self45;correction step does make a large difference. We believe that these findings will inspire further research on understanding exploiting and enhancing self45;correction for building better foundation models.
