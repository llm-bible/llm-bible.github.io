---
layout: publication
title: 'Is Our Chatbot Telling Lies? Assessing Correctness Of An Llm-based Dutch Support Chatbot'
authors: Herman 1 And 2 Lassche, Michiel Overeem, Ayushi Rastogi
conference: "Arxiv"
year: 2024
bibkey: lassche2024is
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.00034'}
tags: ['Reinforcement Learning', 'RAG', 'Training Techniques']
---
Companies support their customers using live chats and chatbots to gain their
loyalty. AFAS is a Dutch company aiming to leverage the opportunity large
language models (LLMs) offer to answer customer queries with minimal to no
input from its customer support team. Adding to its complexity, it is unclear
what makes a response correct, and that too in Dutch. Further, with minimal
data available for training, the challenge is to identify whether an answer
generated by a large language model is correct and do it on the fly.
  This study is the first to define the correctness of a response based on how
the support team at AFAS makes decisions. It leverages literature on natural
language generation and automated answer grading systems to automate the
decision-making of the customer support team. We investigated questions
requiring a binary response (e.g., Would it be possible to adjust tax rates
manually?) or instructions (e.g., How would I adjust tax rate manually?) to
test how close our automated approach reaches support rating. Our approach can
identify wrong messages in 55% of the cases. This work shows the viability of
automatically assessing when our chatbot tell lies.
