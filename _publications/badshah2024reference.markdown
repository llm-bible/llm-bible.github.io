---
layout: publication
title: Reference45;guided Verdict Llms45;as45;judges In Automatic Evaluation Of Free45;form Text
authors: Badshah Sher, Sajjad Hassan
conference: "Arxiv"
year: 2024
bibkey: badshah2024reference
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.09235"}
tags: ['Applications', 'RAG', 'Reinforcement Learning']
---
The emergence of Large Language Models (LLMs) as chat assistants capable of generating human45;like conversations has amplified the need for robust evaluation methods particularly for open45;ended tasks. Conventional metrics like BLEU and ROUGE while useful are increasingly inadequate for capturing the subtle semantics and contextual richness of such generative outputs. We propose a reference45;guided verdict method that automates the evaluation process by leveraging multiple LLMs45;as45;judges. Through experiments on three open45;ended question45;answering tasks we demonstrate that combining multiple LLMs45;as45;judges significantly improves the reliability and accuracy of evaluations particularly in complex tasks where a single model might struggle. Our findings reveal a strong correlation with human evaluations establishing our method as a viable and effective alternative to traditional metrics and human judgments particularly in the context of LLM45;based chat assistants where the complexity and diversity of responses challenge existing benchmarks.
