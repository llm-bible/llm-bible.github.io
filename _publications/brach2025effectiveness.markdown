---
layout: publication
title: 'The Effectiveness Of Large Language Models In Transforming Unstructured Text To Standardized Formats'
authors: William Brach, Kristi치n Ko코콘치l, Michal Ries
conference: "Arxiv"
year: 2025
bibkey: brach2025effectiveness
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.02650"}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Few-Shot', 'GPT', 'Pretraining Methods', 'Fine-Tuning', 'Prompting', 'Applications', 'In-Context Learning']
---
The exponential growth of unstructured text data presents a fundamental
challenge in modern data management and information retrieval. While Large
Language Models (LLMs) have shown remarkable capabilities in natural language
processing, their potential to transform unstructured text into standardized,
structured formats remains largely unexplored - a capability that could
revolutionize data processing workflows across industries. This study breaks
new ground by systematically evaluating LLMs' ability to convert unstructured
recipe text into the structured Cooklang format. Through comprehensive testing
of four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an
innovative evaluation approach is introduced that combines traditional metrics
(WER, ROUGE-L, TER) with specialized metrics for semantic element
identification. Our experiments reveal that GPT-4o with few-shot prompting
achieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating
for the first time that LLMs can reliably transform domain-specific
unstructured text into structured formats without extensive training. Although
model performance generally scales with size, we uncover surprising potential
in smaller models like Llama3.1:8b for optimization through targeted
fine-tuning. These findings open new possibilities for automated structured
data generation across various domains, from medical records to technical
documentation, potentially transforming the way organizations process and
utilize unstructured information.
