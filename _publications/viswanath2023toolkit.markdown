---
layout: publication
title: Fairpy A Toolkit For Evaluation Of Social Biases And Their Mitigation In Large Language Models
authors: Viswanath Hrishikesh, Zhang Tianyi
conference: "Arxiv"
year: 2023
bibkey: viswanath2023toolkit
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2302.05508"}
  - {name: "Code", url: "https://github.com/HrishikeshVish/Fairpy"}
tags: ['BERT', 'Ethics And Bias', 'GPT', 'Has Code', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
Studies have shown that large pretrained language models exhibit biases against social groups based on race gender etc which they inherit from the datasets they are trained on. Various researchers have proposed mathematical tools for quantifying and identifying these biases. There have been methods proposed to mitigate such biases. In this paper we present a comprehensive quantitative evaluation of different kinds of biases such as race gender ethnicity age etc. exhibited by popular pretrained language models such as BERT GPT45;2 etc. and also present a toolkit that provides plug45;and45;play interfaces to connect mathematical tools to identify biases with large pretrained language models such as BERT GPT45;2 etc. and also present users with the opportunity to test custom models against these metrics. The toolkit also allows users to debias existing and custom models using the debiasing techniques proposed so far. The toolkit is available at https://github.com/HrishikeshVish/Fairpy.
