---
layout: publication
title: Mm45;llms Recent Advances In Multimodal Large Language Models
authors: Zhang Duzhen, Yu Yahan, Dong Jiahua, Li Chenxing, Su Dan, Chu Chenhui, Yu Dong
conference: "Arxiv"
year: 2024
bibkey: zhang2024mm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.13601"}
tags: ['Model Architecture', 'Multimodal Models', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
In the past year MultiModal Large Language Models (MM45;LLMs) have undergone substantial advancements augmenting off45;the45;shelf LLMs to support MM inputs or outputs via cost45;effective training strategies. The resulting models not only preserve the inherent reasoning and decision45;making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper we provide a comprehensive survey aimed at facilitating further research of MM45;LLMs. Initially we outline general design formulations for model architecture and training pipeline. Subsequently we introduce a taxonomy encompassing 126 MM45;LLMs each characterized by its specific formulations. Furthermore we review the performance of selected MM45;LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM45;LLMs. Finally we explore promising directions for MM45;LLMs while concurrently maintaining a real45;time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM45;LLMs domain.
