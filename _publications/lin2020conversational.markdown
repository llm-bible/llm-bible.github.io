---
layout: publication
title: Conversational Question Reformulation Via Sequence45;to45;sequence Architectures And Pretrained Language Models
authors: Lin Sheng-chieh, Yang Jheng-hong, Nogueira Rodrigo, Tsai Ming-feng, Wang Chuan-ju, Lin Jimmy
conference: "Arxiv"
year: 2020
bibkey: lin2020conversational
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2004.01909"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Transformer']
---
This paper presents an empirical study of conversational question reformulation (CQR) with sequence45;to45;sequence architectures and pretrained language models (PLMs). We leverage PLMs to address the strong token45;to45;token independence assumption made in the common objective maximum likelihood estimation for the CQR task. In CQR benchmarks of task45;oriented dialogue systems we evaluate fine45;tuned PLMs on the recently45;introduced CANARD dataset as an in45;domain task and validate the models using data from the TREC 2019 CAsT Track as an out45;domain task. Examining a variety of architectures with different numbers of parameters we demonstrate that the recent text45;to45;text transfer transformer (T5) achieves the best results both on CANARD and CAsT with fewer parameters compared to similar transformer architectures.
