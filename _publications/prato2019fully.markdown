---
layout: publication
title: Fully Quantized Transformer For Machine Translation
authors: Gabriele Prato, Ella Charlaix, Mehdi Rezagholizadeh
conference: Arxiv
year: 2019
citations: 25
bibkey: prato2019fully
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1910.10485'}]
tags: [Efficiency and Optimization, Transformer, Quantization]
---
State-of-the-art neural machine translation methods employ massive amounts of
parameters. Drastically reducing computational costs of such methods without
affecting performance has been up to this point unsuccessful. To this end, we
propose FullyQT: an all-inclusive quantization strategy for the Transformer. To
the best of our knowledge, we are the first to show that it is possible to
avoid any loss in translation quality with a fully quantized Transformer.
Indeed, compared to full-precision, our 8-bit models score greater or equal
BLEU on most tasks. Comparing ourselves to all previously proposed methods, we
achieve state-of-the-art quantization results.