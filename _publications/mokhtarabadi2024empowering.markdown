---
layout: publication
title: 'Empowering Persian Llms For Instruction Following: A Novel Dataset And Training Approach'
authors: Hojjat Mokhtarabadi, Ziba Zamani, Abbas Maazallahi, Mohammad Hossein Manshaei
conference: "Arxiv"
year: 2024
bibkey: mokhtarabadi2024empowering
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2407.11186'}
tags: ['Training Techniques', 'Fine-Tuning', 'Prompting', 'Tools']
---
Instruction-tuned large language models have demonstrated remarkable
capabilities in following human instructions across various domains. However,
their proficiency remains notably deficient in many low-resource languages. To
address this challenge, we begin by introducing FarsInstruct a comprehensive
instruction dataset designed to enhance the instruction following ability of
large language models specifically for the Persian language a significant yet
underrepresented language globally. FarsInstruct encompasses a wide range of
task types and datasets, each containing a mix of straightforward to complex
manual written instructions, as well as translations from the Public Pool of
Prompts, ensuring a rich linguistic and cultural representation. Furthermore,
we introduce Co-CoLA, a framework designed to enhance the multi-task
adaptability of LoRA-tuned models. Through extensive experimental analyses, our
study showcases the effectiveness of the FarsInstruct dataset coupled with
training by the Co-CoLA framework, in improving the performance of large
language models within the Persian context. As of the current writing,
FarsInstruct comprises 197 templates across 21 distinct datasets, and we intend
to update it consistently, thus augmenting its applicability.
