---
layout: publication
title: Avatargpt All45;in45;one Framework For Motion Understanding Planning Generation And Beyond
authors: Zhou Zixiang, Wan Yu, Wang Baoyuan
conference: "Arxiv"
year: 2023
bibkey: zhou2023all
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.16468"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Tools']
---
Large Language Models(LLMs) have shown remarkable emergent abilities in unifying almost all (if not every) NLP tasks. In the human motion45;related realm however researchers still develop siloed models for each task. Inspired by InstuctGPT and the generalist concept behind Gato we introduce AvatarGPT an All45;in45;One framework for motion understanding planning generations as well as other tasks such as motion in45;between synthesis. AvatarGPT treats each task as one type of instruction fine45;tuned on the shared LLM. All the tasks are seamlessly interconnected with language as the universal interface constituting a closed45;loop within the framework. To achieve this human motion sequences are first encoded as discrete tokens which serve as the extended vocabulary of LLM. Then an unsupervised pipeline to generate natural language descriptions of human action sequences from in45;the45;wild videos is developed. Finally all tasks are jointly trained. Extensive experiments show that AvatarGPT achieves SOTA on low45;level tasks and promising results on high45;level tasks demonstrating the effectiveness of our proposed All45;in45;One framework. Moreover for the first time AvatarGPT enables a principled approach by iterative traversal of the tasks within the closed45;loop for unlimited long45;motion synthesis.
