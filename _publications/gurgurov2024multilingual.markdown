---
layout: publication
title: 'Multilingual Large Language Models And Curse Of Multilinguality'
authors: Gurgurov Daniil, BÃ¤umel Tanja, Anikina Tatiana
conference: "Arxiv"
year: 2024
bibkey: gurgurov2024multilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.10602"}
tags: ['BERT', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Tokenization', 'Training Techniques']
---
Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models trained on huge datasets show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navigates the landscape of multilingual LLMs providing an introductory overview of their technical aspects. It explains underlying architectures objective functions pre-training data sources and tokenization methods. This work explores the unique features of different model types encoder-only (mBERT XLM-R) decoder-only (XGLM PALM BLOOM GPT-3) and encoder-decoder models (mT5 mBART). Additionally it addresses one of the significant limitations of multilingual LLMs - the curse of multilinguality - and discusses current attempts to overcome it.
