---
layout: publication
title: Multilingual Large Language Models And Curse Of Multilinguality
authors: Gurgurov Daniil, BÃ¤umel Tanja, Anikina Tatiana
conference: "Arxiv"
year: 2024
bibkey: gurgurov2024multilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.10602"}
tags: ['BERT', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Tokenization', 'Training Techniques']
---
Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models trained on huge datasets show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navigates the landscape of multilingual LLMs providing an introductory overview of their technical aspects. It explains underlying architectures objective functions pre45;training data sources and tokenization methods. This work explores the unique features of different model types encoder45;only (mBERT XLM45;R) decoder45;only (XGLM PALM BLOOM GPT45;3) and encoder45;decoder models (mT5 mBART). Additionally it addresses one of the significant limitations of multilingual LLMs 45; the curse of multilinguality 45; and discusses current attempts to overcome it.
