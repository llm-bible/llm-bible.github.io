---
layout: publication
title: 'Benchmarking Llm-based Relevance Judgment Methods'
authors: Negar Arabzadeh, Charles L. A. Clarke
conference: "Arxiv"
year: 2025
bibkey: arabzadeh2025benchmarking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.12558'}
  - {name: "Code", url: 'https://github.com/Narabzad/llm-relevance-judgement-comparison'}
tags: ['Has Code', 'GPT', 'Applications', 'Model Architecture', 'Fine-Tuning', 'Prompting', 'Reinforcement Learning']
---
Large Language Models (LLMs) are increasingly deployed in both academic and
industry settings to automate the evaluation of information seeking systems,
particularly by generating graded relevance judgments. Previous work on
LLM-based relevance assessment has primarily focused on replicating graded
human relevance judgments through various prompting strategies. However, there
has been limited exploration of alternative assessment methods or comprehensive
comparative studies. In this paper, we systematically compare multiple
LLM-based relevance assessment methods, including binary relevance judgments,
graded relevance assessments, pairwise preference-based methods, and two
nugget-based evaluation methods~--~document-agnostic and document-dependent. In
addition to a traditional comparison based on system rankings using Kendall
correlations, we also examine how well LLM judgments align with human
preferences, as inferred from relevance grades. We conduct extensive
experiments on datasets from three TREC Deep Learning tracks 2019, 2020 and
2021 as well as the ANTIQUE dataset, which focuses on non-factoid open-domain
question answering. As part of our data release, we include relevance judgments
generated by both an open-source (Llama3.2b) and a commercial (gpt-4o) model.
Our goal is to \textit\{reproduce\} various LLM-based relevance judgment methods
to provide a comprehensive comparison. All code, data, and resources are
publicly available in our GitHub Repository at
https://github.com/Narabzad/llm-relevance-judgement-comparison.
