---
layout: publication
title: 'Chemmllm: Chemical Multimodal Large Language Model'
authors: Qian Tan, Dongzhan Zhou, Peng Xia, Wanhao Liu, Wanli Ouyang, Lei Bai, Yuqiang Li, Tianfan Fu
conference: "Arxiv"
year: 2025
bibkey: tan2025chemical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.16326"}
  - {name: "Code", url: "https://github.com/bbsbz/ChemMLLM.git"}
tags: ['Efficiency and Optimization', 'Model Architecture', 'Multimodal Models', 'GPT', 'Has Code', 'Applications']
---
Multimodal large language models (MLLMs) have made impressive progress in many applications in recent years. However, chemical MLLMs that can handle cross-modal understanding and generation remain underexplored. To fill this gap, in this paper, we propose ChemMLLM, a unified chemical multimodal large language model for molecule understanding and generation. Also, we design five multimodal tasks across text, molecular SMILES strings, and image, and curate the datasets. We benchmark ChemMLLM against a range of general leading MLLMs and Chemical LLMs on these tasks. Experimental results show that ChemMLLM achieves superior performance across all evaluated tasks. For example, in molecule image optimization task, ChemMLLM outperforms the best baseline (GPT-4o) by 118.9% (4.27 vs 1.95 property improvement). The code is publicly available at https://github.com/bbsbz/ChemMLLM.git.
