---
layout: publication
title: 'Proceedings Of The First International Workshop On Next-generation Language Models For Knowledge Representation And Reasoning (nelamkrr 2024)'
authors: Ken Satoh, Ha-thanh Nguyen, Francesca Toni, Randy Goebel, Kostas Stathis
conference: "Arxiv"
year: 2024
bibkey: satoh2024proceedings
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.05339"}
tags: ['Model Architecture', 'Tools', 'Reinforcement Learning', 'RAG', 'Pretraining Methods', 'Fine-Tuning', 'Transformer']
---
Reasoning is an essential component of human intelligence as it plays a
fundamental role in our ability to think critically, support responsible
decisions, and solve challenging problems. Traditionally, AI has addressed
reasoning in the context of logic-based representations of knowledge. However,
the recent leap forward in natural language processing, with the emergence of
language models based on transformers, is hinting at the possibility that these
models exhibit reasoning abilities, particularly as they grow in size and are
trained on more data. Despite ongoing discussions about what reasoning is in
language models, it is still not easy to pin down to what extent these models
are actually capable of reasoning.
  The goal of this workshop is to create a platform for researchers from
different disciplines and/or AI perspectives, to explore approaches and
techniques with the aim to reconcile reasoning between language models using
transformers and using logic-based representations. The specific objectives
include analyzing the reasoning abilities of language models measured alongside
KR methods, injecting KR-style reasoning abilities into language models
(including by neuro-symbolic means), and formalizing the kind of reasoning
language models carry out. This exploration aims to uncover how language models
can effectively integrate and leverage knowledge and reasoning with it, thus
improving their application and utility in areas where precision and
reliability are a key requirement.
