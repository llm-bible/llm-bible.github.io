---
layout: publication
title: Nmt5 45;45; Is Parallel Data Still Relevant For Pre45;training Massively Multilingual Language Models
authors: Kale Mihir, Siddhant Aditya, Constant Noah, Johnson Melvin, Al-rfou Rami, Xue Linting
conference: "Arxiv"
year: 2021
bibkey: kale2021is
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2106.02171"}
tags: ['Applications', 'Language Modeling', 'RAG', 'Training Techniques']
---
Recently mT5 45; a massively multilingual version of T5 45; leveraged a unified text45;to45;text format to attain state45;of45;the45;art results on a wide variety of multilingual NLP tasks. In this paper we investigate the impact of incorporating parallel data into mT5 pre45;training. We find that multi45;tasking language modeling with objectives such as machine translation during pre45;training is a straightforward way to improve performance on downstream multilingual and cross45;lingual tasks. However the gains start to diminish as the model capacity increases suggesting that parallel data might not be as essential for larger models. At the same time even at larger model sizes we find that pre45;training with parallel data still provides benefits in the limited labelled data regime.
