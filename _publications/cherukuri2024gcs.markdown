---
layout: publication
title: 'GCS-M3VLT: Guided Context Self-attention Based Multi-modal Medical Vision Language Transformer For Retinal Image Captioning'
authors: Teja Krishna Cherukuri, Nagur Shareef Shaik, Jyostna Devi Bodapati, Dong Hye Ye
conference: "Arxiv"
year: 2024
bibkey: cherukuri2024gcs
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.17251"}
tags: ['Multimodal Models', 'Model Architecture', 'Pretraining Methods', 'Transformer', 'Attention Mechanism']
---
Retinal image analysis is crucial for diagnosing and treating eye diseases,
yet generating accurate medical reports from images remains challenging due to
variability in image quality and pathology, especially with limited labeled
data. Previous Transformer-based models struggled to integrate visual and
textual information under limited supervision. In response, we propose a novel
vision-language model for retinal image captioning that combines visual and
textual features through a guided context self-attention mechanism. This
approach captures both intricate details and the global clinical context, even
in data-scarce scenarios. Extensive experiments on the DeepEyeNet dataset
demonstrate a 0.023 BLEU@4 improvement, along with significant qualitative
advancements, highlighting the effectiveness of our model in generating
comprehensive medical captions.
