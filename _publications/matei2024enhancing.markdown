---
layout: publication
title: 'Enhancing Romanian Offensive Language Detection Through Knowledge Distillation, Multi-task Learning, And Data Augmentation'
authors: Vlad-cristian Matei, Iulian-marius Tăiatu, Răzvan-alexandru Smădu, Dumitru-clementin Cercel
conference: "Arxiv"
year: 2024
bibkey: matei2024enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.20498"}
tags: ['Transformer', 'Efficiency and Optimization', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques', 'Attention Mechanism', 'Pretraining Methods', 'Distillation']
---
This paper highlights the significance of natural language processing (NLP)
within artificial intelligence, underscoring its pivotal role in comprehending
and modeling human language. Recent advancements in NLP, particularly in
conversational bots, have garnered substantial attention and adoption among
developers. This paper explores advanced methodologies for attaining smaller
and more efficient NLP models. Specifically, we employ three key approaches:
(1) training a Transformer-based neural network to detect offensive language,
(2) employing data augmentation and knowledge distillation techniques to
increase performance, and (3) incorporating multi-task learning with knowledge
distillation and teacher annealing using diverse datasets to enhance
efficiency. The culmination of these methods has yielded demonstrably improved
outcomes.
