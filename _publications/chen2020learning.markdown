---
layout: publication
title: Learning Modality Interaction For Temporal Sentence Localization And Event
  Captioning In Videos
authors: Shaoxiang Chen, Wenhao Jiang, Wei Liu, Yu-gang Jiang
conference: Arxiv
year: 2020
citations: 57
bibkey: chen2020learning
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2007.14164'}]
tags: [Multimodal Models, Interpretability and Explainability]
---
Automatically generating sentences to describe events and temporally
localizing sentences in a video are two important tasks that bridge language
and videos. Recent techniques leverage the multimodal nature of videos by using
off-the-shelf features to represent videos, but interactions between modalities
are rarely explored. Inspired by the fact that there exist cross-modal
interactions in the human brain, we propose a novel method for learning
pairwise modality interactions in order to better exploit complementary
information for each pair of modalities in videos and thus improve performances
on both tasks. We model modality interaction in both the sequence and channel
levels in a pairwise fashion, and the pairwise interaction also provides some
explainability for the predictions of target tasks. We demonstrate the
effectiveness of our method and validate specific design choices through
extensive ablation studies. Our method turns out to achieve state-of-the-art
performances on four standard benchmark datasets: MSVD and MSR-VTT (event
captioning task), and Charades-STA and ActivityNet Captions (temporal sentence
localization task).