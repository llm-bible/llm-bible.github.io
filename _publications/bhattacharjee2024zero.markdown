---
layout: publication
title: Zero45;shot Llm45;guided Counterfactual Generation For Text
authors: Bhattacharjee Amrita, Moraffah Raha, Garland Joshua, Liu Huan
conference: "Arxiv"
year: 2024
bibkey: bhattacharjee2024zero
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.04793"}
tags: ['Pretraining Methods', 'RAG', 'Training Techniques']
---
Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks. Although methods for automated counterfactual generation have been explored such methods depend on models such as pre45;trained language models that are then fine45;tuned on auxiliary often task45;specific datasets. Collecting and annotating such datasets for counterfactual generation is labor intensive and therefore infeasible in practice. Therefore in this work we focus on a novel problem setting textit123;zero45;shot counterfactual generation125;. To this end we propose a structured way to utilize large language models (LLMs) as general purpose counterfactual example generators. We hypothesize that the instruction45;following and textual understanding capabilities of recent LLMs can be effectively leveraged for generating high quality counterfactuals in a zero45;shot manner without requiring any training or fine45;tuning. Through comprehensive experiments on various downstream tasks in natural language processing (NLP) we demonstrate the efficacy of LLMs as zero45;shot counterfactual generators in evaluating and explaining black45;box NLP models.
