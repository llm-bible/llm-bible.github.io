---
layout: publication
title: 'Unigen: A Unified Framework For Textual Dataset Generation Using Large Language Models'
authors: Wu Siyuan, Huang Yue, Gao Chujie, Chen Dongping, Zhang Qihui, Wan Yao, Zhou Tianyi, Zhang Xiangliang, Gao Jianfeng, Xiao Chaowei, Sun Lichao
conference: "Arxiv"
year: 2024
bibkey: wu2024unified
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.18966"}
tags: ['Agentic', 'GPT', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools']
---
Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly
impacted various fields by enabling high-quality synthetic data generation and
reducing dependence on expensive human-generated datasets. Despite this,
challenges remain in the areas of generalization, controllability, diversity,
and truthfulness within the existing generative frameworks. To address these
challenges, this paper presents UniGen, a comprehensive LLM-powered framework
designed to produce diverse, accurate, and highly controllable datasets. UniGen
is adaptable, supporting all types of text datasets and enhancing the
generative process through innovative mechanisms. To augment data diversity,
UniGen incorporates an attribute-guided generation module and a group checking
feature. For accuracy, it employs a code-based mathematical assessment for
label verification alongside a retrieval-augmented generation technique for
factual validation. The framework also allows for user-specified constraints,
enabling customization of the data generation process to suit particular
requirements. Extensive experiments demonstrate the superior quality of data
generated by UniGen, and each module within UniGen plays a critical role in
this enhancement. Additionally, UniGen is applied in two practical scenarios:
benchmarking LLMs and data augmentation. The results indicate that UniGen
effectively supports dynamic and evolving benchmarking, and that data
augmentation improves LLM capabilities in various domains, including
agent-oriented abilities and reasoning skills.
