---
layout: publication
title: High45;quality Data45;to45;text Generation For Severely Under45;resourced Languages With Out45;of45;the45;box Large Language Models
authors: Lorandi Michela, Belz Anya
conference: "Arxiv"
year: 2024
bibkey: lorandi2024high
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.12267"}
tags: ['Applications', 'Language Modeling']
---
The performance of NLP methods for severely under45;resourced languages cannot currently hope to match the state of the art in NLP methods for well resourced languages. We explore the extent to which pretrained large language models (LLMs) can bridge this gap via the example of data45;to45;text generation for Irish Welsh Breton and Maltese. We test LLMs on these under45;resourced languages and English in a range of scenarios. We find that LLMs easily set the state of the art for the under45;resourced languages by substantial margins as measured by both automatic and human evaluations. For all our languages human evaluation shows on45;a45;par performance with humans for our best systems but BLEU scores collapse compared to English casting doubt on the metrics suitability for evaluating non45;task45;specific systems. Overall our results demonstrate the great potential of LLMs to bridge the performance gap for under45;resourced languages.
