---
layout: publication
title: 'High-quality Data-to-text Generation For Severely Under-resourced Languages With Out-of-the-box Large Language Models'
authors: Michela Lorandi, Anya Belz
conference: "Arxiv"
year: 2024
bibkey: lorandi2024high
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.12267"}
tags: ['Language Modeling', 'Applications']
---
The performance of NLP methods for severely under-resourced languages cannot
currently hope to match the state of the art in NLP methods for well resourced
languages. We explore the extent to which pretrained large language models
(LLMs) can bridge this gap, via the example of data-to-text generation for
Irish, Welsh, Breton and Maltese. We test LLMs on these under-resourced
languages and English, in a range of scenarios. We find that LLMs easily set
the state of the art for the under-resourced languages by substantial margins,
as measured by both automatic and human evaluations. For all our languages,
human evaluation shows on-a-par performance with humans for our best systems,
but BLEU scores collapse compared to English, casting doubt on the metric's
suitability for evaluating non-task-specific systems. Overall, our results
demonstrate the great potential of LLMs to bridge the performance gap for
under-resourced languages.
