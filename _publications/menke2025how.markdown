---
layout: publication
title: 'How Effective Is Constitutional AI In Small Llms? A Study On Deepseek-r1 And Its Peers'
authors: Antonio-gabriel Chac√≥n Shibaura Institute Of Technology, Kempten University Of Applied Sciences Menke, Phan Xuan Shibaura Institute Of Technology Tan
conference: "Arxiv"
year: 2025
bibkey: menke2025how
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.17365"}
tags: ['Responsible AI', 'Model Architecture']
---
Recent incidents highlight safety risks in Large Language Models (LLMs),
motivating research into alignment methods like Constitutional AI (CAI). This
paper explores CAI's self-critique mechanism on small, uncensored 7-9B
parameter models: DeepSeek-R1-8B, Gemma-2-9B, Llama 3.1-8B, and Qwen2.5-7B. We
show that while Llama-based models exhibited significant harm reduction through
self-critique, other architectures demonstrated less improvement in harm
detection after abliteration. These results suggest CAI's effectiveness may
vary depending on model architecture and reasoning capabilities.
