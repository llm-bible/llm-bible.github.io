---
layout: publication
title: From Static To Dynamic&#58; A Continual Learning Framework For Large Language Models
authors: Du Mingzhe, Luu Anh Tuan, Ji Bin, Ng See-kiong
conference: "Arxiv"
year: 2023
bibkey: du2023from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.14248"}
  - {name: "Code", url: "https://github.com/Elfsong/DynaMind"}
tags: ['Has Code', 'Pretraining Methods', 'Tools']
---
The vast number of parameters in large language models (LLMs) endows them with remarkable capabilities allowing them to excel in a variety of natural language processing tasks. However this complexity also presents challenges making LLMs difficult to train and inhibiting their ability to continuously assimilate new knowledge which may lead to inaccuracies in their outputs. To mitigate these issues this paper presents DynaMind a novel continual learning framework designed for LLMs. DynaMind incorporates memory mechanisms to assimilate new knowledge and modular operators to enhance the model inference process with the newly assimilated knowledge consequently improving the accuracies of LLMs outputs. Benchmark experiments demonstrate DynaMinds effectiveness in overcoming these challenges. The code and demo of DynaMind are available on GitHub https://github.com/Elfsong/DynaMind."
