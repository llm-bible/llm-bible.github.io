---
layout: publication
title: 'Grad-sam: Explaining Transformers Via Gradient Self-attention Maps'
authors: Oren Barkan, Edan Hauon, Avi Caciularu, Ori Katz, Itzik Malkiel, Omri Armstrong, Noam Koenigstein
conference: "Arxiv"
year: 2022
bibkey: barkan2022grad
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.11073"}
tags: ['Transformer', 'Attention Mechanism', 'Model Architecture', 'Pretraining Methods']
---
Transformer-based language models significantly advanced the state-of-the-art
in many linguistic tasks. As this revolution continues, the ability to explain
model predictions has become a major area of interest for the NLP community. In
this work, we present Gradient Self-Attention Maps (Grad-SAM) - a novel
gradient-based method that analyzes self-attention units and identifies the
input elements that explain the model's prediction the best. Extensive
evaluations on various benchmarks show that Grad-SAM obtains significant
improvements over state-of-the-art alternatives.
