---
layout: publication
title: Grad45;sam Explaining Transformers Via Gradient Self45;attention Maps
authors: Barkan Oren, Hauon Edan, Caciularu Avi, Katz Ori, Malkiel Itzik, Armstrong Omri, Koenigstein Noam
conference: "Arxiv"
year: 2022
bibkey: barkan2022grad
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.11073"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Transformer45;based language models significantly advanced the state45;of45;the45;art in many linguistic tasks. As this revolution continues the ability to explain model predictions has become a major area of interest for the NLP community. In this work we present Gradient Self45;Attention Maps (Grad45;SAM) 45; a novel gradient45;based method that analyzes self45;attention units and identifies the input elements that explain the models prediction the best. Extensive evaluations on various benchmarks show that Grad45;SAM obtains significant improvements over state45;of45;the45;art alternatives.
