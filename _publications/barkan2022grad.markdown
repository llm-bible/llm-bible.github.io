---
layout: publication
title: Grad-SAM Explaining Transformers via Gradient Self-Attention Maps
authors: Barkan Oren, Hauon Edan, Caciularu Avi, Katz Ori, Malkiel Itzik, Armstrong Omri, Koenigstein Noam
conference: "Arxiv"
year: 2022
bibkey: barkan2022grad
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.11073"}
tags: ['Model Architecture', 'Masked Language Model', 'Transformer', 'Arxiv']
---
Transformer-based language models significantly advanced the state-of-the-art in many linguistic tasks. As this revolution continues the ability to explain model predictions has become a major area of interest for the NLP community. In this work we present Gradient Self-Attention Maps (Grad-SAM) - a novel gradient-based method that analyzes self-attention units and identifies the input elements that explain the models prediction the best. Extensive evaluations on various benchmarks show that Grad-SAM obtains significant improvements over state-of-the-art alternatives.
