---
layout: publication
title: 'Gpt-based Models Meet Simulation: How To Efficiently Use Large-scale Pre-trained Language Models Across Simulation Tasks'
authors: Giabbanelli Philippe J.
conference: "Arxiv"
year: 2023
bibkey: giabbanelli2023gpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.13679"}
tags: ['Attention Mechanism', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Tools', 'Uncategorized']
---
The disruptive technology provided by large-scale pre-trained language models (LLMs) such as ChatGPT or GPT-4 has received significant attention in several application domains, often with an emphasis on high-level opportunities and concerns. This paper is the first examination regarding the use of LLMs for scientific simulations. We focus on four modeling and simulation tasks, each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved. The first task is devoted to explaining the structure of a conceptual model to promote the engagement of participants in the modeling process. The second task focuses on summarizing simulation outputs, so that model users can identify a preferred scenario. The third task seeks to broaden accessibility to simulation platforms by conveying the insights of simulation visualizations via text. Finally, the last task evokes the possibility of explaining simulation errors and providing guidance to resolve them.
