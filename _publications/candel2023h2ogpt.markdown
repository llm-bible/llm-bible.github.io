---
layout: publication
title: h2oGPT Democratizing Large Language Models
authors: Candel Arno, Mckinney Jon, Singer Philipp, Pfeiffer Pascal, Jeblick Maximilian, Prabhu Prithvi, Gambera Jeff, Landry Mark, Bansal Shivam, Chesler Ryan, Lee Chun Ming, Conde Marcos V., Stetsenko Pasha, Grellier Olivier, Ambati Srisatish
conference: "Arxiv"
year: 2023
bibkey: candel2023h2ogpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.08161"}
tags: ['Applications', 'Bias Mitigation', 'Ethics And Bias', 'Fairness', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
Applications built on top of Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their human-level capabilities in natural language processing. However they also pose many significant risks such as the presence of biased private or harmful text and the unauthorized inclusion of copyrighted material. We introduce h2oGPT a suite of open-source code repositories for the creation and use of LLMs based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the worlds best truly open-source alternative to closed-source approaches. In collaboration with and as part of the incredible and unstoppable open-source community we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100 private document search using natural language. Open-source language models help boost AI development and make it more accessible and trustworthy. They lower entry hurdles allowing people and groups to tailor these models to their needs. This openness increases innovation transparency and fairness. An open-source strategy is needed to share AI benefits fairly and H2O.ai will continue to democratize AI and LLMs.
