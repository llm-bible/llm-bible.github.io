---
layout: publication
title: Low45;resource Cross45;lingual Summarization Through Few45;shot Learning With Large Language Models
authors: Park Gyutae, Hwang Seojin, Lee Hwanhee
conference: "Arxiv"
year: 2024
bibkey: park2024low
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.04630"}
tags: ['Applications', 'Fine Tuning', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
Cross45;lingual summarization (XLS) aims to generate a summary in a target language different from the source language document. While large language models (LLMs) have shown promising zero45;shot XLS performance their few45;shot capabilities on this task remain unexplored especially for low45;resource languages with limited parallel data. In this paper we investigate the few45;shot XLS performance of various models including Mistral45;7B45;Instruct45;v0.2 GPT45;3.5 and GPT45;4. Our experiments demonstrate that few45;shot learning significantly improves the XLS performance of LLMs particularly GPT45;3.5 and GPT45;4 in low45;resource settings. However the open45;source model Mistral45;7B45;Instruct45;v0.2 struggles to adapt effectively to the XLS task with limited examples. Our findings highlight the potential of few45;shot learning for improving XLS performance and the need for further research in designing LLM architectures and pre45;training objectives tailored for this task. We provide a future work direction to explore more effective few45;shot learning strategies and to investigate the transfer learning capabilities of LLMs for cross45;lingual summarization.
