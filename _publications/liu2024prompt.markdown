---
layout: publication
title: 'Prompt Framework For Role-playing: Generation And Evaluation'
authors: Liu Xun, Ni Zhengwei
conference: "Arxiv"
year: 2024
bibkey: liu2024prompt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.00627"}
tags: ['Applications', 'Prompting', 'RAG', 'Reinforcement Learning', 'Tools', 'Uncategorized']
---
Large language models (LLM) have demonstrated remarkable abilities in
generating natural language, understanding user instruction, and mimicking
human language use. These capabilities have garnered considerable interest in
applications such as role-playing. However, the process of collecting
individual role scripts (or profiles) data and manually evaluating the
performance can be costly. We introduce a framework that uses prompts to
leverage the state-of-the-art (SOTA) LLMs to construct role-playing dialogue
datasets and evaluate the role-playing performance. Additionally, we employ
recall-oriented evaluation Rouge-L metric to support the result of the LLM
evaluator.
