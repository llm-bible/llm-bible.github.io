---
layout: publication
title: 'Towards Knowledge-grounded Natural Language Understanding And Generation'
authors: Whitehouse Chenxi
conference: "Arxiv"
year: 2024
bibkey: whitehouse2024towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.15364"}
tags: ['Applications', 'Fine Tuning', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
This thesis investigates how natural language understanding and generation with transformer models can benefit from grounding the models with knowledge representations and addresses the following key research questions (i) Can knowledge of entities extend its benefits beyond entity-centric tasks such as entity linking (ii) How can we faithfully and effectively extract such structured knowledge from raw text especially noisy web text (iii) How do other types of knowledge beyond structured knowledge contribute to improving NLP tasks Studies in this thesis find that incorporating relevant and up-to-date knowledge of entities benefits fake news detection and entity-focused code-switching significantly enhances zero-shot cross-lingual transfer on entity-centric tasks. In terms of effective and faithful approaches to extracting structured knowledge it is observed that integrating negative examples and training with entity planning significantly improves performance. Additionally it is established that other general forms of knowledge such as parametric and distilled knowledge enhance multimodal and multilingual knowledge-intensive tasks. This research shows the tangible benefits of diverse knowledge integration and motivates further exploration in this direction.
