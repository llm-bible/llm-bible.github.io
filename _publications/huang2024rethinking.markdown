---
layout: publication
title: Conme Rethinking Evaluation Of Compositional Reasoning For Modern Vlms
authors: Huang Irene, Lin Wei, Mirza M. Jehanzeb, Hansen Jacob A., Doveh Sivan, Butoi Victor Ion, Herzig Roei, Arbelle Assaf, Kuhene Hilde, Darrel Trevor, Gan Chuang, Oliva Aude, Feris Rogerio, Karlinsky Leonid
conference: "Arxiv"
year: 2024
bibkey: huang2024rethinking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.08164"}
tags: ['Applications', 'Language Modeling', 'Prompting', 'RAG']
---
Compositional Reasoning (CR) entails grasping the significance of attributes relations and word order. Recent Vision45;Language Models (VLMs) comprising a visual encoder and a Large Language Model (LLM) decoder have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question have VLMs effectively tackled the CR challenge We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM45;only negative text generation pipeline. Consequently the negatives produced either appear as outliers from the natural language distribution learned by VLMs LLM decoders or as improbable within the corresponding image context. To address these limitations we introduce ConMe 45;45; a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce hard CR Qamp;A. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses our pipeline autonomously generates evaluates and selects challenging compositional reasoning questions establishing a robust CR benchmark also subsequently validated manually. Our benchmark provokes a noteworthy up to 3337; decrease in CR performance compared to preceding benchmarks reinstating the CR challenge even for state45;of45;the45;art VLMs.
