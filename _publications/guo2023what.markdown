---
layout: publication
title: What Can Large Language Models Do In Chemistry? A Comprehensive Benchmark On Eight Tasks
authors: Guo Taicheng, Guo Kehan, Nan Bozhao, Liang Zhenwen, Guo Zhichun, Chawla Nitesh V., Wiest Olaf, Zhang Xiangliang
conference: "Arxiv"
year: 2023
bibkey: guo2023what
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.18365"}
  - {name: "Code", url: "https://github.com/ChemFoundationModels/ChemLLMBench"}
tags: ['Few Shot', 'Fine Tuning', 'GPT', 'Has Code', 'In Context Learning', 'Model Architecture', 'Prompting']
---
Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science finance and software engineering. However the capability of LLMs to advance the field of chemistry remains unclear. In this paper rather than pursuing state-of-the-art performance we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4 GPT-3.5 Davinci-003 Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench."
