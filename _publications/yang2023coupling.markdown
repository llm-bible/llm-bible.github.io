---
layout: publication
title: Coupling Large Language Models With Logic Programming For Robust And General Reasoning From Text
authors: Yang Zhun, Ishay Adam, Lee Joohyung
conference: "Arxiv"
year: 2023
bibkey: yang2023coupling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.07696"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
While large language models (LLMs) such as GPT45;3 appear to be robust and general their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study we observe that a large language model can serve as a highly effective few45;shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs a logic45;based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question45;answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLMs adaptation to a specific task along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state45;of45;the45;art performance on several NLP benchmarks including bAbI StepGame CLUTRR and gSCAN. Additionally it successfully tackles robot planning tasks that an LLM alone fails to solve.
