---
layout: publication
title: Is DPO Superior To PPO For LLM Alignment A Comprehensive Study
authors: Xu Shusheng, Fu Wei, Gao Jiaxuan, Ye Wenjie, Liu Weilin, Mei Zhiyu, Wang Guangju, Yu Chao, Wu Yi
conference: "Arxiv"
year: 2024
bibkey: xu2024is
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.10719"}
tags: ['Agentic', 'Applications', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'RAG', 'Reinforcement Learning']
---
Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward45;based or reward45;free. Novel applications such as ChatGPT and Claude leverage reward45;based methods that first learn a reward model and apply actor45;critic algorithms such as Proximal Policy Optimization (PPO). However in academic benchmarks state45;of45;the45;art results are often achieved via reward45;free methods such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO Why does PPO perform poorly on these benchmarks In this paper we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine45;tuning LLMs. Finally we benchmark DPO and PPO across a collection of RLHF testbeds ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state45;of45;the45;art results in challenging code competitions.
