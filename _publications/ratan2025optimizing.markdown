---
layout: publication
title: 'Optimizing Language Models For Grammatical Acceptability: A Comparative Study Of Fine-tuning Techniques'
authors: Shobhit Ratan, Farley Knight, Ghada Jerfel, Sze Chung Ho
conference: "Arxiv"
year: 2025
bibkey: ratan2025optimizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.07853"}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Distillation', 'Pretraining Methods', 'Transformer', 'Fine-Tuning']
---
This study explores the fine-tuning (FT) of the Open Pre-trained Transformer
(OPT-125M) for grammatical acceptability tasks using the CoLA dataset. By
comparing Vanilla-Fine-Tuning (VFT), Pattern-Based-Fine-Tuning (PBFT), and
Parameter-Efficient Fine-Tuning techniques (PEFT) like Low-Rank Adaptation
(LoRA), we demonstrate significant improvements in computational efficiency
while maintaining high accuracy. Our experiments reveal that while VFT achieves
the highest accuracy (81.2%), LoRA enhancing FT by reducing memory usage and
iteration time by more than 50%, and increases accuracy in PBFT case. Context
Distillation (CD), though computationally efficient, underperformed with
accuracy around 31%. Our findings contribute to democratizing access to large
language models (LLM) by reducing computational barriers.
