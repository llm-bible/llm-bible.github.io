---
layout: publication
title: 'Make Every Move Count: Llm-based High-quality RTL Code Generation Using MCTS'
authors: Matthew Delorenzo, Animesh Basak Chowdhury, Vasudev Gohil, Shailja Thakur, Ramesh Karri, Siddharth Garg, Jeyavijayan Rajendran
conference: "Arxiv"
year: 2024
bibkey: delorenzo2024make
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2402.03289'}
tags: ['Transformer', 'Efficiency and Optimization', 'Model Architecture', 'Applications', 'Prompting', 'Reinforcement Learning', 'Pretraining Methods']
---
Existing large language models (LLMs) for register transfer level code
generation face challenges like compilation failures and suboptimal power,
performance, and area (PPA) efficiency. This is due to the lack of PPA
awareness in conventional transformer decoding algorithms. In response, we
present an automated transformer decoding algorithm that integrates Monte Carlo
tree-search for lookahead, guiding the transformer to produce compilable,
functionally correct, and PPA-optimized code. Empirical evaluation with a
fine-tuned language model on RTL codesets shows that our proposed technique
consistently generates functionally correct code compared to prompting-only
methods and effectively addresses the PPA-unawareness drawback of naive large
language models. For the largest design generated by the state-of-the-art LLM
(16-bit adder), our technique can achieve a 31.8% improvement in the area-delay
product.
