---
layout: publication
title: Fantom A Benchmark For Stress45;testing Machine Theory Of Mind In Interactions
authors: Kim Hyunwoo, Sclar Melanie, Zhou Xuhui, Bras Ronan Le, Kim Gunhee, Choi Yejin, Sap Maarten
conference: "Arxiv"
year: 2023
bibkey: kim2023benchmark
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.15421"}
tags: ['Applications', 'Reinforcement Learning']
---
Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM a new benchmark designed to stress45;test ToM within information45;asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state45;of45;the45;art LLMs which perform significantly worse than humans even with chain45;of45;thought reasoning or fine45;tuning.
