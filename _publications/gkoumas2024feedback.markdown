---
layout: publication
title: Feedback-aligned Mixed LLMs for Machine Language-Molecule Translation
authors: Gkoumas Dimitris, Liakata Maria
conference: "Arxiv"
year: 2024
bibkey: gkoumas2024feedback
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.13984"}
tags: ['ARXIV', 'Cross Modal', 'Multimodal Models']
---
The intersection of chemistry and Artificial Intelligence (AI) is an active area of research focused on accelerating scientific discovery. While using large language models (LLMs) with scientific modalities has shown potential there are significant challenges to address such as improving training efficiency and dealing with the out-of-distribution problem. Focussing on the task of automated language-molecule translation we are the first to use state-of-the art (SOTA) human-centric optimisation algorithms in the cross-modal setting successfully aligning cross-language-molecule modals. We empirically show that we can augment the capabilities of scientific LLMs without the need for extensive data or large models. We conduct experiments using only 10 of the available data to mitigate memorisation effects associated with training large models on extensive datasets. We achieve significant performance gains surpassing the best benchmark model trained on extensive in-distribution data by a large margin and reach new SOTA levels. Additionally we are the first to propose employing non-linear fusion for mixing cross-modal LLMs which further boosts performance gains without increasing training costs or data needs. Finally we introduce a fine-grained domain-agnostic evaluation method to assess hallucination in LLMs and promote responsible use.
