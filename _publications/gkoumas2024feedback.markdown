---
layout: publication
title: Feedback45;aligned Mixed Llms For Machine Language45;molecule Translation
authors: Gkoumas Dimitris, Liakata Maria
conference: "Arxiv"
year: 2024
bibkey: gkoumas2024feedback
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.13984"}
tags: ['Efficiency And Optimization', 'Ethics And Bias', 'Merging', 'Training Techniques']
---
The intersection of chemistry and Artificial Intelligence (AI) is an active area of research focused on accelerating scientific discovery. While using large language models (LLMs) with scientific modalities has shown potential there are significant challenges to address such as improving training efficiency and dealing with the out45;of45;distribution problem. Focussing on the task of automated language45;molecule translation we are the first to use state45;of45;the art (SOTA) human45;centric optimisation algorithms in the cross45;modal setting successfully aligning cross45;language45;molecule modals. We empirically show that we can augment the capabilities of scientific LLMs without the need for extensive data or large models. We conduct experiments using only 1037; of the available data to mitigate memorisation effects associated with training large models on extensive datasets. We achieve significant performance gains surpassing the best benchmark model trained on extensive in45;distribution data by a large margin and reach new SOTA levels. Additionally we are the first to propose employing non45;linear fusion for mixing cross45;modal LLMs which further boosts performance gains without increasing training costs or data needs. Finally we introduce a fine45;grained domain45;agnostic evaluation method to assess hallucination in LLMs and promote responsible use.
