---
layout: publication
title: Testing AI On Language Comprehension Tasks Reveals Insensitivity To Underlying Meaning
authors: Dentella Vittoria, Guenther Fritz, Murphy Elliot, Marcus Gary, Leivada Evelina
conference: "Arxiv"
year: 2023
bibkey: dentella2023testing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2302.12313"}
tags: ['Applications', 'Prompting', 'Reinforcement Learning']
---
Large Language Models (LLMs) are recruited in applications that span from clinical assistance and legal support to question answering and education. Their success in specialized tasks has led to the claim that they possess human-like linguistic capabilities related to compositional understanding and reasoning. Yet reverse-engineering is bound by Moravecs Paradox according to which easy skills are hard. We systematically assess 7 state-of-the-art models on a novel benchmark. Models answered a series of comprehension questions each prompted multiple times in two settings permitting one-word or open-length replies. Each question targets a short text featuring high-frequency linguistic constructions. To establish a baseline for achieving human-like performance we tested 400 humans on the same prompts. Based on a dataset of n=26680 datapoints we discovered that LLMs perform at chance accuracy and waver considerably in their answers. Quantitatively the tested models are outperformed by humans and qualitatively their answers showcase distinctly non-human errors in language understanding. We interpret this evidence as suggesting that despite their usefulness in various tasks current AI models fall short of understanding language in a way that matches humans and we argue that this may be due to their lack of a compositional operator for regulating grammatical and semantic information.
