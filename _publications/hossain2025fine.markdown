---
layout: publication
title: 'Fine-tuning Llama 2 Interference: A Comparative Study Of Language Implementations For Optimal Efficiency'
authors: Sazzad Hossain, Touhidul Alam Seyam, Avijit Chowdhury, Munis Xamidov, Rajib Ghose, Abhijit Pathak
conference: "Arxiv"
year: 2025
bibkey: hossain2025fine
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.01651"}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Tools', 'Pretraining Methods', 'Fine-Tuning']
---
This paper presents a comparative study aimed at optimizing Llama2 inference,
a critical aspect of machine learning and natural language processing (NLP). We
evaluate various programming languages and frameworks, including TensorFlow,
PyTorch, Python, Mojo, C++, and Java, analyzing their performance in terms of
speed, memory consumption, and ease of implementation through extensive
benchmarking. Strengths and limitations of each approach are highlighted, along
with proposed optimization strategies for parallel processing and hardware
utilization. Furthermore, we investigate the Mojo SDK, a novel framework
designed for large language model (LLM) inference on Apple Silicon,
benchmarking its performance against implementations in C, C++, Rust, Zig, Go,
and Julia. Our experiments, conducted on an Apple M1 Max, demonstrate Mojo
SDK's competitive performance, ease of use, and seamless Python compatibility,
positioning it as a strong alternative for LLM inference on Apple Silicon. We
also discuss broader implications for LLM deployment on resource-constrained
hardware and identify potential directions for future research.
