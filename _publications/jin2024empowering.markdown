---
layout: publication
title: Crimson Empowering Strategic Reasoning In Cybersecurity Through Large Language Models
authors: Jin Jiandong, Tang Bowen, Ma Mingxuan, Liu Xiao, Wang Yunfei, Lai Qingnan, Yang Jia, Zhou Changling
conference: "Arxiv"
year: 2024
bibkey: jin2024empowering
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.00878"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Security', 'Training Techniques']
---
We introduces Crimson a system that enhances the strategic reasoning capabilities of Large Language Models (LLMs) within the realm of cybersecurity. By correlating CVEs with MITRE ATTamp;CK techniques Crimson advances threat anticipation and strategic defense efforts. Our approach includes defining and evaluating cybersecurity strategic tasks alongside implementing a comprehensive human45;in45;the45;loop data45;synthetic workflow to develop the CVE45;to45;ATTamp;CK Mapping (CVEM) dataset. We further enhance LLMs reasoning abilities through a novel Retrieval45;Aware Training (RAT) process and its refined iteration RAT45;R. Our findings demonstrate that an LLM fine45;tuned with our techniques possessing 7 billion parameters approaches the performance level of GPT45;4 showing markedly lower rates of hallucination and errors and surpassing other models in strategic reasoning tasks. Moreover domain45;specific fine45;tuning of embedding models significantly improves performance within cybersecurity contexts underscoring the efficacy of our methodology. By leveraging Crimson to convert raw vulnerability data into structured and actionable insights we bolster proactive cybersecurity defenses.
