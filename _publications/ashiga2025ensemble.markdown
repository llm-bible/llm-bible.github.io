---
layout: publication
title: 'Ensemble Learning For Large Language Models In Text And Code Generation: A Survey'
authors: Mari Ashiga, Wei Jie, Fan Wu, Vardan Voskanyan, Fateme Dinmohammadi, Paul Brookes, Jingzhi Gong, Zheng Wang
conference: "Arxiv"
year: 2025
bibkey: ashiga2025ensemble
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.13505'}
tags: ['Language Modeling', 'Transformer', 'RAG', 'Applications', 'Model Architecture', 'Training Techniques', 'GPT', 'Merging', 'Multimodal Models', 'Survey Paper', 'Reinforcement Learning', 'Ethics and Bias', 'Pretraining Methods']
---
Generative pretrained transformers (GPT) are the common large language models
(LLMs) used for generating text from natural language inputs. However, the
fixed properties of language parameters in individual LLMs can lead to
inconsistencies in the generated outputs. This limitation also restricts the
models' ability to represent diverse language patterns due to inherent biases.
Moreover, many powerful LLMs are closed-source. This prevents organizations
from integrating their data into these systems, raising concerns about data
privacy and limiting industry applications. Inspired by the successful
application of LLM ensemble models in text generation, recent literature has
also investigated their potential in code generation. This article reviews
these emerging LLM ensemble approaches. Our goal is to enhance readers'
understanding of existing techniques and encourage further research and
practical implementation, aiming to expand the real-world applications of LLM
ensemble models in both text and code generation. We categorize these
approaches into seven main methods: weight merging, knowledge fusion, mixture
of experts, reward ensemble, output ensemble, routing, and cascading. From this
list, we focus on four methods and models that show strong performance and
potential for broader applications. We analyze their modeling steps, training
methods, and output features to provide a clear understanding of their
capabilities. Our findings highlight the benefits of LLM ensemble techniques.
These include better representation of diversity, improved output quality, and
greater flexibility in applications. This information offers valuable insights
for selecting models for various real-world tasks involving text and code
generation, and potentially applying methods to multimodal LLMs.
