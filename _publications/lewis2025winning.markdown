---
layout: publication
title: 'Winning Big With Small Models: Knowledge Distillation Vs. Self-training For Reducing Hallucination In QA Agents'
authors: Ashley Lewis, Michael White, Jing Liu, Toshiaki Koike-akino, Kieran Parsons, Ye Wang
conference: "Arxiv"
year: 2025
bibkey: lewis2025winning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.19545"}
tags: ['Agentic', 'Security', 'Training Techniques', 'Efficiency and Optimization', 'Model Architecture', 'Tools', 'Reinforcement Learning', 'RAG', 'Distillation', 'GPT', 'Ethics and Bias', 'Pretraining Methods', 'Fine-Tuning']
---
The deployment of Large Language Models (LLMs) in customer support is
constrained by hallucination-generating false information-and the high cost of
proprietary models. To address these challenges, we propose a
retrieval-augmented question-answering (QA) pipeline and explore how to balance
human input and automation. Using a dataset of questions about a Samsung Smart
TV user manual, we demonstrate that synthetic data generated by LLMs
outperforms crowdsourced data in reducing hallucination in finetuned models. We
also compare self-training (fine-tuning models on their own outputs) and
knowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o),
and find that self-training achieves comparable hallucination reduction. We
conjecture that this surprising finding can be attributed to increased exposure
bias issues in the knowledge distillation case and support this conjecture with
post hoc analysis. We also improve robustness to unanswerable questions and
retrieval failures with contextualized "I don't know" responses. These findings
show that scalable, cost-efficient QA systems can be built using synthetic data
and self-training with open-source models, reducing reliance on proprietary
tools or costly human annotations.
