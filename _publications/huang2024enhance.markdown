---
layout: publication
title: 'Longsafety: Enhance Safety For Long-context Llms'
authors: Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Qipeng Guo, Linyang Li, Chenkun Tan, Yang Gao, Pengyu Wang, Linlin Li, Qun Liu, Yaqian Zhou, Xipeng Qiu, Xuanjing Huang
conference: "Arxiv"
year: 2024
bibkey: huang2024enhance
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.06899"}
tags: ['Responsible AI', 'RAG', 'Training Techniques', 'Model Architecture']
---
Recent advancements in model architectures and length extrapolation
techniques have significantly extended the context length of large language
models (LLMs), paving the way for their application in increasingly complex
tasks. However, despite the growing capabilities of long-context LLMs, the
safety issues in long-context scenarios remain underexplored. While safety
alignment in short context has been widely studied, the safety concerns of
long-context LLMs have not been adequately addressed. In this work, we
introduce \textbf\{LongSafety\}, a comprehensive safety alignment dataset for
long-context LLMs, containing 10 tasks and 17k samples, with an average length
of 40.9k tokens. Our experiments demonstrate that training with LongSafety can
enhance long-context safety performance while enhancing short-context safety
and preserving general capabilities. Furthermore, we demonstrate that
long-context safety does not equal long-context alignment with short-context
safety data and LongSafety has generalizing capabilities in context length and
long-context safety scenarios.
