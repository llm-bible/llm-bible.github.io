---
layout: publication
title: Towards End45;to45;end Embodied Decision Making Via Multi45;modal Large Language Model Explorations With Gpt445;vision And Beyond
authors: Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Tianyu Liu, Baobao Chang
conference: "Arxiv"
year: 2023
bibkey: chen2023towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2310.02071v4"}
  - {name: "Code", url: "https://github.com/pkunlp&#45;icler/PCA&#45;EVAL/"}
tags: ['Agentic', 'Fine Tuning', 'GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'RAG', 'Reinforcement Learning', 'Tools']
---
In this study we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision45;making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge MLLMs like GPT445;Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state45;of45;the45;art MLLMs can handle embodied decision45;making in an end45;to45;end manner and whether collaborations between LLMs and MLLMs can enhance decision45;making. To address these questions we introduce a new benchmark called PCA45;EVAL which evaluates embodied decision45;making from the perspectives of Perception Cognition and Action. Additionally we propose HOLMES a multi45;agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision45;making. We compare end45;to45;end embodied decision45;making and HOLMES on our benchmark and find that the GPT445;Vision model demonstrates strong end45;to45;end embodied decision45;making abilities outperforming GPT445;HOLMES in terms of average decision accuracy (+337;). However this performance is exclusive to the latest GPT445;Vision model surpassing the open45;source state45;of45;the45;art MLLM by 2637;. Our results indicate that powerful MLLMs like GPT445;Vision hold promise for decision45;making in embodied agents offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp&#45;icler/PCA&#45;EVAL/.
