---
layout: publication
title: Teaching Large Language Models An Unseen Language On The Fly
authors: Zhang Chen, Liu Xiao, Lin Jiuheng, Feng Yansong
conference: "Arxiv"
year: 2024
bibkey: zhang2024teaching
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.19167"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Existing large language models struggle to support numerous low45;resource languages particularly the extremely low45;resource ones for which there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question we collect a research suite for Zhuang a language supported by no LLMs currently. We introduce DiPMT++ a framework for adapting LLMs to unseen languages by in45;context learning. Using a dictionary and 5K parallel sentences only DiPMT++ significantly enhances the performance of GPT45;4 from 0 to 16 BLEU for Chinese45;to45;Zhuang translation and achieves 32 BLEU for Zhuang45;to45;Chinese translation. We also validate the effectiveness of our framework on Kalamang another unseen language. Furthermore we demonstrate the practical utility of DiPMT++ in aiding humans in translating completely unseen languages which could contribute to the preservation of linguistic diversity.
