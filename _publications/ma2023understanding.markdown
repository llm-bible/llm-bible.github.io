---
layout: publication
title: 'Lms: Understanding Code Syntax And Semantics For Code Analysis'
authors: Wei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Li Li, Yang Liu
conference: "Arxiv"
year: 2023
bibkey: ma2023understanding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.12138"}
tags: ['Interpretability and Explainability', 'Model Architecture', 'GPT', 'Reinforcement Learning']
---
Large language models~(LLMs) demonstrate significant potential to
revolutionize software engineering (SE) by exhibiting outstanding performance
in SE tasks such as code and document generation. However, the high reliability
and risk control requirements in software engineering raise concerns about the
lack of interpretability of LLMs. To address this concern, we conducted a study
to evaluate the capabilities of LLMs and their limitations for code analysis in
SE. We break down the abilities needed for artificial intelligence~(AI) models
to address SE tasks related to code analysis into three categories: 1) syntax
understanding, 2) static behavior understanding, and 3) dynamic behavior
understanding. Our investigation focused on the ability of LLMs to comprehend
code syntax and semantic structures, which include abstract syntax trees (AST),
control flow graphs (CFG), and call graphs (CG). We employed four
state-of-the-art foundational models, GPT4, GPT3.5, StarCoder and
CodeLlama-13b-instruct. We assessed the performance of LLMs on cross-language
tasks involving C, Java, Python, and Solidity.
  Our findings revealed that while LLMs have a talent for understanding code
syntax, they struggle with comprehending code semantics, particularly dynamic
semantics. We conclude that LLMs possess capabilities similar to an Abstract
Syntax Tree (AST) parser, demonstrating initial competencies in static code
analysis. Furthermore, our study highlights that LLMs are susceptible to
hallucinations when interpreting code semantic structures and fabricating
nonexistent facts. These results indicate the need to explore methods to verify
the correctness of LLM output to ensure its dependability in SE. More
importantly, our study provides an initial answer to why the codes generated by
LLM are usually syntax-correct but vulnerable.
