---
layout: publication
title: 'Kuaiji: The First Chinese Accounting Large Language Model'
authors: Jiayuan Luo, Songhua Yang, Xiaoling Qiu, Panyu Chen, Yufei Nai, Wenxuan Zeng, Wentao Zhang, Xinke Jiang
conference: "Arxiv"
year: 2024
bibkey: luo2024first
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2402.13866'}
tags: ['Model Architecture', 'Tools', 'Training Techniques', 'Fine-Tuning', 'GPT', 'Reinforcement Learning', 'Pre-Training', 'Pretraining Methods']
---
Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated
impressive proficiency in comprehending and generating natural language.
However, they encounter difficulties when tasked with adapting to specialized
domains such as accounting. To address this challenge, we introduce Kuaiji, a
tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned
using the Baichuan framework, which encompasses continuous pre-training and
supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing
large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy
and response speed. Our contributions encompass the creation of the first
Chinese accounting dataset, the establishment of Kuaiji as a leading
open-source Chinese accounting LLM, and the validation of its efficacy through
real-world accounting scenarios.
