---
layout: publication
title: Logic-enhanced Language Model Agents For Trustworthy Social Simulations
authors: Mensfelt Agnieszka, Stathis Kostas, Trencsenyi Vince
conference: "Arxiv"
year: 2024
bibkey: mensfelt2024logic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.16081"}
tags: ['Agentic', 'Attention Mechanism', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
We introduce the Logic-Enhanced Language Model Agents (LELMA) framework a novel approach to enhance the trustworthiness of social simulations that utilize large language models (LLMs). While LLMs have gained attention as agents for simulating human behaviour their applicability in this role is limited by issues such as inherent hallucinations and logical inconsistencies. LELMA addresses these challenges by integrating LLMs with symbolic AI enabling logical verification of the reasoning generated by LLMs. This verification process provides corrective feedback refining the reasoning output. The framework consists of three main components an LLM-Reasoner for producing strategic reasoning an LLM-Translator for mapping natural language reasoning to logic queries and a Solver for evaluating these queries. This study focuses on decision-making in game-theoretic scenarios as a model of human interaction. Experiments involving the Hawk-Dove game Prisoners Dilemma and Stag Hunt highlight the limitations of state-of-the-art LLMs GPT-4 Omni and Gemini 1.0 Pro in producing correct reasoning in these contexts. LELMA demonstrates high accuracy in error detection and improves the reasoning correctness of LLMs via self-refinement particularly in GPT-4 Omni.
