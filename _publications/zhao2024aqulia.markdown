---
layout: publication
title: Aqulia45;med LLM Pioneering Full45;process Open45;source Medical Language Models
authors: Zhao Lulu, Zeng Weihao, Shi Xiaofeng, Zhou Hua, Hao Donglin, Lin Yonghua
conference: "Arxiv"
year: 2024
bibkey: zhao2024aqulia
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.12182"}
tags: ['Agentic', 'Efficiency And Optimization', 'Fine Tuning', 'Reinforcement Learning', 'Training Techniques']
---
Recently both closed45;source LLMs and open45;source communities have made significant strides outperforming humans in various general domains. However their performance in specific professional fields such as medicine especially within the open45;source community remains suboptimal due to the complexity of medical knowledge. We propose Aquila45;Med a bilingual medical LLM based on Aquila addressing these challenges through continue pre45;training supervised fine45;tuning (SFT) and reinforcement learning from human feedback (RLHF). We construct a large45;scale Chinese and English medical dataset for continue pre45;training and a high45;quality SFT dataset covering extensive medical specialties. Additionally we develop a high45;quality Direct Preference Optimization (DPO) dataset for further alignment. Aquila45;Med achieves notable results across single45;turn multi45;turn dialogues and medical multiple45;choice questions demonstrating the effectiveness of our approach. We open45;source the datasets and the entire training process contributing valuable resources to the research community. Our models and datasets will released at https://huggingface.co/BAAI/AquilaMed&#45;RL.
