---
layout: publication
title: 'Mind The Data Gap: Bridging Llms To Enterprise Data Integration'
authors: Moe Kayali, Fabian Wenz, Nesime Tatbul, Çağatay Demiralp
conference: "Arxiv"
year: 2024
bibkey: kayali2024mind
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.20331"}
tags: ['Reinforcement Learning']
---
Leading large language models (LLMs) are trained on public data. However,
most of the world's data is dark data that is not publicly accessible, mainly
in the form of private organizational or enterprise data. We show that the
performance of methods based on LLMs seriously degrades when tested on
real-world enterprise datasets. Current benchmarks, based on public data,
overestimate the performance of LLMs. We release a new benchmark dataset, the
GOBY Benchmark, to advance discovery in enterprise data integration. Based on
our experience with this enterprise benchmark, we propose techniques to uplift
the performance of LLMs on enterprise data, including (1) hierarchical
annotation, (2) runtime class-learning, and (3) ontology synthesis. We show
that, once these techniques are deployed, the performance on enterprise data
becomes on par with that of public data. The Goby benchmark can be obtained at
https://goby-benchmark.github.io/.
