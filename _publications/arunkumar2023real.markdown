---
layout: publication
title: Real45;time Visual Feedback To Guide Benchmark Creation A Human45;and45;metric45;in45;the45;loop Workflow
authors: Arunkumar Anjana, Mishra Swaroop, Sachdeva Bhavdeep, Baral Chitta, Bryan Chris
conference: "Arxiv"
year: 2023
bibkey: arunkumar2023real
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2302.04434"}
tags: ['BERT', 'GPT', 'Model Architecture', 'Multimodal Models', 'Security', 'Survey Paper']
---
Recent research has shown that language models exploit artifacts in benchmarks to solve tasks rather than truly learning them leading to inflated model performance. In pursuit of creating better benchmarks we propose VAIDA a novel benchmark creation paradigm for NLP that focuses on guiding crowdworkers an under45;explored facet of addressing benchmark idiosyncrasies. VAIDA facilitates sample correction by providing realtime visual feedback and recommendations to improve sample quality. Our approach is domain model task and metric agnostic and constitutes a paradigm shift for robust validated and dynamic benchmark creation via human45;and45;metric45;in45;the45;loop workflows. We evaluate via expert review and a user study with NASA TLX. We find that VAIDA decreases effort frustration mental and temporal demands of crowdworkers and analysts simultaneously increasing the performance of both user groups with a 45.837; decrease in the level of artifacts in created samples. As a by product of our user study we observe that created samples are adversarial across models leading to decreases of 31.337; (BERT) 22.537; (RoBERTa) 14.9837; (GPT45;3 fewshot) in performance.
