---
layout: publication
title: 'RAG And RAU: A Survey On Retrieval-augmented Language Model In Natural Language Processing'
authors: Yucheng Hu, Yuxing Lu
conference: "Arxiv"
year: 2024
bibkey: hu2024rag
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.19543"}
  - {name: "Code", url: "https://github.com/2471023025/RALM_Survey"}
tags: ['Efficiency and Optimization', 'Survey Paper', 'Applications', 'RAG', 'Reinforcement Learning', 'Security', 'Has Code']
---
Large Language Models (LLMs) have catalyzed significant advancements in
Natural Language Processing (NLP), yet they encounter challenges such as
hallucination and the need for domain-specific knowledge. To mitigate these,
recent methodologies have integrated information retrieved from external
resources with LLMs, substantially enhancing their performance across NLP
tasks. This survey paper addresses the absence of a comprehensive overview on
Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented
Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an
in-depth examination of their paradigm, evolution, taxonomy, and applications.
The paper discusses the essential components of RALMs, including Retrievers,
Language Models, and Augmentations, and how their interactions lead to diverse
model structures and applications. RALMs demonstrate utility in a spectrum of
tasks, from translation and dialogue systems to knowledge-intensive
applications. The survey includes several evaluation methods of RALMs,
emphasizing the importance of robustness, accuracy, and relevance in their
assessment. It also acknowledges the limitations of RALMs, particularly in
retrieval quality and computational efficiency, offering directions for future
research. In conclusion, this survey aims to offer a structured insight into
RALMs, their potential, and the avenues for their future development in NLP.
The paper is supplemented with a Github Repository containing the surveyed
works and resources for further study:
https://github.com/2471023025/RALM_Survey.
