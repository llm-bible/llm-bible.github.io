---
layout: publication
title: 'A Judge-free LLM Open-ended Generation Benchmark Based On The Distributional Hypothesis'
authors: Kentaro Imajo, Masanori Hirano, Shuji Suzuki, Hiroaki Mikami
conference: "Arxiv"
year: 2025
bibkey: imajo2025judge
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.09316"}
tags: ['Language Modeling', 'GPT', 'Applications', 'Model Architecture']
---
Evaluating the open-ended text generation of large language models (LLMs) is
challenging because of the lack of a clear ground truth and the high cost of
human or LLM-based assessments. We propose a novel benchmark that evaluates
LLMs using n-gram statistics and rules, without relying on human judgement or
LLM-as-a-judge approaches. Using 50 question and reference answer sets, we
introduce three new metrics based on n-grams and rules: Fluency, Truthfulness,
and Helpfulness. Our benchmark strongly correlates with GPT-4o-based
evaluations while requiring significantly fewer computational resources,
demonstrating its effectiveness as a scalable alternative for assessing LLMs'
open-ended generation capabilities.
