---
layout: publication
title: MIMIR A Streamlined Platform For Personalized Agent Tuning In Domain Expertise
authors: Deng Chunyuan, Tang Xiangru, Zhao Yilun, Wang Hanming, Wang Haoran, Zhou Wangchunshu, Cohan Arman, Gerstein Mark
conference: "Arxiv"
year: 2024
bibkey: deng2024mimir
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.04285"}
tags: ['Agentic', 'Efficiency And Optimization', 'Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Recently large language models (LLMs) have evolved into interactive agents proficient in planning tool use and task execution across a wide variety of tasks. However without specific agent tuning open-source models like LLaMA currently struggle to match the efficiency of GPT- 4 particularly given the scarcity of agent-tuning datasets for fine-tuning. In response we introduce a streamlined platform offering a customizable pipeline that enables users to leverage both private knowledge and publicly available legally compliant datasets at scale for . Additionally supports the generation of general instruction-tuning datasets from the same input. This dual capability ensures that language agents developed through the platform possess both specific agent abilities and general competencies. integrates these features into a cohesive end-to-end platform facilitating everything from the uploading of personalized files to one-click agent fine-tuning.
