---
layout: publication
title: 'Conversational AI Powered By Large Language Models Amplifies False Memories In Witness Interviews'
authors: Chan Samantha, Pataranutaporn Pat, Suri Aditya, Zulfikar Wazeer, Maes Pattie, Loftus Elizabeth F.
conference: "Arxiv"
year: 2024
bibkey: chan2024conversational
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.04681"}
tags: ['Survey Paper']
---
This study examines the impact of AI on human false memories -- recollections
of events that did not occur or deviate from actual occurrences. It explores
false memory induction through suggestive questioning in Human-AI interactions,
simulating crime witness interviews. Four conditions were tested: control,
survey-based, pre-scripted chatbot, and generative chatbot using a large
language model (LLM). Participants (N=200) watched a crime video, then
interacted with their assigned AI interviewer or survey, answering questions
including five misleading ones. False memories were assessed immediately and
after one week. Results show the generative chatbot condition significantly
increased false memory formation, inducing over 3 times more immediate false
memories than the control and 1.7 times more than the survey method. 36.4% of
users' responses to the generative chatbot were misled through the interaction.
After one week, the number of false memories induced by generative chatbots
remained constant. However, confidence in these false memories remained higher
than the control after one week. Moderating factors were explored: users who
were less familiar with chatbots but more familiar with AI technology, and more
interested in crime investigations, were more susceptible to false memories.
These findings highlight the potential risks of using advanced AI in sensitive
contexts, like police interviews, emphasizing the need for ethical
considerations.
