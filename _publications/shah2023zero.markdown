---
layout: publication
title: Zero Is Not Hero Yet Benchmarking Zero45;shot Performance Of Llms For Financial Tasks
authors: Shah Agam, Chava Sudheer
conference: "Arxiv"
year: 2023
bibkey: shah2023zero
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.16633"}
tags: ['BERT', 'GPT', 'Model Architecture', 'Pretraining Methods']
---
Recently large language models (LLMs) like ChatGPT have shown impressive performance on many natural language processing tasks with zero45;shot. In this paper we investigate the effectiveness of zero45;shot LLMs in the financial domain. We compare the performance of ChatGPT along with some open45;source generative LLMs in zero45;shot mode with RoBERTa fine45;tuned on annotated data. We address three inter45;related research questions on data annotation performance gaps and the feasibility of employing generative models in the finance domain. Our findings demonstrate that ChatGPT performs well even without labeled data but fine45;tuned models generally outperform it. Our research also highlights how annotating with generative models can be time45;intensive. Our codebase is publicly available on GitHub under CC BY45;NC 4.0 license.
