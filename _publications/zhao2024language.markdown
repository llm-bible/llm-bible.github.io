---
layout: publication
title: Language Model Council Benchmarking Foundation Models On Highly Subjective Tasks By Consensus
authors: Zhao Justin, Plaza-del-arco Flor Miriam, Curry Amanda Cercas
conference: "Arxiv"
year: 2024
bibkey: zhao2024language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.08598"}
tags: ['Ethics And Bias', 'Tools']
---
The rapid advancement of Large Language Models (LLMs) necessitates robust and challenging benchmarks. Leaderboards like Chatbot Arena rank LLMs based on how well their responses align with human preferences. However many tasks such as those related to emotional intelligence creative writing or persuasiveness are highly subjective and often lack majoritarian human agreement. Judges may have irreconcilable disagreements about what constitutes a better response. To address the challenge of ranking LLMs on highly subjective tasks we propose a novel benchmarking framework the Language Model Council (LMC). The LMC operates through a democratic process to 1) formulate a test set through equal participation 2) administer the test among council members and 3) evaluate responses as a collective jury. We deploy a council of 20 newest LLMs on an open45;ended emotional intelligence task responding to interpersonal dilemmas. Our results show that the LMC produces rankings that are more separable robust and less biased than those from any individual LLM judge and is more consistent with a human45;established leaderboard compared to other benchmarks.
