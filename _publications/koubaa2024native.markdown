---
layout: publication
title: Arabiangpt Native Arabic Gpt45;based Large Language Model
authors: Koubaa Anis, Ammar Adel, Ghouti Lahouari, Najar Omar, Sibaee Serry
conference: "Arxiv"
year: 2024
bibkey: koubaa2024native
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.15313"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
The predominance of English and Latin45;based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models detracting from their efficacy in processing native Arabics intricate morphology and syntax. Consequently there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap this paper proposes ArabianGPT a series of transformer45;based models within the ArabianLLM suite designed explicitly for Arabic. These models including ArabianGPT45;0.1B and ArabianGPT45;0.3B vary in size and complexity aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer integral to these models addresses the unique morphological aspects of Arabic script ensuring more accurate text processing. Empirical results from fine45;tuning the models on tasks like sentiment analysis and summarization demonstrate significant improvements. For sentiment analysis the fine45;tuned ArabianGPT45;0.1B model achieved a remarkable accuracy of 9537; a substantial increase from the base models 5637;. Similarly in summarization tasks fine45;tuned models showed enhanced F1 scores indicating improved precision and recall in generating concise summaries. Comparative analysis of fine45;tuned ArabianGPT models against their base versions across various benchmarks reveals nuanced differences in performance with fine45;tuning positively impacting specific tasks like question answering and summarization. These findings underscore the efficacy of fine45;tuning in aligning ArabianGPT models more closely with specific NLP tasks highlighting the potential of tailored transformer architectures in advancing Arabic NLP.
