---
layout: publication
title: Adavae Exploring Adaptive Gpt45;2s In Variational Auto45;encoders For Language Modeling
authors: Tu Haoqin, Yang Zhongliang, Yang Jinshuai, Huang Yongfeng
conference: "Arxiv"
year: 2022
bibkey: tu2022exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2205.05862"}
  - {name: "Code", url: "https://github.com/ImKeTT/AdaVAE&#125;"}
tags: ['Applications', 'Attention Mechanism', 'GPT', 'Has Code', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Training Techniques', 'Transformer']
---
Variational Auto45;Encoder (VAE) has become the de45;facto learning paradigm in achieving representation learning and generation for natural language at the same time. Nevertheless existing VAE45;based language models either employ elementary RNNs which is not powerful to handle complex works in the multi45;task situation or fine45;tunes two pre45;trained language models (PLMs) for any downstream task which is a huge drain on resources. In this paper we propose the first VAE framework empowered with adaptive GPT45;2s (AdaVAE). Different from existing systems we unify both the encoderamp;decoder of the VAE model using GPT45;2s with adaptive parameter45;efficient components and further introduce Latent Attention operation to better construct latent space from transformer models. Experiments from multiple dimensions validate that AdaVAE is competent to effectively organize language in three related tasks (language modeling representation modeling and guided text generation) even with less than 1537; activated parameters in training. Our code is available at url123;https://github.com/ImKeTT/AdaVAE&#125;.
