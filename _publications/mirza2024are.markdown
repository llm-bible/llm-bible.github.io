---
layout: publication
title: 'Are Large Language Models Superhuman Chemists?'
authors: Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Martiño Ríos-garcía, Benedict Emoekabu, Aswanth Krishnan, Tanya Gupta, Mara Schilling-wilhelmi, Macjonathan Okereke, Anagha Aneesh, Amir Mohammad Elahi, Mehrdad Asgari, Juliane Eberhardt, Hani M. Elbeheiry, María Victoria Gil, Maximilian Greiner, Caroline T. Holick, Christina Glaubitz, Tim Hoffmann, Abdelrahman Ibrahim, Lea C. Klepsch, Yannik Köster, Fabian Alexander Kreth, Jakob Meyer, Santiago Miret, Jan Matthias Peschel, Michael Ringleb, Nicole Roesner, Johanna Schreiber, Ulrich S. Schubert, Leanne M. Stafast, Dinga Wonanke, Michael Pieler, Philippe Schwaller, Kevin Maik Jablonka
conference: "Arxiv"
year: 2024
bibkey: mirza2024are
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.01475"}
tags: ['Responsible AI', 'RAG', 'Tools']
---
Large language models (LLMs) have gained widespread interest due to their
ability to process human language and perform tasks on which they have not been
explicitly trained.
  However, we possess only a limited systematic understanding of the chemical
capabilities of LLMs, which would be required to improve models and mitigate
potential harm. Here, we introduce "ChemBench," an automated framework for
evaluating the chemical knowledge and reasoning abilities of state-of-the-art
LLMs against the expertise of chemists.
  We curated more than 2,700 question-answer pairs, evaluated leading open- and
closed-source LLMs, and found that the best models outperformed the best human
chemists in our study on average. However, the models struggle with some basic
tasks and provide overconfident predictions.
  These findings reveal LLMs' impressive chemical capabilities while
emphasizing the need for further research to improve their safety and
usefulness. They also suggest adapting chemistry education and show the value
of benchmarking frameworks for evaluating LLMs in specific domains.
