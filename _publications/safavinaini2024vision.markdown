---
layout: publication
title: Vision45;language And Large Language Model Performance In Gastroenterology GPT Claude Llama Phi Mistral Gemma And Quantized Models
authors: Safavi-naini Seyed Amir Ahmad, Ali Shuhaib, Shahab Omer, Shahhoseini Zahra, Savage Thomas, Rafiee Sara, Samaan Jamil S, Shabeeb Reem Al, Ladak Farah, Yang Jamie O, Echavarria Juan, Babar Sumbal, Shaukat Aasma, Margolis Samuel, Tatonetti Nicholas P, Nadkarni Girish, Kurdi Bara El, Soroush Ali
conference: "Arxiv"
year: 2024
bibkey: safavinaini2024vision
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.00084"}
tags: ['Efficiency And Optimization', 'GPT', 'Model Architecture', 'Prompting', 'Quantization', 'RAG', 'Tools']
---
Background and Aims This study evaluates the medical reasoning performance of large language models (LLMs) and vision language models (VLMs) in gastroenterology. Methods We used 300 gastroenterology board exam45;style multiple45;choice questions 138 of which contain images to systematically assess the impact of model configurations and parameters and prompt engineering strategies utilizing GPT45;3.5. Next we assessed the performance of proprietary and open45;source LLMs (versions) including GPT (3.5 4 4o 4omini) Claude (3 3.5) Gemini (1.0) Mistral Llama (2 3 3.1) Mixtral and Phi (3) across different interfaces (web and API) computing environments (cloud and local) and model precisions (with and without quantization). Finally we assessed accuracy using a semiautomated pipeline. Results Among the proprietary models GPT45;4o (73.737;) and Claude3.545;Sonnet (74.037;) achieved the highest accuracy outperforming the top open45;source models Llama3.145;405b (6437;) Llama3.145;70b (58.337;) and Mixtral45;8x7b (54.337;). Among the quantized open45;source models the 645;bit quantized Phi345;14b (48.737;) performed best. The scores of the quantized models were comparable to those of the full45;precision models Llama245;7b Llama245;45;13b and Gemma245;9b. Notably VLM performance on image45;containing questions did not improve when the images were provided and worsened when LLM45;generated captions were provided. In contrast a 1037; increase in accuracy was observed when images were accompanied by human45;crafted image descriptions. Conclusion In conclusion while LLMs exhibit robust zero45;shot performance in medical reasoning the integration of visual data remains a challenge for VLMs. Effective deployment involves carefully determining optimal model configurations encouraging users to consider either the high performance of proprietary models or the flexible adaptability of open45;source models.
