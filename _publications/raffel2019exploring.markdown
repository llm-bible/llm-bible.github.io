---
layout: publication
title: Exploring The Limits Of Transfer Learning With A Unified Text45;to45;text Transformer
authors: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
conference: "Arxiv"
year: 2019
bibkey: raffel2019exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/1910.10683v4"}
tags: ['Applications', 'Fine Tuning', 'Model Architecture', 'Pretraining Methods', 'Tools', 'Training Techniques', 'Transformer']
---
Transfer learning where a model is first pre45;trained on a data45;rich task before being fine45;tuned on a downstream task has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches methodology and practice. In this paper we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text45;based language problems into a text45;to45;text format. Our systematic study compares pre45;training objectives architectures unlabeled data sets transfer approaches and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new Colossal Clean Crawled Corpus we achieve state45;of45;the45;art results on many benchmarks covering summarization question answering text classification and more. To facilitate future work on transfer learning for NLP we release our data set pre45;trained models and code.
