---
layout: publication
title: 'Poison Attacks And Adversarial Prompts Against An Informed University Virtual Assistant'
authors: Ivan A. Fernandez, Subash Neupane, Sudip Mittal, Shahram Rahimi
conference: "Arxiv"
year: 2024
bibkey: fernandez2024poison
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.06788'}
tags: ['Security', 'GPT', 'Model Architecture', 'Tools', 'Prompting', 'Reinforcement Learning', 'Interpretability']
---
Recent research has shown that large language models (LLMs) are particularly
vulnerable to adversarial attacks. Since the release of ChatGPT, various
industries are adopting LLM-based chatbots and virtual assistants in their data
workflows. The rapid development pace of AI-based systems is being driven by
the potential of Generative AI (GenAI) to assist humans in decision making. The
immense optimism behind GenAI often overshadows the adversarial risks
associated with these technologies. A threat actor can use security gaps, poor
safeguards, and limited data governance to carry out attacks that grant
unauthorized access to the system and its data. As a proof-of-concept, we
assess the performance of BarkPlug, the Mississippi State University chatbot,
against data poison attacks from a red team perspective.
