---
layout: publication
title: 'Telechat Technical Report'
authors: He Zhongjiang, Wang Zihan, Liu Xinzhang, Liu Shixuan, Yao Yitong, Huang Yuyao, Li Xuelong, Li Yongxiang, Che Zhonghao, Zhang Zhaoxi, Wang Yan, Wang Xin, Pu Luwen, Xu Huinan, Fang Ruiyu, Zhao Yu, Zhang Jie, Huang Xiaomeng, Lu Zhilong, Peng Jiaxin, Zheng Wenjun, Wang Shiquan, Yang Bingkai, He Xuewei, Jiang Zhuoru, Xie Qiyi, Zhang Yanhan, Li Zhongqiu, Shi Lingling, Fu Weiwei, Zhang Yin, Huang Zilu, Xiong Sishi, Zhang Yuxiang, Wang Chao, Song Shuangyong
conference: "Arxiv"
year: 2024
bibkey: he2024telechat
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.03804"}
tags: ['Applications', 'Fine Tuning', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
In this technical report we present TeleChat a collection of large language models (LLMs) with parameters of 3 billion 7 billion and 12 billion. It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages including trillions of tokens. Subsequently the model undergoes fine-tuning to align with human preferences following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks including language understanding mathematics reasoning code generation and knowledge-based question answering. Our findings indicate that TeleChat achieves comparable performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing LLMs we release the fine-tuned model checkpoints of TeleChats 7B and 12B variant along with code and a portion of our pretraining data to the public community.
