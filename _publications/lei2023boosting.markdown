---
layout: publication
title: Boosting Logical Reasoning in Large Language Models through a New Framework The Graph of Thought
authors: Lei Bin, Lin Pei-hung, Liao Chunhua, Ding Caiwen
conference: "Arxiv"
year: 2023
bibkey: lei2023boosting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.08614"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG', 'Tools']
---
Recent advancements in large-scale models such as GPT-4 have showcased remarkable capabilities in addressing standard queries. However when facing complex problems that require multi-step logical reasoning their accuracy dramatically decreases. Current research has explored the realm of to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique dubbed . Through testing on a trio of escalating challenges the 24-point game resolution of high-degree polynomial equations and derivation of formulas for recursive sequences our method outperformed GPT-4 achieving accuracy improvements of 89.7 86 and 56 for each respective task. Moreover when juxtaposed with the state-of-the-art (SOTA) prompting method our approach registered an average accuracy boost of 23 24 and 15.
