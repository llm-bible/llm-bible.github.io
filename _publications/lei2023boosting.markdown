---
layout: publication
title: Boosting Logical Reasoning In Large Language Models Through A New Framework The Graph Of Thought
authors: Lei Bin, Lin Pei-hung, Liao Chunhua, Ding Caiwen
conference: "Arxiv"
year: 2023
bibkey: lei2023boosting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.08614"}
tags: ['GPT', 'Model Architecture', 'Prompting', 'RAG', 'Tools']
---
Recent advancements in large45;scale models such as GPT45;4 have showcased remarkable capabilities in addressing standard queries. However when facing complex problems that require multi45;step logical reasoning their accuracy dramatically decreases. Current research has explored the realm of textit123;prompting engineering125; to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique dubbed textit123;Graph of Thoughts (GoT)125;. Through testing on a trio of escalating challenges the 2445;point game resolution of high45;degree polynomial equations and derivation of formulas for recursive sequences our method outperformed GPT45;4 achieving accuracy improvements of 89.737; 8637; and 5637; for each respective task. Moreover when juxtaposed with the state45;of45;the45;art (SOTA) prompting method textit123;Tree of Thought (ToT)125; our approach registered an average accuracy boost of 2337; 2437; and 1537;.
