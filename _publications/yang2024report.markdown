---
layout: publication
title: Report Cards Qualitative Evaluation of Language Models Using Natural Language Summaries
authors: Yang Blair, Cui Fuyang, Paster Keiran, Ba Jimmy, Vaezipoor Pashootan, Pitis Silviu, Zhang Michael R.
conference: "Arxiv"
year: 2024
bibkey: yang2024report
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.00844"}
tags: ['Interpretability And Explainability', 'Tools']
---
The rapid development and dynamic nature of large language models (LLMs) make it difficult for conventional quantitative benchmarks to accurately assess their capabilities. We propose report cards which are human-interpretable natural language summaries of model behavior for specific skills or topics. We develop a framework to evaluate report cards based on three criteria specificity (ability to distinguish between models) faithfulness (accurate representation of model capabilities) and interpretability (clarity and relevance to humans). We also propose an iterative algorithm for generating report cards without human supervision and explore its efficacy by ablating various design choices. Through experimentation with popular LLMs we demonstrate that report cards provide insights beyond traditional benchmarks and can help address the need for a more interpretable and holistic evaluation of LLMs.
