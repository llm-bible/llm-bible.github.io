---
layout: publication
title: Potential and Limitations of LLMs in Capturing Structured Semantics A Case Study on SRL
authors: Cheng Ning, Yan Zhaohui, Wang Ziming, Li Zhijie, Yu Jiaming, Zheng Zilong, Tu Kewei, Xu Jinan, Han Wenjuan
conference: "Arxiv"
year: 2024
bibkey: cheng2024potential
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.06410"}
tags: ['ARXIV', 'Interpretability']
---
Large Language Models (LLMs) play a crucial role in capturing structured semantics to enhance language understanding improve interpretability and reduce bias. Nevertheless an ongoing controversy exists over the extent to which LLMs can grasp structured semantics. To assess this we propose using Semantic Role Labeling (SRL) as a fundamental task to explore LLMs ability to extract structured semantics. In our assessment we employ the prompting approach which leads to the creation of our few-shot SRL parser called PromptSRL. PromptSRL enables LLMs to map natural languages to explicit semantic structures which provides an interpretable window into the properties of LLMs. We find interesting potential LLMs can indeed capture semantic structures and scaling-up doesnt always mirror potential. Additionally limitations of LLMs are observed in C-arguments etc. Lastly we are surprised to discover that significant overlap in the errors is made by both LLMs and untrained humans accounting for almost 30 of all errors.
