---
layout: publication
title: 'Towards Adaptive Feedback With AI: Comparing The Feedback Quality Of Llms And Teachers On Experimentation Protocols'
authors: Kathrin Seßler, Arne Bewersdorff, Claudia Nerdel, Enkelejda Kasneci
conference: "Arxiv"
year: 2025
bibkey: seßler2025towards
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.12842'}
tags: ['Reinforcement Learning', 'Agentic', 'Efficiency and Optimization', 'RAG']
---
Effective feedback is essential for fostering students' success in scientific
inquiry. With advancements in artificial intelligence, large language models
(LLMs) offer new possibilities for delivering instant and adaptive feedback.
However, this feedback often lacks the pedagogical validation provided by
real-world practitioners. To address this limitation, our study evaluates and
compares the feedback quality of LLM agents with that of human teachers and
science education experts on student-written experimentation protocols. Four
blinded raters, all professionals in scientific inquiry and science education,
evaluated the feedback texts generated by 1) the LLM agent, 2) the teachers and
3) the science education experts using a five-point Likert scale based on six
criteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive
Tone, Linguistic Clarity, and Technical Terminology. Our results indicate that
LLM-generated feedback shows no significant difference to that of teachers and
experts in overall quality. However, the LLM agent's performance lags in the
Feed Back dimension, which involves identifying and explaining errors within
the student's work context. Qualitative analysis highlighted the LLM agent's
limitations in contextual understanding and in the clear communication of
specific errors. Our findings suggest that combining LLM-generated feedback
with human expertise can enhance educational practices by leveraging the
efficiency of LLMs and the nuanced understanding of educators.
