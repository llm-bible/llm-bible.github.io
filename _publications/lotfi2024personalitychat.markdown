---
layout: publication
title: PersonalityChat Conversation Distillation for Personalized Dialog Modeling with Facts and Traits
authors: Lotfi Ehsan, De Bruyn Maxime, Buhmann Jeska, Daelemans Walter
conference: "Arxiv"
year: 2024
bibkey: lotfi2024personalitychat
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.07363"}
tags: ['Agentic', 'Distillation', 'Efficiency And Optimization', 'Prompting', 'Training Techniques']
---
The new wave of Large Language Models (LLM) has offered an efficient tool to curate sizeable conversational datasets. So far studies have mainly focused on task-oriented or generic open-domain dialogs and have not fully explored the ability of LLMs in following complicated prompts. In this work we focus on personalization and employ LLMs to curate a dataset which is difficult and costly to crowd-source PersonalityChat is a synthetic conversational dataset based upon the popular PersonaChat dataset but conditioned on both personas and (Big-5) personality traits. Evaluating models fine-tuned on this dataset we show that the personality trait labels can be used for trait-based personalization of generative dialogue models. We also perform a head-to-head comparison between PersonalityChat and PersonaChat and show that training on the distilled dataset results in more fluent and coherent dialog agents in the small-model regime.
