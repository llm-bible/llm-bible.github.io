---
layout: publication
title: Llm45;eval Unified Multi45;dimensional Automatic Evaluation For Open45;domain Conversations With Large Language Models
authors: Yen-ting Lin, Yun-nung Chen
conference: "Arxiv"
year: 2023
bibkey: lin2023llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2305.13711v1"}
tags: ['Efficiency And Optimization', 'Pretraining Methods', 'Prompting', 'RAG']
---
We propose LLM45;Eval a unified multi45;dimensional automatic evaluation method for open45;domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations ground45;truth responses or multiple LLM prompts which can be expensive and time45;consuming. To address these issues we design a single prompt45;based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM45;Eval on various benchmark datasets demonstrating its effectiveness efficiency and adaptability compared to state45;of45;the45;art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM45;Eval offers a versatile and robust solution for evaluating open45;domain conversation systems streamlining the evaluation process and providing consistent performance across diverse scenarios.
