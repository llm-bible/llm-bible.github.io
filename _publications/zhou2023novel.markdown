---
layout: publication
title: "CONA: A Novel Context-aware Instruction Paradigm For Communication Using Large Language Model"
authors: Zhou Nan, Tao Xinghui, Chen Xi
conference: "Arxiv"
year: 2023
bibkey: zhou2023novel
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.18620"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG', 'Reinforcement Learning', 'Tools', 'Transformer']
---
We introduce CONA a novel context-aware instruction paradigm for effective knowledge dissemination using generative pre-trained transformer (GPT) models. CONA is a flexible framework designed to leverage the capabilities of Large Language Models (LLMs) and incorporate DIKW (Data Information Knowledge Wisdom) hierarchy to automatically instruct and optimise presentation content anticipate potential audience inquiries and provide context-aware answers that adaptive to the knowledge level of the audience group. The unique aspect of the CONA paradigm lies in its combination of an independent advisory mechanism and a recursive feedback loop rooted on the DIKW hierarchy. This synergy significantly enhances context-aware contents ensuring they are accessible and easily comprehended by the audience. This paradigm is an early pioneer to explore new methods for knowledge dissemination and communication in the LLM era offering effective support for everyday knowledge sharing scenarios. We conduct experiments on a range of audience roles along with materials from various disciplines using GPT4. Both quantitative and qualitative results demonstrated that the proposed CONA paradigm achieved remarkable performance compared to the outputs guided by conventional prompt engineering.
