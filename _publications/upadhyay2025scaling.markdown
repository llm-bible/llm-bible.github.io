---
layout: publication
title: 'Synlexlm: Scaling Legal Llms With Synthetic Data And Curriculum Learning'
authors: Ojasw Upadhyay, Abishek Saravanakumar, Ayman Ismail
conference: "Arxiv"
year: 2025
bibkey: upadhyay2025scaling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.18762"}
tags: ['Training Techniques', 'Tools', 'Pretraining Methods', 'Fine-Tuning', 'Pre-Training']
---
Large Language Models (LLMs) are powerful but often require extensive
fine-tuning and large datasets for specialized domains like law.
General-purpose pre-training may not capture legal nuances, and acquiring
sufficient legal data is challenging. We introduce SynLexLM, a novel approach
to efficiently pre-train a legal LLM. Our method employs curriculum learning,
progressing from simple to complex legal texts and queries, combined with
synthetic data augmentation using models like Gemini Pro to address data
scarcity. We aim to achieve improved performance on legal benchmarks
(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned
versions. Preliminary work involves generating synthetic QA pairs reflecting
legal reasoning. This work aims to enhance legal document analysis and research
tools, potentially democratizing access to advanced legal AI.
