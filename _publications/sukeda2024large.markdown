---
layout: publication
title: 70b45;parameter Large Language Models In Japanese Medical Question45;answering
authors: Sukeda Issey, Kishikawa Risa, Kodera Satoshi
conference: "Arxiv"
year: 2024
bibkey: sukeda2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.14882"}
tags: ['Fine Tuning', 'Pretraining Methods', 'Prompting', 'Training Techniques']
---
Since the rise of large language models (LLMs) the domain adaptation has been one of the hot topics in various domains. Many medical LLMs trained with English medical dataset have made public recently. However Japanese LLMs in medical domain still lack its research. Here we utilize multiple 70B45;parameter LLMs for the first time and show that instruction tuning using Japanese medical question45;answering dataset significantly improves the ability of Japanese LLMs to solve Japanese medical license exams surpassing 5037; in accuracy. In particular the Japanese45;centric models exhibit a more significant leap in improvement through instruction tuning compared to their English45;centric counterparts. This underscores the importance of continual pretraining and the adjustment of the tokenizer in our local language. We also examine two slightly different prompt formats resulting in non45;negligible performance improvement.
