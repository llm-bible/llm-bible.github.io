---
layout: publication
title: 'An Empirical Study Of Validating Synthetic Data For Formula Generation'
authors: Usneek Singh, Jos√© Cambronero, Sumit Gulwani, Aditya Kanade, Anirudh Khatry, Vu Le, Mukul Singh, Gust Verbruggen
conference: "Arxiv"
year: 2024
bibkey: singh2024empirical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.10657"}
tags: ['Pretraining Methods', 'RAG', 'Training Techniques', 'Fine-Tuning']
---
Large language models (LLMs) can be leveraged to help with writing formulas
in spreadsheets, but resources on these formulas are scarce, impacting both the
base performance of pre-trained models and limiting the ability to fine-tune
them. Given a corpus of formulas, we can use a(nother) model to generate
synthetic natural language utterances for fine-tuning. However, it is important
to validate whether the NL generated by the LLM is indeed accurate to be
beneficial for fine-tuning. In this paper, we provide empirical results on the
impact of validating these synthetic training examples with surrogate
objectives that evaluate the accuracy of the synthetic annotations. We
demonstrate that validation improves performance over raw data across four
models (2 open and 2 closed weight). Interestingly, we show that although
validation tends to prune more challenging examples, it increases the
complexity of problems that models can solve after being fine-tuned on
validated data.
