---
layout: publication
title: 'Analyze The Neurons, Not The Embeddings: Understanding When And Where LLM Representations Align With Humans'
authors: Masha Fedzechkina, Eleonora Gualdoni, Sinead Williamson, Katherine Metcalf, Skyler Seto, Barry-john Theobald
conference: "Arxiv"
year: 2025
bibkey: fedzechkina2025analyze
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.15090'}
tags: ['Uncategorized']
---
Modern large language models (LLMs) achieve impressive performance on some
tasks, while exhibiting distinctly non-human-like behaviors on others. This
raises the question of how well the LLM's learned representations align with
human representations. In this work, we introduce a novel approach to the study
of representation alignment: we adopt a method from research on activation
steering to identify neurons responsible for specific concepts (e.g., 'cat')
and then analyze the corresponding activation patterns. Our findings reveal
that LLM representations closely align with human representations inferred from
behavioral data. Notably, this alignment surpasses that of word embeddings,
which have been center stage in prior work on human and model alignment.
Additionally, our approach enables a more granular view of how LLMs represent
concepts. Specifically, we show that LLMs organize concepts in a way that
reflects hierarchical relationships interpretable to humans (e.g.,
'animal'-'dog').
