---
layout: publication
title: 'Energy-conscious LLM Decoding: Impact Of Text Generation Strategies On GPU Energy Consumption'
authors: Alireza Nik, Michael A. Riegler, PÃ¥l Halvorsen
conference: "Arxiv"
year: 2025
bibkey: nik2025energy
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.11723"}
tags: ['Language Modeling', 'Efficiency and Optimization', 'Applications', 'Reinforcement Learning']
---
Decoding strategies significantly influence the quality and diversity of the
generated texts in large language models (LLMs), yet their impact on
computational resource consumption, particularly GPU energy usage, is
insufficiently studied. This paper investigates the relationship between text
generation decoding methods and energy efficiency, focusing on the trade-off
between generation quality and GPU energy consumption across diverse tasks and
decoding configurations. By benchmarking multiple strategies across different
text generation tasks, such as Translation, Code Summarization, and Math
Problem Solving, we reveal how selecting appropriate decoding techniques with
their tuned hyperparameters affects text quality and has measurable
implications for resource utilization, emphasizing the need for balanced
optimization. To the best of our knowledge, this study is among the first to
explore decoding strategies in LLMs through the lens of energy consumption,
offering actionable insights for designing resource-aware applications that
maintain high-quality text generation.
