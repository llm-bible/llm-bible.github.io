---
layout: publication
title: 'Llama: Open And Efficient Foundation Language Models'
authors: Hugo Touvron et al.
conference: Arxiv
year: 2023
citations: 2718
bibkey: touvron2023open
additional_links:
- name: Paper
  url: https://arxiv.org/abs/2302.13971
tags:
- GPT
- Transformer
---
We introduce LLaMA, a collection of foundation language models ranging from
7B to 65B parameters. We train our models on trillions of tokens, and show that
it is possible to train state-of-the-art models using publicly available
datasets exclusively, without resorting to proprietary and inaccessible
datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,
and LLaMA-65B is competitive with the best models, Chinchilla-70B and
PaLM-540B. We release all our models to the research community.