---
layout: publication
title: Llama Open And Efficient Foundation Language Models
authors: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample
conference: "Arxiv"
year: 2023
bibkey: touvron2023open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2302.13971v1"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
We introduce LLaMA a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens and show that it is possible to train state45;of45;the45;art models using publicly available datasets exclusively without resorting to proprietary and inaccessible datasets. In particular LLaMA45;13B outperforms GPT45;3 (175B) on most benchmarks and LLaMA45;65B is competitive with the best models Chinchilla45;70B and PaLM45;540B. We release all our models to the research community.
