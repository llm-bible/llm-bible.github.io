---
layout: publication
title: 'Benchmarking Large Vision-language Models On Fine-grained Image Tasks: A Comprehensive Evaluation'
authors: Hong-tao Yu, Xiu-shen Wei, Yuxin Peng, Serge Belongie
conference: "Arxiv"
year: 2025
bibkey: yu2025benchmarking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.14988'}
  - {name: "Code", url: 'https://github.com/SEU-VIPGroup/FG-BMK'}
tags: ['Attention Mechanism', 'Has Code', 'Training Techniques', 'Model Architecture', 'Multimodal Models']
---
Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal perception capabilities, garnering significant attention. While numerous evaluation studies have emerged, assessing LVLMs both holistically and on specialized tasks, fine-grained image tasks-fundamental to computer vision-remain largely unexplored. To fill this gap, we introduce a comprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 1.01 million questions and 0.33 million images. Our evaluation systematically examines LVLMs from both human-oriented and machine-oriented perspectives, focusing on their semantic recognition and fine-grained feature representation capabilities. Through extensive experiments on twelve representative LVLMs/VLMs, we uncover key findings regarding the influence of training paradigms, modality alignment, perturbation susceptibility, and fine-grained category reasoning on task performance. This work provides critical insights into the limitations of current LVLMs and offers guidance for future data construction and model design in the development of more advanced LVLMs. Our code is open-source and available at https://github.com/SEU-VIPGroup/FG-BMK.
