---
layout: publication
title: Bp45;transformer Modelling Long45;range Context Via Binary Partitioning
authors: Ye Zihao, Guo Qipeng, Gan Quan, Qiu Xipeng, Zhang Zheng
conference: "Arxiv"
year: 2019
bibkey: ye2019bp
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1911.04070"}
tags: ['Applications', 'Attention Mechanism', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
The Transformer model is widely successful on many natural language processing tasks. However the quadratic complexity of self45;attention limit its application on long text. In this paper adopting a fine45;to45;coarse attention mechanism on multi45;scale spans via binary partitioning (BP) we propose BP45;Transformer (BPT for short). BPT yields O(kâ‹… nlog (n/k)) connections where k is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification machine translation and language modeling shows BPT has a superior performance for long text than previous self45;attention models. Our code hyperparameters and CUDA kernels for sparse attention are available in PyTorch.
