---
layout: publication
title: Quite Good but Not Enough Nationality Bias in Large Language Models -- A Case Study of ChatGPT
authors: Zhu Shucheng, Wang Weikang, Liu Ying
conference: "Arxiv"
year: 2024
bibkey: zhu2024quite
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.06996"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Language Modeling', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Tools']
---
While nationality is a pivotal demographic element that enhances the performance of language models it has received far less scrutiny regarding inherent biases. This study investigates nationality bias in ChatGPT (GPT-3.5) a large language model (LLM) designed for text generation. The research covers 195 countries 4 temperature settings and 3 distinct prompt types generating 4680 discourses about nationality descriptions in Chinese and English. Automated metrics were used to analyze the nationality bias and expert annotators alongside ChatGPT itself evaluated the perceived bias. The results show that ChatGPTs generated discourses are predominantly positive especially compared to its predecessor GPT-2. However when prompted with negative inclinations it occasionally produces negative content. Despite ChatGPT considering its generated text as neutral it shows consistent self-awareness about nationality bias when subjected to the same pair-wise comparison annotation framework used by human annotators. In conclusion while ChatGPTs generated texts seem friendly and positive they reflect the inherent nationality biases in the real world. This bias may vary across different language versions of ChatGPT indicating diverse cultural perspectives. The study highlights the subtle and pervasive nature of biases within LLMs emphasizing the need for further scrutiny.
