---
layout: publication
title: 'Align, Generate, Learn: A Novel Closed-loop Framework For Cross-lingual In-context Learning'
authors: Mateo Alejandro Rojas, Rafael Carranza
conference: "Arxiv"
year: 2024
bibkey: rojas2024novel
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.08955"}
tags: ['Security', 'Training Techniques', 'Tools', 'RAG', 'Pretraining Methods', 'Fine-Tuning', 'Prompting', 'In-Context Learning']
---
Cross-lingual in-context learning (XICL) has emerged as a transformative
paradigm for leveraging large language models (LLMs) to tackle multilingual
tasks, especially for low-resource languages. However, existing approaches
often rely on external retrievers or task-specific fine-tuning, limiting their
scalability and generalizability. In this paper, we propose a novel
self-supervised framework that harnesses the generative capabilities of LLMs to
internally select and utilize task-relevant examples. Our method introduces two
key objectives: a retrieval-generation alignment loss to optimize the quality
of selected examples and a semantic coherence loss to ensure cross-lingual
consistency. Through extensive experiments on multilingual benchmarks, our
approach achieves state-of-the-art performance, significantly outperforming
existing baselines. Further analysis highlights its robustness across diverse
language families and its ability to generalize to unseen tasks. Human
evaluations confirm the superior fluency, relevance, and semantic correctness
of outputs generated by our method. This work provides a scalable, effective,
and generalizable solution for cross-lingual in-context learning.
