---
layout: publication
title: 'Proto-lm: A Prototypical Network-based Framework For Built-in Interpretability In Large Language Models'
authors: Sean Xie, Soroush Vosoughi, Saeed Hassanpour
conference: "Arxiv"
year: 2023
bibkey: xie2023proto
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2311.01732'}
tags: ['Interpretability and Explainability', 'Training Techniques', 'Tools', 'Fine-Tuning', 'Interpretability', 'Pretraining Methods']
---
Large Language Models (LLMs) have significantly advanced the field of Natural
Language Processing (NLP), but their lack of interpretability has been a major
concern. Current methods for interpreting LLMs are post hoc, applied after
inference time, and have limitations such as their focus on low-level features
and lack of explainability at higher level text units. In this work, we
introduce proto-lm, a prototypical network-based white-box framework that
allows LLMs to learn immediately interpretable embeddings during the
fine-tuning stage while maintaining competitive performance. Our method's
applicability and interpretability are demonstrated through experiments on a
wide range of NLP tasks, and our results indicate a new possibility of creating
interpretable models without sacrificing performance. This novel approach to
interpretability in LLMs can pave the way for more interpretable models without
the need to sacrifice performance.
