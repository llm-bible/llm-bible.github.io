---
layout: publication
title: 'Policy-to-language: Train Llms To Explain Decisions With Flow-matching Generated Rewards'
authors: Xinyi Yang, Liang Zeng, Heng Dong, Chao Yu, Xiaoran Wu, Huazhong Yang, Yu Wang, Milind Tambe, Tonghan Wang
conference: "Arxiv"
year: 2025
bibkey: yang2025policy
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.12530'}
tags: ['Reinforcement Learning', 'Agentic', 'Interpretability and Explainability', 'Training Techniques']
---
As humans increasingly share environments with diverse agents powered by RL,
LLMs, and beyond, the ability to explain their policies in natural language
will be vital for reliable coexistence. In this paper, we build a
model-agnostic explanation generator based on an LLM. The technical novelty is
that the rewards for training this LLM are generated by a generative flow
matching model. This model has a specially designed structure with a hidden
layer merged with an LLM to harness the linguistic cues of explanations into
generating appropriate rewards. Experiments on both RL and LLM tasks
demonstrate that our method can generate dense and effective rewards while
saving on expensive human feedback; it thus enables effective explanations and
even improves the accuracy of the decisions in original tasks.
