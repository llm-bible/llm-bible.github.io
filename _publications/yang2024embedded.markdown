---
layout: publication
title: Embedded Translations For Low45;resource Automated Glossing
authors: Yang Changbing, Nicolai Garrett, Silfverberg Miikka
conference: "Arxiv"
year: 2024
bibkey: yang2024embedded
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.08189"}
tags: ['Attention Mechanism', 'BERT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning']
---
We investigate automatic interlinear glossing in low45;resource settings. We augment a hard45;attentional neural model with embedded translation information extracted from interlinear glossed text. After encoding these translations using large language models specifically BERT and T5 we introduce a character45;level decoder for generating glossed output. Aided by these enhancements our model demonstrates an average improvement of 3.9737;45;points over the previous state of the art on datasets from the SIGMORPHON 2023 Shared Task on Interlinear Glossing. In a simulated ultra low45;resource setting trained on as few as 100 sentences our system achieves an average 9.7837;45;point improvement over the plain hard45;attentional baseline. These results highlight the critical role of translation information in boosting the systems performance especially in processing and interpreting modest data sources. Our findings suggest a promising avenue for the documentation and preservation of languages with our experiments on shared task datasets indicating significant advancements over the existing state of the art.
