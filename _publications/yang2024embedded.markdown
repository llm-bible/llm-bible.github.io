---
layout: publication
title: 'Embedded Translations For Low-resource Automated Glossing'
authors: Yang Changbing, Nicolai Garrett, Silfverberg Miikka
conference: "Arxiv"
year: 2024
bibkey: yang2024embedded
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.08189"}
tags: ['Attention Mechanism', 'BERT', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Uncategorized']
---
We investigate automatic interlinear glossing in low-resource settings. We augment a hard-attentional neural model with embedded translation information extracted from interlinear glossed text. After encoding these translations using large language models, specifically BERT and T5, we introduce a character-level decoder for generating glossed output. Aided by these enhancements, our model demonstrates an average improvement of 3.97\&#37;-points over the previous state of the art on datasets from the SIGMORPHON 2023 Shared Task on Interlinear Glossing. In a simulated ultra low-resource setting, trained on as few as 100 sentences, our system achieves an average 9.78\&#37;-point improvement over the plain hard-attentional baseline. These results highlight the critical role of translation information in boosting the system's performance, especially in processing and interpreting modest data sources. Our findings suggest a promising avenue for the documentation and preservation of languages, with our experiments on shared task datasets indicating significant advancements over the existing state of the art.
