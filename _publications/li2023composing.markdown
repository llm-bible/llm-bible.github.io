---
layout: publication
title: Covlm Composing Visual Entities And Relationships In Large Language Models Via Communicative Decoding
authors: Li Junyan, Chen Delin, Hong Yining, Chen Zhenfang, Chen Peihao, Shen Yikang, Gan Chuang
conference: "Arxiv"
year: 2023
bibkey: li2023composing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.03354"}
tags: ['Applications', 'Tools']
---
A remarkable ability of human beings resides in compositional reasoning i.e. the capacity to make infinite use of finite means. However current large vision45;language foundation models (VLMs) fall short of such compositional abilities due to their bag45;of45;words behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end we propose CoVLM which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision45;language communicative decoding. Specifically we first devise a set of novel communication tokens for the LLM for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions45;of45;interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision45;to45;language and language45;to45;vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g. ~2037; in HICO45;DET mAP ~1437; in Cola top45;1 accuracy and ~337; on ARO top45;1 accuracy). We also achieve state45;of45;the45;art performances on traditional vision45;language tasks such as referring expression comprehension and visual question answering.
