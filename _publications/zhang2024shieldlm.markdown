---
layout: publication
title: ShieldLM Empowering LLMs as Aligned Customizable and Explainable Safety Detectors
authors: Zhang Zhexin, Lu Yida, Ma Jingyuan, Zhang Di, Li Rui, Ke Pei, Sun Hao, Sha Lei, Sui Zhifang, Wang Hongning, Huang Minlie
conference: "Arxiv"
year: 2024
bibkey: zhang2024shieldlm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.16444"}
  - {name: "Code", url: "https://github.com/thu-coai/ShieldLM}"}
tags: ['ARXIV', 'Explainability', 'Has Code', 'Interpretability', 'LLM']
---
The safety of Large Language Models (LLMs) has gained increasing attention in recent years but there still lacks a comprehensive approach for detecting safety issues within LLMs responses in an aligned customizable and explainable manner. In this paper we propose ShieldLM an LLM-based safety detector which aligns with general human safety standards supports customizable detection rules and provides explanations for its decisions. To train ShieldLM we compile a large bilingual dataset comprising 14387 query-response pairs annotating the safety of responses based on various safety standards. Through extensive experiments we demonstrate that ShieldLM surpasses strong baselines across four test sets showcasing remarkable customizability and explainability. Besides performing well on standard detection datasets ShieldLM has also been shown to be effective in real-world situations as a safety evaluator for advanced LLMs. We release ShieldLM at urlhttps://github.com/thu-coai/ShieldLM} to support accurate and explainable safety detection under various safety standards contributing to the ongoing efforts to enhance the safety of LLMs.
