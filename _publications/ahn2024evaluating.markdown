---
layout: publication
title: Timechara Evaluating Point45;in45;time Character Hallucination Of Role45;playing Large Language Models
authors: Ahn Jaewoo, Lee Taehyun, Lim Junyoung, Kim Jin-hwa, Yun Sangdoo, Lee Hwaran, Kim Gunhee
conference: "Arxiv"
year: 2024
bibkey: ahn2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.18027"}
tags: ['Agentic', 'GPT', 'Model Architecture', 'Pretraining Methods']
---
While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e. role45;playing agents) we emphasize the importance of point45;in45;time role45;playing. This situates characters at specific moments in the narrative progression for three main reasons (i) enhancing users narrative immersion (ii) avoiding spoilers and (iii) fostering engagement in fandom role45;playing. To accurately represent characters at specific time points agents must avoid character hallucination where they display knowledge that contradicts their characters identities and historical timelines. We introduce TimeChara a new benchmark designed to evaluate point45;in45;time character hallucination in role45;playing LLMs. Comprising 10895 instances generated through an automated pipeline this benchmark reveals significant hallucination issues in current state45;of45;the45;art LLMs (e.g. GPT45;4o). To counter this challenge we propose Narrative45;Experts a method that decomposes the reasoning steps and utilizes narrative experts to reduce point45;in45;time character hallucinations effectively. Still our findings with TimeChara highlight the ongoing challenges of point45;in45;time character hallucination calling for further study.
