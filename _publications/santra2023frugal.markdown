---
layout: publication
title: 'Frugal Prompting For Dialog Models'
authors: Santra Bishal, Basak Sakya, De Abhinandan, Gupta Manish, Goyal Pawan
conference: "Arxiv"
year: 2023
bibkey: santra2023frugal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.14919"}
tags: ['Applications', 'Prompting', 'Tools']
---
The use of large language models (LLMs) in natural language processing (NLP) tasks is rapidly increasing leading to changes in how researchers approach problems in the field. To fully utilize these models abilities a better understanding of their behavior for different input protocols is required. With LLMs users can directly interact with the models through a text-based interface to define and solve various tasks. Hence understanding the conversational abilities of these LLMs which may not have been specifically trained for dialog modeling is also important. This study examines different approaches for building dialog systems using LLMs by considering various aspects of the prompt. As part of prompt tuning we experiment with various ways of providing instructions exemplars current query and additional context. The research also analyzes the representations of dialog history that have the optimal usable-information density. Based on the findings the paper suggests more compact ways of providing dialog history information while ensuring good performance and reducing models inference-API costs. The research contributes to a better understanding of how LLMs can be effectively used for building interactive systems.
