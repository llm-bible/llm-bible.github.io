---
layout: publication
title: Exploiting Sentence45;level Representations For Passage Ranking
authors: Leonhardt Jurek, Beringer Fabian, Anand Avishek
conference: "Arxiv"
year: 2021
bibkey: leonhardt2021exploiting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2106.07316"}
tags: ['Applications', 'BERT', 'Efficiency And Optimization', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
Recently pre45;trained contextual models such as BERT have shown to perform well in language related tasks. We revisit the design decisions that govern the applicability of these models for the passage re45;ranking task in open45;domain question answering. We find that common approaches in the literature rely on fine45;tuning a pre45;trained BERT model and using a single global representation of the input discarding useful fine45;grained relevance signals in token45; or sentence45;level representations. We argue that these discarded tokens hold useful information that can be leveraged. In this paper we explicitly model the sentence45;level representations by using Dynamic Memory Networks (DMNs) and conduct empirical evaluation to show improvements in passage re45;ranking over fine45;tuned vanilla BERT models by memory45;enhanced explicit sentence modelling on a diverse set of open45;domain QA datasets. We further show that freezing the BERT model and only training the DMN layer still comes close to the original performance while improving training efficiency drastically. This indicates that the usual fine45;tuning step mostly helps to aggregate the inherent information in a single output token as opposed to adapting the whole model to the new task and only achieves rather small gains.
