---
layout: publication
title: Re45;evaluating Germeval17 Using German Pre45;trained Language Models
authors: Aßenmacher M., Corvonato A., Heumann C.
conference: "Arxiv"
year: 2021
bibkey: aßenmacher2021re
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2102.12330"}
tags: ['BERT', 'Model Architecture', 'Pretraining Methods', 'Tools', 'Transformer']
---
The lack of a commonly used benchmark data set (collection) such as (Super45;)GLUE (Wang et al. 2018 2019) for the evaluation of non45;English pre45;trained language models is a severe shortcoming of current English45;centric NLP45;research. It concentrates a large part of the research on English neglecting the uncertainty when transferring conclusions found for the English language to other languages. We evaluate the performance of the German and multilingual BERT45;based models currently available via the huggingface transformers library on the four tasks of the GermEval17 workshop. We compare them to pre45;BERT architectures (Wojatzki et al. 2017; Schmitt et al. 2018; Attia et al. 2018) as well as to an ELMo45;based architecture (Biesialska et al. 2020) and a BERT45;based approach (Guhr et al. 2020). The observed improvements are put in relation to those for similar tasks and similar models (pre45;BERT vs. BERT45;based) for the English language in order to draw tentative conclusions about whether the observed improvements are transferable to German or potentially other related languages.
