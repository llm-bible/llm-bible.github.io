---
layout: publication
title: 'Automatic Item Generation For Personality Situational Judgment Tests With Large Language Models'
authors: Chang-jin Li, Jiyuan Zhang, Yun Tang, Jian Li
conference: "Arxiv"
year: 2024
bibkey: li2024automatic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.12144"}
tags: ['GPT', 'Ethics and Bias', 'Model Architecture', 'Reinforcement Learning', 'Prompting']
---
Personality assessment, particularly through situational judgment tests
(SJTs), is a vital tool for psychological research, talent selection, and
educational evaluation. This study explores the potential of GPT-4, a
state-of-the-art large language model (LLM), to automate the generation of
personality situational judgment tests (PSJTs) in Chinese. Traditional SJT
development is labor-intensive and prone to biases, while GPT-4 offers a
scalable, efficient alternative. Two studies were conducted: Study 1 evaluated
the impact of prompt design and temperature settings on content validity,
finding that optimized prompts with a temperature of 1.0 produced creative and
accurate items. Study 2 assessed the psychometric properties of GPT-4-generated
PSJTs, revealing that they demonstrated satisfactory reliability and validity,
surpassing the performance of manually developed tests in measuring the Big
Five personality traits. This research highlights GPT-4's effectiveness in
developing high-quality PSJTs, providing a scalable and innovative method for
psychometric test development. These findings expand the possibilities of
automatic item generation and the application of LLMs in psychology, and offer
practical implications for streamlining test development processes in
resource-limited settings.
