---
layout: publication
title: Bjtu45;wechats Systems For The WMT22 Chat Translation Task
authors: Liang Yunlong, Meng Fandong, Xu Jinan, Chen Yufeng, Zhou Jie
conference: "Arxiv"
year: 2022
bibkey: liang2022bjtu
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.15009"}
tags: ['Distillation', 'Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Training Techniques', 'Transformer']
---
This paper introduces the joint submission of the Beijing Jiaotong University and WeChat AI to the WMT22 chat translation task for English45;German. Based on the Transformer we apply several effective variants. In our experiments we utilize the pre45;training45;then45;fine45;tuning paradigm. In the first pre45;training stage we employ data filtering and synthetic data generation (i.e. back45;translation forward45;translation and knowledge distillation). In the second fine45;tuning stage we investigate speaker45;aware in45;domain data generation speaker adaptation prompt45;based context modeling target denoising fine45;tuning and boosted self45;COMET45;based model ensemble. Our systems achieve 0.810 and 0.946 COMET scores. The COMET scores of English45;German and German45;English are the highest among all submissions.
