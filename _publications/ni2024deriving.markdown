---
layout: publication
title: Mixeval Deriving Wisdom Of The Crowd From LLM Benchmark Mixtures
authors: Ni Jinjie, Xue Fuzhao, Yue Xiang, Deng Yuntian, Shah Mahir, Jain Kabir, Neubig Graham, You Yang
conference: "Arxiv"
year: 2024
bibkey: ni2024deriving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.06565"}
tags: ['Ethics And Bias', 'Reinforcement Learning', 'Tools']
---
Evaluating large language models (LLMs) is challenging. Traditional ground45;truth45;based benchmarks fail to capture the comprehensiveness and nuance of real45;world queries while LLM45;as45;judge benchmarks suffer from grading biases and limited query quantity. Both of them may also become contaminated over time. User45;facing evaluation such as Chatbot Arena provides reliable signals but is costly and slow. In this work we propose MixEval a new paradigm for establishing efficient gold45;standard LLM evaluation by strategically mixing off45;the45;shelf benchmarks. It bridges (1) comprehensive and well45;distributed real45;world user queries and (2) efficient and fairly45;graded ground45;truth45;based benchmarks by matching queries mined from the web with similar queries from existing benchmarks. Based on MixEval we further build MixEval45;Hard which offers more room for model improvement. Our benchmarks advantages lie in (1) a 0.96 model ranking correlation with Chatbot Arena arising from the highly impartial query distribution and grading mechanism (2) fast cheap and reproducible execution (637; of the time and cost of MMLU) and (3) dynamic evaluation enabled by the rapid and stable data update pipeline. We provide extensive meta45;evaluation and analysis for our and existing LLM benchmarks to deepen the communitys understanding of LLM evaluation and guide future research directions.
