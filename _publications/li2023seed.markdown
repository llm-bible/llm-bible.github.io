---
layout: publication
title: Seed45;bench45;2 Benchmarking Multimodal Large Language Models
authors: Li Bohao, Ge Yuying, Ge Yixiao, Wang Guangzhi, Wang Rui, Zhang Ruimao, Shan Ying
conference: "Arxiv"
year: 2023
bibkey: li2023seed
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.17092"}
  - {name: "Code", url: "https://github.com/AILab&#45;CVC/SEED&#45;Bench&#125;"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning']
---
Multimodal large language models (MLLMs) building upon the foundation of powerful large language models (LLMs) have recently demonstrated exceptional capabilities in generating not only texts but also images given interleaved multimodal inputs (acting like a combination of GPT45;4V and DALL45;E 3). However existing MLLM benchmarks remain limited to assessing only models comprehension ability of single image45;text inputs failing to keep up with the strides made in MLLMs. A comprehensive benchmark is imperative for investigating the progress and uncovering the limitations of current MLLMs. In this work we categorize the capabilities of MLLMs into hierarchical levels from L95;0 to L95;4 based on the modalities they can accept and generate and propose SEED45;Bench45;2 a comprehensive benchmark that evaluates the textbf123;hierarchical125; capabilities of MLLMs. Specifically SEED45;Bench45;2 comprises 24K multiple45;choice questions with accurate human annotations which spans 27 dimensions including the evaluation of both text and image generation. Multiple45;choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 23 prominent open45;source MLLMs and summarize valuable observations. By revealing the limitations of existing MLLMs through extensive evaluations we aim for SEED45;Bench45;2 to provide insights that will motivate future research towards the goal of General Artificial Intelligence. Dataset and evaluation code are available at href123;https://github.com/AILab&#45;CVC/SEED&#45;Bench&#125;
