---
layout: publication
title: Language Models Are Multilingual Chain-of-thought Reasoners
authors: Freda Shi et al.
conference: Arxiv
year: 2022
citations: 37
bibkey: shi2022language
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2210.03057'}, {name: Code,
    url: 'https://github.com/google-research/url-nlp'}]
tags: [Prompting, Reinforcement Learning]
---
We evaluate the reasoning abilities of large language models in multilingual
settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by
manually translating 250 grade-school math problems from the GSM8K dataset
(Cobbe et al., 2021) into ten typologically diverse languages. We find that the
ability to solve MGSM problems via chain-of-thought prompting emerges with
increasing model scale, and that models have strikingly strong multilingual
reasoning abilities, even in underrepresented languages such as Bengali and
Swahili. Finally, we show that the multilingual reasoning abilities of language
models extend to other tasks such as commonsense reasoning and word-in-context
semantic judgment. The MGSM benchmark is publicly available at
https://github.com/google-research/url-nlp.