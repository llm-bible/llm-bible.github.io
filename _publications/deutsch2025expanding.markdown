---
layout: publication
title: 'WMT24++: Expanding The Language Coverage Of WMT24 To 55 Languages & Dialects'
authors: Daniel Deutsch, Eleftheria Briakou, Isaac Caswell, Mara Finkelstein, Rebecca Galor, Juraj Juraska, Geza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, Shruti Rijhwani, Parker Riley, Elizabeth Salesky, Firas Trabelsi, Stephanie Winkler, Biao Zhang, Markus Freitag
conference: "Arxiv"
year: 2025
bibkey: deutsch2025expanding
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.12404'}
tags: ['RAG', 'WMT']
---
As large language models (LLM) become more and more capable in languages
other than English, it is important to collect benchmark datasets in order to
evaluate their multilingual performance, including on tasks like machine
translation (MT). In this work, we extend the WMT24 dataset to cover 55
languages by collecting new human-written references and post-edits for 46 new
languages and dialects in addition to post-edits of the references in 8 out of
9 languages in the original WMT24 dataset. The dataset covers four domains:
literary, news, social, and speech. We benchmark a variety of MT providers and
LLMs on the collected dataset using automatic metrics and find that LLMs are
the best-performing MT systems in all 55 languages. These results should be
confirmed using a human-based evaluation, which we leave for future work.
