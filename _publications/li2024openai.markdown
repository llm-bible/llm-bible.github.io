---
layout: publication
title: 'Openai-o1 AB Testing: Does The O1 Model Really Do Good Reasoning In Math Problem Solving?'
authors: Leo Li, Ye Luo, Tingyou Pan
conference: "Arxiv"
year: 2024
bibkey: li2024openai
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.06198"}
tags: ['Prompting', 'Training Techniques']
---
The Orion-1 model by OpenAI is claimed to have more robust logical reasoning
capabilities than previous large language models. However, some suggest the
excellence might be partially due to the model "memorizing" solutions,
resulting in less satisfactory performance when prompted with problems not in
the training data. We conduct a comparison experiment using two datasets: one
consisting of International Mathematics Olympiad (IMO) problems, which is
easily accessible; the other one consisting of Chinese National Team Training
camp (CNT) problems, which have similar difficulty but not as publically
accessible. We label the response for each problem and compare the performance
between the two datasets. We conclude that there is no significant evidence to
show that the model relies on memorizing problems and solutions. Also, we
perform case studies to analyze some features of the model's response.
