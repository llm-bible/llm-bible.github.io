---
layout: publication
title: Look Before You Leap Towards Decision45;aware And Generalizable Tool45;usage For Large Language Models
authors: Gui Anchun, Li Jian, Dai Yong, Du Nan, Xiao Han
conference: "Arxiv"
year: 2024
bibkey: gui2024look
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.16696"}
tags: ['Attention Mechanism', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Tools']
---
Tool45;augmented large language models (LLMs) are attracting widespread attention when accessing up45;to45;date knowledge and alleviating hallucination issues. Nowadays advanced closed45;source LLMs (e.g. ChatGPT) have demonstrated surprising tool45;usage capabilities through prompting and in45;context learning techniques. To empower the capabilities of open45;source LLMs (e.g. LLaMA) in manipulating tools current efforts focus on either template45;driven or token45;triggered tool45;usage. However the former hampers LLMs flexibility to address diverse users queries due to constrained tool interactions while the latter limits the generalizability when engaging with new tools since tool45;usage learning is based on task45; and tool45;specific datasets. To alleviate these concerns in this paper we propose a decision45;aware and generalizable tool45;usage framework (DEER). Specifically we first construct the tool45;usage samples with multiple decision branches via an automatic generation pipeline thereby inspiring the decision45;making awareness of LLMs under diverse scenarios. Meanwhile we propose a novel tool sampling strategy to enhance the generalizability of LLMs over unseen tools. Extensive experiments demonstrate that our proposed DEER is effective and significantly outperforms baselines across various datasets.
