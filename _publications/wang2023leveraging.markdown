---
layout: publication
title: Lifelongmemory Leveraging Llms For Answering Queries In Long45;form Egocentric Videos
authors: Wang Ying, Yang Yanlai, Ren Mengye
conference: "Arxiv"
year: 2023
bibkey: wang2023leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.05269"}
  - {name: "Code", url: "https://github.com/Agentic&#45;Learning&#45;AI&#45;Lab/lifelong&#45;memory"}
tags: ['Agentic', 'Applications', 'Has Code', 'Interpretability And Explainability', 'RAG', 'Tools']
---
In this paper we introduce LifelongMemory a new framework for accessing long45;form egocentric videographic memory through natural language question answering and retrieval. LifelongMemory generates concise video activity descriptions of the camera wearer and leverages the zero45;shot capabilities of pretrained large language models to perform reasoning over long45;form video context. Furthermore Lifelong Memory uses a confidence and explanation module to produce confident high45;quality and interpretable answers. Our approach achieves state45;of45;the45;art performance on the EgoSchema benchmark for question answering and is highly competitive on the natural language query (NLQ) challenge of Ego4D. Code is available at https://github.com/Agentic&#45;Learning&#45;AI&#45;Lab/lifelong&#45;memory.
