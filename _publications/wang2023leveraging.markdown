---
layout: publication
title: "Lifelongmemory: Leveraging Llms For Answering Queries In Long-form Egocentric Videos"
authors: Wang Ying, Yang Yanlai, Ren Mengye
conference: "Arxiv"
year: 2023
bibkey: wang2023leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.05269"}
  - {name: "Code", url: "https://github.com/Agentic-Learning-AI-Lab/lifelong-memory"}
tags: ['Agentic', 'Applications', 'Has Code', 'Interpretability And Explainability', 'RAG', 'Tools']
---
In this paper we introduce LifelongMemory a new framework for accessing long-form egocentric videographic memory through natural language question answering and retrieval. LifelongMemory generates concise video activity descriptions of the camera wearer and leverages the zero-shot capabilities of pretrained large language models to perform reasoning over long-form video context. Furthermore Lifelong Memory uses a confidence and explanation module to produce confident high-quality and interpretable answers. Our approach achieves state-of-the-art performance on the EgoSchema benchmark for question answering and is highly competitive on the natural language query (NLQ) challenge of Ego4D. Code is available at https://github.com/Agentic-Learning-AI-Lab/lifelong-memory."
