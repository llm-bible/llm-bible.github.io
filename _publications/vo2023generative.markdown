---
layout: publication
title: Generative Pre45;trained Transformer For Vietnamese Community45;based COVID45;19 Question Answering
authors: Vo Tam Minh, Tran Khiem Vinh
conference: "Arxiv"
year: 2023
bibkey: vo2023generative
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.14602"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Recent studies have provided empirical evidence of the wide45;ranging potential of Generative Pre45;trained Transformer (GPT) a pretrained language model in the field of natural language processing. GPT has been effectively employed as a decoder within state45;of45;the45;art (SOTA) question answering systems yielding exceptional performance across various tasks. However the current research landscape concerning GPTs application in Vietnamese remains limited. This paper aims to address this gap by presenting an implementation of GPT45;2 for community45;based question answering specifically focused on COVID45;19 related queries in Vietnamese. We introduce a novel approach by conducting a comparative analysis of different Transformers vs SOTA models in the community45;based COVID45;19 question answering dataset. The experimental findings demonstrate that the GPT45;2 models exhibit highly promising outcomes outperforming other SOTA models as well as previous community45;based COVID45;19 question answering models developed for Vietnamese.
