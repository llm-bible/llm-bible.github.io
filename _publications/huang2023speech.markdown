---
layout: publication
title: Speech Translation With Large Language Models An Industrial Practice
authors: Huang Zhichao, Ye Rong, Ko Tom, Dong Qianqian, Cheng Shanbo, Wang Mingxuan, Li Hang
conference: "Arxiv"
year: 2023
bibkey: huang2023speech
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.13585"}
  - {name: "Code", url: "https://speechtranslation.github.io/llm&#45;st/"}
tags: ['Has Code', 'Pretraining Methods', 'Prompting']
---
Given the great success of large language models (LLMs) across various tasks in this paper we introduce LLM45;ST a novel and effective speech translation model constructed upon a pre45;trained LLM. By integrating the large language model (LLM) with a speech encoder and employing multi45;task instruction tuning LLM45;ST can produce accurate timestamped transcriptions and translations even from long audio inputs. Furthermore our findings indicate that the implementation of Chain45;of45;Thought (CoT) prompting can yield advantages in the context of LLM45;ST. Through rigorous experimentation on English and Chinese datasets we showcase the exceptional performance of LLM45;ST establishing a new benchmark in the field of speech translation. Demo https://speechtranslation.github.io/llm&#45;st/.
