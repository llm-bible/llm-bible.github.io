---
layout: publication
title: 'Detectiveqa: Evaluating Long-context Reasoning On Detective Novels'
authors: Zhe Xu, Jiasheng Ye, Xiaoran Liu, Xiangyang Liu, Tianxiang Sun, Zhigeng Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, Xipeng Qiu
conference: "Arxiv"
year: 2024
bibkey: xu2024evaluating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2409.02465'}
tags: ['Reinforcement Learning', 'RAG', 'GPT', 'Model Architecture']
---
Recently, significant efforts have been devoted to enhancing the long-context
capabilities of Large Language Models (LLMs), particularly in long-context
reasoning. To facilitate this research, we propose \textbf\{DetectiveQA\}, a
dataset specifically designed for narrative reasoning within long contexts. We
leverage detective novels, averaging over 100k tokens, to create a dataset
containing 1200 human-annotated questions in both Chinese and English, each
paired with corresponding reference reasoning steps. Furthermore, we introduce
a step-wise reasoning metric, which enhances the evaluation of LLMs' reasoning
processes. We validate our approach and evaluate the mainstream LLMs, including
GPT-4, Claude, and LLaMA, revealing persistent long-context reasoning
challenges and demonstrating their evidence-retrieval challenges. Our findings
offer valuable insights into the study of long-context reasoning and lay the
base for more rigorous evaluations.
