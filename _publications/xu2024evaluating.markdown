---
layout: publication
title: Detectiveqa Evaluating Long45;context Reasoning On Detective Novels
authors: Xu Zhe, Ye Jiasheng, Liu Xiangyang, Sun Tianxiang, Liu Xiaoran, Guo Qipeng, Li Linlin, Liu Qun, Huang Xuanjing, Qiu Xipeng
conference: "Arxiv"
year: 2024
bibkey: xu2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.02465"}
tags: ['Pretraining Methods', 'RAG', 'Tools']
---
With the rapid advancement of Large Language Models (LLMs) long45;context information understanding and processing have become a hot topic in academia and industry. However benchmarks for evaluating the ability of LLMs to handle long45;context information do not seem to have kept pace with the development of LLMs. Despite the emergence of various long45;context evaluation benchmarks the types of capability assessed are still limited without new capability dimensions. In this paper we introduce DetectiveQA a narrative reasoning benchmark featured with an average context length of over 100K tokens. DetectiveQA focuses on evaluating the long45;context reasoning ability of LLMs which not only requires a full understanding of context but also requires extracting important evidences from the context and reasoning according to extracted evidences to answer the given questions. This is a new dimension of capability evaluation which is more in line with the current intelligence level of LLMs. We use detective novels as data sources which naturally have various reasoning elements. Finally we manually annotated 600 questions in Chinese and then also provided an English edition of the context information and questions. We evaluate many long45;context LLMs on DetectiveQA including commercial and open45;sourced models and the results indicate that existing long45;context LLMs still require significant advancements to effectively process true long45;context dependency questions.
