---
layout: publication
title: "Explainability For Large Language Models: A Survey"
authors: Zhao Haiyan, Chen Hanjie, Yang Fan, Liu Ninghao, Deng Huiqi, Cai Hengyi, Wang Shuaiqiang, Yin Dawei, Du Mengnan
conference: "Arxiv"
year: 2023
bibkey: zhao2023explainability
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.01029"}
tags: ['Applications', 'Ethics And Bias', 'Fine Tuning', 'Interpretability And Explainability', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques', 'Transformer']
---
Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore understanding and explaining these models is crucial for elucidating their behaviors limitations and social impacts. In this paper we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.
