---
layout: publication
title: 'Demonstrating Specification Gaming In Reasoning Models'
authors: Alexander Bondarenko, Denis Volk, Dmitrii Volkov, Jeffrey Ladish
conference: "Arxiv"
year: 2025
bibkey: bondarenko2025demonstrating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.13295"}
tags: ['Agentic', 'Prompting', 'Model Architecture', 'GPT']
---
We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack.
  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing.
