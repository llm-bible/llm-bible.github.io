---
layout: publication
title: Ignore Previous Prompt Attack Techniques For Language Models
authors: Perez FÃ¡bio, Ribeiro Ian
conference: "Arxiv"
year: 2022
bibkey: perez2022ignore
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.09527"}
  - {name: "Code", url: "https://github.com/agencyenterprise/PromptInject"}
tags: ['Agentic', 'Applications', 'GPT', 'Has Code', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Security', 'Tools', 'Transformer']
---
Transformer45;based large language models (LLMs) provide a powerful foundation for natural language tasks in large45;scale customer45;facing applications. However studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject a prosaic alignment framework for mask45;based iterative adversarial prompt composition we examine how GPT45;3 the most widely deployed language model in production can be easily misaligned by simple handcrafted inputs. In particular we investigate two types of attacks 45;45; goal hijacking and prompt leaking 45;45; and demonstrate that even low45;aptitude but sufficiently ill45;intentioned agents can easily exploit GPT45;3s stochastic nature creating long45;tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.
