---
layout: publication
title: 'On The Calibration Of Massively Multilingual Language Models'
authors: Ahuja Kabir, Sitaram Sunayana, Dandapat Sandipan, Choudhury Monojit
conference: "Arxiv"
year: 2022
bibkey: ahuja2022calibration
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2210.12265"}
tags: ['Attention Mechanism', 'Few Shot', 'Masked Language Model', 'Model Architecture', 'Uncategorized']
---
Massively Multilingual Language Models (MMLMs) have recently gained
popularity due to their surprising effectiveness in cross-lingual transfer.
While there has been much work in evaluating these models for their performance
on a variety of tasks and languages, little attention has been paid on how well
calibrated these models are with respect to the confidence in their
predictions. We first investigate the calibration of MMLMs in the zero-shot
setting and observe a clear case of miscalibration in low-resource languages or
those which are typologically diverse from English. Next, we empirically show
that calibration methods like temperature scaling and label smoothing do
reasonably well towards improving calibration in the zero-shot scenario. We
also find that few-shot examples in the language can further help reduce the
calibration errors, often substantially. Overall, our work contributes towards
building more reliable multilingual models by highlighting the issue of their
miscalibration, understanding what language and model specific factors
influence it, and pointing out the strategies to improve the same.
