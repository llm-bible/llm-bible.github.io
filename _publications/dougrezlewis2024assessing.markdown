---
layout: publication
title: 'Assessing The Reasoning Capabilities Of Llms In The Context Of Evidence-based Claim Verification'
authors: John Dougrez-lewis, Mahmud Elahi Akhter, Federico Ruggeri, Sebastian LÃ¶bbers, Yulan He, Maria Liakata
conference: "Arxiv"
year: 2024
bibkey: dougrezlewis2024assessing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.10735"}
tags: ['Prompting', 'Tools', 'Reinforcement Learning']
---
Although LLMs have shown great performance on Mathematics and Coding related
reasoning tasks, the reasoning capabilities of LLMs regarding other forms of
reasoning are still an open problem. Here, we examine the issue of reasoning
from the perspective of claim verification. We propose a framework designed to
break down any claim paired with evidence into atomic reasoning types that are
necessary for verification. We use this framework to create Reasoning in
Evidence-based Claim Verification (RECV), the first claim verification
benchmark, incorporating real-world claims, to assess the deductive and
abductive reasoning capabilities of LLMs. The benchmark comprises of three
datasets, covering reasoning problems of increasing complexity. We evaluate
three state-of-the-art proprietary LLMs under multiple prompt settings. Our
results show that while LLMs can address deductive reasoning problems, they
consistently fail in cases of abductive reasoning. Moreover, we observe that
enhancing LLMs with rationale generation is not always beneficial. Nonetheless,
we find that generated rationales are semantically similar to those provided by
humans, especially in deductive reasoning cases.
