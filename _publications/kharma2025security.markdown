---
layout: publication
title: 'Security And Quality In Llm-generated Code: A Multi-language, Multi-model Analysis'
authors: Mohammed Kharma, Soohyeon Choi, Mohammed Alkhanafseh, David Mohaisen
conference: "Arxiv"
year: 2025
bibkey: kharma2025security
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.01853"}
tags: ['Security', 'Tools', 'Reinforcement Learning', 'Merging', 'Applications']
---
Artificial Intelligence (AI)-driven code generation tools are increasingly
used throughout the software development lifecycle to accelerate coding tasks.
However, the security of AI-generated code using Large Language Models (LLMs)
remains underexplored, with studies revealing various risks and weaknesses.
This paper analyzes the security of code generated by LLMs across different
programming languages. We introduce a dataset of 200 tasks grouped into six
categories to evaluate the performance of LLMs in generating secure and
maintainable code. Our research shows that while LLMs can automate code
creation, their security effectiveness varies by language. Many models fail to
utilize modern security features in recent compiler and toolkit updates, such
as Java 17. Moreover, outdated methods are still commonly used, particularly in
C++. This highlights the need for advancing LLMs to enhance security and
quality while incorporating emerging best practices in programming languages.
