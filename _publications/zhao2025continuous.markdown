---
layout: publication
title: 'CAMEL: Continuous Action Masking Enabled By Large Language Models For Reinforcement Learning'
authors: Yanxiao Zhao, Yangge Qian, Jingyang Shan, Xiaolin Qin
conference: "Arxiv"
year: 2025
bibkey: zhao2025continuous
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.11896"}
tags: ['Agentic', 'Efficiency and Optimization', 'Training Techniques', 'Multimodal Models', 'Tools', 'Reinforcement Learning', 'RAG', 'Fine-Tuning']
---
Reinforcement learning (RL) in continuous action spaces encounters persistent
challenges, such as inefficient exploration and convergence to suboptimal
solutions. To address these limitations, we propose CAMEL, a novel framework
integrating LLM-generated suboptimal policies into the RL training pipeline.
CAMEL leverages dynamic action masking and an adaptive epsilon-masking
mechanism to guide exploration during early training stages while gradually
enabling agents to optimize policies independently. At the core of CAMEL lies
the integration of Python-executable suboptimal policies generated by LLMs
based on environment descriptions and task objectives. Although simplistic and
hard-coded, these policies offer valuable initial guidance for RL agents. To
effectively utilize these priors, CAMEL employs masking-aware optimization to
dynamically constrain the action space based on LLM outputs. Additionally,
epsilon-masking gradually reduces reliance on LLM-generated guidance, enabling
agents to transition from constrained exploration to autonomous policy
refinement. Experimental validation on Gymnasium MuJoCo environments
demonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated
policies significantly improve sample efficiency, achieving performance
comparable to or surpassing expert masking baselines. For Walker2d-v4, where
LLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust
RL performance without notable degradation, highlighting the framework's
adaptability across diverse tasks. While CAMEL shows promise in enhancing
sample efficiency and mitigating convergence challenges, these issues remain
open for further research. Future work aims to generalize CAMEL to multimodal
LLMs for broader observation-action spaces and automate policy evaluation,
reducing human intervention and enhancing scalability in RL training pipelines.
