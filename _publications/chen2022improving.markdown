---
layout: publication
title: Improving In45;context Few45;shot Learning Via Self45;supervised Training
authors: Chen Mingda, Du Jingfei, Pasunuru Ramakanth, Mihaylov Todor, Iyer Srini, Stoyanov Veselin, Kozareva Zornitsa
conference: "Arxiv"
year: 2022
bibkey: chen2022improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2205.01703"}
tags: ['Pretraining Methods', 'Training Techniques']
---
Self45;supervised pretraining has made few45;shot learning possible for many NLP tasks. But the pretraining objectives are not typically adapted specifically for in45;context few45;shot learning. In this paper we propose to use self45;supervision in an intermediate training stage between pretraining and downstream few45;shot usage with the goal to teach the model to perform in45;context few shot learning. We propose and evaluate four self45;supervised objectives on two benchmarks. We find that the intermediate self45;supervision stage produces models that outperform strong baselines. Ablation study shows that several factors affect the downstream performance such as the amount of training data and the diversity of the self45;supervised objectives. Human45;annotated cross45;task supervision and self45;supervision are complementary. Qualitative analysis suggests that the self45;supervised45;trained models are better at following task requirements.
