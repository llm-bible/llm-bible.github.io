---
layout: publication
title: ROSE Doesnt Do That Boosting The Safety Of Instruction45;tuned Large Language Models With Reverse Prompt Contrastive Decoding
authors: Zhong Qihuang, Ding Liang, Liu Juhua, Du Bo, Tao Dacheng
conference: "Arxiv"
year: 2024
bibkey: zhong2024rose
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.11889"}
tags: ['Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Responsible AI', 'Training Techniques']
---
With the development of instruction45;tuned large language models (LLMs) improving the safety of LLMs has become more critical. However the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts e.g. high45;quality safety data and expensive computational resources which are costly and inefficient. To this end we present reverse prompt contrastive decoding (ROSE) a simple45;yet45;effective method to directly boost the safety of existing instruction45;tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully45;designed reverse prompts. Experiments on 6 safety and 2 general45;purpose tasks show that our ROSE not only brings consistent and significant safety improvements (up to +13.837; safety score) upon 5 types of instruction45;tuned LLMs but also benefits the general45;purpose ability of LLMs. In45;depth analyses explore the underlying mechanism of ROSE and reveal when and where to use it.
