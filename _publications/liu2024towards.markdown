---
layout: publication
title: Towards Truthful Multilingual Large Language Models: Benchmarking And Alignment Strategies
authors: Liu Weihao, Wu Ning, Ding Wenbiao, Liang Shining, Gong Ming, Zhang Dongmei
conference: "Arxiv"
year: 2024
bibkey: liu2024towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.14434"}
tags: ['Pretraining Methods', 'Reinforcement Learning']
---
In the era of large language models (LLMs) building multilingual large language models (MLLMs) that can serve users worldwide holds great significance. However existing research seldom focuses on the truthfulness of MLLMs. Meanwhile contemporary multilingual aligning technologies struggle to balance massive languages and often exhibit serious truthfulness gaps across different languages especially those that differ greatly from English. In our work we construct a benchmark for truthfulness evaluation in multilingual scenarios and explore the ways to align facts across languages to enhance the truthfulness of MLLMs. Furthermore we propose Fact-aware Multilingual Selective Synergy (FaMSS) to optimize the data allocation across a large number of languages and different data types. Experimental results demonstrate that our approach can effectively reduce the multilingual representation disparity and enhance the multilingual capabilities of LLMs.
