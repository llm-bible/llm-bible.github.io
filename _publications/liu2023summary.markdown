---
layout: publication
title: 'Summary Of Chatgpt-related Research And Perspective Towards The Future Of Large Language Models'
authors: Liu Yiheng, Han Tianle, Ma Siyuan, Zhang Jiayue, Yang Yuanyuan, Tian Jiaming, He Hao, Li Antong, He Mengshen, Liu Zhengliang, Wu Zihao, Zhao Lin, Zhu Dajiang, Li Xiang, Qiang Ning, Shen Dingang, Liu Tianming, Ge Bao
conference: "Meta-Radiology"
year: 2023
bibkey: liu2023summary
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.01852"}
tags: ['Agentic', 'Applications', 'Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and
GPT-4) research, state-of-the-art large language models (LLM) from the GPT
series, and their prospective applications across diverse domains. Indeed, key
innovations such as large-scale pre-training that captures knowledge across the
entire world wide web, instruction fine-tuning and Reinforcement Learning from
Human Feedback (RLHF) have played significant roles in enhancing LLMs'
adaptability and performance. We performed an in-depth analysis of 194 relevant
papers on arXiv, encompassing trend analysis, word cloud representation, and
distribution analysis across various application domains. The findings reveal a
significant and increasing interest in ChatGPT-related research, predominantly
centered on direct natural language processing applications, while also
demonstrating considerable potential in areas ranging from education and
history to mathematics, medicine, and physics. This study endeavors to furnish
insights into ChatGPT's capabilities, potential implications, ethical concerns,
and offer direction for future advancements in this field.
