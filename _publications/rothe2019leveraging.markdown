---
layout: publication
title: Leveraging Pre45;trained Checkpoints For Sequence Generation Tasks
authors: Rothe Sascha, Narayan Shashi, Severyn Aliaksei
conference: "Arxiv"
year: 2019
bibkey: rothe2019leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1907.12461"}
tags: ['Applications', 'BERT', 'GPT', 'Merging', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques', 'Transformer']
---
Unsupervised pre45;training of large neural models has recently revolutionized Natural Language Processing. By warm45;starting from the publicly released checkpoints NLP practitioners have pushed the state45;of45;the45;art on multiple benchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language Understanding tasks. In this paper we demonstrate the efficacy of pre45;trained checkpoints for Sequence Generation. We developed a Transformer45;based sequence45;to45;sequence model that is compatible with publicly available pre45;trained BERT GPT45;2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model both encoder and decoder with these checkpoints. Our models result in new state45;of45;the45;art results on Machine Translation Text Summarization Sentence Splitting and Sentence Fusion.
