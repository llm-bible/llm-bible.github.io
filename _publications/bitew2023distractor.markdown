---
layout: publication
title: Distractor Generation For Multiple45;choice Questions With Predictive Prompting And Large Language Models
authors: Bitew Semere Kiros, Deleu Johannes, Develder Chris, Demeester Thomas
conference: "Arxiv"
year: 2023
bibkey: bitew2023distractor
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.16338"}
tags: ['Applications', 'Attention Mechanism', 'GPT', 'Model Architecture', 'Prompting', 'RAG']
---
Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable performance across various tasks and have garnered significant attention from both researchers and practitioners. However in an educational context we still observe a performance gap in generating distractors 45;45; i.e. plausible yet incorrect answers 45;45; with LLMs for multiple45;choice questions (MCQs). In this study we propose a strategy for guiding LLMs such as ChatGPT in generating relevant distractors by prompting them with question items automatically retrieved from a question bank as well45;chosen in45;context examples. We evaluate our LLM45;based solutions using a quantitative assessment on an existing test set as well as through quality annotations by human experts i.e. teachers. We found that on average 5337; of the generated distractors presented to the teachers were rated as high45;quality i.e. suitable for immediate use as is outperforming the state45;of45;the45;art model. We also show the gains of our approach 1 in generating high45;quality distractors by comparing it with a zero45;shot ChatGPT and a few45;shot ChatGPT prompted with static examples.
