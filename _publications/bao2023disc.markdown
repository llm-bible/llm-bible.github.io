---
layout: publication
title: 'Disc-medllm: Bridging General Large Language Models And Real-world Medical Consultation'
authors: Zhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao Wu, Cheng Zhong, Jiajie Peng, Xuanjing Huang, Zhongyu Wei
conference: "Arxiv"
year: 2023
bibkey: bao2023disc
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2308.14346'}
  - {name: "Code", url: 'https://github.com/FudanDISC/DISC-MedLLM'}
tags: ['Has Code', 'RAG', 'Training Techniques', 'Fine-Tuning', 'Reinforcement Learning', 'Pretraining Methods']
---
We propose DISC-MedLLM, a comprehensive solution that leverages Large
Language Models (LLMs) to provide accurate and truthful medical response in
end-to-end conversational healthcare services. To construct high-quality
Supervised Fine-Tuning (SFT) datasets, we employ three strategies: utilizing
medical knowledge-graphs, reconstructing real-world dialogues, and
incorporating human-guided preference rephrasing. These datasets are
instrumental in training DISC-MedLLM, surpassing existing medical LLMs in both
single-turn and multi-turn consultation scenarios. Extensive experimental
results demonstrate the effectiveness of the proposed model in bridging the gap
between general language models and real-world medical consultation.
Additionally, we release the constructed dataset and model weights to further
contribute to research and development. Further details and resources can be
found at https://github.com/FudanDISC/DISC-MedLLM
