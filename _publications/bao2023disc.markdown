---
layout: publication
title: Disc45;medllm Bridging General Large Language Models And Real45;world Medical Consultation
authors: Bao Zhijie, Chen Wei, Xiao Shengze, Ren Kuang, Wu Jiaao, Zhong Cheng, Peng Jiajie, Huang Xuanjing, Wei Zhongyu
conference: "Arxiv"
year: 2023
bibkey: bao2023disc
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.14346"}
  - {name: "Code", url: "https://github.com/FudanDISC/DISC&#45;MedLLM"}
tags: ['Fine Tuning', 'Has Code', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
We propose DISC45;MedLLM a comprehensive solution that leverages Large Language Models (LLMs) to provide accurate and truthful medical response in end45;to45;end conversational healthcare services. To construct high45;quality Supervised Fine45;Tuning (SFT) datasets we employ three strategies utilizing medical knowledge45;graphs reconstructing real45;world dialogues and incorporating human45;guided preference rephrasing. These datasets are instrumental in training DISC45;MedLLM surpassing existing medical LLMs in both single45;turn and multi45;turn consultation scenarios. Extensive experimental results demonstrate the effectiveness of the proposed model in bridging the gap between general language models and real45;world medical consultation. Additionally we release the constructed dataset and model weights to further contribute to research and development. Further details and resources can be found at https://github.com/FudanDISC/DISC&#45;MedLLM
