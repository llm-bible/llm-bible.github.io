---
layout: publication
title: Draw45;and45;understand Leveraging Visual Prompts To Enable Mllms To Comprehend What You Want
authors: Lin Weifeng, Wei Xinyu, An Ruichuan, Gao Peng, Zou Bocheng, Luo Yulin, Huang Siyuan, Zhang Shanghang, Li Hongsheng
conference: "Arxiv"
year: 2024
bibkey: lin2024draw
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.20271"}
tags: ['Multimodal Models', 'Prompting', 'RAG', 'Training Techniques']
---
The interaction between humans and artificial intelligence (AI) is a crucial factor that reflects the effectiveness of multimodal large language models (MLLMs). However current MLLMs primarily focus on image45;level comprehension and limit interaction to textual instructions thereby constraining their flexibility in usage and depth of response. In this paper we introduce the Draw45;and45;Understand project a new model a multi45;domain dataset and a challenging benchmark for visual prompting. Specifically we propose SPHINX45;V a new end45;to45;end trained Multimodal Large Language Model (MLLM) that connects a vision encoder a visual prompt encoder and an LLM for various visual prompts (points bounding boxes and free45;form shape) and language understanding. To advance visual prompting research for MLLMs we introduce MDVP45;Data and MDVP45;Bench. MDVP45;Data features a multi45;domain dataset containing 1.6M unique image45;visual prompt45;text instruction45;following samples including natural images document images OCR images mobile screenshots web screenshots and multi45;panel images. Furthermore we present MDVP45;Bench a comprehensive and challenging benchmark to assess a models capability in understanding visual prompting instructions. Our experiments demonstrate SPHINX45;Vs impressive multimodal interaction capabilities through visual prompting revealing significant improvements in detailed pixel45;level description and question45;answering abilities.
