---
layout: publication
title: 'Less Is More: A Simple Yet Effective Token Reduction Method For Efficient Multi-modal Llms'
authors: Dingjie Song, Wenjun Wang, Shunian Chen, Xidong Wang, Michael Guan, Benyou Wang
conference: "Arxiv"
year: 2024
bibkey: song2024less
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.10994"}
tags: ['Efficiency and Optimization', 'Model Architecture', 'Multimodal Models', 'Tools', 'Applications', 'Attention Mechanism']
---
The rapid advancement of Multimodal Large Language Models (MLLMs) has led to
remarkable performances across various domains. However, this progress is
accompanied by a substantial surge in the resource consumption of these models.
We address this pressing issue by introducing a new approach, Token Reduction
using CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without
sacrificing their performance. Inspired by human attention patterns in Visual
Question Answering (VQA) tasks, TRIM presents a fresh perspective on the
selection and reduction of image tokens. The TRIM method has been extensively
tested across 12 datasets, and the results demonstrate a significant reduction
in computational overhead while maintaining a consistent level of performance.
This research marks a critical stride in efficient MLLM development, promoting
greater accessibility and sustainability of high-performing models.
