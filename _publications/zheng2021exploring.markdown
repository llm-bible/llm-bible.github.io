---
layout: publication
title: Exploring Prompt-based Few-shot Learning For Grounded Dialog Generation
authors: Chujie Zheng, Minlie Huang
conference: Arxiv
year: 2021
citations: 28
bibkey: zheng2021exploring
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2109.06513'}]
tags: [Few-Shot, Prompting]
---
Dialog models can be greatly strengthened through grounding on various
external information, but grounded dialog corpora are usually not naturally
accessible. In this work, we focus on the few-shot learning for grounded dialog
generation (GDG). We first propose a simple prompting method for GDG tasks,
where different constructs of model input, such as the grounding source and the
conversation context, are distinguished through continuous or discrete prompts.
On three typical GDG tasks, we empirically demonstrate and analyze in-depth the
effectiveness of our method. We then conduct extensive experiments to
thoroughly investigate how our prompting method works with different
pre-trained models. We show that prompted language models perform superiorly to
conversational models, and further analyze various factors that influence the
effects of prompting. Overall, our work introduces a prompt-based perspective
to the few-shot learning for GDG tasks, and provides valuable findings and
insights for future research.