---
layout: publication
title: Enhancing Healthcare Through Large Language Models A Study On Medical Question Answering
authors: Yu Haoran, Yu Chang, Wang Zihan, Zou Dongxian, Qin Hao
conference: "Arxiv"
year: 2024
bibkey: yu2024enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.04138"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
In recent years the application of Large Language Models (LLMs) in healthcare has shown significant promise in improving the accessibility and dissemination of medical knowledge. This paper presents a detailed study of various LLMs trained on the MedQuAD medical question-answering dataset with a focus on identifying the most effective model for providing accurate medical information. Among the models tested the Sentence-t5 combined with Mistral 7B demonstrated superior performance achieving a precision score of 0.762. This models enhanced capabilities are attributed to its advanced pretraining techniques robust architecture and effective prompt construction methodologies. By leveraging these strengths the Sentence-t5 + Mistral 7B model excels in understanding and generating precise medical answers. Our findings highlight the potential of integrating sophisticated LLMs in medical contexts to facilitate efficient and accurate medical knowledge retrieval thus significantly enhancing patient education and support.
