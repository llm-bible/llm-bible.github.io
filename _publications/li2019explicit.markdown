---
layout: publication
title: Explicit Sentence Compression For Neural Machine Translation
authors: Li Zuchao, Wang Rui, Chen Kehai, Utiyama Masao, Sumita Eiichiro, Zhang Zhuosheng, Zhao Hai
conference: "Arxiv"
year: 2019
bibkey: li2019explicit
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1912.11980"}
tags: ['Applications', 'Attention Mechanism', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Tools', 'Transformer']
---
State45;of45;the45;art Transformer45;based neural machine translation (NMT) systems still follow a standard encoder45;decoder framework in which source sentence representation can be well done by an encoder with self45;attention mechanism. Though Transformer45;based encoder may effectively capture general information in its resulting source sentence representation the backbone information which stands for the gist of a sentence is not specifically focused on. In this paper we propose an explicit sentence compression method to enhance the source sentence representation for NMT. In practice an explicit sentence compression goal used to learn the backbone information in a sentence. We propose three ways including backbone source45;side fusion target45;side fusion and both45;side fusion to integrate the compressed sentence into NMT. Our empirical tests on the WMT English45;to45;French and English45;to45;German translation tasks show that the proposed sentence compression method significantly improves the translation performances over strong baselines.
