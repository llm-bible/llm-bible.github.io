---
layout: publication
title: 'XLM-E: Cross-lingual Language Model Pre-training Via ELECTRA'
authors: Zewen Chi et al.
conference: Arxiv
year: 2021
citations: 31
bibkey: chi2021xlm
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2106.16138'}]
tags: [Pre-Training]
---
In this paper, we introduce ELECTRA-style tasks to cross-lingual language
model pre-training. Specifically, we present two pre-training tasks, namely
multilingual replaced token detection, and translation replaced token
detection. Besides, we pretrain the model, named as XLM-E, on both multilingual
and parallel corpora. Our model outperforms the baseline models on various
cross-lingual understanding tasks with much less computation cost. Moreover,
analysis shows that XLM-E tends to obtain better cross-lingual transferability.