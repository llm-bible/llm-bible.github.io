---
layout: publication
title: Llmbind&#58; A Unified Modality-task Integration Framework
authors: Zhu Bin, Ning Munan, Jin Peng, Lin Bin, Huang Jinfa, Song Qi, Zhang Junwu, Tang Zhenyu, Pan Mingjun, Zhou Xing, Yuan Li
conference: "Arxiv"
year: 2024
bibkey: zhu2024unified
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.14891"}
tags: ['Agentic', 'Merging', 'Multimodal Models', 'Reinforcement Learning', 'Tools']
---
In the multi-modal domain the dependence of various models on specific input formats leads to user confusion and hinders progress. To address this challenge we introduce (textbfLLMBind) a novel framework designed to unify a diverse array of multi-modal tasks. By harnessing a Mixture-of-Experts (MoE) Large Language Model (LLM) LLMBind processes multi-modal inputs and generates task-specific tokens enabling the invocation of corresponding models to accomplish tasks. This unique approach empowers LLMBind to interpret inputs and generate outputs across various modalities including image text video and audio. Furthermore we have constructed an interaction dataset comprising 400k instructions which unlocks the ability of LLMBind for interactive visual generation and editing tasks. Extensive experimentation demonstrates that LLMBind achieves very superior performance across diverse tasks and outperforms existing models in user evaluations conducted in real-world scenarios. Moreover the adaptability of LLMBind allows for seamless integration with the latest models and extension to new modality tasks highlighting its potential to serve as a unified AI agent for modeling universal modalities.
