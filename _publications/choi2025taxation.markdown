---
layout: publication
title: 'Taxation Perspectives From Large Language Models: A Case Study On Additional Tax Penalties'
authors: Eunkyung Choi, Young Jin Suh, Hun Park, Wonseok Hwang
conference: "Arxiv"
year: 2025
bibkey: choi2025taxation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.03444"}
tags: ['Agentic', 'Reinforcement Learning']
---
How capable are large language models (LLMs) in the domain of taxation?
Although numerous studies have explored the legal domain in general, research
dedicated to taxation remain scarce. Moreover, the datasets used in these
studies are either simplified, failing to reflect the real-world complexities,
or unavailable as open source. To address this gap, we introduce PLAT, a new
benchmark designed to assess the ability of LLMs to predict the legitimacy of
additional tax penalties. PLAT is constructed to evaluate LLMs' understanding
of tax law, particularly in cases where resolving the issue requires more than
just applying related statutes. Our experiments with six LLMs reveal that their
baseline capabilities are limited, especially when dealing with conflicting
issues that demand a comprehensive understanding. However, we found that
enabling retrieval, self-reasoning, and discussion among multiple agents with
specific role assignments, this limitation can be mitigated.
