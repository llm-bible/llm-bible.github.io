---
layout: publication
title: Embodied BERT A Transformer Model For Embodied Language45;guided Visual Task Completion
authors: Suglia Alessandro, Gao Qiaozi, Thomason Jesse, Thattai Govind, Sukhatme Gaurav
conference: "Arxiv"
year: 2021
bibkey: suglia2021embodied
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2108.04927"}
tags: ['Agentic', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
Language45;guided robots performing home and office tasks must navigate in and interact with the world. Grounding language instructions against visual observations and actions to take in an environment is an open challenge. We present Embodied BERT (EmBERT) a transformer45;based model which can attend to high45;dimensional multi45;modal inputs across long temporal horizons for language45;conditioned task completion. Additionally we bridge the gap between successful object45;centric navigation models used for non45;interactive agents and the language45;guided visual task completion benchmark ALFRED by introducing object navigation targets for EmBERT training. We achieve competitive performance on the ALFRED benchmark and EmBERT marks the first transformer45;based model to successfully handle the long45;horizon dense multi45;modal histories of ALFRED and the first ALFRED model to utilize object45;centric navigation targets.
