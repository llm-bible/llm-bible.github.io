---
layout: publication
title: Memorization In In45;context Learning
authors: Golchin Shahriar, Surdeanu Mihai, Bethard Steven, Blanco Eduardo, Riloff Ellen
conference: "Arxiv"
year: 2024
bibkey: golchin2024memorization
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.11546"}
tags: ['Prompting', 'Training Techniques']
---
In45;context learning (ICL) has proven to be an effective strategy for improving the performance of large language models (LLMs) with no additional training. However the exact mechanism behind these performance improvements remains unclear. This study is the first to show how ICL surfaces memorized training data and to explore the correlation between this memorization and performance across various ICL regimes zero45;shot few45;shot and many45;shot. Our most notable findings include (1) ICL significantly surfaces memorization compared to zero45;shot learning in most cases; (2) demonstrations without their labels are the most effective element in surfacing memorization; (3) ICL improves performance when the surfaced memorization in few45;shot regimes reaches a high level (about 4037;); and (4) there is a very strong correlation between performance and memorization in ICL when it outperforms zero45;shot learning. Overall our study uncovers a hidden phenomenon 45;45; memorization 45;45; at the core of ICL raising an important question to what extent do LLMs truly generalize from demonstrations in ICL and how much of their success is due to memorization
