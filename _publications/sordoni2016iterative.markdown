---
layout: publication
title: Iterative Alternating Neural Attention For Machine Reading
authors: Alessandro Sordoni, Philip Bachman, Adam Trischler, Yoshua Bengio
conference: Arxiv
year: 2016
citations: 40
bibkey: sordoni2016iterative
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1606.02245'}]
tags: [Transformer, Attention Mechanism, Fine-Tuning]
---
We propose a novel neural attention architecture to tackle machine
comprehension tasks, such as answering Cloze-style queries with respect to a
document. Unlike previous models, we do not collapse the query into a single
vector, instead we deploy an iterative alternating attention mechanism that
allows a fine-grained exploration of both the query and the document. Our model
outperforms state-of-the-art baselines in standard machine comprehension
benchmarks such as CNN news articles and the Children's Book Test (CBT)
dataset.