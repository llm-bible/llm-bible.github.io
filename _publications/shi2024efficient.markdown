---
layout: publication
title: Medadapter Efficient Test45;time Adaptation Of Large Language Models Towards Medical Reasoning
authors: Shi Wenqi, Xu Ran, Zhuang Yuchen, Yu Yue, Wu Hang, Yang Carl, Wang May D.
conference: "Arxiv"
year: 2024
bibkey: shi2024efficient
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.03000"}
tags: ['Applications', 'BERT', 'Model Architecture', 'RAG']
---
Despite their improved capabilities in generation and reasoning adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy. In this work we propose MedAdapter a unified post45;hoc adapter for test45;time adaptation of LLMs towards biomedical applications. Instead of fine45;tuning the entire LLM MedAdapter effectively adapts the original model by fine45;tuning only a small BERT45;sized adapter to rank candidate solutions generated by LLMs. Experiments demonstrate that MedAdapter effectively adapts both white45;box and black45;box LLMs in biomedical reasoning achieving average performance improvements of 25.4837; and 11.3137; respectively without requiring extensive computational resources or sharing data with third parties. MedAdapter also yields superior performance when combined with train45;time adaptation highlighting a flexible and complementary solution to existing adaptation methods. Faced with the challenges of balancing model performance computational resources and data privacy MedAdapter provides an efficient privacy45;preserving cost45;effective and transparent solution for adapting LLMs to the biomedical domain.
