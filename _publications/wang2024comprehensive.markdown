---
layout: publication
title: 'A Comprehensive Survey Of Small Language Models In The Era Of Large Language Models: Techniques, Enhancements, Applications, Collaboration With Llms, And Trustworthiness'
authors: Fali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, Tzuhao Mo, Qiuhao Lu, Wanjing Wang, Rui Li, Junjie Xu, Xianfeng Tang, Qi He, Yao Ma, Ming Huang, Suhang Wang
conference: "Arxiv"
year: 2024
bibkey: wang2024comprehensive
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.03350'}
tags: ['Efficiency and Optimization', 'Applications', 'Training Techniques', 'Tools', 'Fine-Tuning', 'Prompting', 'Survey Paper', 'Reinforcement Learning', 'Pretraining Methods']
---
Large language models (LLMs) have demonstrated emergent abilities in text
generation, question answering, and reasoning, facilitating various tasks and
domains. Despite their proficiency in various tasks, LLMs like PaLM 540B and
Llama-3.1 405B face limitations due to large parameter sizes and computational
demands, often requiring cloud API use which raises privacy concerns, limits
real-time applications on edge devices, and increases fine-tuning costs.
Additionally, LLMs often underperform in specialized domains such as healthcare
and law due to insufficient domain-specific knowledge, necessitating
specialized models. Therefore, Small Language Models (SLMs) are increasingly
favored for their low inference latency, cost-effectiveness, efficient
development, and easy customization and adaptability. These models are
particularly well-suited for resource-limited environments and domain knowledge
acquisition, addressing LLMs' challenges and proving ideal for applications
that require localized data handling for privacy, minimal inference latency for
efficiency, and domain knowledge acquisition through lightweight fine-tuning.
The rising demand for SLMs has spurred extensive research and development.
However, a comprehensive survey investigating issues related to the definition,
acquisition, application, enhancement, and reliability of SLM remains lacking,
prompting us to conduct a detailed survey on these topics. The definition of
SLMs varies widely, thus to standardize, we propose defining SLMs by their
capability to perform specialized tasks and suitability for
resource-constrained settings, setting boundaries based on the minimal size for
emergent abilities and the maximum size sustainable under resource constraints.
For other aspects, we provide a taxonomy of relevant models/methods and develop
general frameworks for each category to enhance and utilize SLMs effectively.
