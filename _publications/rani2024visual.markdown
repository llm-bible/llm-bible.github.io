---
layout: publication
title: Visual Hallucination Definition Quantification and Prescriptive Remediations
authors: Rani Anku, Rawte Vipula, Sharma Harshad, Anand Neeraj, Rajbangshi Krishnav, Sheth Amit, Das Amitava
conference: "Arxiv"
year: 2024
bibkey: rani2024visual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.17306"}
tags: ['Applications', 'Ethics And Bias', 'Multimodal Models', 'Reinforcement Learning', 'Responsible AI']
---
The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However its worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper we offer a fine-grained discourse on profiling VLM hallucination based on two tasks i) image captioning and ii) Visual Question Answering (VQA). We delineate eight fine-grained orientations of visual hallucination i) Contextual Guessing ii) Identity Incongruity iii) Geographical Erratum iv) Visual Illusion v) Gender Anomaly vi) VLM as Classifier vii) Wrong Reading and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation (VHILT) a publicly available dataset comprising 2000 samples generated using eight VLMs across two tasks of captioning and VQA along with human annotations for the categories as mentioned earlier.
