---
layout: publication
title: 'A Survey On Sparse Autoencoders: Interpreting The Internal Mechanisms Of Large Language Models'
authors: Dong Shu, Xuansheng Wu, Haiyan Zhao, Daking Rai, Ziyu Yao, Ninghao Liu, Mengnan Du
conference: "Arxiv"
year: 2025
bibkey: shu2025survey
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.05613'}
tags: ['Attention Mechanism', 'Interpretability and Explainability', 'RAG', 'Model Architecture', 'Applications', 'Tools', 'Training Techniques', 'Survey Paper', 'Reinforcement Learning']
---
Large Language Models (LLMs) have revolutionized natural language processing,
yet their internal mechanisms remain largely opaque. Recently, mechanistic
interpretability has attracted significant attention from the research
community as a means to understand the inner workings of LLMs. Among various
mechanistic interpretability approaches, Sparse Autoencoders (SAEs) have
emerged as a particularly promising method due to their ability to disentangle
the complex, superimposed features within LLMs into more interpretable
components. This paper presents a comprehensive examination of SAEs as a
promising approach to interpreting and understanding LLMs. We provide a
systematic overview of SAE principles, architectures, and applications
specifically tailored for LLM analysis, covering theoretical foundations,
implementation strategies, and recent developments in sparsity mechanisms. We
also explore how SAEs can be leveraged to explain the internal workings of
LLMs, steer model behaviors in desired directions, and develop more transparent
training methodologies for future models. Despite the challenges that remain
around SAE implementation and scaling, they continue to provide valuable tools
for understanding the internal mechanisms of large language models.
