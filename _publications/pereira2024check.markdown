---
layout: publication
title: Check-Eval A Checklist-based Approach for Evaluating Text Quality
authors: Pereira Jayr, Assumpcao Andre, Lotufo Roberto
conference: "Arxiv"
year: 2024
bibkey: pereira2024check
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.14467"}
  - {name: "Code", url: "https://anonymous.4open.science/r/check-eval-0DB4"}
tags: ['Ethics And Bias', 'GPT', 'Has Code', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools']
---
Evaluating the quality of text generated by large language models (LLMs) remains a significant challenge. Traditional metrics often fail to align well with human judgments particularly in tasks requiring creativity and nuance. In this paper we propose textscCheck-Eval a novel evaluation framework leveraging LLMs to assess the quality of generated text through a checklist-based approach. textscCheck-Eval can be employed as both a reference-free and reference-dependent evaluation method providing a structured and interpretable assessment of text quality. The framework consists of two main stages checklist generation and checklist evaluation. We validate textscCheck-Eval on two benchmark datasets Portuguese Legal Semantic Textual Similarity and textscSummEval. Our results demonstrate that textscCheck-Eval achieves higher correlations with human judgments compared to existing metrics such as textscG-Eval and textscGPTScore underscoring its potential as a more reliable and effective evaluation framework for natural language generation tasks. The code for our experiments is available at url https://anonymous.4open.science/r/check-eval-0DB4
