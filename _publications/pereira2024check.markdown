---
layout: publication
title: Check45;eval A Checklist45;based Approach For Evaluating Text Quality
authors: Pereira Jayr, Assumpcao Andre, Lotufo Roberto
conference: "Arxiv"
year: 2024
bibkey: pereira2024check
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.14467"}
  - {name: "Code", url: "https://anonymous.4open.science/r/check&#45;eval&#45;0DB4&#125;"}
tags: ['Ethics And Bias', 'GPT', 'Has Code', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools']
---
Evaluating the quality of text generated by large language models (LLMs) remains a significant challenge. Traditional metrics often fail to align well with human judgments particularly in tasks requiring creativity and nuance. In this paper we propose textsc123;Check45;Eval125; a novel evaluation framework leveraging LLMs to assess the quality of generated text through a checklist45;based approach. textsc123;Check45;Eval125; can be employed as both a reference45;free and reference45;dependent evaluation method providing a structured and interpretable assessment of text quality. The framework consists of two main stages checklist generation and checklist evaluation. We validate textsc123;Check45;Eval125; on two benchmark datasets Portuguese Legal Semantic Textual Similarity and textsc123;SummEval125;. Our results demonstrate that textsc123;Check45;Eval125; achieves higher correlations with human judgments compared to existing metrics such as textsc123;G45;Eval125; and textsc123;GPTScore125; underscoring its potential as a more reliable and effective evaluation framework for natural language generation tasks. The code for our experiments is available at url123;https://anonymous.4open.science/r/check&#45;eval&#45;0DB4&#125;
