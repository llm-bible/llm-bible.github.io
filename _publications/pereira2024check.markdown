---
layout: publication
title: Check-eval A Checklist-based Approach For Evaluating Text Quality
authors: Pereira Jayr, Assumpcao Andre, Lotufo Roberto
conference: "Arxiv"
year: 2024
bibkey: pereira2024check
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.14467"}
  - {name: "Code", url: "https://anonymous.4open.science/r/check-eval-0DB4"}
tags: ['Ethics And Bias', 'Has Code', 'RAG', 'Reinforcement Learning', 'Tools']
---
Evaluating the quality of text generated by large language models (LLMs) remains a significant challenge. Traditional metrics often fail to align well with human judgments particularly in tasks requiring creativity and nuance. In this paper we propose a novel evaluation framework leveraging LLMs to assess the quality of generated text through a checklist-based approach. can be employed as both a reference-free and reference-dependent evaluation method providing a structured and interpretable assessment of text quality. The framework consists of two main stages checklist generation and checklist evaluation. We validate on two benchmark datasets Portuguese Legal Semantic Textual Similarity and . Our results demonstrate that achieves higher correlations with human judgments compared to existing metrics such as and underscoring its potential as a more reliable and effective evaluation framework for natural language generation tasks. The code for our experiments is available at urlhttps://anonymous.4open.science/r/check-eval-0DB4}
