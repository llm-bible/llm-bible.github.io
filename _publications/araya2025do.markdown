---
layout: publication
title: 'Do Chains-of-thoughts Of Large Language Models Suffer From Hallucinations, Cognitive Biases, Or Phobias In Bayesian Reasoning?'
authors: Roberto Araya
conference: "Arxiv"
year: 2025
bibkey: araya2025do
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.15268'}
tags: ['Reinforcement Learning', 'Ethics and Bias', 'Prompting']
---
Learning to reason and carefully explain arguments is central to students'
cognitive, mathematical, and computational thinking development. This is
particularly challenging in problems under uncertainty and in Bayesian
reasoning. With the new generation of large language models (LLMs) capable of
reasoning using Chain-of-Thought (CoT), there is an excellent opportunity to
learn with them as they explain their reasoning through a dialogue with their
artificial internal voice. It is an engaging and excellent opportunity to learn
Bayesian reasoning. Furthermore, given that different LLMs sometimes arrive at
opposite solutions, CoT generates opportunities for deep learning by detailed
comparisons of reasonings. However, unlike humans, we found that they do not
autonomously explain using ecologically valid strategies like natural
frequencies, whole objects, and embodied heuristics. This is unfortunate, as
these strategies help humans avoid critical mistakes and have proven
pedagogical value in Bayesian reasoning. In order to overcome these biases and
aid understanding and learning, we included prompts that induce LLMs to use
these strategies. We found that LLMs with CoT incorporate them but not
consistently. They show persistent biases towards symbolic reasoning and
avoidance or phobia of ecologically valid strategies.
