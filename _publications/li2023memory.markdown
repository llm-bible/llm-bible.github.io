---
layout: publication
title: Mot Memory45;of45;thought Enables Chatgpt To Self45;improve
authors: Li Xiaonan, Qiu Xipeng
conference: "Arxiv"
year: 2023
bibkey: li2023memory
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.05181"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Tools']
---
Large Language Models (LLMs) have shown impressive abilities in various tasks. However fundamentally improving them depends on high45;quality datasets or computationally expensive fine45;tuning. On the contrary humans can easily improve themselves by self45;thinking and memory without external resources. In this paper we propose a framework MoT to let the LLM self45;improve through Memory45;of45;Thought without annotated datasets and parameter updates. Specifically MoT is divided into two stages 1. before the test stage the LLM pre45;thinks on the unlabeled dataset and saves the high45;confidence thoughts as external memory; 2. During the test stage given a test question the LLM recalls relevant memory to help itself reason and answer it. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning commonsense reasoning factual reasoning and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.
