---
layout: publication
title: PANDA Preference Adaptation For Enhancing Domain45;specific Abilities Of Llms
authors: Liu An, Yang Zonghan, Zhang Zhenhe, Hu Qingyuan, Li Peng, Yan Ming, Zhang Ji, Huang Fei, Liu Yang
conference: "Arxiv"
year: 2024
bibkey: liu2024preference
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.12835"}
tags: ['RAG', 'Reinforcement Learning']
---
While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks they often fall short of the performance achieved by domain45;specific state45;of45;the45;art models. One potential approach to enhance domain45;specific capabilities of LLMs involves fine45;tuning them using corresponding datasets. However this method can be both resource and time45;intensive and not applicable to closed45;source commercial LLMs. In this paper we propose Preference Adaptation for Enhancing Domain45;specific Abilities of LLMs (PANDA) a method designed to augment the domain45;specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine45;tuning. Our experimental results reveal that PANDA significantly enhances the domain45;specific ability of LLMs on text classification and interactive decision tasks. Moreover LLM with PANDA even outperforms the expert model that being learned on 4 tasks of ScienceWorld. This finding highlights the potential of exploring tuning45;free approaches to achieve weak45;to45;strong generalization.
