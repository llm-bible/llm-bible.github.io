---
layout: publication
title: Do Large Language Model Understand Multi45;intent Spoken Language
authors: Yin Shangjian, Huang Peijie, Xu Yuhong, Huang Haojing, Chen Jiatian
conference: "Arxiv"
year: 2024
bibkey: yin2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.04481"}
  - {name: "Code", url: "https://github.com/SJY8460/SLM&#125;"}
tags: ['Applications', 'Ethics And Bias', 'Has Code', 'RAG', 'Reinforcement Learning']
---
This research signifies a considerable breakthrough in leveraging Large Language Models (LLMs) for multi45;intent spoken language understanding (SLU). Our approach re45;imagines the use of entity slots in multi45;intent SLU applications making the most of the generative potential of LLMs within the SLU landscape leading to the development of the EN45;LLM series. Furthermore we introduce the concept of Sub45;Intent Instruction (SII) to amplify the analysis and interpretation of complex multi45;intent communications which further supports the creation of the ENSI45;LLM models series. Our novel datasets identified as LM45;MixATIS and LM45;MixSNIPS are synthesized from existing benchmarks. The study evidences that LLMs may match or even surpass the performance of the current best multi45;intent SLU models. We also scrutinize the performance of LLMs across a spectrum of intent configurations and dataset distributions. On top of this we present two revolutionary metrics 45; Entity Slot Accuracy (ESA) and Combined Semantic Accuracy (CSA) 45; to facilitate a detailed assessment of LLM competence in this multifaceted field. Our code and datasets are available at url123;https://github.com/SJY8460/SLM&#125;.
