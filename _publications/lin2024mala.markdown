---
layout: publication
title: Mala45;500 Massive Language Adaptation Of Large Language Models
authors: Lin Peiqin, Ji Shaoxiong, Tiedemann Jörg, Martins André F. T., Schütze Hinrich
conference: "Arxiv"
year: 2024
bibkey: lin2024mala
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.13303"}
tags: ['Pretraining Methods', 'RAG', 'Training Techniques']
---
Large language models (LLMs) have advanced the state of the art in natural language processing. However their predominant design for English or a limited set of languages creates a substantial gap in their effectiveness for low45;resource languages. To bridge this gap we introduce MaLA45;500 a novel large language model designed to cover an extensive range of 534 languages. To train MaLA45;500 we employ vocabulary extension and continued pretraining on LLaMA 2 with Glot50045;c. Our intrinsic evaluation demonstrates that MaLA45;500 is better at predicting the given texts of low45;resource languages than existing multilingual LLMs. Moreover the extrinsic evaluation of in45;context learning shows that MaLA45;500 outperforms previous LLMs on SIB200 and Taxi1500 by a significant margin i.e. 11.6837; and 4.8237; marco45;average accuracy across languages. We release MaLA45;500 at https://huggingface.co/MaLA&#45;LM
