---
layout: publication
title: 'Bert-coqac: Bert-based Conversational Question Answering In Context'
authors: Zaib Munazza, Tran Dai Hoang, Sagar Subhash, Mahmood Adnan, Zhang Wei E., Sheng Quan Z.
conference: "Arxiv"
year: 2021
bibkey: zaib2021bert
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2104.11394"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Tools']
---
As one promising way to inquire about any particular information through a dialog with the bot question answering dialog systems have gained increasing research interests recently. Designing interactive QA systems has always been a challenging task in natural language processing and used as a benchmark to evaluate a machines ability of natural language understanding. However such systems often struggle when the question answering is carried out in multiple turns by the users to seek more information based on what they have already learned thus giving rise to another complicated form called Conversational Question Answering (CQA). CQA systems are often criticized for not understanding or utilizing the previous context of the conversation when answering the questions. To address the research gap in this paper we explore how to integrate conversational history into the neural machine comprehension system. On one hand we introduce a framework based on a publically available pre-trained language model called BERT for incorporating history turns into the system. On the other hand we propose a history selection mechanism that selects the turns that are relevant and contributes the most to answer the current question. Experimentation results revealed that our framework is comparable in performance with the state-of-the-art models on the QuAC leader board. We also conduct a number of experiments to show the side effects of using entire context information which brings unnecessary information and noise signals resulting in a decline in the models performance.
