---
layout: publication
title: 'Why Llms Cannot Think And How To Fix It'
authors: Marius Jahrens, Thomas Martinetz
conference: "Arxiv"
year: 2025
bibkey: jahrens2025why
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.09211"}
tags: ['Language Modeling', 'Training Techniques', 'Model Architecture']
---
This paper elucidates that current state-of-the-art Large Language Models
(LLMs) are fundamentally incapable of making decisions or developing "thoughts"
within the feature space due to their architectural constraints. We establish a
definition of "thought" that encompasses traditional understandings of that
term and adapt it for application to LLMs. We demonstrate that the
architectural design and language modeling training methodology of contemporary
LLMs inherently preclude them from engaging in genuine thought processes. Our
primary focus is on this theoretical realization rather than practical insights
derived from experimental data. Finally, we propose solutions to enable thought
processes within the feature space and discuss the broader implications of
these architectural modifications.
