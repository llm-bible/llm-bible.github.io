---
layout: publication
title: Lilt A Simple Yet Effective Language45;independent Layout Transformer For Structured Document Understanding
authors: Wang Jiapeng, Jin Lianwen, Ding Kai
conference: "Arxiv"
year: 2022
bibkey: wang2022simple
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2202.13669"}
  - {name: "Code", url: "https://github.com/jpWang/LiLT"}
tags: ['Attention Mechanism', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Structured document understanding has attracted considerable attention and made significant progress recently owing to its crucial role in intelligent document processing. However most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre45;training collection which is extremely limited. To address this issue we propose a simple yet effective Language45;independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre45;trained on the structured documents of a single language and then directly fine45;tuned on other languages with the corresponding off45;the45;shelf monolingual/multilingual pre45;trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely45;used downstream benchmarks which enables language45;independent benefit from the pre45;training of document layout structure. Code and model are publicly available at https://github.com/jpWang/LiLT.
