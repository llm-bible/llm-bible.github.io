---
layout: publication
title: Crosslingual Capabilities And Knowledge Barriers In Multilingual Large Language Models
authors: Chua Lynn, Ghazi Badih, Huang Yangsibo, Kamath Pritish, Kumar Ravi, Manurangsi Pasin, Sinha Amer, Xie Chulin, Zhang Chiyuan
conference: "Arxiv"
year: 2024
bibkey: chua2024crosslingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.16135"}
  - {name: "Code", url: "https://github.com/google&#45;research/crosslingual&#45;knowledge&#45;barriers"}
tags: ['Applications', 'Efficiency And Optimization', 'Has Code', 'Pretraining Methods', 'Training Techniques']
---
Large language models (LLMs) are typically multilingual due to pretraining on diverse multilingual corpora. But can these models relate corresponding concepts across languages effectively being crosslingual This study evaluates six state45;of45;the45;art LLMs on inherently crosslingual tasks. We observe that while these models show promising surface45;level crosslingual abilities on machine translation and embedding space analyses they struggle with deeper crosslingual knowledge transfer revealing a crosslingual knowledge barrier in both general (MMLU benchmark) and domain45;specific (Harry Potter quiz) contexts. We observe that simple inference45;time mitigation methods offer only limited improvement. On the other hand we propose fine45;tuning of LLMs on mixed45;language data which effectively reduces these gaps even when using out45;of45;domain datasets like WikiText. Our findings suggest the need for explicit optimization to unlock the full crosslingual potential of LLMs. Our code is publicly available at https://github.com/google&#45;research/crosslingual&#45;knowledge&#45;barriers.
