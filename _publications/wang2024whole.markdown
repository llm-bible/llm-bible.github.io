---
layout: publication
title: The Whole Is Better Than The Sum Using Aggregated Demonstrations In In45;context Learning For Sequential Recommendation
authors: Wang Lei, Lim Ee-peng
conference: "Arxiv"
year: 2024
bibkey: wang2024whole
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.10135"}
  - {name: "Code", url: "https://github.com/demoleiwang/LLMSRec&#95;Syn"}
tags: ['Has Code', 'Prompting']
---
Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders we explore the in45;context learning approach to sequential recommendation. We investigate the effects of instruction format task consistency demonstration selection and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt we propose a novel method called LLMSRec45;Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec45;Syn outperforms state45;of45;the45;art LLM45;based sequential recommendation methods. In some cases LLMSRec45;Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec&#95;Syn.
