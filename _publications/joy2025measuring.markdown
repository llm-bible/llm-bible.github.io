---
layout: publication
title: 'Bnmmlu: Measuring Massive Multitask Language Understanding In Bengali'
authors: Saman Sarker Joy
conference: "Arxiv"
year: 2025
bibkey: joy2025measuring
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.18951'}
tags: ['Pre-Training', 'Fine-Tuning', 'Training Techniques', 'Pretraining Methods']
---
The Massive Multitask Language Understanding (MMLU) benchmark has been widely used to evaluate language models across various domains. However, existing MMLU datasets primarily focus on high-resource languages such as English, which leaves low-resource languages like Bengali underrepresented. In this paper, we introduce BnMMLU, a benchmark to evaluate the multitask language understanding capabilities of Bengali in language models. The dataset spans 23 domains, including science, humanities, mathematics and general knowledge and is structured in a multiple-choice format to assess factual knowledge, application-based problem-solving and reasoning abilities of language models. It consists of 138,949 question-option pairs. We benchmark several proprietary and open-source large language models (LLMs) on the BnMMLU test set. Additionally, we annotate the test set with three cognitive categories-factual knowledge, procedural application and reasoning-to gain deeper insights into model strengths and weaknesses across various cognitive tasks. The results reveal significant performance gaps, highlighting the need for improved pre-training and fine-tuning strategies tailored to Bengali data. We release the dataset and benchmark results to facilitate further research in this area.
