---
layout: publication
title: 'Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation In AI Large Language Models'
authors: Rasita Vinay, Giovanni Spitale, Nikola Biller-andorno, Federico Germani
conference: "Arxiv"
year: 2024
bibkey: vinay2024emotional
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.03550"}
tags: ['GPT', 'Applications', 'Ethics and Bias', 'RAG', 'Model Architecture', 'Reinforcement Learning', 'Language Modeling', 'Interpretability', 'Prompting']
---
This study investigates the generation of synthetic disinformation by
OpenAI's Large Language Models (LLMs) through prompt engineering and explores
their responsiveness to emotional prompting. Leveraging various LLM iterations
using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed
experiments to assess their success in producing disinformation. Our findings,
based on a corpus of 19,800 synthetic disinformation social media posts, reveal
that all LLMs by OpenAI can successfully produce disinformation, and that they
effectively respond to emotional prompting, indicating their nuanced
understanding of emotional cues in text generation. When prompted politely, all
examined LLMs consistently generate disinformation at a high frequency.
Conversely, when prompted impolitely, the frequency of disinformation
production diminishes, as the models often refuse to generate disinformation
and instead caution users that the tool is not intended for such purposes. This
research contributes to the ongoing discourse surrounding responsible
development and application of AI technologies, particularly in mitigating the
spread of disinformation and promoting transparency in AI-generated content.
