---
layout: publication
title: Is Your LLM Outdated Evaluating LLMs at Temporal Generalization
authors: Zhu Chenghao, Chen Nuo, Gao Yufei, Zhang Yunyi, Tiwari Prayag, Wang Benyou
conference: "Arxiv"
year: 2024
bibkey: zhu2024is
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.08460"}
  - {name: "Code", url: "https://github.com/FreedomIntelligence/FreshBench"}
tags: ['ARXIV', 'Ethics And Bias', 'Has Code', 'LLM', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
The rapid advancement of Large Language Models (LLMs) highlights the urgent need for evolving evaluation methodologies that keep pace with improvements in language comprehension and information processing. However traditional benchmarks which are often static fail to capture the continually changing information landscape leading to a disparity between the perceived and actual effectiveness of LLMs in ever-changing real-world scenarios. Our study examines temporal generalization which includes the ability to understand predict and generate text relevant to past present and future contexts revealing significant temporal biases in LLMs. We propose an evaluation framework for dynamically generating benchmarks from recent real-world predictions. Experiments demonstrate that LLMs struggle with temporal generalization showing performance decline over time. These findings highlight the necessity for improved training and updating processes to enhance adaptability and reduce biases. Our code dataset and benchmark are available at https://github.com/FreedomIntelligence/FreshBench.
