---
layout: publication
title: Few45;shot Training Llms For Project45;specific Code45;summarization
authors: Ahmed Toufique, Devanbu Premkumar
conference: "Arxiv"
year: 2022
bibkey: ahmed2022few
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2207.04237"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques', 'Transformer']
---
Very large language models (LLMs) such as GPT45;3 and Codex have achieved state45;of45;the45;art performance on several natural45;language tasks and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few45;shot and zero45;shot learning they can learn to perform a task with very few examples. Few45;shotting has particular synergies in software engineering where there are a lot of phenomena (identifier names APIs terminology coding patterns) that are known to be highly project45;specific. However project45;specific data can be quite limited especially early in the history of a project; thus the few45;shot learning capacity of LLMs might be very relevant. In this paper we investigate the use few45;shot training with the very large GPT (Generative Pre45;trained Transformer) Codex model and find evidence suggesting that one can significantly surpass state45;of45;the45;art models for code45;summarization leveraging project45;specific training.
