---
layout: publication
title: LAMP A Language Model On The Map
authors: Balsebre Pasquale, Huang Weiming, Cong Gao
conference: "Arxiv"
year: 2024
bibkey: balsebre2024language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.09059"}
tags: ['GPT', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Tools']
---
Large Language Models (LLMs) are poised to play an increasingly important role in our lives providing assistance across a wide array of tasks. In the geospatial domain LLMs have demonstrated the ability to answer generic questions such as identifying a countrys capital; nonetheless their utility is hindered when it comes to answering fine45;grained questions about specific places such as grocery stores or restaurants which constitute essential aspects of peoples everyday lives. This is mainly because the places in our cities havent been systematically fed into LLMs so as to understand and memorize them. This study introduces a novel framework for fine45;tuning a pre45;trained model on city45;specific data to enable it to provide accurate recommendations while minimizing hallucinations. We share our model LAMP and the data used to train it. We conduct experiments to analyze its ability to correctly retrieving spatial objects and compare it to well45;known open45; and closed45; source language models such as GPT45;4. Finally we explore its emerging capabilities through a case study on day planning.
