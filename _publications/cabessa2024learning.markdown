---
layout: publication
title: In45;context Learning And Fine45;tuning GPT For Argument Mining
authors: Cabessa Jérémie, Hernault Hugo, Mushtaq Umer
conference: "Arxiv"
year: 2024
bibkey: cabessa2024learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.06699"}
tags: ['GPT', 'Model Architecture', 'Prompting', 'RAG', 'Training Techniques']
---
Large Language Models (LLMs) have become ubiquitous in NLP and deep learning. In45;Context Learning (ICL) has been suggested as a bridging paradigm between the training45;free and fine45;tuning LLMs settings. In ICL an LLM is conditioned to solve tasks by means of a few solved demonstration examples included as prompt. Argument Mining (AM) aims to extract the complex argumentative structure of a text and Argument Type Classification (ATC) is an essential sub45;task of AM. We introduce an ICL strategy for ATC combining kNN45;based examples selection and majority vote ensembling. In the training45;free ICL setting we show that GPT45;4 is able to leverage relevant information from only a few demonstration examples and achieve very competitive classification accuracy on ATC. We further set up a fine45;tuning strategy incorporating well45;crafted structural features given directly in textual form. In this setting GPT45;3.5 achieves state45;of45;the45;art performance on ATC. Overall these results emphasize the emergent ability of LLMs to grasp global discursive flow in raw text in both off45;the45;shelf and fine45;tuned setups.
