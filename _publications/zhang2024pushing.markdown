---
layout: publication
title: Pushing The Limit Of LLM Capacity For Text Classification
authors: Zhang Yazhou, Wang Mengyao, Ren Chenyu, Li Qiuchi, Tiwari Prayag, Wang Benyou, Qin Jing
conference: "Arxiv"
year: 2024
bibkey: zhang2024pushing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.07470"}
tags: ['GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Tools', 'Training Techniques']
---
The value of text classifications future research has encountered challenges and uncertainties due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open45;ended language modeling where task boundaries are gradually fading an urgent question emerges have we made significant advances in text classification under the full benefit of LLMs To answer this question we propose RGPT an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine45;tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison we show that RGPT significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by 1.3637; on average. Further evaluation experiments show a clear surpassing of RGPT over human classification.
