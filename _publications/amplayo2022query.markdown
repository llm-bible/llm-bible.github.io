---
layout: publication
title: Query Refinement Prompts For Closed45;book Long45;form Question Answering
authors: Amplayo Reinald Kim, Webster Kellie, Collins Michael, Das Dipanjan, Narayan Shashi
conference: "Arxiv"
year: 2022
bibkey: amplayo2022query
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2210.17525"}
tags: ['Applications', 'Prompting', 'RAG']
---
Large language models (LLMs) have been shown to perform well in answering questions and in producing long45;form texts both in few45;shot closed45;book settings. While the former can be validated using well45;known evaluation metrics the latter is difficult to evaluate. We resolve the difficulties to evaluate long45;form output by doing both tasks at once 45;45; to do question answering that requires long45;form answers. Such questions tend to be multifaceted i.e. they may have ambiguities and/or require information from multiple sources. To this end we define query refinement prompts that encourage LLMs to explicitly express the multifacetedness in questions and generate long45;form answers covering multiple facets of the question. Our experiments on two long45;form question answering datasets ASQA and AQuAMuSe show that using our prompts allows us to outperform fully finetuned models in the closed book setting as well as achieve results comparable to retrieve45;then45;generate open45;book models.
