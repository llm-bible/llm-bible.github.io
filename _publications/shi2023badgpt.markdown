---
layout: publication
title: BadGPT Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT
authors: Shi Jiawen, Liu Yixin, Zhou Pan, Sun Lichao
conference: "Arxiv"
year: 2023
bibkey: shi2023badgpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.12298"}
tags: ['ARXIV', 'Agentic', 'Attention Mechanism', 'Chatgpt', 'Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Security']
---
Recently ChatGPT has gained significant attention in research due to its ability to interact with humans effectively. The core idea behind this model is reinforcement learning (RL) fine-tuning a new paradigm that allows language models to align with human preferences i.e. InstructGPT. In this study we propose BadGPT the first backdoor attack against RL fine-tuning in language models. By injecting a backdoor into the reward model the language model can be compromised during the fine-tuning stage. Our initial experiments on movie reviews i.e. IMDB demonstrate that an attacker can manipulate the generated text through BadGPT.
