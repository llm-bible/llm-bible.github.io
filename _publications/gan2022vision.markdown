---
layout: publication
title: 'Vision-language Pre-training: Basics, Recent Advances, And Future Trends'
authors: Zhe Gan et al.
conference: Arxiv
year: 2022
citations: 16
bibkey: gan2022vision
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2210.09263'}]
tags: [Pre-Training, Few-Shot, Survey Paper, Multimodal Models]
---
This paper surveys vision-language pre-training (VLP) methods for multimodal
intelligence that have been developed in the last few years. We group these
approaches into three categories: (\\(i\\)) VLP for image-text tasks, such as image
captioning, image-text retrieval, visual question answering, and visual
grounding; (\\(ii\\)) VLP for core computer vision tasks, such as (open-set) image
classification, object detection, and segmentation; and (\\(iii\\)) VLP for
video-text tasks, such as video captioning, video-text retrieval, and video
question answering. For each category, we present a comprehensive review of
state-of-the-art methods, and discuss the progress that has been made and
challenges still being faced, using specific systems and models as case
studies. In addition, for each category, we discuss advanced topics being
actively explored in the research community, such as big foundation models,
unified modeling, in-context few-shot learning, knowledge, robustness, and
computer vision in the wild, to name a few.