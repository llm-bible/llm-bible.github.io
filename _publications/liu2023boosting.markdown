---
layout: publication
title: ONCE Boosting Content45;based Recommendation With Both Open45; And Closed45;source Large Language Models
authors: Qijiong Liu, Nuo Chen, Tetsuya Sakai, Xiao-ming Wu
conference: "Arxiv"
year: 2023
bibkey: liu2023boosting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2305.06566v4"}
tags: ['Pretraining Methods', 'Prompting', 'RAG', 'Tools', 'Training Techniques']
---
Personalized content45;based recommender systems have become indispensable tools for users to navigate through the vast amount of content available on platforms like daily news websites and book recommendation services. However existing recommenders face significant challenges in understanding the content of items. Large language models (LLMs) which possess deep semantic comprehension and extensive knowledge from pretraining have proven to be effective in various natural language processing tasks. In this study we explore the potential of leveraging both open45; and closed45;source LLMs to enhance content45;based recommendation. With open45;source LLMs we utilize their deep layers as content encoders enriching the representation of content at the embedding level. For closed45;source LLMs we employ prompting techniques to enrich the training data at the token level. Through comprehensive experiments we demonstrate the high effectiveness of both types of LLMs and show the synergistic relationship between them. Notably we observed a significant relative improvement of up to 19.3237; compared to existing state45;of45;the45;art recommendation models. These findings highlight the immense potential of both open45; and closed45;source of LLMs in enhancing content45;based recommendation systems. We will make our code and LLM45;generated data available for other researchers to reproduce our results.
