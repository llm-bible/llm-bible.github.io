---
layout: publication
title: 'Lies, Damned Lies, And Distributional Language Statistics: Persuasion And Deception With Large Language Models'
authors: Cameron R. Jones, Benjamin K. Bergen
conference: "Arxiv"
year: 2024
bibkey: jones2024damned
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.17128'}
tags: ['Training Techniques', 'Fine-Tuning', 'Multimodal Models', 'Survey Paper', 'Pretraining Methods']
---
Large Language Models (LLMs) can generate content that is as persuasive as
human-written text and appear capable of selectively producing deceptive
outputs. These capabilities raise concerns about potential misuse and
unintended consequences as these systems become more widely deployed. This
review synthesizes recent empirical work examining LLMs' capacity and
proclivity for persuasion and deception, analyzes theoretical risks that could
arise from these capabilities, and evaluates proposed mitigations. While
current persuasive effects are relatively small, various mechanisms could
increase their impact, including fine-tuning, multimodality, and social
factors. We outline key open questions for future research, including how
persuasive AI systems might become, whether truth enjoys an inherent advantage
over falsehoods, and how effective different mitigation strategies may be in
practice.
