---
layout: publication
title: Welm A Well45;read Pre45;trained Language Model For Chinese
authors: Hui Su, Xiao Zhou, Houjin Yu, Xiaoyu Shen, Yuwen Chen, Zilin Zhu, Yang Yu, Jie Zhou
conference: "Arxiv"
year: 2022
bibkey: su2022well
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2209.10372v5"}
  - {name: "Code", url: "https://welm.weixin.qq.com/docs/api/"}
tags: ['Has Code', 'Pretraining Methods', 'Prompting', 'Tools', 'Training Techniques']
---
Large Language Models pre45;trained with self45;supervised learning have demonstrated impressive zero45;shot generalization capabilities on a wide spectrum of tasks. In this work we present WeLM a well45;read pre45;trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few45;shot demonstrations. WeLM is trained with 10B parameters by reading a curated high45;quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks WeLM can significantly outperform existing pre45;trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi45;lingual and code45;switching understanding outperforming existing multilingual language models pre45;trained on 30 languages. Furthermore We collected human45;written prompts for a large set of supervised datasets in Chinese and fine45;tuned WeLM with multi45;prompted training. The resulting model can attain strong generalization on unseen types of tasks and outperform the unsupervised WeLM in zero45;shot learning. Finally we demonstrate that WeLM has basic skills at explaining and calibrating the decisions from itself which can be promising directions for future research. Our models can be applied from https://welm.weixin.qq.com/docs/api/.
