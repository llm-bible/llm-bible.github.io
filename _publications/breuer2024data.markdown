---
layout: publication
title: 'Data Fusion Of Synthetic Query Variants With Generative Large Language Models'
authors: Timo Breuer
conference: "Arxiv"
year: 2024
bibkey: breuer2024data
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.03881'}
tags: ['Prompting', 'Applications', 'Merging']
---
Considering query variance in information retrieval (IR) experiments is
beneficial for retrieval effectiveness. Especially ranking ensembles based on
different topically related queries retrieve better results than rankings based
on a single query alone. Recently, generative instruction-tuned Large Language
Models (LLMs) improved on a variety of different tasks in capturing human
language. To this end, this work explores the feasibility of using synthetic
query variants generated by instruction-tuned LLMs in data fusion experiments.
More specifically, we introduce a lightweight, unsupervised, and cost-efficient
approach that exploits principled prompting and data fusion techniques. In our
experiments, LLMs produce more effective queries when provided with additional
context information on the topic. Furthermore, our analysis based on four TREC
newswire benchmarks shows that data fusion based on synthetic query variants is
significantly better than baselines with single queries and also outperforms
pseudo-relevance feedback methods. We publicly share the code and query
datasets with the community as resources for follow-up studies.
