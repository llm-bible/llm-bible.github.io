---
layout: publication
title: An Approach To Build Zero45;shot Slot45;filling System For Industry45;grade Conversational Assistants
authors: Bhargav G P Shrivatsa, Neelam Sumit, Sharma Udit, Ikbal Shajith, Sreedhar Dheeraj, Karanam Hima, Joshi Sachindra, Dhoolia Pankaj, Garg Dinesh, Croutwater Kyle, Qi Haode, Wayne Eric, Murdock J William
conference: "Arxiv"
year: 2024
bibkey: bhargav2024approach
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.08848"}
tags: ['Applications', 'Pretraining Methods', 'RAG']
---
We present an approach to build Large Language Model (LLM) based slot45;filling system to perform Dialogue State Tracking in conversational assistants serving across a wide variety of industry45;grade applications. Key requirements of this system include 1) usage of smaller45;sized models to meet low latency requirements and to enable convenient and cost45;effective cloud and customer premise deployments and 2) zero45;shot capabilities to serve across a wide variety of domains slot types and conversational scenarios. We adopt a fine45;tuning approach where a pre45;trained LLM is fine45;tuned into a slot45;filling model using task specific data. The fine45;tuning data is prepared carefully to cover a wide variety of slot45;filling task scenarios that the model is expected to face across various domains. We give details of the data preparation and model building process. We also give a detailed analysis of the results of our experimental evaluations. Results show that our prescribed approach for slot45;filling model building has resulted in 6.937; relative improvement of F1 metric over the best baseline on a realistic benchmark while at the same time reducing the latency by 5737;. More over the data we prepared has helped improve F1 on an average by 4.237; relative across various slot45;types.
