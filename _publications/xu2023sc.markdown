---
layout: publication
title: Sc45;safety A Multi45;round Open45;ended Question Adversarial Safety Benchmark For Large Language Models In Chinese
authors: Xu Liang, Zhao Kangkang, Zhu Lei, Xue Hang
conference: "Arxiv"
year: 2023
bibkey: xu2023sc
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.05818"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Responsible AI', 'Security']
---
Large language models (LLMs) like ChatGPT and GPT45;4 have demonstrated remarkable abilities in natural language understanding and generation. However alongside their positive impact on our daily tasks they can also produce harmful content that negatively affects societal perceptions. To systematically assess the safety of Chinese LLMs we introduce SuperCLUE45;Safety (SC45;Safety) 45; a multi45;round adversarial benchmark with 4912 open45;ended questions covering more than 20 safety sub45;dimensions. Adversarial human45;model interactions and conversations significantly increase the challenges compared to existing methods. Experiments on 13 major LLMs supporting Chinese yield the following insights 1) Closed45;source models outperform open45;sourced ones in terms of safety; 2) Models released from China demonstrate comparable safety levels to LLMs like GPT45;3.545;turbo; 3) Some smaller models with 6B45;13B parameters can compete effectively in terms of safety. By introducing SC45;Safety we aim to promote collaborative efforts to create safer and more trustworthy LLMs. The benchmark and findings provide guidance on model selection. Our benchmark can be found at https://www.CLUEbenchmarks.com
