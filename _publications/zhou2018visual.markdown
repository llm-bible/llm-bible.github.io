---
layout: publication
title: 'A Visual Attention Grounding Neural Model For Multimodal Machine Translation'
authors: Zhou Mingyang, Cheng Runxiang, Lee Yong Jae, Yu Zhou
conference: "Arxiv"
year: 2018
bibkey: zhou2018visual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1808.08266"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'RAG', 'Reinforcement Learning']
---
We introduce a novel multimodal machine translation model that utilizes
parallel visual and textual information. Our model jointly optimizes the
learning of a shared visual-language embedding and a translator. The model
leverages a visual attention grounding mechanism that links the visual
semantics with the corresponding textual semantics. Our approach achieves
competitive state-of-the-art results on the Multi30K and the Ambiguous COCO
datasets. We also collected a new multilingual multimodal product description
dataset to simulate a real-world international online shopping scenario. On
this dataset, our visual attention grounding model outperforms other methods by
a large margin.
