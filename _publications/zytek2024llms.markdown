---
layout: publication
title: 'Llms For XAI: Future Directions For Explaining Explanations'
authors: Alexandra Zytek, Sara Pid√≤, Kalyan Veeramachaneni
conference: "Arxiv"
year: 2024
bibkey: zytek2024llms
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.06064"}
tags: ['Prompting', 'Training Techniques', 'Interpretability and Explainability']
---
In response to the demand for Explainable Artificial Intelligence (XAI), we
investigate the use of Large Language Models (LLMs) to transform ML
explanations into natural, human-readable narratives. Rather than directly
explaining ML models using LLMs, we focus on refining explanations computed
using existing XAI algorithms. We outline several research directions,
including defining evaluation metrics, prompt design, comparing LLM models,
exploring further training methods, and integrating external data. Initial
experiments and user study suggest that LLMs offer a promising way to enhance
the interpretability and usability of XAI.
