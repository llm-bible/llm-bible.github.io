---
layout: publication
title: 'Rigochat 2: An Adapted Language Model To Spanish Using A Bounded Dataset And Reduced Hardware'
authors: Gonzalo Santamaría Gómez, Guillem García Subies, Pablo Gutiérrez Ruiz, Mario González Valero, Natàlia Fuertes, Helena Montoro Zamorano, Carmen Muñoz Sanz, Leire Rosado Plaza, Nuria Aldama García, David Betancur Sánchez, Kateryna Sushkova, Marta Guerrero Nieto, Álvaro Barbero Jiménez
conference: "Arxiv"
year: 2025
bibkey: gómez2025rigochat
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.08188'}
tags: ['Training Techniques']
---
Large Language Models (LLMs) have become a key element of modern artificial
intelligence, demonstrating the ability to address a wide range of language
processing tasks at unprecedented levels of accuracy without the need of
collecting problem-specific data. However, these versatile models face a
significant challenge: both their training and inference processes require
substantial computational resources, time, and memory. Consequently, optimizing
this kind of models to minimize these requirements is crucial. In this article,
we demonstrate that, with minimal resources and in a remarkably short time, it
is possible to enhance a state-of-the-art model, specifically for a given
language task, without compromising its overall capabilities using a relatively
small pretrained LLM as a basis. Specifically, we present our use case,
RigoChat 2, illustrating how LLMs can be adapted to achieve superior results in
Spanish-language tasks.
