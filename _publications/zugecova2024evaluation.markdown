---
layout: publication
title: 'Evaluation Of LLM Vulnerabilities To Being Misused For Personalized Disinformation Generation'
authors: Aneta Zugecova, Dominik Macko, Ivan Srba, Robert Moro, Jakub Kopal, Katarina Marcincinova, Matus Mesarcik
conference: "Arxiv"
year: 2024
bibkey: zugecova2024evaluation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.13666"}
tags: ['Responsible AI', 'Reinforcement Learning']
---
The capabilities of recent large language models (LLMs) to generate
high-quality content indistinguishable by humans from human-written texts rises
many concerns regarding their misuse. Previous research has shown that LLMs can
be effectively misused for generating disinformation news articles following
predefined narratives. Their capabilities to generate personalized (in various
aspects) content have also been evaluated and mostly found usable. However, a
combination of personalization and disinformation abilities of LLMs has not
been comprehensively studied yet. Such a dangerous combination should trigger
integrated safety filters of the LLMs, if there are some. This study fills this
gap by evaluation of vulnerabilities of recent open and closed LLMs, and their
willingness to generate personalized disinformation news articles in English.
We further explore whether the LLMs can reliably meta-evaluate the
personalization quality and whether the personalization affects the
generated-texts detectability. Our results demonstrate the need for stronger
safety-filters and disclaimers, as those are not properly functioning in most
of the evaluated LLMs. Additionally, our study revealed that the
personalization actually reduces the safety-filter activations; thus
effectively functioning as a jailbreak. Such behavior must be urgently
addressed by LLM developers and service providers.
