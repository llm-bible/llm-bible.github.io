---
layout: publication
title: 'Icleval: Evaluating In-context Learning Ability Of Large Language Models'
authors: Chen Wentong, Lin Yankai, Zhou Zhenhao, Huang Hongyun, Jia Yantao, Cao Zhao, Wen Ji-rong
conference: "Arxiv"
year: 2024
bibkey: chen2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.14955"}
  - {name: "Code", url: "https://github.com/yiye3/ICLEval"}
tags: ['Has Code', 'In Context Learning', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
In-Context Learning (ICL) is a critical capability of Large Language Models (LLMs) as it empowers them to comprehend and reason across interconnected inputs. Evaluating the ICL ability of LLMs can enhance their utilization and deepen our understanding of how this ability is acquired at the training stage. However existing evaluation frameworks primarily focus on language abilities and knowledge often overlooking the assessment of ICL ability. In this work we introduce the ICLEval benchmark to evaluate the ICL abilities of LLMs which encompasses two key sub-abilities exact copying and rule learning. Through the ICLEval benchmark we demonstrate that ICL ability is universally present in different LLMs and model size is not the sole determinant of ICL efficacy. Surprisingly we observe that ICL abilities particularly copying develop early in the pretraining process and stabilize afterward. Our source codes and benchmark are released at https://github.com/yiye3/ICLEval."
