---
layout: publication
title: 'Walking In Others'' Shoes: How Perspective-taking Guides Large Language Models In Reducing Toxicity And Bias'
authors: Xu Rongwu, Zhou Zi'an, Zhang Tianwei, Qi Zehan, Yao Su, Xu Ke, Xu Wei, Qiu Han
conference: "Arxiv"
year: 2024
bibkey: xu2024walking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.15366"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'Training Techniques']
---
'The common toxicity and societal bias in contents generated by large language models (LLMs) necessitate strategies to reduce harm. Present solutions often demand white-box access to the model or substantial training, which is impractical for cutting-edge commercial LLMs. Moreover, prevailing prompting methods depend on external tool feedback and fail to simultaneously lessen toxicity and bias. Motivated by social psychology principles, we propose a novel strategy named \textbf\{perspective-taking prompting (\textsc\{PeT\})\} that inspires LLMs to integrate diverse human perspectives and self-regulate their responses. This self-correction mechanism can significantly diminish toxicity (up to \(89\%\)) and bias (up to \(73\%\)) in LLMs'' responses. Rigorous evaluations and ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and three open-source LLMs, revealing \textsc\{PeT\}''s superiority in producing less harmful responses, outperforming five strong baselines.'
