---
layout: publication
title: Walking in Others Shoes How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias
authors: Xu Rongwu, Zhou Zi'an, Zhang Tianwei, Qi Zehan, Yao Su, Xu Ke, Xu Wei, Qiu Han
conference: "Arxiv"
year: 2024
bibkey: xu2024walking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.15366"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'Training Techniques']
---
The common toxicity and societal bias in contents generated by large language models (LLMs) necessitate strategies to reduce harm. Present solutions often demand white-box access to the model or substantial training which is impractical for cutting-edge commercial LLMs. Moreover prevailing prompting methods depend on external tool feedback and fail to simultaneously lessen toxicity and bias. Motivated by social psychology principles we propose a novel strategy named textbfperspective-taking prompting (textscPeT) that inspires LLMs to integrate diverse human perspectives and self-regulate their responses. This self-correction mechanism can significantly diminish toxicity (up to 89) and bias (up to 73) in LLMs responses. Rigorous evaluations and ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and three open-source LLMs revealing textscPeTs superiority in producing less harmful responses outperforming five strong baselines.
