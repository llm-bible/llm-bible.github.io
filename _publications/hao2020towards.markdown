---
layout: publication
title: Towards Learning A Generic Agent For Vision45;and45;language Navigation Via Pre45;training
authors: Hao Weituo, Li Chunyuan, Li Xiujun, Carin Lawrence, Gao Jianfeng
conference: "Arxiv"
year: 2020
bibkey: hao2020towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2002.10638"}
tags: ['Agentic', 'Multimodal Models', 'Tools', 'Training Techniques']
---
Learning to navigate in a visual environment following natural45;language instructions is a challenging task because the multimodal inputs to the agent are highly variable and the training data on a new task is often limited. In this paper we present the first pre45;training and fine45;tuning paradigm for vision45;and45;language navigation (VLN) tasks. By training on a large amount of image45;text45;action triplets in a self45;supervised learning manner the pre45;trained model provides generic representations of visual environments and language instructions. It can be easily used as a drop45;in for existing VLN frameworks leading to the proposed agent called Prevalent. It learns more effectively in new tasks and generalizes better in a previously unseen environment. The performance is validated on three VLN tasks. On the Room45;to45;Room benchmark our model improves the state45;of45;the45;art from 4737; to 5137; on success rate weighted by path length. Further the learned representation is transferable to other VLN tasks. On two recent tasks vision45;and45;dialog navigation and Help Anna! the proposed Prevalent leads to significant improvement over existing methods achieving a new state of the art.
