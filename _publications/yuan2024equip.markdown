---
layout: publication
title: Remamba Equip Mamba With Effective Long45;sequence Modeling
authors: Yuan Danlong, Liu Jiahao, Li Bei, Zhang Huishuai, Wang Jingang, Cai Xunliang, Zhao Dongyan
conference: "Arxiv"
year: 2024
bibkey: yuan2024equip
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.15496"}
tags: ['Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
While the Mamba architecture demonstrates superior inference efficiency and competitive performance on short45;context natural language processing (NLP) tasks empirical evidence suggests its capacity to comprehend long contexts is limited compared to transformer45;based models. In this study we investigate the long45;context efficiency issues of the Mamba models and propose ReMamba which enhances Mambas ability to comprehend long contexts. ReMamba incorporates selective compression and adaptation techniques within a two45;stage re45;forward process incurring minimal additional inference costs overhead. Experimental results on the LongBench and L45;Eval benchmarks demonstrate ReMambas efficacy improving over the baselines by 3.2 and 1.6 points respectively and attaining performance almost on par with same45;size transformer models.
