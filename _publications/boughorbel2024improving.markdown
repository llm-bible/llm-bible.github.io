---
layout: publication
title: Improving Language Models Trained On Translated Data With Continual Pre45;training And Dictionary Learning Analysis
authors: Boughorbel Sabri, Parvez Md Rizwan, Hawasly Majd
conference: "Arxiv"
year: 2024
bibkey: boughorbel2024improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.14277"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Interpretability And Explainability', 'Model Architecture', 'Training Techniques']
---
Training LLMs for low45;resource languages usually utilizes data augmentation from English using machine translation (MT). This however brings a number of challenges to LLM training there are large costs attached to translating and curating huge amounts of content with high45;end machine translation solutions; the translated content carries over cultural biases; and if the translation is not faithful and accurate data quality degrades causing issues in the trained model. In this work we investigate the role of translation and synthetic data in training language models. We translate TinyStories a dataset of 2.2M short stories for 345;4 year old children from English to Arabic using the open NLLB45;3B MT model. We train a number of story generation models of size 1M45;33M parameters using this data. We identify a number of quality and task45;specific issues in the resulting models. To rectify these issues we further pre45;train the models with a small dataset of synthesized high45;quality Arabic stories generated by a capable LLM representing 137; of the original training data. We show using GPT45;4 as a judge and Dictionary Learning Analysis from mechanistic interpretability that the suggested approach is a practical means to resolve some of the machine translation pitfalls. We illustrate the improvements through case studies of linguistic and cultural bias issues.
