---
layout: publication
title: "Larger Language Models Do In-context Learning Differently"
authors: Wei Jerry, Wei Jason, Tay Yi, Tran Dustin, Webson Albert, Lu Yifeng, Chen Xinyun, Liu Hanxiao, Huang Da, Zhou Denny, Ma Tengyu
conference: "Arxiv"
year: 2023
bibkey: wei2023larger
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2303.03846"}
tags: ['GPT', 'In Context Learning', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Training Techniques']
---
We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3 InstructGPT Codex PaLM and Flan-PaLM). First experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining large models can override semantic priors when presented with in-context exemplars that contradict priors despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL) in which labels are semantically unrelated to their inputs (e.g. foo/bar instead of negative/positive) thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings but more of the former.
