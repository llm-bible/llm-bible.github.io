---
layout: publication
title: Apollo Unified Adapter And Prompt Learning For Vision Language Models
authors: Chowdhury Sanjoy, Nag Sayan, Manocha Dinesh
conference: "Arxiv"
year: 2023
bibkey: chowdhury2023unified
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.01564"}
tags: ['Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Prompting']
---
The choice of input text prompt plays a critical role in the performance of Vision45;Language Pretrained (VLP) models such as CLIP. We present APoLLo a unified multi45;modal approach that combines Adapter and Prompt learning for Vision45;Language models. Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine45;tuned in a few45;shot setting. We introduce trainable cross45;attention45;based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. We enforce consistency between the respective encoder branches (receiving augmented inputs) to prevent overfitting in downstream tasks. Our method is evaluated on three representative tasks generalization to novel classes cross45;dataset evaluation and unseen domain shifts. In practice APoLLo achieves a relative gain up to 6.0337; over MaPLe (SOTA) on novel classes for 10 diverse image recognition datasets.
