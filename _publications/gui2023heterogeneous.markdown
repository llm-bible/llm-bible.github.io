---
layout: publication
title: Hiformer Heterogeneous Feature Interactions Learning With Transformers For Recommender Systems
authors: Gui Huan, Wang Ruoxi, Yin Ke, Jin Long, Kula Maciej, Xu Taibai, Hong Lichan, Chi Ed H.
conference: "Arxiv"
year: 2023
bibkey: gui2023heterogeneous
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.05884"}
tags: ['Applications', 'Attention Mechanism', 'Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Pruning', 'RAG', 'Reinforcement Learning', 'Transformer']
---
Learning feature interaction is the critical backbone to building recommender systems. In web45;scale applications learning feature interaction is extremely challenging due to the sparse and large input feature space; meanwhile manually crafting effective feature interactions is infeasible because of the exponential solution space. We propose to leverage a Transformer45;based architecture with attention layers to automatically capture feature interactions. Transformer architectures have witnessed great success in many domains such as natural language processing and computer vision. However there has not been much adoption of Transformer architecture for feature interaction modeling in industry. We aim at closing the gap. We identify two key challenges for applying the vanilla Transformer architecture to web45;scale recommender systems (1) Transformer architecture fails to capture the heterogeneous feature interactions in the self45;attention layer; (2) The serving latency of Transformer architecture might be too high to be deployed in web45;scale recommender systems. We first propose a heterogeneous self45;attention layer which is a simple yet effective modification to the self45;attention layer in Transformer to take into account the heterogeneity of feature interactions. We then introduce textsc123;Hiformer125; (textbf123;H125;eterogeneous textbf123;I125;nteraction Transtextbf123;former125;) to further improve the model expressiveness. With low45;rank approximation and model pruning hiformer enjoys fast inference for online deployment. Extensive offline experiment results corroborates the effectiveness and efficiency of the textsc123;Hiformer125; model. We have successfully deployed the textsc123;Hiformer125; model to a real world large scale App ranking model at Google Play with significant improvement in key engagement metrics (up to +2.6637;).
