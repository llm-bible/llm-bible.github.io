---
layout: publication
title: 'G-boost: Boosting Private Slms With General Llms'
authors: Yijiang Fan, Yuren Mao, Longbin Lai, Ying Zhang, Zhengping Qian, Yunjun Gao
conference: "Arxiv"
year: 2025
bibkey: fan2025g
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.10367'}
tags: ['Reinforcement Learning', 'Tools']
---
Due to the limited computational resources, most Large Language Models (LLMs)
developers can only fine-tune Small Language Models (SLMs) on their own data.
These private SLMs typically have limited effectiveness. To boost the
performance of private SLMs, this paper proposes to ask general LLMs for help.
The general LLMs can be APIs or larger LLMs whose inference cost the developers
can afford. Specifically, we propose the G-Boost framework where a private SLM
adaptively performs collaborative inference with a general LLM under the guide
of process reward. Experiments demonstrate that our framework can significantly
boost the performance of private SLMs.
