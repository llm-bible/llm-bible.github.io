---
layout: publication
title: Attention Link An Efficient Attention45;based Low Resource Machine Translation Architecture
authors: Min Zeping
conference: "Arxiv"
year: 2023
bibkey: min2023attention
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2302.00340"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Transformers have achieved great success in machine translation but transformer45;based NMT models often require millions of bilingual parallel corpus for training. In this paper we propose a novel architecture named as attention link (AL) to help improve transformer models performance especially in low training resources. We theoretically demonstrate the superiority of our attention link architecture in low training resources. Besides we have done a large number of experiments including en45;de de45;en en45;fr en45;it it45;en en45;ro translation tasks on the IWSLT14 dataset as well as real low resources scene on bn45;gu and gu45;ta translation tasks on the CVIT PIB dataset. All the experiment results show our attention link is powerful and can lead to a significant improvement. In addition we achieve a 37.9 BLEU score a new sota on the IWSLT14 de45;en task by combining our attention link and other advanced methods.
