---
layout: publication
title: 'Who Is Chatgpt? Benchmarking Llms'' Psychological Portrayal Using Psychobench'
authors: Huang Jen-tse, Wang Wenxuan, Li Eric John, Lam Man Ho, Ren Shujie, Yuan Youliang, Jiao Wenxiang, Tu Zhaopeng, Lyu Michael R.
conference: "Arxiv"
year: 2023
bibkey: huang2023who
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.01386"}
  - {name: "Code", url: "https://github.com/CUHK-ARISE/PsychoBench"}
tags: ['Agentic', 'Applications', 'GPT', 'Has Code', 'Model Architecture', 'Responsible AI', 'Tools', 'Uncategorized']
---
Large Language Models (LLMs) have recently showcased their remarkable
capacities, not only in natural language processing tasks but also across
diverse domains such as clinical medicine, legal consultation, and education.
LLMs become more than mere applications, evolving into assistants capable of
addressing diverse user requests. This narrows the distinction between human
beings and artificial intelligence agents, raising intriguing questions
regarding the potential manifestation of personalities, temperaments, and
emotions within LLMs. In this paper, we propose a framework, PsychoBench, for
evaluating diverse psychological aspects of LLMs. Comprising thirteen scales
commonly used in clinical psychology, PsychoBench further classifies these
scales into four distinct categories: personality traits, interpersonal
relationships, motivational tests, and emotional abilities. Our study examines
five popular models, namely text-davinci-003, gpt-3.5-turbo, gpt-4, LLaMA-2-7b,
and LLaMA-2-13b. Additionally, we employ a jailbreak approach to bypass the
safety alignment protocols and test the intrinsic natures of LLMs. We have made
PsychoBench openly accessible via https://github.com/CUHK-ARISE/PsychoBench.
