---
layout: publication
title: 'Phantom: Persona-based Prompting Has An Effect On Theory-of-mind Reasoning In Large Language Models'
authors: Fiona Anting Tan, Gerard Christopher Yeo, Kokil Jaidka, Fanyou Wu, Weijie Xu, Vinija Jain, Aman Chadha, Yang Liu, See-kiong Ng
conference: "Arxiv"
year: 2024
bibkey: tan2024persona
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2403.02246'}
tags: ['Prompting']
---
The use of LLMs in natural language reasoning has shown mixed results,
sometimes rivaling or even surpassing human performance in simpler
classification tasks while struggling with social-cognitive reasoning, a domain
where humans naturally excel. These differences have been attributed to many
factors, such as variations in prompting and the specific LLMs used. However,
no reasons appear conclusive, and no clear mechanisms have been established in
prior work. In this study, we empirically evaluate how role-playing prompting
influences Theory-of-Mind (ToM) reasoning capabilities. Grounding our rsearch
in psychological theory, we propose the mechanism that, beyond the inherent
variance in the complexity of reasoning tasks, performance differences arise
because of socially-motivated prompting differences. In an era where prompt
engineering with role-play is a typical approach to adapt LLMs to new contexts,
our research advocates caution as models that adopt specific personas might
potentially result in errors in social-cognitive reasoning.
