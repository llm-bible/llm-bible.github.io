---
layout: publication
title: "Do Llms Find Human Answers To Fact-driven Questions Perplexing? A Case Study On Reddit"
authors: Seegmiller Parker, Gatto Joseph, Sharif Omar, Basak Madhusudan, Preum Sarah Masud
conference: "Arxiv"
year: 2024
bibkey: seegmiller2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.01147"}
tags: ['Pretraining Methods', 'Reinforcement Learning']
---
Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities or subreddits. We collect and release a dataset of 409 fact-driven questions and 7534 diverse human-rated answers from 15 r/AskTopic communities across 3 categories profession social identity and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.
