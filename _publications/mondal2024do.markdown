---
layout: publication
title: 'Do Large Language Models Exhibit Cognitive Dissonance? Studying The Difference Between Revealed Beliefs And Stated Answers'
authors: Mondal Manuel, Dolamic Ljiljana, Bovet Gérôme, Cudré-mauroux Philippe, Audiffren Julien
conference: "Arxiv"
year: 2024
bibkey: mondal2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.14986"}
tags: ['Ethics And Bias', 'Prompting']
---
Prompting and Multiple Choices Questions (MCQ) have become the preferred approach to assess the capabilities of Large Language Models (LLMs) due to their ease of manipulation and evaluation. Such experimental appraisals have pointed toward the LLMs apparent ability to perform causal reasoning or to grasp uncertainty. In this paper we investigate whether these abilities are measurable outside of tailored prompting and MCQ by reformulating these issues as direct text completion - the foundation of LLMs. To achieve this goal we define scenarios with multiple possible outcomes and we compare the prediction made by the LLM through prompting (their Stated Answer) to the probability distributions they compute over these outcomes during next token prediction (their Revealed Belief). Our findings suggest that the Revealed Belief of LLMs significantly differs from their Stated Answer and hint at multiple biases and misrepresentations that their beliefs may yield in many scenarios and outcomes. As text completion is at the core of LLMs these results suggest that common evaluation methods may only provide a partial picture and that more research is needed to assess the extent and nature of their capabilities.
