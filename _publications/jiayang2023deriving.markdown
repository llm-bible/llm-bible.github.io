---
layout: publication
title: Storyanalogy Deriving Story45;level Analogies From Large Language Models To Unlock Analogical Understanding
authors: Jiayang Cheng, Qiu Lin, Chan Tsz Ho, Fang Tianqing, Wang Weiqi, Chan Chunkit, Ru Dongyu, Guo Qipeng, Zhang Hongming, Song Yangqiu, Zhang Yue, Zhang Zheng
conference: "Arxiv"
year: 2023
bibkey: jiayang2023deriving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.12874"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
Analogy45;making between narratives is crucial for human reasoning. In this paper we evaluate the ability to identify and generate analogies by constructing a first45;of45;its45;kind large45;scale story45;level analogy corpus textsc123;StoryAnalogy125; which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure45;Mapping Theory. We design a set of tests on textsc123;StoryAnalogy125; presenting the first evaluation of story45;level analogy identification and generation. Interestingly we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT for example only achieved around 3037; accuracy in multiple45;choice questions (compared to over 8537; accuracy for humans). Furthermore we observe that the data in textsc123;StoryAnalogy125; can improve the quality of analogy generation in LLMs where a fine45;tuned FlanT545;xxl model achieves comparable performance to zero45;shot ChatGPT.
