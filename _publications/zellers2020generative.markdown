---
layout: publication
title: Turingadvice A Generative And Dynamic Evaluation Of Language Use
authors: Zellers Rowan, Holtzman Ari, Clark Elizabeth, Qin Lianhui, Farhadi Ali, Choi Yejin
conference: "Arxiv"
year: 2020
bibkey: zellers2020generative
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2004.03607"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Tools', 'Training Techniques']
---
We propose TuringAdvice a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding our ability to use language to resolve open45;ended situations by communicating with each other. Empirical results show that todays models struggle at TuringAdvice even multibillion parameter models finetuned on 600k in45;domain training examples. The best model a finetuned T5 writes advice that is at least as helpful as human45;written advice in only 1437; of cases; a much larger non45;finetunable GPT3 model does even worse at 437;. This low performance reveals language understanding errors that are hard to spot outside of a generative setting showing much room for progress.
