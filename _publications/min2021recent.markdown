---
layout: publication
title: Recent Advances In Natural Language Processing Via Large Pre-trained Language Models&#58; A Survey
authors: Min Bonan, Ross Hayley, Sulem Elior, Veyseh Amir Pouran Ben, Nguyen Thien Huu, Sainz Oscar, Agirre Eneko, Heinz Ilana, Roth Dan
conference: "Arxiv"
year: 2021
bibkey: min2021recent
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2111.01243"}
tags: ['Applications', 'BERT', 'Fine Tuning', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Survey Paper', 'Training Techniques', 'Transformer']
---
Large pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning prompting or text generation approaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research.
