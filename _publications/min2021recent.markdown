---
layout: publication
title: 'Recent Advances In Natural Language Processing Via Large Pre-trained Language Models: A Survey'
authors: Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heinz, Dan Roth
conference: "Arxiv"
year: 2021
bibkey: min2021recent
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2111.01243'}
tags: ['Language Modeling', 'Transformer', 'Training Techniques', 'Model Architecture', 'BERT', 'Fine-Tuning', 'Prompting', 'Applications', 'Survey Paper', 'Pre-Training', 'Pretraining Methods']
---
Large, pre-trained transformer-based language models such as BERT have
drastically changed the Natural Language Processing (NLP) field. We present a
survey of recent work that uses these large language models to solve NLP tasks
via pre-training then fine-tuning, prompting, or text generation approaches. We
also present approaches that use pre-trained language models to generate data
for training augmentation or other purposes. We conclude with discussions on
limitations and suggested directions for future research.
