---
layout: publication
title: 'Automated Rationale Generation: A Technique For Explainable AI And Its Effects
  On Human Perceptions'
authors: Upol Ehsan, Pradyumna Tambwekar, Larry Chan, Brent Harrison, Mark Riedl
conference: Arxiv
year: 2019
citations: 44
bibkey: ehsan2019automated
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1901.03729'}]
tags: [Interpretability and Explainability, Agentic]
---
Automated rationale generation is an approach for real-time explanation
generation whereby a computational model learns to translate an autonomous
agent's internal state and action data representations into natural language.
Training on human explanation data can enable agents to learn to generate
human-like explanations for their behavior. In this paper, using the context of
an agent that plays Frogger, we describe (a) how to collect a corpus of
explanations, (b) how to train a neural rationale generator to produce
different styles of rationales, and (c) how people perceive these rationales.
We conducted two user studies. The first study establishes the plausibility of
each type of generated rationale and situates their user perceptions along the
dimensions of confidence, humanlike-ness, adequate justification, and
understandability. The second study further explores user preferences between
the generated rationales with regard to confidence in the autonomous agent,
communicating failure and unexpected behavior. Overall, we find alignment
between the intended differences in features of the generated rationales and
the perceived differences by users. Moreover, context permitting, participants
preferred detailed rationales to form a stable mental model of the agent's
behavior.