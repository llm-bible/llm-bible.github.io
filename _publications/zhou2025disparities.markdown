---
layout: publication
title: 'Disparities In LLM Reasoning Accuracy And Explanations: A Case Study On African American English'
authors: Runtao Zhou, Guangya Wan, Saadia Gabriel, Sheng Li, Alexander J Gates, Maarten Sap, Thomas Hartvigsen
conference: "Arxiv"
year: 2025
bibkey: zhou2025disparities
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.04099"}
  - {name: "Code", url: "https://github.com/Runtaozhou/dialect_bias_eval"}
tags: ['Tools', 'Ethics and Bias', 'Interpretability and Explainability', 'Reinforcement Learning', 'Has Code', 'Prompting']
---
Large Language Models (LLMs) have demonstrated remarkable capabilities in
reasoning tasks, leading to their widespread deployment. However, recent
studies have highlighted concerning biases in these models, particularly in
their handling of dialectal variations like African American English (AAE). In
this work, we systematically investigate dialectal disparities in LLM reasoning
tasks. We develop an experimental framework comparing LLM performance given
Standard American English (SAE) and AAE prompts, combining LLM-based dialect
conversion with established linguistic analyses. We find that LLMs consistently
produce less accurate responses and simpler reasoning chains and explanations
for AAE inputs compared to equivalent SAE questions, with disparities most
pronounced in social science and humanities domains. These findings highlight
systematic differences in how LLMs process and reason about different language
varieties, raising important questions about the development and deployment of
these systems in our multilingual and multidialectal world. Our code repository
is publicly available at https://github.com/Runtaozhou/dialect_bias_eval.
