---
layout: publication
title: STEER Assessing The Economic Rationality Of Large Language Models
authors: Raman Narun, Lundy Taylor, Amouyal Samuel, Levine Yoav, Leyton-brown Kevin, Tennenholtz Moshe
conference: "Arxiv"
year: 2024
bibkey: raman2024assessing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.09552"}
tags: ['Agentic', 'Prompting', 'Survey Paper']
---
There is increasing interest in using LLMs as decision45;making agents. Doing so includes many degrees of freedom which model should be used; how should it be prompted; should it be asked to introspect conduct chain45;of45;thought reasoning etc Settling these questions 45;45; and more broadly determining whether an LLM agent is reliable enough to be trusted 45;45; requires a methodology for assessing such an agents economic rationality. In this paper we provide one. We begin by surveying the economic literature on rational decision making taxonomizing a large set of fine45;grained elements that an agent should exhibit along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and combined with a user45;provided rubric produces a STEER report card. Finally we describe the results of a large45;scale empirical experiment with 14 different LLMs characterizing the both current state of the art and the impact of different model sizes on models ability to exhibit rational behavior.
