---
layout: publication
title: Factuality Of Large Language Models In The Year 2024
authors: Wang Yuxia, Wang Minghan, Manzoor Muhammad Arslan, Liu Fei, Georgiev Georgi, Das Rocktim Jyoti, Nakov Preslav
conference: "Arxiv"
year: 2024
bibkey: wang2024factuality
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.02420"}
tags: ['Applications', 'Attention Mechanism', 'Language Modeling', 'Model Architecture', 'Reinforcement Learning', 'Survey Paper', 'TACL']
---
Large language models (LLMs) especially when instruction45;tuned for chat have become part of our daily lives freeing people from the process of searching extracting and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately in many cases LLM responses are factually incorrect which limits their applicability in real45;world scenarios. As a result research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey we critically analyze existing work with the aim to identify the major challenges and their associated causes pointing out to potential solutions for improving the factuality of LLMs and analyzing the obstacles to automated factuality evaluation for open45;ended text generation. We further offer an outlook on where future research should go.
