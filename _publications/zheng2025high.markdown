---
layout: publication
title: 'GREATERPROMPT: A Unified, Customizable, And High-performing Open-source Toolkit For Prompt Optimization'
authors: Wenliang Zheng, Sarkar Snigdha Sarathi Das, Yusen Zhang, Rui Zhang
conference: "Arxiv"
year: 2025
bibkey: zheng2025high
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.03975"}
  - {name: "Code", url: "https://github.com/psunlpgroup/GreaterPrompt"}
tags: ['Tools', 'Efficiency and Optimization', 'RAG', 'Has Code', 'Prompting']
---
LLMs have gained immense popularity among researchers and the general public
for its impressive capabilities on a variety of tasks. Notably, the efficacy of
LLMs remains significantly dependent on the quality and structure of the input
prompts, making prompt design a critical factor for their performance. Recent
advancements in automated prompt optimization have introduced diverse
techniques that automatically enhance prompts to better align model outputs
with user expectations. However, these methods often suffer from the lack of
standardization and compatibility across different techniques, limited
flexibility in customization, inconsistent performance across model scales, and
they often exclusively rely on expensive proprietary LLM APIs. To fill in this
gap, we introduce GREATERPROMPT, a novel framework that democratizes prompt
optimization by unifying diverse methods under a unified, customizable API
while delivering highly effective prompts for different tasks. Our framework
flexibly accommodates various model scales by leveraging both text
feedback-based optimization for larger LLMs and internal gradient-based
optimization for smaller models to achieve powerful and precise prompt
improvements. Moreover, we provide a user-friendly Web UI that ensures
accessibility for non-expert users, enabling broader adoption and enhanced
performance across various user groups and application scenarios. GREATERPROMPT
is available at https://github.com/psunlpgroup/GreaterPrompt via GitHub, PyPI,
and web user interfaces.
