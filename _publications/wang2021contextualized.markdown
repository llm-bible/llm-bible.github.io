---
layout: publication
title: 'Contextualized Scene Imagination For Generative Commonsense Reasoning'
authors: Wang Peifeng, Zamora Jonathan, Liu Junfeng, Ilievski Filip, Chen Muhao, Ren Xiang
conference: "Arxiv"
year: 2021
bibkey: wang2021contextualized
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2112.06318"}
tags: ['Applications', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Transformer']
---
Humans use natural language to compose common concepts from their environment into plausible, day-to-day scene descriptions. However, such generative commonsense reasoning (GCSR) skills are lacking in state-of-the-art text generation methods. Descriptive sentences about arbitrary concepts generated by neural text generation models (e.g., pre-trained text-to-text Transformers) are often grammatically fluent but may not correspond to human common sense, largely due to their lack of mechanisms to capture concept relations, to identify implicit concepts, and to perform generalizable reasoning about unseen concept compositions. In this paper, we propose an Imagine-and-Verbalize (I&amp;V) method, which learns to imagine a relational scene knowledge graph (SKG) with relations between the input concepts, and leverage the SKG as a constraint when generating a plausible scene description. We collect and harmonize a set of knowledge resources from different domains and modalities, providing a rich auxiliary supervision signal for I&amp;V. The experiments demonstrate the effectiveness of I&amp;V in improving language models on both concept-to-sentence and concept-to-story generation tasks, while enabling the model to learn well from fewer task examples and generate SKGs that make common sense to human annotators.
