---
layout: publication
title: 'Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions'
authors: Shih-han Chan
conference: "Arxiv"
year: 2025
bibkey: chan2025encrypted
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.23250"}
tags: ['Responsible AI', 'Security', 'Tools', 'Prompting', 'Applications']
---
Security threats like prompt injection attacks pose significant risks to
applications that integrate Large Language Models (LLMs), potentially leading
to unauthorized actions such as API misuse. Unlike previous approaches that aim
to detect these attacks on a best-effort basis, this paper introduces a novel
method that appends an Encrypted Prompt to each user prompt, embedding current
permissions. These permissions are verified before executing any actions (such
as API calls) generated by the LLM. If the permissions are insufficient, the
LLM's actions will not be executed, ensuring safety. This approach guarantees
that only actions within the scope of the current permissions from the LLM can
proceed. In scenarios where adversarial prompts are introduced to mislead the
LLM, this method ensures that any unauthorized actions from LLM wouldn't be
executed by verifying permissions in Encrypted Prompt. Thus, threats like
prompt injection attacks that trigger LLM to generate harmful actions can be
effectively mitigated.
