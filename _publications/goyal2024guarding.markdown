---
layout: publication
title: 'Llmguard: Guarding Against Unsafe LLM Behavior'
authors: Shubh Goyal, Medha Hira, Shubham Mishra, Sukriti Goyal, Arnav Goel, Niharika Dadu, Kirushikesh Db, Sameep Mehta, Nishtha Madaan
conference: "Arxiv"
year: 2024
bibkey: goyal2024guarding
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2403.00826'}
tags: ['Reinforcement Learning', 'Ethics and Bias']
---
Although the rise of Large Language Models (LLMs) in enterprise settings
brings new opportunities and capabilities, it also brings challenges, such as
the risk of generating inappropriate, biased, or misleading content that
violates regulations and can have legal concerns. To alleviate this, we present
"LLMGuard", a tool that monitors user interactions with an LLM application and
flags content against specific behaviours or conversation topics. To do this
robustly, LLMGuard employs an ensemble of detectors.
