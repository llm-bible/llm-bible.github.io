---
layout: publication
title: "Badgpt: Exploring Security Vulnerabilities Of Chatgpt Via Backdoor Attacks To Instructgpt"
authors: Shi Jiawen, Liu Yixin, Zhou Pan, Sun Lichao
conference: "Arxiv"
year: 2023
bibkey: shi2023exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.12298"}
tags: ['Agentic', 'Attention Mechanism', 'Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Security', 'Training Techniques']
---
Recently ChatGPT has gained significant attention in research due to its ability to interact with humans effectively. The core idea behind this model is reinforcement learning (RL) fine-tuning a new paradigm that allows language models to align with human preferences i.e. InstructGPT. In this study we propose BadGPT the first backdoor attack against RL fine-tuning in language models. By injecting a backdoor into the reward model the language model can be compromised during the fine-tuning stage. Our initial experiments on movie reviews i.e. IMDB demonstrate that an attacker can manipulate the generated text through BadGPT.
