---
layout: publication
title: Metacognitive Myopia in Large Language Models
authors: Scholten Florian, Rebholz Tobias R., HÃ¼tter Mandy
conference: "Arxiv"
year: 2024
bibkey: scholten2024metacognitive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.05568"}
tags: ['Agentic', 'Ethics And Bias', 'Interpretability And Explainability', 'Merging', 'Model Architecture', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Large Language Models (LLMs) exhibit potentially harmful biases that reinforce culturally inherent stereotypes cloud moral judgments or amplify positive evaluations of majority groups. Previous explanations mainly attributed bias in LLMs to human annotators and the selection of training data. Consequently they have typically been addressed with bottom-up approaches such as reinforcement learning or debiasing corpora. However these methods only treat the effects of LLM biases by indirectly influencing the model architecture but do not address the underlying causes in the computational process. Here we propose metacognitive myopia as a cognitive-ecological framework that can account for a conglomerate of established and emerging LLM biases and provide a lever to address problems in powerful but vulnerable tools. Our theoretical framework posits that a lack of the two components of metacognition monitoring and control causes five symptoms of metacognitive myopia in LLMs integration of invalid tokens and embeddings susceptibility to redundant information neglect of base rates in conditional computation decision rules based on frequency and inappropriate higher-order statistical inference for nested data structures. As a result LLMs produce erroneous output that reaches into the daily high-stakes decisions of humans. By introducing metacognitive regulatory processes into LLMs engineers and scientists can develop precise remedies for the underlying causes of these biases. Our theory sheds new light on flawed human-machine interactions and raises ethical concerns regarding the increasing imprudent implementation of LLMs in organizational structures.
