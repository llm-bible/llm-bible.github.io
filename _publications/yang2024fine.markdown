---
layout: publication
title: Fine45;tuning Large Language Models With Human45;inspired Learning Strategies In Medical Question Answering
authors: Yang Yushi, Bean Andrew M., Mccraith Robert, Mahdi Adam
conference: "Arxiv"
year: 2024
bibkey: yang2024fine
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.07888"}
tags: ['Applications', 'Training Techniques']
---
Training Large Language Models (LLMs) incurs substantial data45;related costs motivating the development of data45;efficient training methods through optimised data ordering and selection. Human45;inspired learning strategies such as curriculum learning offer possibilities for efficient training by organising data according to common human learning practices. Despite evidence that fine45;tuning with curriculum learning improves the performance of LLMs for natural language understanding tasks its effectiveness is typically assessed using a single model. In this work we extend previous research by evaluating both curriculum45;based and non45;curriculum45;based learning strategies across multiple LLMs using human45;defined and automated data labels for medical question answering. Our results indicate a moderate impact of using human45;inspired learning strategies for fine45;tuning LLMs with maximum accuracy gains of 1.7737; per model and 1.8137; per dataset. Crucially we demonstrate that the effectiveness of these strategies varies significantly across different model45;dataset combinations emphasising that the benefits of a specific human45;inspired strategy for fine45;tuning LLMs do not generalise. Additionally we find evidence that curriculum learning using LLM45;defined question difficulty outperforms human45;defined difficulty highlighting the potential of using model45;generated measures for optimal curriculum design.
