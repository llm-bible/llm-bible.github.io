---
layout: publication
title: Llm45;dcache Improving Tool45;augmented Llms With Gpt45;driven Localized Data Caching
authors: Singh Simranjit, Fore Michael, Karatzas Andreas, Lee Chaehong, Jian Yanan, Shangguan Longfei, Yu Fuxun, Anagnostopoulos Iraklis, Stamoulis Dimitrios
conference: "Arxiv"
year: 2024
bibkey: singh2024llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.06799"}
tags: ['Agentic', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG', 'Reinforcement Learning', 'Tools']
---
As Large Language Models (LLMs) broaden their capabilities to manage thousands of API calls they are confronted with complex data operations across vast datasets with significant overhead to the underlying system. In this work we introduce LLM45;dCache to optimize data accesses by treating cache operations as callable API functions exposed to the tool45;augmented agent. We grant LLMs the autonomy to manage cache decisions via prompting seamlessly integrating with existing function45;calling mechanisms. Tested on an industry45;scale massively parallel platform that spans hundreds of GPT endpoints and terabytes of imagery our method improves Copilot times by an average of 1.24x across various LLMs and prompting techniques.
