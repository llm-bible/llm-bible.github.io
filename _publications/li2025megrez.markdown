---
layout: publication
title: 'Megrez-omni Technical Report'
authors: Boxun Li, Yadong Li, Zhiyuan Li, Congyi Liu, Weilin Liu, Guowei Niu, Zheyue Tan, Haiyang Xu, Zhuyu Yao, Tao Yuan, Dong Zhou, Yueqing Zhuang, Shengen Yan, Guohao Dai, Yu Wang
conference: "Arxiv"
year: 2025
bibkey: li2025megrez
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.15803'}
tags: ['Reinforcement Learning', 'Security', 'Multimodal Models', 'Applications']
---
In this work, we present the Megrez models, comprising a language model
(Megrez-3B-Instruct) and a multimodal model (Megrez-3B-Omni). These models are
designed to deliver fast inference, compactness, and robust edge-side
intelligence through a software-hardware co-design approach. Megrez-3B-Instruct
offers several advantages, including high accuracy, high speed, ease of use,
and a wide range of applications. Building on Megrez-3B-Instruct,
Megrez-3B-Omni is an on-device multimodal understanding LLM that supports
image, text, and audio analysis. It achieves state-of-the-art accuracy across
all three modalities and demonstrates strong versatility and robustness,
setting a new benchmark for multimodal AI models.
