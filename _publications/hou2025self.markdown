---
layout: publication
title: 'SMART: Self-generating And Self-validating Multi-dimensional Assessment For Llms'' Mathematical Problem Solving'
authors: Yujie Hou, Ting Zhang, Mei Wang, Xuetao Ma, Hua Huang
conference: "Arxiv"
year: 2025
bibkey: hou2025self
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.16646'}
tags: ['Reinforcement Learning', 'Tools']
---
Large Language Models have achieved remarkable results on a variety of mathematical benchmarks. However, concerns remain as to whether these successes reflect genuine mathematical reasoning or superficial pattern recognition. Common evaluation metrics, such as final answer accuracy, fail to disentangle the underlying competencies involved, offering limited diagnostic value. To address these limitations, we introduce SMART: a Self-Generating and Self-Validating Multi-Dimensional Assessment Framework. SMART decomposes mathematical problem solving into four distinct dimensions: understanding, reasoning, arithmetic, and reflection \& refinement. Each dimension is evaluated independently through tailored tasks, enabling interpretable and fine-grained analysis of LLM behavior. Crucially, SMART integrates an automated self-generating and self-validating mechanism to produce and verify benchmark data, ensuring both scalability and reliability. We apply SMART to 21 state-of-the-art open- and closed-source LLMs, uncovering significant discrepancies in their abilities across different dimensions. Our findings demonstrate the inadequacy of final answer accuracy as a sole metric and motivate a new holistic metric to better capture true problem-solving capabilities. Code and benchmarks will be released upon acceptance.
