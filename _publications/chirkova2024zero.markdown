---
layout: publication
title: Zero45;shot Cross45;lingual Transfer In Instruction Tuning Of Large Language Models
authors: Chirkova Nadezhda, Nikoulina Vassilina
conference: "Arxiv"
year: 2024
bibkey: chirkova2024zero
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.14778"}
tags: ['Pretraining Methods', 'Prompting', 'Training Techniques']
---
Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions but is under45;studied in multilingual settings. In this work we conduct a systematic study of zero45;shot cross45;lingual transfer in IT when an LLM is instruction45;tuned on English45;only data and then tested on user prompts in other languages. We advocate for the importance of evaluating various aspects of model responses in multilingual instruction following and investigate the influence of different model configuration choices. We find that cross45;lingual transfer does happen successfully in IT even if all stages of model training are English45;centric but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data. English45;trained LLMs are capable of generating correct45;language comprehensive and helpful responses in other languages but suffer from low factuality and may occasionally have fluency errors.
