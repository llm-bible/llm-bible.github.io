---
layout: publication
title: Numeracy from Literacy Data Science as an Emergent Skill from Large Language Models
authors: Noever David, Mckee Forrest
conference: "Arxiv"
year: 2023
bibkey: noever2023numeracy
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2301.13382"}
tags: ['Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Large language models (LLM) such as OpenAIs ChatGPT and GPT-3 offer unique testbeds for exploring the translation challenges of turning literacy into numeracy. Previous publicly-available transformer models from eighteen months prior and 1000 times smaller failed to provide basic arithmetic. The statistical analysis of four complex datasets described here combines arithmetic manipulations that cannot be memorized or encoded by simple rules. The work examines whether next-token prediction succeeds from sentence completion into the realm of actual numerical understanding. For example the work highlights cases for descriptive statistics on in-memory datasets that the LLM initially loads from memory or generates randomly using python libraries. The resulting exploratory data analysis showcases the models capabilities to group by or pivot categorical sums infer feature importance derive correlations and predict unseen test cases using linear regression. To extend the models testable range the research deletes and appends random rows such that recall alone cannot explain emergent numeracy.
