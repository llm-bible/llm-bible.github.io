---
layout: publication
title: Pargo Bridging Vision45;language With Partial And Global Views
authors: Wang An-lan, Shan Bin, Shi Wei, Lin Kun-yu, Fei Xiang, Tang Guozhi, Liao Lei, Tang Jingqun, Huang Can, Zheng Wei-shi
conference: "Arxiv"
year: 2024
bibkey: wang2024bridging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.12928"}
tags: ['Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning', 'Training Techniques']
---
This work presents ParGo a novel Partial45;Global projector designed to connect the vision and language modalities for Multimodal Large Language Models (MLLMs). Unlike previous works that rely on global attention45;based projectors our ParGo bridges the representation gap between the separately pre45;trained vision encoders and the LLMs by integrating global and partial views which alleviates the overemphasis on prominent regions. To facilitate the effective training of ParGo we collect a large45;scale detail45;captioned image45;text dataset named ParGoCap45;1M45;PT consisting of 1 million images paired with high45;quality captions. Extensive experiments on several MLLM benchmarks demonstrate the effectiveness of our ParGo highlighting its superiority in aligning vision and language modalities. Compared to conventional Q45;Former projector our ParGo achieves an improvement of 259.96 in MME benchmark. Furthermore our experiments reveal that ParGo significantly outperforms other projectors particularly in tasks that emphasize detail perception ability.
