---
layout: publication
title: 'All-in-one Image-grounded Conversational Agents'
authors: Da Ju, Kurt Shuster, Y-lan Boureau, Jason Weston
conference: "Arxiv"
year: 2019
bibkey: ju2019all
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1912.12394"}
tags: ['Agentic', 'Multimodal Models', 'Training Techniques', 'Model Architecture', 'RAG', 'Pretraining Methods', 'Transformer', 'Applications']
---
As single-task accuracy on individual language and image tasks has improved
substantially in the last few years, the long-term goal of a generally skilled
agent that can both see and talk becomes more feasible to explore. In this
work, we focus on leveraging individual language and image tasks, along with
resources that incorporate both vision and language towards that objective. We
design an architecture that combines state-of-the-art Transformer and ResNeXt
modules fed into a novel attentive multimodal module to produce a combined
model trained on many tasks. We provide a thorough analysis of the components
of the model, and transfer performance when training on one, some, or all of
the tasks. Our final models provide a single system that obtains good results
on all vision and language tasks considered, and improves the state-of-the-art
in image-grounded conversational applications.
