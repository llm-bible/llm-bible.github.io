---
layout: publication
title: ECONET Effective Continual Pretraining Of Language Models For Event Temporal Reasoning
authors: Han Rujun, Ren Xiang, Peng Nanyun
conference: "Arxiv"
year: 2020
bibkey: han2020effective
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2012.15283"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Tools', 'Training Techniques']
---
While pre45;trained language models (PTLMs) have achieved noticeable success on many NLP tasks they still struggle for tasks that require event temporal reasoning which is essential for event45;centric applications. We present a continual pre45;training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self45;supervised learning objectives to recover masked45;out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre45;training a PTLM with these objectives jointly we reinforce its attention to event and temporal information yielding enhanced capability on event temporal reasoning. This effective continual pre45;training framework for event temporal reasoning (ECONET) improves the PTLMs fine45;tuning performances across five relation extraction and question answering tasks and achieves new or on45;par state45;of45;the45;art performances in most of our downstream tasks.
