---
layout: publication
title: P45;transformer Towards Better Document45;to45;document Neural Machine Translation
authors: Li Yachao, Li Junhui, Jiang Jing, Tao Shimin, Yang Hao, Zhang Min
conference: "Arxiv"
year: 2022
bibkey: li2022p
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2212.05830"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Directly training a document45;to45;document (Doc2Doc) neural machine translation (NMT) via Transformer from scratch especially on small datasets usually fails to converge. Our dedicated probing tasks show that 1) both the absolute position and relative position information gets gradually weakened or even vanished once it reaches the upper encoder layers and 2) the vanishing of absolute position information in encoder output causes the training failure of Doc2Doc NMT. To alleviate this problem we propose a position45;aware Transformer (P45;Transformer) to enhance both the absolute and relative position information in both self45;attention and cross45;attention. Specifically we integrate absolute positional information i.e. position embeddings into the query45;key pairs both in self45;attention and cross45;attention through a simple yet effective addition operation. Moreover we also integrate relative position encoding in self45;attention. The proposed P45;Transformer utilizes sinusoidal position encoding and does not require any task45;specified position embedding segment embedding or attention mechanism. Through the above methods we build a Doc2Doc NMT model with P45;Transformer which ingests the source document and completely generates the target document in a sequence45;to45;sequence (seq2seq) way. In addition P45;Transformer can be applied to seq2seq45;based document45;to45;sentence (Doc2Sent) and sentence45;to45;sentence (Sent2Sent) translation. Extensive experimental results of Doc2Doc NMT show that P45;Transformer significantly outperforms strong baselines on widely45;used 9 document45;level datasets in 7 language pairs covering small45; middle45; and large45;scales and achieves a new state45;of45;the45;art. Experimentation on discourse phenomena shows that our Doc2Doc NMT models improve the translation quality in both BLEU and discourse coherence. We make our code available on Github.
