---
layout: publication
title: Large Language Models For Data Annotation A Survey
authors: Tan Zhen, Li Dawei, Wang Song, Beigi Alimohammad, Jiang Bohan, Bhattacharjee Amrita, Karami Mansooreh, Li Jundong, Cheng Lu, Liu Huan
conference: "Arxiv"
year: 2024
bibkey: tan2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.13446"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
Data annotation generally refers to the labeling or generating of raw data with relevant information which could be used for improving the efficacy of machine learning models. The process however is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs) exemplified by GPT-4 presents an unprecedented opportunity to automate the complicated process of data annotation. While existing surveys have extensively covered LLM architecture training and general applications we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects LLM-Based Annotation Generation LLM-Generated Annotations Assessment and LLM-Generated Annotations Utilization. Furthermore this survey includes an in-depth taxonomy of data types that LLMs can annotate a comprehensive review of learning strategies for models utilizing LLM-generated annotations and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation. Serving as a key guide this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation thereby fostering future advancements in this critical field.
