---
layout: publication
title: 'Redefining Information Retrieval Of Structured Database Via Large Language Models'
authors: Mingzhu Wang, Yuzhe Zhang, Qihang Zhao, Junyi Yang, Hong Zhang
conference: "Arxiv"
year: 2024
bibkey: wang2024redefining
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2405.05508'}
tags: ['Training Techniques', 'Applications', 'Tools', 'Fine-Tuning', 'Pretraining Methods']
---
Retrieval augmentation is critical when Language Models (LMs) exploit
non-parametric knowledge related to the query through external knowledge bases
before reasoning. The retrieved information is incorporated into LMs as context
alongside the query, enhancing the reliability of responses towards factual
questions. Prior researches in retrieval augmentation typically follow a
retriever-generator paradigm. In this context, traditional retrievers encounter
challenges in precisely and seamlessly extracting query-relevant information
from knowledge bases. To address this issue, this paper introduces a novel
retrieval augmentation framework called ChatLR that primarily employs the
powerful semantic understanding ability of Large Language Models (LLMs) as
retrievers to achieve precise and concise information retrieval. Additionally,
we construct an LLM-based search and question answering system tailored for the
financial domain by fine-tuning LLM on two tasks including Text2API and API-ID
recognition. Experimental results demonstrate the effectiveness of ChatLR in
addressing user queries, achieving an overall information retrieval accuracy
exceeding 98.8%.
