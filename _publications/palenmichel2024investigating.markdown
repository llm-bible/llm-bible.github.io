---
layout: publication
title: Investigating LLM Applications In E45;commerce
authors: Palen-michel Chester, Wang Ruixiang, Zhang Yipeng, Yu David, Xu Canran, Wu Zhe
conference: "Arxiv"
year: 2024
bibkey: palenmichel2024investigating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.12779"}
tags: ['Applications', 'Efficiency And Optimization', 'Fine Tuning', 'Merging', 'Training Techniques']
---
The emergence of Large Language Models (LLMs) has revolutionized natural language processing in various applications especially in e45;commerce. One crucial step before the application of such LLMs in these fields is to understand and compare the performance in different use cases in such tasks. This paper explored the efficacy of LLMs in the e45;commerce domain focusing on instruction45;tuning an open source LLM model with public e45;commerce datasets of varying sizes and comparing the performance with the conventional models prevalent in industrial applications. We conducted a comprehensive comparison between LLMs and traditional pre45;trained language models across specific tasks intrinsic to the e45;commerce domain namely classification generation summarization and named entity recognition (NER). Furthermore we examined the effectiveness of the current niche industrial application of very large LLM using in45;context learning in e45;commerce specific tasks. Our findings indicate that few45;shot inference with very large LLMs often does not outperform fine45;tuning smaller pre45;trained models underscoring the importance of task45;specific model optimization.Additionally we investigated different training methodologies such as single45;task training mixed45;task training and LoRA merging both within domain/tasks and between different tasks. Through rigorous experimentation and analysis this paper offers valuable insights into the potential effectiveness of LLMs to advance natural language processing capabilities within the e45;commerce industry.
