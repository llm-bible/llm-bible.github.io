---
layout: publication
title: Olympiadbench A Challenging Benchmark For Promoting AGI With Olympiad45;level Bilingual Multimodal Scientific Problems
authors: He Chaoqun, Luo Renjie, Bai Yuzhuo, Hu Shengding, Thai Zhen Leng, Shen Junhao, Hu Jinyi, Han Xu, Huang Yujie, Zhang Yuxiang, Liu Jie, Qi Lei, Liu Zhiyuan, Sun Maosong
conference: "Arxiv"
year: 2024
bibkey: he2024challenging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.14008"}
  - {name: "Code", url: "https://github.com/OpenBMB/OlympiadBench&#125;"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'RAG', 'Reinforcement Learning']
---
Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models new rigorous challenges are essential to gauge their advanced abilities. In this work we present OlympiadBench an Olympiad45;level bilingual multimodal scientific benchmark featuring 8476 problems from Olympiad45;level mathematics and physics competitions including the Chinese college entrance exam. Each problem is detailed with expert45;level annotations for step45;by45;step reasoning. Evaluating top45;tier models on OlympiadBench we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably the best45;performing model GPT45;4V attains an average score of 17.9737; on OlympiadBench with a mere 10.7437; in physics highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT45;4V points out prevalent issues with hallucinations knowledge omissions and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at url123;https://github.com/OpenBMB/OlympiadBench&#125;
