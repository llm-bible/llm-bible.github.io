---
layout: publication
title: Conversational Simulmt Efficient Simultaneous Translation With Large Language Models
authors: Wang Minghan, Vu Thuy-trang, Wang Yuxia, Shareghi Ehsan, Haffari Gholamreza
conference: "Arxiv"
year: 2024
bibkey: wang2024conversational
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.10552"}
tags: ['Applications', 'Efficiency And Optimization', 'Tools']
---
Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However this often comes at the expense of high inference cost and latency. In this paper we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.
