---
layout: publication
title: Efficiency In Focus Layernorm As A Catalyst For Fine45;tuning Medical Visual Language Pre45;trained Models
authors: Chen Jiawei, Yang Dingkang, Jiang Yue, Li Mingcheng, Wei Jinjie, Hou Xiaolu, Zhang Lihua
conference: "Arxiv"
year: 2024
bibkey: chen2024efficiency
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.16385"}
tags: ['Applications', 'Attention Mechanism', 'Efficiency And Optimization', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
In the realm of Medical Visual Language Models (Med45;VLMs) the quest for universal efficient fine45;tuning mechanisms remains paramount especially given researchers in interdisciplinary fields are often extremely short of training resources yet largely unexplored. Given the unique challenges in the medical domain such as limited data scope and significant domain45;specific requirements evaluating and adapting Parameter45;Efficient Fine45;Tuning (PEFT) methods specifically for Med45;VLMs is essential. Most of the current PEFT methods on Med45;VLMs have yet to be comprehensively investigated but mainly focus on adding some components to the models structure or input. However fine45;tuning intrinsic model components often yields better generality and consistency and its impact on the ultimate performance of Med45;VLMs has been widely overlooked and remains understudied. In this paper we endeavour to explore an alternative to traditional PEFT methods especially the impact of fine45;tuning LayerNorm layers FFNs and Attention layers on the Med45;VLMs. Our comprehensive studies span both small45;scale and large45;scale Med45;VLMs evaluating their performance under various fine45;tuning paradigms across tasks such as Medical Visual Question Answering and Medical Imaging Report Generation. The findings reveal unique insights into the effects of intrinsic parameter fine45;tuning methods on fine45;tuning Med45;VLMs to downstream tasks and expose fine45;tuning solely the LayerNorm layers not only surpasses the efficiency of traditional PEFT methods but also retains the models accuracy and generalization capabilities across a spectrum of medical downstream tasks. The experiments show LayerNorm fine45;tunings superior adaptability and scalability particularly in the context of large45;scale Med45;VLMs.
