---
layout: publication
title: Synth^2 Boosting Visual45;language Models With Synthetic Captions And Image Embeddings
authors: Sharifzadeh Sahand, Kaplanis Christos, Pathak Shreya, Kumaran Dharshan, Ilic Anastasija, Mitrovic Jovana, Blundell Charles, Banino Andrea
conference: "Arxiv"
year: 2024
bibkey: sharifzadeh2024boosting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.07750"}
tags: ['Pretraining Methods', 'RAG', 'Training Techniques']
---
The creation of high45;quality human45;labeled image45;caption datasets presents a significant bottleneck in the development of Visual45;Language Models (VLMs). In this work we investigate an approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image45;text pairs for efficient and effective VLM training. Our method employs a pretrained text45;to45;image model to synthesize image embeddings from captions generated by an LLM. Despite the text45;to45;image model and VLM initially being trained on the same data our approach leverages the image generators ability to create novel compositions resulting in synthetic image embeddings that expand beyond the limitations of the original dataset. Extensive experiments demonstrate that our VLM finetuned on synthetic data achieves comparable performance to models trained solely on human45;annotated data while requiring significantly less data. Furthermore we perform a set of analyses on captions which reveals that semantic diversity and balance are key aspects for better downstream performance. Finally we show that synthesizing images in the image embedding space is 2537; faster than in the pixel space. We believe our work not only addresses a significant challenge in VLM training but also opens up promising avenues for the development of self45;improving multi45;modal models.
