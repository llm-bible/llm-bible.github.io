---
layout: publication
title: Examining User45;friendly And Open45;sourced Large GPT Models A Survey On Language Multimodal And Scientific GPT Models
authors: Gao Kaiyuan, He Sunan, He Zhenyu, Lin Jiacheng, Pei Qizhi, Shao Jie, Zhang Wei
conference: "Arxiv"
year: 2023
bibkey: gao2023examining
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.14149"}
  - {name: "Code", url: "https://github.com/GPT&#45;Alternatives/gpt&#95;alternatives"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Survey Paper', 'Transformer']
---
Generative pre45;trained transformer (GPT) models have revolutionized the field of natural language processing (NLP) with remarkable performance in various tasks and also extend their power to multimodal domains. Despite their success large GPT models like GPT45;4 face inherent limitations such as considerable size high computational requirements complex deployment processes and closed development loops. These constraints restrict their widespread adoption and raise concerns regarding their responsible development and usage. The need for user45;friendly relatively small and open45;sourced alternative GPT models arises from the desire to overcome these limitations while retaining high performance. In this survey paper we provide an examination of alternative open45;sourced models of large GPTs focusing on user45;friendly and relatively small models that facilitate easier deployment and accessibility. Through this extensive survey we aim to equip researchers practitioners and enthusiasts with a thorough understanding of user45;friendly and relatively small open45;sourced models of large GPTs their current state challenges and future research directions inspiring the development of more efficient accessible and versatile GPT models that cater to the broader scientific community and advance the field of general artificial intelligence. The source contents are continuously updating in https://github.com/GPT&#45;Alternatives/gpt&#95;alternatives.
