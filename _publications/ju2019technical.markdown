---
layout: publication
title: Technical Report On Conversational Question Answering
authors: Ying Ju et al.
conference: Arxiv
year: 2019
citations: 46
bibkey: ju2019technical
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1909.10772'}]
tags: [Distillation, BERT, Efficiency and Optimization]
---
Conversational Question Answering is a challenging task since it requires
understanding of conversational history. In this project, we propose a new
system RoBERTa + AT +KD, which involves rationale tagging multi-task,
adversarial training, knowledge distillation and a linguistic post-process
strategy. Our single model achieves 90.4(F1) on the CoQA test set without data
augmentation, outperforming the current state-of-the-art single model by 2.6%
F1.