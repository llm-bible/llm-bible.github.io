---
layout: publication
title: 'Beyond Next Word Prediction: Developing Comprehensive Evaluation Frameworks For Measuring LLM Performance On Real World Applications'
authors: Vishakha Agrawal, Archie Chaudhury, Shreya Agrawal
conference: "Arxiv"
year: 2025
bibkey: agrawal2025beyond
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.04828"}
tags: ['Responsible AI', 'Model Architecture', 'Tools', 'Reinforcement Learning', 'Language Modeling', 'Ethics and Bias', 'Prompting', 'Applications']
---
While Large Language Models (LLMs) are fundamentally next-token prediction
systems, their practical applications extend far beyond this basic function.
From natural language processing and text generation to conversational
assistants and software use, LLMs have numerous use-cases, and have already
acquired a significant degree of enterprise adoption. To evaluate such models,
static evaluation datasets, consisting of a set of prompts and their
corresponding ground truths, are often used to benchmark the efficacy of the
model for a particular task. In this paper, we provide the basis for a more
comprehensive evaluation framework, based upon a traditional game and
tool-based architecture that enables a more overarching measurement of a
model's capabilities. For simplicity, we provide a generalized foundation that
can be extended, without significant alteration, to numerous scenarios, from
specific use cases such as supply chain management or financial reasoning, to
abstract measurements such as ethics or safety.
