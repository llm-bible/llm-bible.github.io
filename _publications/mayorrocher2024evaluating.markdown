---
layout: publication
title: 'Evaluating Large Language Models With Tests Of Spanish As A Foreign Language: Pass Or Fail?'
authors: Marina Mayor-rocher, Nina Melero, Elena Merino-gómez, María Grandury, Javier Conde, Pedro Reviriego
conference: "Arxiv"
year: 2024
bibkey: mayorrocher2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.15334"}
tags: ['RAG']
---
Large Language Models (LLMs) have been profusely evaluated on their ability
to answer questions on many topics and their performance on different natural
language understanding tasks. Those tests are usually conducted in English, but
most LLM users are not native English speakers. Therefore, it is of interest to
analyze how LLMs understand other languages at different levels: from
paragraphs to morphems. In this paper, we evaluate the performance of
state-of-the-art LLMs in TELEIA, a recently released benchmark with similar
questions to those of Spanish exams for foreign students, covering topics such
as reading comprehension, word formation, meaning and compositional semantics,
and grammar. The results show that LLMs perform well at understanding Spanish
but are still far from achieving the level of a native speaker in terms of
grammatical competence.
