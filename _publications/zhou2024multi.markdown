---
layout: publication
title: Multi45;stage Balanced Distillation Addressing Long45;tail Challenges In Sequence45;level Knowledge Distillation
authors: Zhou Yuhang, Zhu Jing, Xu Paiheng, Liu Xiaoyu, Wang Xiyao, Koutra Danai, Ai Wei, Huang Furong
conference: "Arxiv"
year: 2024
bibkey: zhou2024multi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.13114"}
tags: ['Distillation', 'Efficiency And Optimization', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Large language models (LLMs) have significantly advanced various natural language processing tasks but deploying them remains computationally expensive. Knowledge distillation (KD) is a promising solution enabling the transfer of capabilities from larger teacher LLMs to more compact student models. Particularly sequence45;level KD which distills rationale45;based reasoning processes instead of merely final outcomes shows great potential in enhancing students reasoning capabilities. However current methods struggle with sequence level KD under long45;tailed data distributions adversely affecting generalization on sparsely represented domains. We introduce the Multi45;Stage Balanced Distillation (BalDistill) framework which iteratively balances training data within a fixed computational budget. By dynamically selecting representative head domain examples and synthesizing tail domain examples BalDistill achieves state45;of45;the45;art performance across diverse long45;tailed datasets enhancing both the efficiency and efficacy of the distilled models.
