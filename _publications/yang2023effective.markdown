---
layout: publication
title: Effective Distillation Of Table45;based Reasoning Ability From Llms
authors: Yang Bohao, Tang Chen, Zhao Kun, Xiao Chenghao, Lin Chenghua
conference: "Arxiv"
year: 2023
bibkey: yang2023effective
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.13182"}
  - {name: "Code", url: "https://github.com/Bernard&#45;Yang/DistillTableCoT"}
tags: ['Applications', 'Distillation', 'Efficiency And Optimization', 'Has Code', 'Language Modeling', 'RAG']
---
Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However their enormous parameter size and extremely high requirements for compute power pose challenges for their practical deployment. Recent research has revealed that specific capabilities of LLMs such as numerical reasoning can be transferred to smaller models through distillation. Some studies explore the potential of leveraging LLMs to perform table45;based reasoning. However there has been no prior work focusing on table reasoning skills in smaller models specifically tailored for scientific table45;to45;text generation tasks. In this paper we propose a novel table45;based reasoning distillation approach with the aim of distilling LLMs into tailored smaller models. Our experimental results have shown that a 220 million parameter model (Flan45;T545;base) fine45;tuned using distilled data not only achieves a significant improvement compared to traditionally fine45;tuned baselines but also surpasses specific LLMs on a scientific table45;to45;text generation dataset. Our code is available at https://github.com/Bernard&#45;Yang/DistillTableCoT.
