---
layout: publication
title: LLMGuard Guarding Against Unsafe LLM Behavior
authors: Goyal Shubh, Hira Medha, Mishra Shubham, Goyal Sukriti, Goel Arnav, Dadu Niharika, Db Kirushikesh, Mehta Sameep, Madaan Nishtha
conference: "Arxiv"
year: 2024
bibkey: goyal2024llmguard
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.00826"}
tags: ['ARXIV', 'Applications', 'LLM', 'Tools']
---
Although the rise of Large Language Models (LLMs) in enterprise settings brings new opportunities and capabilities it also brings challenges such as the risk of generating inappropriate biased or misleading content that violates regulations and can have legal concerns. To alleviate this we present LLMGuard a tool that monitors user interactions with an LLM application and flags content against specific behaviours or conversation topics. To do this robustly LLMGuard employs an ensemble of detectors.
