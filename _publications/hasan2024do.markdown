---
layout: publication
title: Do Large Language Models Speak All Languages Equally A Comparative Study In Low45;resource Settings
authors: Hasan Md. Arid, Tarannum Prerona, Dey Krishno, Razzak Imran, Naseem Usman
conference: "Arxiv"
year: 2024
bibkey: hasan2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.02237"}
tags: ['Attention Mechanism', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Large language models (LLMs) have garnered significant interest in natural language processing (NLP) particularly their remarkable performance in various downstream tasks in resource45;rich languages. Recent studies have highlighted the limitations of LLMs in low45;resource languages primarily focusing on binary classification tasks and giving minimal attention to South Asian languages. These limitations are primarily attributed to constraints such as dataset scarcity computational costs and research gaps specific to low45;resource languages. To address this gap we present datasets for sentiment and hate speech tasks by translating from English to Bangla Hindi and Urdu facilitating research in low45;resource language processing. Further we comprehensively examine zero45;shot learning using multiple LLMs in English and widely spoken South Asian languages. Our findings indicate that GPT45;4 consistently outperforms Llama 2 and Gemini with English consistently demonstrating superior performance across diverse tasks compared to low45;resource languages. Furthermore our analysis reveals that natural language inference (NLI) exhibits the highest performance among the evaluated tasks with GPT45;4 demonstrating superior capabilities.
