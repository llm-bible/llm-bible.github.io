---
layout: publication
title: Do Large Language Models Speak All Languages Equally A Comparative Study in Low-Resource Settings
authors: Hasan Md. Arid, Tarannum Prerona, Dey Krishno, Razzak Imran, Naseem Usman
conference: "Arxiv"
year: 2024
bibkey: hasan2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.02237"}
tags: ['Attention Mechanism', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Large language models (LLMs) have garnered significant interest in natural language processing (NLP) particularly their remarkable performance in various downstream tasks in resource-rich languages. Recent studies have highlighted the limitations of LLMs in low-resource languages primarily focusing on binary classification tasks and giving minimal attention to South Asian languages. These limitations are primarily attributed to constraints such as dataset scarcity computational costs and research gaps specific to low-resource languages. To address this gap we present datasets for sentiment and hate speech tasks by translating from English to Bangla Hindi and Urdu facilitating research in low-resource language processing. Further we comprehensively examine zero-shot learning using multiple LLMs in English and widely spoken South Asian languages. Our findings indicate that GPT-4 consistently outperforms Llama 2 and Gemini with English consistently demonstrating superior performance across diverse tasks compared to low-resource languages. Furthermore our analysis reveals that natural language inference (NLI) exhibits the highest performance among the evaluated tasks with GPT-4 demonstrating superior capabilities.
