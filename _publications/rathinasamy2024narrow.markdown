---
layout: publication
title: Narrow Transformer Starcoder45;based Java45;lm For Desktop
authors: Rathinasamy Kamalkumar, J Balaji A, Kumar Ankush, Gayari Gagan, K Harshini, Mondal Rajab Ali, S Sreenivasa Raghavan K, Singh Swayam, Tarafdar Mohammed Rafee
conference: "Arxiv"
year: 2024
bibkey: rathinasamy2024narrow
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.03941"}
tags: ['Model Architecture', 'Pretraining Methods', 'Transformer']
---
This paper presents NT45;Java45;1.1B an open45;source specialized code language model built on StarCoderBase45;1.1B designed for coding tasks in Java programming. NT45;Java45;1.1B achieves state45;of45;the45;art performance surpassing its base model and majority of other models of similar size on MultiPL45;E Java code benchmark. While there have been studies on extending large generic pre45;trained models to improve proficiency in specific programming languages like Python similar investigations on small code models for other programming languages are lacking. Large code models require specialized hardware like GPUs for inference highlighting the need for research into building small code models that can be deployed on developer desktops. This paper addresses this research gap by focusing on the development of a small Java code model NT45;Java45;1.1B and its quantized versions which performs comparably to open models around 1.1B on MultiPL45;E Java code benchmarks making them ideal for desktop deployment. This paper establishes the foundation for specialized models across languages and sizes for a family of NT Models.
