---
layout: publication
title: Exploring Straightforward Conversational Red-teaming
authors: Kour George, Zwerdling Naama, Zalmanovici Marcel, Anaby-tavor Ateret, Fandina Ora Nova, Farchi Eitan
conference: "Arxiv"
year: 2024
bibkey: kour2024exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.04822"}
tags: ['Applications', 'Security']
---
Large language models (LLMs) are increasingly used in business dialogue systems but they pose security and ethical risks. Multi-turn conversations where context influences the models behavior can be exploited to produce undesired responses. In this paper we examine the effectiveness of utilizing off-the-shelf LLMs in straightforward red-teaming approaches where an attacker LLM aims to elicit undesired output from a target LLM comparing both single-turn and conversational red-teaming tactics. Our experiments offer insights into various usage strategies that significantly affect their performance as red teamers. They suggest that off-the-shelf models can act as effective red teamers and even adjust their attack strategy based on past attempts although their effectiveness decreases with greater alignment.
