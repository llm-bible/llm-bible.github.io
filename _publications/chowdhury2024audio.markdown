---
layout: publication
title: Meerkat Audio45;visual Large Language Model For Grounding In Space And Time
authors: Chowdhury Sanjoy, Nag Sayan, Dasgupta Subhrajyoti, Chen Jun, Elhoseiny Mohamed, Gao Ruohan, Manocha Dinesh
conference: "Arxiv"
year: 2024
bibkey: chowdhury2024audio
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.01851"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'RAG']
---
Leveraging Large Language Models remarkable proficiency in text45;based tasks recent works on Multi45;modal LLMs (MLLMs) extend them to other modalities like vision and audio. However the progress in these directions has been mostly focused on tasks that only require a coarse45;grained understanding of the audio45;visual semantics. We present Meerkat an audio45;visual LLM equipped with a fine45;grained understanding of image and audio both spatially and temporally. With a new modality alignment module based on optimal transport and a cross45;attention module that enforces audio45;visual consistency Meerkat can tackle challenging tasks such as audio referred image grounding image guided audio temporal localization and audio45;visual fact45;checking. Moreover we carefully curate a large dataset AVFIT that comprises 3M instruction tuning samples collected from open45;source datasets and introduce MeerkatBench that unifies five challenging audio45;visual tasks. We achieve state45;of45;the45;art performance on all these downstream tasks with a relative improvement of up to 37.1237;.
