---
layout: publication
title: 'Decoding Decoded: Understanding Hyperparameter Effects In Open-ended Text Generation'
authors: Esteban Garces Arias, Meimingwei Li, Christian Heumann, Matthias AÃŸenmacher
conference: "Arxiv"
year: 2024
bibkey: arias2024decoding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.06097"}
tags: ['Applications', 'Language Modeling']
---
Decoding strategies for generative large language models (LLMs) are a
critical but often underexplored aspect of text generation tasks. Guided by
specific hyperparameters, these strategies aim to transform the raw probability
distributions produced by language models into coherent, fluent text. In this
study, we undertake a large-scale empirical assessment of a range of decoding
methods, open-source LLMs, textual domains, and evaluation protocols to
determine how hyperparameter choices shape the outputs. Our experiments include
both factual (e.g., news) and creative (e.g., fiction) domains, and incorporate
a broad suite of automatic evaluation metrics alongside human judgments.
Through extensive sensitivity analyses, we distill practical recommendations
for selecting and tuning hyperparameters, noting that optimal configurations
vary across models and tasks. By synthesizing these insights, this study
provides actionable guidance for refining decoding strategies, enabling
researchers and practitioners to achieve higher-quality, more reliable, and
context-appropriate text generation outcomes.
