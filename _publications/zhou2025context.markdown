---
layout: publication
title: 'CCJA: Context-coherent Jailbreak Attack For Aligned Large Language Models'
authors: Guanghao Zhou, Panjia Qiu, Mingyuan Fan, Cen Chen, Mingyuan Chu, Xin Zhang, Jun Zhou
conference: "Arxiv"
year: 2025
bibkey: zhou2025context
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.11379"}
tags: ['Responsible AI', 'Efficiency and Optimization', 'Reinforcement Learning', 'Masked Language Model', 'Security', 'Pretraining Methods', 'BERT', 'Prompting']
---
Despite explicit alignment efforts for large language models (LLMs), they can
still be exploited to trigger unintended behaviors, a phenomenon known as
"jailbreaking." Current jailbreak attack methods mainly focus on discrete
prompt manipulations targeting closed-source LLMs, relying on manually crafted
prompt templates and persuasion rules. However, as the capabilities of
open-source LLMs improve, ensuring their safety becomes increasingly crucial.
In such an environment, the accessibility of model parameters and gradient
information by potential attackers exacerbates the severity of jailbreak
threats. To address this research gap, we propose a novel
\underline\{C\}ontext-\underline\{C\}oherent \underline\{J\}ailbreak
\underline\{A\}ttack (CCJA). We define jailbreak attacks as an optimization
problem within the embedding space of masked language models. Through
combinatorial optimization, we effectively balance the jailbreak attack success
rate with semantic coherence. Extensive evaluations show that our method not
only maintains semantic consistency but also surpasses state-of-the-art
baselines in attack effectiveness. Additionally, by integrating semantically
coherent jailbreak prompts generated by our method into widely used black-box
methodologies, we observe a notable enhancement in their success rates when
targeting closed-source commercial LLMs. This highlights the security threat
posed by open-source LLMs to commercial counterparts. We will open-source our
code if the paper is accepted.
