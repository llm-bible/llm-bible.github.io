---
layout: publication
title: 'Combating Toxic Language: A Review Of Llm-based Strategies For Software Engineering'
authors: Hao Zhuo, Yicheng Yang, Kewen Peng
conference: "Arxiv"
year: 2025
bibkey: zhuo2025combating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.15439"}
tags: ['RAG', 'Survey Paper', 'Reinforcement Learning']
---
Large Language Models (LLMs) have become integral to software engineering
(SE), where they are increasingly used in development workflows. However, their
widespread use raises concerns about the presence and propagation of toxic
language--harmful or offensive content that can foster exclusionary
environments. This paper provides a comprehensive review of recent research on
toxicity detection and mitigation, focusing on both SE-specific and
general-purpose datasets. We examine annotation and preprocessing techniques,
assess detection methodologies, and evaluate mitigation strategies,
particularly those leveraging LLMs. Additionally, we conduct an ablation study
demonstrating the effectiveness of LLM-based rewriting for reducing toxicity.
By synthesizing existing work and identifying open challenges, this review
highlights key areas for future research to ensure the responsible deployment
of LLMs in SE and beyond.
