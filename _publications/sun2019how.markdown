---
layout: publication
title: How To Fine-tune BERT For Text Classification?
authors: Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang
conference: Arxiv
year: 2019
citations: 779
bibkey: sun2019how
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1905.05583'}]
tags: [Fine-Tuning, BERT, Transformer, Pre-Training]
---
Language model pre-training has proven to be useful in learning universal
language representations. As a state-of-the-art language model pre-training
model, BERT (Bidirectional Encoder Representations from Transformers) has
achieved amazing results in many language understanding tasks. In this paper,
we conduct exhaustive experiments to investigate different fine-tuning methods
of BERT on text classification task and provide a general solution for BERT
fine-tuning. Finally, the proposed solution obtains new state-of-the-art
results on eight widely-studied text classification datasets.