---
layout: publication
title: 'Demo-craft: Using In-context Learning To Improve Code Generation In Large Language Models'
authors: Nirmal Joshua Kapu, Mihit Sreejith
conference: "Arxiv"
year: 2024
bibkey: kapu2024demo
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.00865"}
tags: ['Applications', 'RAG', 'Reinforcement Learning', 'Prompting', 'In-Context Learning']
---
Generating executable code from natural language instructions using Large
Language Models (LLMs) poses challenges such as semantic ambiguity and
understanding taskspecific contexts. To address these issues, we propose a
system called DemoCraft, which enhances code generation by leveraging
in-context learning and demonstration selection, combined with latent concept
learning. Latent concept learning introduces additional concept tokens, which
are trainable embeddings that capture task-specific knowledge. We then test our
system on two major datasets: MBPP and Humaneval. Our experimental results
demonstrate that the proposed system achieves an approximate 2x increase in the
pass@k metric compared to baseline models. Furthermore, we introduce two novel
evaluation metrics: correctness@k and similarity@k. Our empirical studies
indicate that our system attains nearly a 3x improvement in these metrics as
well.
