---
layout: publication
title: Gradual Fine-tuning For Low-resource Domain Adaptation
authors: Haoran Xu et al.
conference: Adapt-NLP EACL 2021
year: 2021
citations: 18
bibkey: xu2021gradual
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2103.02205'}]
tags: [Fine-Tuning]
---
Fine-tuning is known to improve NLP models by adapting an initial model
trained on more plentiful but less domain-salient examples to data in a target
domain. Such domain adaptation is typically done using one stage of
fine-tuning. We demonstrate that gradually fine-tuning in a multi-stage process
can yield substantial further gains and can be applied without modifying the
model or learning objective.