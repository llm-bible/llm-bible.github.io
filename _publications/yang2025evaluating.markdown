---
layout: publication
title: 'Evaluating The Generalization Capabilities Of Large Language Models On Code Reasoning'
authors: Rem Yang, Julian Dai, Nikos Vasilakis, Martin Rinard
conference: "Arxiv"
year: 2025
bibkey: yang2025evaluating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.05518'}
tags: ['Reinforcement Learning']
---
We assess how the code reasoning abilities of large language models (LLMs)
generalize to different kinds of programs. We present techniques for obtaining
in- and out-of-distribution programs with different characteristics: code
sampled from a domain-specific language, code automatically generated by an
LLM, code collected from competitive programming contests, and mutated versions
of these programs. We also present an experimental methodology for evaluating
LLM generalization by comparing their performance on these programs. We perform
an extensive evaluation across 10 state-of-the-art models from the past year,
obtaining insights into their generalization capabilities over time and across
different classes of programs. Our results highlight that while earlier models
exhibit behavior consistent with pattern matching, the latest models exhibit
strong generalization abilities on code reasoning.
