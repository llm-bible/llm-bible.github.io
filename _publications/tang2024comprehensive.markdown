---
layout: publication
title: Llmbox: A Comprehensive Library For Large Language Models
authors: Tang Tianyi, Hu Yiwen, Li Bingqian, Luo Wenyang, Qin Zijing, Sun Haoxiang, Wang Jiapeng, Xu Shiyi, Cheng Xiaoxue, Guo Geyang, Peng Han, Zheng Bowen, Tang Yiru, Min Yingqian, Chen Yushuo, Chen Jie, Zhao Yuanqian, Ding Luran, Wang Yuhao, Dong Zican, Xia Chunxuan, Li Junyi, Zhou Kun, Zhao Wayne Xin, Wen Ji-rong
conference: "Arxiv"
year: 2024
bibkey: tang2024comprehensive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.05563"}
  - {name: "Code", url: "https://github.com/RUCAIBox/LLMBox"}
tags: ['Efficiency And Optimization', 'Has Code', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
To facilitate the research on large language models (LLMs) this paper presents a comprehensive and unified library LLMBox to ease the development use and evaluation of LLMs. This library is featured with three main merits (1) a unified data interface that supports the flexible implementation of various training strategies (2) a comprehensive evaluation that covers extensive tasks datasets and models and (3) more practical consideration especially on user-friendliness and efficiency. With our library users can easily reproduce existing methods train new models and conduct comprehensive performance comparisons. To rigorously test LLMBox we conduct extensive experiments in a diverse coverage of evaluation settings and experimental results demonstrate the effectiveness and efficiency of our library in supporting various implementations related to LLMs. The detailed introduction and usage guidance can be found at https://github.com/RUCAIBox/LLMBox."
