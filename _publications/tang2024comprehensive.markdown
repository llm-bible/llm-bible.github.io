---
layout: publication
title: 'Llmbox: A Comprehensive Library For Large Language Models'
authors: Tianyi Tang, Yiwen Hu, Bingqian Li, Wenyang Luo, Zijing Qin, Haoxiang Sun, Jiapeng Wang, Shiyi Xu, Xiaoxue Cheng, Geyang Guo, Han Peng, Bowen Zheng, Yiru Tang, Yingqian Min, Yushuo Chen, Jie Chen, Yuanqian Zhao, Luran Ding, Yuhao Wang, Zican Dong, Chunxuan Xia, Junyi Li, Kun Zhou, Wayne Xin Zhao, Ji-rong Wen
conference: "Arxiv"
year: 2024
bibkey: tang2024comprehensive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.05563"}
  - {name: "Code", url: "https://github.com/RUCAIBox/LLMBox"}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Tools', 'Reinforcement Learning', 'RAG', 'Has Code']
---
To facilitate the research on large language models (LLMs), this paper
presents a comprehensive and unified library, LLMBox, to ease the development,
use, and evaluation of LLMs. This library is featured with three main merits:
(1) a unified data interface that supports the flexible implementation of
various training strategies, (2) a comprehensive evaluation that covers
extensive tasks, datasets, and models, and (3) more practical consideration,
especially on user-friendliness and efficiency. With our library, users can
easily reproduce existing methods, train new models, and conduct comprehensive
performance comparisons. To rigorously test LLMBox, we conduct extensive
experiments in a diverse coverage of evaluation settings, and experimental
results demonstrate the effectiveness and efficiency of our library in
supporting various implementations related to LLMs. The detailed introduction
and usage guidance can be found at https://github.com/RUCAIBox/LLMBox.
