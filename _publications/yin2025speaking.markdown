---
layout: publication
title: 'SWI: Speaking With Intent In Large Language Models'
authors: Yuwei Yin, Eunjeong Hwang, Giuseppe Carenini
conference: "Arxiv"
year: 2025
bibkey: yin2025speaking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.21544"}
tags: ['Tools', 'Applications', 'Interpretability and Explainability', 'Reinforcement Learning', 'Prompting']
---
Intent, typically clearly formulated and planned, functions as a cognitive
framework for reasoning and problem-solving. This paper introduces the concept
of Speaking with Intent (SWI) in large language models (LLMs), where the
explicitly generated intent encapsulates the model's underlying intention and
provides high-level planning to guide subsequent analysis and communication. By
emulating deliberate and purposeful thoughts in the human mind, SWI is
hypothesized to enhance the reasoning capabilities and generation quality of
LLMs. Extensive experiments on mathematical reasoning benchmarks consistently
demonstrate the superiority of Speaking with Intent over Baseline (i.e.,
generation without explicit intent). Moreover, SWI outperforms answer-trigger
prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive
performance with the strong method ARR (Analyzing, Retrieving, and Reasoning).
Additionally, the effectiveness and generalizability of SWI are solidified on
reasoning-intensive question answering (QA) and text summarization benchmarks,
where SWI brings consistent improvement to the Baseline generation. In text
summarization, SWI-generated summaries exhibit greater accuracy, conciseness,
and factual correctness, with fewer hallucinations. Furthermore, human
evaluations verify the coherence, effectiveness, and interpretability of the
intent produced by SWI. This proof-of-concept study creates a novel avenue for
enhancing LLMs' reasoning abilities with cognitive notions.
