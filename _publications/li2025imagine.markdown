---
layout: publication
title: 'Imagine While Reasoning In Space: Multimodal Visualization-of-thought'
authors: Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan VuliÄ‡, Furu Wei
conference: "Arxiv"
year: 2025
bibkey: li2025imagine
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.07542"}
tags: ['Prompting', 'Pretraining Methods', 'Multimodal Models', 'GPT']
---
Chain-of-Thought (CoT) prompting has proven highly effective for enhancing
complex reasoning in Large Language Models (LLMs) and Multimodal Large Language
Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks.
Nonetheless, human cognition extends beyond language alone, enabling the
remarkable capability to think in both words and images. Inspired by this
mechanism, we propose a new reasoning paradigm, Multimodal
Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by
generating image visualizations of their reasoning traces. To ensure
high-quality visualization, we introduce token discrepancy loss into
autoregressive MLLMs. This innovation significantly improves both visual
coherence and fidelity. We validate this approach through several dynamic
spatial reasoning tasks. Experimental results reveal that MVoT demonstrates
competitive performance across tasks. Moreover, it exhibits robust and reliable
improvements in the most challenging scenarios where CoT fails. Ultimately,
MVoT establishes new possibilities for complex reasoning tasks where visual
thinking can effectively complement verbal reasoning.
