---
layout: publication
title: Text45;to45;model Text45;conditioned Neural Network Diffusion For Train45;once45;for45;all Personalization
authors: Li Zexi, Gao Lingzhi, Wu Chao
conference: "Arxiv"
year: 2024
bibkey: li2024text
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.14132"}
tags: ['Merging', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'RAG', 'Reinforcement Learning', 'Transformer']
---
Generative artificial intelligence (GenAI) has made significant progress in understanding world knowledge and generating content from human languages across various modalities like text45;to45;text large language models text45;to45;image stable diffusion and text45;to45;video Sora. While in this paper we investigate the capability of GenAI for text45;to45;model generation to see whether GenAI can comprehend hyper45;level knowledge embedded within AI itself parameters. Specifically we study a practical scenario termed train45;once45;for45;all personalization aiming to generate personalized models for diverse end45;users and tasks using text prompts. Inspired by the recent emergence of neural network diffusion we present Tina a text45;conditioned neural network diffusion for train45;once45;for45;all personalization. Tina leverages a diffusion transformer model conditioned on task descriptions embedded using a CLIP model. Despite the astronomical number of potential personalized tasks (e.g. 1.73Ã—10^123;13125;) by our design Tina demonstrates remarkable in45;distribution and out45;of45;distribution generalization even trained on small datasets (sim 1000). We further verify whether and how Tina understands world knowledge by analyzing its capabilities under zero45;shot/few45;shot image prompts different numbers of personalized classes prompts of natural language descriptions and predicting unseen entities.
