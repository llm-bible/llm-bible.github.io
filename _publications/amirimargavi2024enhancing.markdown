---
layout: publication
title: 'Enhancing Answer Reliability Through Inter-model Consensus Of Large Language Models'
authors: Alireza Amiri-margavi, Iman Jebellat, Ehsan Jebellat, Seyed Pouyan Mousavi Davoudi
conference: "Arxiv"
year: 2024
bibkey: amirimargavi2024enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.16797"}
tags: ['Model Architecture', 'GPT', 'Tools']
---
We propose a collaborative framework in which multiple large language models
-- including GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and
Gemini-1.5-Flash -- generate and answer complex, PhD-level statistical
questions when definitive ground truth is unavailable. Our study examines how
inter-model consensus improves both response reliability and identifies the
quality of the generated questions. Employing chi-square tests, Fleiss' Kappa,
and confidence interval analysis, we quantify consensus rates and inter-rater
agreement to assess both response precision and question quality. Key results
indicate that Claude and GPT-4 produce well-structured, less ambiguous
questions with a higher inter-rater agreement, as shown by narrower confidence
intervals and greater alignment with question-generating models. In contrast,
Gemini and LLaMA exhibit greater variability and lower reliability in question
formulation. These findings demonstrate that collaborative interactions among
large language models enhance response reliability and provide valuable
insights for optimizing AI-driven collaborative reasoning systems.
