---
layout: publication
title: Audio45;visual LLM For Video Understanding
authors: Shu Fangxun, Zhang Lei, Jiang Hao, Xie Cihang
conference: "Arxiv"
year: 2023
bibkey: shu2023audio
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.06720"}
tags: ['GPT', 'Model Architecture', 'Multimodal Models', 'Training Techniques']
---
This paper presents Audio45;Visual LLM a Multimodal Large Language Model that takes both visual and auditory inputs for holistic video understanding. A key design is the modality45;augmented training which involves the integration of modality45;specific tokens engineered to activate the appropriate visual and/or auditory encoder selectively. This mechanism is pivotal in enabling end45;to45;end joint training with video data at different modalities including visual45;only audio45;only and audio45;visual formats. Moreover we introduce a high45;quality video instruction dataset derived from GPT45;4. This dataset allows Audio45;Visual LLM to adeptly process a variety of task45;oriented video instructions ranging from multi45;turn conversations and audio45;visual narratives to complex reasoning tasks. Extensive experiments demonstrate that Audio45;Visual LLM impressively achieves strong zero45;shot results across a range of video understanding tasks. For example Audio45;Visual LLM achieves an accuracy of 53.737; on MSRVTT45;QA outperforming non45;LLM45;based InterVideo by 6.637; and LLM45;based Valley by 4.437; respectively. Additionally our Audio45;Visual LLM also achieves competitive performance on audio tasks (e.g. AudioCaps).
