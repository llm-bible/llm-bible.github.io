---
layout: publication
title: 'Self-explanation Prompting Improves Dialogue Understanding In Large Language Models'
authors: Gao Haoyu, Lin Ting-en, Li Hangyu, Yang Min, Wu Yuchuan, Ma Wentao, Li Yongbin
conference: "Arxiv"
year: 2023
bibkey: gao2023self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.12940"}
tags: ['Few Shot', 'Interpretability And Explainability', 'Prompting', 'Uncategorized']
---
Task-oriented dialogue (TOD) systems facilitate users in executing various
activities via multi-turn dialogues, but Large Language Models (LLMs) often
struggle to comprehend these intricate contexts. In this study, we propose a
novel "Self-Explanation" prompting strategy to enhance the comprehension
abilities of LLMs in multi-turn dialogues. This task-agnostic approach requires
the model to analyze each dialogue utterance before task execution, thereby
improving performance across various dialogue-centric tasks. Experimental
results from six benchmark datasets confirm that our method consistently
outperforms other zero-shot prompts and matches or exceeds the efficacy of
few-shot prompts, demonstrating its potential as a powerful tool in enhancing
LLMs' comprehension in complex dialogue tasks.
