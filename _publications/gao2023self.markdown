---
layout: publication
title: Self45;explanation Prompting Improves Dialogue Understanding In Large Language Models
authors: Gao Haoyu, Lin Ting-en, Li Hangyu, Yang Min, Wu Yuchuan, Ma Wentao, Li Yongbin
conference: "Arxiv"
year: 2023
bibkey: gao2023self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.12940"}
tags: ['Interpretability And Explainability', 'Pretraining Methods', 'Prompting']
---
Task45;oriented dialogue (TOD) systems facilitate users in executing various activities via multi45;turn dialogues but Large Language Models (LLMs) often struggle to comprehend these intricate contexts. In this study we propose a novel Self45;Explanation prompting strategy to enhance the comprehension abilities of LLMs in multi45;turn dialogues. This task45;agnostic approach requires the model to analyze each dialogue utterance before task execution thereby improving performance across various dialogue45;centric tasks. Experimental results from six benchmark datasets confirm that our method consistently outperforms other zero45;shot prompts and matches or exceeds the efficacy of few45;shot prompts demonstrating its potential as a powerful tool in enhancing LLMs comprehension in complex dialogue tasks.
