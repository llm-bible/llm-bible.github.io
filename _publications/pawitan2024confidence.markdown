---
layout: publication
title: 'Confidence In The Reasoning Of Large Language Models'
authors: Yudi Pawitan, Chris Holmes
conference: "Arxiv"
year: 2024
bibkey: pawitan2024confidence
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.15296"}
tags: ['GPT', 'Prompting', 'Model Architecture', 'Reinforcement Learning']
---
There is a growing literature on reasoning by large language models (LLMs),
but the discussion on the uncertainty in their responses is still lacking. Our
aim is to assess the extent of confidence that LLMs have in their answers and
how it correlates with accuracy. Confidence is measured (i) qualitatively in
terms of persistence in keeping their answer when prompted to reconsider, and
(ii) quantitatively in terms of self-reported confidence score. We investigate
the performance of three LLMs -- GPT4o, GPT4-turbo and Mistral -- on two
benchmark sets of questions on causal judgement and formal fallacies and a set
of probability and statistical puzzles and paradoxes. Although the LLMs show
significantly better performance than random guessing, there is a wide
variability in their tendency to change their initial answers. There is a
positive correlation between qualitative confidence and accuracy, but the
overall accuracy for the second answer is often worse than for the first
answer. There is a strong tendency to overstate the self-reported confidence
score. Confidence is only partially explained by the underlying token-level
probability. The material effects of prompting on qualitative confidence and
the strong tendency for overconfidence indicate that current LLMs do not have
any internally coherent sense of confidence.
