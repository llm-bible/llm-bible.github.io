---
layout: publication
title: 'Dogerm: Equipping Reward Models With Domain Knowledge Through Model Merging'
authors: Lin Tzu-han, Li Chen-an, Lee Hung-yi, Chen Yun-nung
conference: "Arxiv"
year: 2024
bibkey: lin2024equipping
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.01470"}
tags: ['Agentic', 'Merging', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Reinforcement learning from human feedback (RLHF) is a popular strategy for aligning large language models (LLMs) with desired behaviors. Reward modeling is a crucial step in RLHF. However collecting paired preference data for training reward models is often costly and time-consuming especially for domain-specific preferences requiring expert annotation. To address this challenge we propose the textbfDomain knowledtextbfge merged textbfReward textbfModel (DogeRM) a novel framework that integrates domain-specific knowledge into a general reward model by model merging. The experiments demonstrate that DogeRM enhances performance across different benchmarks and provide a detailed analysis showcasing the effects of model merging showing the great potential of facilitating model alignment.
