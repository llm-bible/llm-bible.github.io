---
layout: publication
title: 'Survey Of Different Large Language Model Architectures: Trends, Benchmarks, And Challenges'
authors: Minghao Shao, Abdul Basit, Ramesh Karri, Muhammad Shafique
conference: "Arxiv"
year: 2024
bibkey: shao2024survey
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.03220'}
tags: ['Transformer', 'Model Architecture', 'Applications', 'Merging', 'Prompting', 'Multimodal Models', 'Survey Paper', 'Pretraining Methods']
---
Large Language Models (LLMs) represent a class of deep learning models adept
at understanding natural language and generating coherent responses to various
prompts or queries. These models far exceed the complexity of conventional
neural networks, often encompassing dozens of neural network layers and
containing billions to trillions of parameters. They are typically trained on
vast datasets, utilizing architectures based on transformer blocks. Present-day
LLMs are multi-functional, capable of performing a range of tasks from text
generation and language translation to question answering, as well as code
generation and analysis. An advanced subset of these models, known as
Multimodal Large Language Models (MLLMs), extends LLM capabilities to process
and interpret multiple data modalities, including images, audio, and video.
This enhancement empowers MLLMs with capabilities like video editing, image
comprehension, and captioning for visual content. This survey provides a
comprehensive overview of the recent advancements in LLMs. We begin by tracing
the evolution of LLMs and subsequently delve into the advent and nuances of
MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical
features, strengths, and limitations. Additionally, we present a comparative
analysis of these models and discuss their challenges, potential limitations,
and prospects for future development.
