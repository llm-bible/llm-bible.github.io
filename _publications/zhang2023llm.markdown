---
layout: publication
title: Llm45;based Medical Assistant Personalization With Short45; And Long45;term Memory Coordination
authors: Zhang Kai, Kang Yangyang, Zhao Fubang, Liu Xiaozhong
conference: "Arxiv"
year: 2023
bibkey: zhang2023llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.11696"}
tags: ['Efficiency And Optimization', 'Fine Tuning', 'GPT', 'Model Architecture', 'Training Techniques']
---
Large Language Models (LLMs) such as GPT3.5 have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand medical assistants hold the potential to offer substantial benefits for individuals. However the exploration of LLM45;based personalized medical assistant remains relatively scarce. Typically patients converse differently based on their background and preferences which necessitates the task of enhancing user45;oriented medical assistant. While one can fully train an LLM for this objective the resource consumption is unaffordable. Prior research has explored memory45;based methods to enhance the response with aware of previous mistakes for new queries during a dialogue session. We contend that a mere memory module is inadequate and fully training an LLM can be excessively costly. In this study we propose a novel computational bionic memory mechanism equipped with a parameter45;efficient fine45;tuning (PEFT) schema to personalize medical assistants.
