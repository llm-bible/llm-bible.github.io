---
layout: publication
title: From Zero To Hero Examining The Power Of Symbolic Tasks In Instruction Tuning
authors: Liu Qian, Zhou Fan, Jiang Zhengbao, Dou Longxu, Lin Min
conference: "Arxiv"
year: 2023
bibkey: liu2023from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.07995"}
tags: ['Efficiency And Optimization', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
Fine45;tuning language models on tasks with instructions has demonstrated potential in facilitating zero45;shot generalization to unseen tasks. In this paper we introduce a straightforward yet effective method for enhancing instruction tuning by employing symbolic tasks. Compared to crowdsourced human tasks or model45;generated tasks symbolic tasks present a unique advantage as they can be easily generated in vast quantities theoretically providing an infinite supply of high45;quality training instances. To explore the potential of symbolic tasks we carry out an extensive case study on the representative symbolic task of SQL execution. Empirical results on various benchmarks validate that the integration of SQL execution leads to significant improvements in zero45;shot scenarios particularly in table reasoning. Notably our 3B model surpasses both the 175B GPT45;3 and ChatGPT in zero45;shot table reasoning across four benchmarks. Furthermore experimental results on BBH (27 tasks) and MMLU (57 tasks) reveal that language models can be enhanced through symbolic tasks without compromising their generality. We hope that our paper serves as a catalyst inspiring increased efforts to incorporate symbolic tasks in instruction tuning.
