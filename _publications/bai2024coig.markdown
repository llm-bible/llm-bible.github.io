---
layout: publication
title: 'COIG-CQIA: Quality Is All You Need For Chinese Instruction Fine-tuning'
authors: Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Junting Zhou, Ziqiang Liu, Feiteng Fang, Mingshan Chang, Tianyu Zheng, Xincheng Zhang, Nuo Ma, Zekun Wang, Ruibin Yuan, Haihong Wu, Hongquan Lin, Wenhao Huang, Jiajun Zhang, Chenghua Lin, Jie Fu, Min Yang, Shiwen Ni, Ge Zhang
conference: "Arxiv"
year: 2024
bibkey: bai2024coig
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.18058"}
tags: ['Pretraining Methods', 'Training Techniques', 'Fine-Tuning', 'Reinforcement Learning']
---
Remarkable progress on English instruction tuning has facilitated the
efficacy and reliability of large language models (LLMs). However, there
remains a noticeable gap in instruction tuning for Chinese, where the complex
linguistic features pose significant challenges. Existing datasets, generally
distilled from English-centric LLMs, are not well-aligned with Chinese users'
interaction patterns. To bridge this gap, we introduce COIG-CQIA, a new Chinese
instruction tuning dataset derived from various real-world resources and
undergoing rigorous human verification. We conduct extensive experiments on
COIG-CQIA, and compare them with strong baseline models and datasets. The
experimental results show that models trained on COIG-CQIA achieve highly
competitive performance in diverse benchmarks. Additionally, our findings offer
several insights for designing effective Chinese instruction-tuning datasets
and data-mixing strategies. Our dataset are available at
https://huggingface.co/datasets/m-a-p/COIG-CQIA.
