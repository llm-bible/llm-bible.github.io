---
layout: publication
title: Improving Neural Machine Translation With Pre45;trained Representation
authors: Weng Rongxiang, Yu Heng, Huang Shujian, Luo Weihua, Chen Jiajun
conference: "Arxiv"
year: 2019
bibkey: weng2019improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1908.07688"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Tools', 'Transformer']
---
Monolingual data has been demonstrated to be helpful in improving the translation quality of neural machine translation (NMT). The current methods stay at the usage of word45;level knowledge such as generating synthetic parallel data or extracting information from word embedding. In contrast the power of sentence45;level contextual knowledge which is more complex and diverse playing an important role in natural language generation has not been fully exploited. In this paper we propose a novel structure which could leverage monolingual data to acquire sentence45;level contextual representations. Then we design a framework for integrating both source and target sentence45;level representations into NMT model to improve the translation quality. Experimental results on Chinese45;English German45;English machine translation tasks show that our proposed model achieves improvement over strong Transformer baselines while experiments on English45;Turkish further demonstrate the effectiveness of our approach in the low45;resource scenario.
