---
layout: publication
title: Understanding Telecom Language Through Large Language Models
authors: Bariah Lina, Zou Hang, Zhao Qiyang, Mouhouche Belkacem, Bader Faouzi, Debbah Merouane
conference: "Arxiv"
year: 2023
bibkey: bariah2023understanding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.07933"}
tags: ['Agentic', 'BERT', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Tools', 'Training Techniques']
---
The recent progress of artificial intelligence (AI) opens up new frontiers in the possibility of automating many tasks involved in Telecom networks design implementation and deployment. This has been further pushed forward with the evolution of generative artificial intelligence (AI) including the emergence of large language models (LLMs) which is believed to be the cornerstone toward realizing self45;governed interactive AI agents. Motivated by this in this paper we aim to adapt the paradigm of LLMs to the Telecom domain. In particular we fine45;tune several LLMs including BERT distilled BERT RoBERTa and GPT45;2 to the Telecom domain languages and demonstrate a use case for identifying the 3rd Generation Partnership Project (3GPP) standard working groups. We consider training the selected models on 3GPP technical documents (Tdoc) pertinent to years 200945;2019 and predict the Tdoc categories in years 202045;2023. The results demonstrate that fine45;tuning BERT and RoBERTa model achieves 84.637; accuracy while GPT45;2 model achieves 8337; in identifying 3GPP working groups. The distilled BERT model with around 5037; less parameters achieves similar performance as others. This corroborates that fine45;tuning pretrained LLM can effectively identify the categories of Telecom language. The developed framework shows a stepping stone towards realizing intent45;driven and self45;evolving wireless networks from Telecom languages and paves the way for the implementation of generative AI in the Telecom domain.
