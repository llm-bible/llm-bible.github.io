---
layout: publication
title: Negotiationtom A Benchmark For Stress45;testing Machine Theory Of Mind On Negotiation Surrounding
authors: Chan Chunkit, Jiayang Cheng, Yim Yauwai, Deng Zheye, Fan Wei, Li Haoran, Liu Xin, Zhang Hongming, Wang Weiqi, Song Yangqiu
conference: "Arxiv"
year: 2024
bibkey: chan2024benchmark
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.13627"}
tags: ['Agentic', 'Pretraining Methods', 'Reinforcement Learning']
---
Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability. Theory of mind evaluations currently focuses on testing models using machine45;generated data or game settings prone to shortcuts and spurious correlations which lacks evaluation of machine ToM ability in real45;world human interaction scenarios. This poses a pressing demand to develop new real45;world scenario benchmarks. We introduce NegotiationToM a new benchmark designed to stress45;test machine ToM in real45;world negotiation surrounding covered multi45;dimensional mental states (i.e. desires beliefs and intentions). Our benchmark builds upon the Belief45;Desire45;Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models. Our findings demonstrate that NegotiationToM is challenging for state45;of45;the45;art LLMs as they consistently perform significantly worse than humans even when employing the chain45;of45;thought (CoT) method.
