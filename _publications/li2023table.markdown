---
layout: publication
title: Table45;gpt Table45;tuned GPT For Diverse Table Tasks
authors: Li Peng, He Yeye, Yashar Dror, Cui Weiwei, Ge Song, Zhang Haidong, Fainman Danielle Rifinski, Zhang Dongmei, Chaudhuri Surajit
conference: "Arxiv"
year: 2023
bibkey: li2023table
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.09263"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
Language models such as GPT45;3.5 and ChatGPT demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However when probing language models using a range of basic table45;understanding tasks we observe that todays language models are still sub45;optimal in many table45;related tasks likely because they are pre45;trained predominantly on emph123;one45;dimensional125; natural45;language texts whereas relational tables are emph123;two45;dimensional125; objects. In this work we propose a new emph123;table45;tuning125; paradigm where we continue to train/fine45;tune language models like GPT45;3.5 and ChatGPT using diverse table45;tasks synthesized from real tables as training data with the goal of enhancing language models ability to understand tables and perform table tasks. We show that our resulting Table45;GPT models demonstrate (1) better emph123;table45;understanding125; capabilities by consistently outperforming the vanilla GPT45;3.5 and ChatGPT on a wide45;range of table tasks including holdout unseen tasks and (2) strong emph123;generalizability125; in its ability to respond to diverse human instructions to perform new table45;tasks in a manner similar to GPT45;3.5 and ChatGPT.
