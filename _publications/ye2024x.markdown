---
layout: publication
title: X45;VILA Cross45;modality Alignment For Large Language Model
authors: Ye Hanrong, Huang De-an, Lu Yao, Yu Zhiding, Ping Wei, Tao Andrew, Kautz Jan, Han Song, Xu Dan, Molchanov Pavlo, Yin Hongxu
conference: "Arxiv"
year: 2024
bibkey: ye2024x
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.19335"}
tags: ['Merging', 'Multimodal Models', 'Reinforcement Learning', 'Training Techniques']
---
We introduce X45;VILA an omni45;modality model designed to extend the capabilities of large language models (LLMs) by incorporating image video and audio modalities. By aligning modality45;specific encoders with LLM inputs and diffusion decoders with LLM outputs X45;VILA achieves cross45;modality understanding reasoning and generation. To facilitate this cross45;modality alignment we curate an effective interleaved any45;to45;any modality instruction45;following dataset. Furthermore we identify a significant problem with the current cross45;modality alignment method which results in visual information loss. To address the issue we propose a visual alignment mechanism with a visual embedding highway module. We then introduce a resource45;efficient recipe for training X45;VILA that exhibits proficiency in any45;to45;any modality conversation surpassing previous approaches by large margins. X45;VILA also showcases emergent properties across modalities even in the absence of similar training data. The project will be made open45;source.
