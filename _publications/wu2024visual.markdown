---
layout: publication
title: 'Visual Prompting In Multimodal Large Language Models: A Survey'
authors: Junda Wu, Zhehao Zhang, Yu Xia, Xintong Li, Zhaoyang Xia, Aaron Chang, Tong Yu, Sungchul Kim, Ryan A. Rossi, Ruiyi Zhang, Subrata Mitra, Dimitris N. Metaxas, Lina Yao, Jingbo Shang, Julian Mcauley
conference: "Arxiv"
year: 2024
bibkey: wu2024visual
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2409.15310'}
tags: ['Training Techniques', 'Prompting', 'Multimodal Models', 'Survey Paper', 'In-Context Learning']
---
Multimodal large language models (MLLMs) equip pre-trained large-language
models (LLMs) with visual capabilities. While textual prompting in LLMs has
been widely studied, visual prompting has emerged for more fine-grained and
free-form visual instructions. This paper presents the first comprehensive
survey on visual prompting methods in MLLMs, focusing on visual prompting,
prompt generation, compositional reasoning, and prompt learning. We categorize
existing visual prompts and discuss generative methods for automatic prompt
annotations on the images. We also examine visual prompting methods that enable
better alignment between visual encoders and backbone LLMs, concerning MLLM's
visual grounding, object referring, and compositional reasoning abilities. In
addition, we provide a summary of model training and in-context learning
methods to improve MLLM's perception and understanding of visual prompts. This
paper examines visual prompting methods developed in MLLMs and provides a
vision of the future of these methods.
