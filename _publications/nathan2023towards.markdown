---
layout: publication
title: Towards Probing Contact Center Large Language Models
authors: Nathan Varun, Kumar Ayush, Ingle Digvijay, Vepa Jithendra
conference: "Arxiv"
year: 2023
bibkey: nathan2023towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.15922"}
tags: ['Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Fine45;tuning large language models (LLMs) with domain45;specific instructions has emerged as an effective method to enhance their domain45;specific understanding. Yet there is limited work that examines the core characteristics acquired during this process. In this study we benchmark the fundamental characteristics learned by contact45;center (CC) specific instruction fine45;tuned LLMs with out45;of45;the45;box (OOB) LLMs via probing tasks encompassing conversational channel and automatic speech recognition (ASR) properties. We explore different LLM architectures (Flan45;T5 and Llama) sizes (3B 7B 11B 13B) and fine45;tuning paradigms (full fine45;tuning vs PEFT). Our findings reveal remarkable effectiveness of CC45;LLMs on the in45;domain downstream tasks with improvement in response acceptability by over 4837; compared to OOB45;LLMs. Additionally we compare the performance of OOB45;LLMs and CC45;LLMs on the widely used SentEval dataset and assess their capabilities in terms of surface syntactic and semantic information through probing tasks. Intriguingly we note a relatively consistent performance of probing classifiers on the set of probing tasks. Our observations indicate that CC45;LLMs while outperforming their out45;of45;the45;box counterparts exhibit a tendency to rely less on encoding surface syntactic and semantic properties highlighting the intricate interplay between domain45;specific adaptation and probing task performance opening up opportunities to explore behavior of fine45;tuned language models in specialized contexts.
