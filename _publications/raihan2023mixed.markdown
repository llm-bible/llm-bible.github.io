---
layout: publication
title: Mixed45;distil45;bert Code45;mixed Language Modeling For Bangla English And Hindi
authors: Raihan Md Nishat, Goswami Dhiman, Mahmud Antara
conference: "Arxiv"
year: 2023
bibkey: raihan2023mixed
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.10272"}
tags: ['BERT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
One of the most popular downstream tasks in the field of Natural Language Processing is text classification. Text classification tasks have become more daunting when the texts are code45;mixed. Though they are not exposed to such text during pre45;training different BERT models have demonstrated success in tackling Code45;Mixed NLP challenges. Again in order to enhance their performance Code45;Mixed NLP models have depended on combining synthetic data with real45;world data. It is crucial to understand how the BERT models performance is impacted when they are pretrained using corresponding code45;mixed languages. In this paper we introduce Tri45;Distil45;BERT a multilingual model pre45;trained on Bangla English and Hindi and Mixed45;Distil45;BERT a model fine45;tuned on code45;mixed data. Both models are evaluated across multiple NLP tasks and demonstrate competitive performance against larger models like mBERT and XLM45;R. Our two45;tiered pre45;training approach offers efficient alternatives for multilingual and code45;mixed language understanding contributing to advancements in the field.
