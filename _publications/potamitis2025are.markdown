---
layout: publication
title: 'Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback'
authors: Nearchos Potamitis, Akhil Arora
conference: "Arxiv"
year: 2025
bibkey: potamitis2025are
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.12951"}
tags: ['Prompting', 'Agent', 'Agentic', 'Tools']
---
Recent advancements in large language models (LLMs) have catalyzed the
development of general-purpose autonomous agents, demonstrating remarkable
performance in complex reasoning tasks across various domains. This surge has
spurred the evolution of a plethora of prompt-based reasoning frameworks. A
recent focus has been on iterative reasoning strategies that refine outputs
through self-evaluation and verbalized feedback. However, these strategies
require additional computational complexity to enable models to recognize and
correct their mistakes, leading to a significant increase in their cost. In
this work, we introduce the concept of ``retrials without feedback'', an
embarrassingly simple yet powerful mechanism for enhancing reasoning frameworks
by allowing LLMs to retry problem-solving attempts upon identifying incorrect
answers. Unlike conventional iterative refinement methods, our method does not
require explicit self-reflection or verbalized feedback, simplifying the
refinement process. Our findings indicate that simpler retrial-based approaches
often outperform more sophisticated reasoning frameworks, suggesting that the
benefits of complex methods may not always justify their computational costs.
By challenging the prevailing assumption that more intricate reasoning
strategies inherently lead to better performance, our work offers new insights
into how simpler, more efficient approaches can achieve optimal results. So,
are retrials all you need?
