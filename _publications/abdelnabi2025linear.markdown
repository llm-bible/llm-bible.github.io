---
layout: publication
title: 'Linear Control Of Test Awareness Reveals Differential Compliance In Reasoning Models'
authors: Sahar Abdelnabi, Ahmed Salem
conference: "Arxiv"
year: 2025
bibkey: abdelnabi2025linear
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.14617"}
tags: ['Responsible AI', 'Prompting', 'Tools', 'Reinforcement Learning']
---
Reasoning-focused large language models (LLMs) sometimes alter their behavior when they detect that they are being evaluated, an effect analogous to the Hawthorne phenomenon, which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such "test awareness" impacts model behavior, particularly its safety alignment. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-source reasoning LLMs across both realistic and hypothetical tasks. Our results demonstrate that test awareness significantly impact safety alignment, and is different for different models. By providing fine-grained control over this latent effect, our work aims to increase trust in how we perform safety evaluation.
