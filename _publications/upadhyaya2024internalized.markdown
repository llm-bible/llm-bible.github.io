---
layout: publication
title: 'Internalized Self-correction For Large Language Models'
authors: Nishanth Upadhyaya, Raghavendra Sridharamurthy
conference: "Arxiv"
year: 2024
bibkey: upadhyaya2024internalized
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.16653"}
tags: ['Training Techniques']
---
In this article, we introduce 'Internalized Self-Correction' (InSeC) for
large language models (LLMs). While many approaches exist for self-reflection
at inference time, we propose a novel method that combines ideas from negative
sampling, self-reflection during training, and inference time. InSeC allows
LLMs to correct themselves by introducing mistakes and their corresponding
corrections during training, thereby converting the learning process into a
true supervised learning task with both positive and negative examples. This
approach can be extended to improve instruction following and correct
hallucinations or incorrect sentences generated by LLMs.
