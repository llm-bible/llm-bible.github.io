---
layout: publication
title: DUMA Reading Comprehension With Transposition Thinking
authors: Zhu Pengfei, Zhao Hai, Li Xiaoguang
conference: "IEEE/ACM.Transactions.on.Audio.Speech.and.Language.Processing"
year: 2020
bibkey: zhu2020reading
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2001.09415"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Multi45;choice Machine Reading Comprehension (MRC) requires model to decide the correct answer from a set of answer options when given a passage and a question. Thus in addition to a powerful Pre45;trained Language Model (PrLM) as encoder multi45;choice MRC especially relies on a matching network design which is supposed to effectively capture the relationships among the triplet of passage question and answers. While the newer and more powerful PrLMs have shown their mightiness even without the support from a matching network we propose a new DUal Multi45;head Co45;Attention (DUMA) model which is inspired by humans transposition thinking process solving the multi45;choice MRC problem respectively considering each others focus from the standpoint of passage and question. The proposed DUMA has been shown effective and is capable of generally promoting PrLMs. Our proposed method is evaluated on two benchmark multi45;choice MRC tasks DREAM and RACE showing that in terms of powerful PrLMs DUMA can still boost the model to reach new state45;of45;the45;art performance.
