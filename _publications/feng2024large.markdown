---
layout: publication
title: Large Language Model-based Human-Agent Collaboration for Complex Task Solving
authors: Feng Xueyang, Chen Zhi-yuan, Qin Yujia, Lin Yankai, Chen Xu, Liu Zhiyuan, Wen Ji-rong
conference: "Arxiv"
year: 2024
bibkey: feng2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.12914"}
  - {name: "Code", url: "https://github.com/XueyangFeng/ReHAC"}
tags: ['Agent', 'Agentic', 'Has Code', 'Reinforcement Learning']
---
In recent developments within the research community the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work we introduce the problem of LLM-based human-agent collaboration for complex task-solving exploring their synergistic potential. In addition we propose a Reinforcement Learning-based Human-Agent Collaboration method ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the models effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks primarily through well-planned limited human intervention. Datasets and code are available at https://github.com/XueyangFeng/ReHAC.
