---
layout: publication
title: 'Fuximt: Sparsifying Large Language Models For Chinese-centric Multilingual Machine Translation'
authors: Shaolin Zhu, Tianyu Dong, Bo Li, Deyi Xiong
conference: "Arxiv"
year: 2025
bibkey: zhu2025sparsifying
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.14256"}
tags: ['Fine-Tuning', 'Applications', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods']
---
In this paper, we present FuxiMT, a novel Chinese-centric multilingual machine translation model powered by a sparsified large language model (LLM). We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on a massive Chinese corpus and then conduct multilingual fine-tuning on a large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable.
