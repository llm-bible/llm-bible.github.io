---
layout: publication
title: 'Beyond Final Answers: Evaluating Large Language Models For Math Tutoring'
authors: Adit Gupta, Jennifer Reddig, Tommaso Calo, Daniel Weitekamp, Christopher J. Maclellan
conference: "Arxiv"
year: 2025
bibkey: gupta2025beyond
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.16460"}
tags: ['GPT', 'Prompting', 'Model Architecture', 'Reinforcement Learning']
---
Researchers have made notable progress in applying Large Language Models
(LLMs) to solve math problems, as demonstrated through efforts like GSM8k,
ProofNet, AlphaGeometry, and MathOdyssey. This progress has sparked interest in
their potential use for tutoring students in mathematics. However, the
reliability of LLMs in tutoring contexts -- where correctness and instructional
quality are crucial -- remains underexplored. Moreover, LLM problem-solving
capabilities may not necessarily translate into effective tutoring support for
students. In this work, we present two novel approaches to evaluate the
correctness and quality of LLMs in math tutoring contexts. The first approach
uses an intelligent tutoring system for college algebra as a testbed to assess
LLM problem-solving capabilities. We generate benchmark problems using the
tutor, prompt a diverse set of LLMs to solve them, and compare the solutions to
those generated by the tutor. The second approach evaluates LLM as tutors
rather than problem solvers. We employ human evaluators, who act as students
seeking tutoring support from each LLM. We then assess the quality and
correctness of the support provided by the LLMs via a qualitative coding
process. We applied these methods to evaluate several ChatGPT models, including
3.5 Turbo, 4, 4o, o1-mini, and o1-preview. Our findings show that when used as
problem solvers, LLMs generate correct final answers for 85.5% of the college
algebra problems tested. When employed interactively as tutors, 90% of LLM
dialogues show high-quality instructional support; however, many contain errors
-- only 56.6% are entirely correct. We conclude that, despite their potential,
LLMs are not yet suitable as intelligent tutors for math without human
oversight or additional mechanisms to ensure correctness and quality.
