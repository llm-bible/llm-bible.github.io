---
layout: publication
title: Evaluating Open45;domain Question Answering In The Era Of Large Language Models
authors: Kamalloo Ehsan, Dziri Nouha, Clarke Charles L. A., Rafiei Davood
conference: "Arxiv"
year: 2023
bibkey: kamalloo2023evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.06984"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Reinforcement Learning']
---
Lexical matching remains the de facto evaluation method for open45;domain question answering (QA). Unfortunately lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer thereby making matching with the gold answers even more challenging. Without accurate evaluation the true progress in open45;domain QA remains unknown. In this paper we conduct a thorough analysis of various open45;domain QA models including LLMs by manually evaluating their answers on a subset of NQ45;open a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated the performance of the InstructGPT (zero45;shot) LLM increases by nearly +6037; making it on par with existing top models and the InstructGPT (few45;shot) model actually achieves a new state45;of45;the45;art on NQ45;open. We also find that more than 5037; of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments although still suffering from unnecessary strictness. Finally we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances but not for long45;form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time there appears to be no substitute for human evaluation.
