---
layout: publication
title: Continual Learning Of Large Language Models\: A Comprehensive Survey
authors: Shi Haizhou, Xu Zihao, Wang Hengyi, Qin Weiyi, Wang Wenyuan, Wang Yibin, Wang Zifeng, Ebrahimi Sayna, Wang Hao
conference: "Arxiv"
year: 2024
bibkey: shi2024continual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.16789"}
  - {name: "Code", url: "https://github.com/Wang-ML-Lab/llm-continual-learning-survey"}
tags: ['Applications', 'Fine Tuning', 'Has Code', 'Pretraining Methods', 'Survey Paper', 'Training Techniques']
---
The recent success of large language models (LLMs) trained on static pre-collected general datasets has sparked numerous research directions and applications. One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions task structures and user preferences. Pre-trained LLMs when tailored for specific needs often experience significant performance degradation in previous knowledge domains -- a phenomenon known as catastrophic forgetting. While extensively studied in the continual learning (CL) community it presents new manifestations in the realm of LLMs. In this survey we provide a comprehensive overview of the current research progress on LLMs within the context of CL. This survey is structured into four main sections we first describe an overview of continually learning LLMs consisting of two directions of continuity vertical continuity (or vertical continual learning) i.e. continual adaptation from general to specific capabilities and horizontal continuity (or horizontal continual learning) i.e. continual adaptation across time and domains (Section 3). We then summarize three stages of learning LLMs in the context of modern CL Continual Pre-Training (CPT) Domain-Adaptive Pre-training (DAP) and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of evaluation protocols for continual learning with LLMs along with the current available data sources (Section 5). Finally we discuss intriguing questions pertaining to continual learning for LLMs (Section 6). The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey."
