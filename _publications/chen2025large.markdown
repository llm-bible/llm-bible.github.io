---
layout: publication
title: 'Large Language Models For Predictive Analysis: How Far Are They?'
authors: Qin Chen, Yuanyi Ren, Xiaojun Ma, Yuyang Shi
conference: "Arxiv"
year: 2025
bibkey: chen2025large
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.17149'}
  - {name: "Code", url: 'https://github.com/Cqkkkkkk/PredictiQ}{Github'}
tags: ['Reinforcement Learning', 'Has Code', 'Applications', 'Tools']
---
Predictive analysis is a cornerstone of modern decision-making, with applications in various domains. Large Language Models (LLMs) have emerged as powerful tools in enabling nuanced, knowledge-intensive conversations, thus aiding in complex decision-making tasks. With the burgeoning expectation to harness LLMs for predictive analysis, there is an urgent need to systematically assess their capability in this domain. However, there is a lack of relevant evaluations in existing studies. To bridge this gap, we introduce the \textbf\{PredictiQ\} benchmark, which integrates 1130 sophisticated predictive analysis queries originating from 44 real-world datasets of 8 diverse fields. We design an evaluation protocol considering text analysis, code generation, and their alignment. Twelve renowned LLMs are evaluated, offering insights into their practical use in predictive analysis. Generally, we believe that existing LLMs still face considerable challenges in conducting predictive analysis. See \href\{https://github.com/Cqkkkkkk/PredictiQ\}\{Github\}.
