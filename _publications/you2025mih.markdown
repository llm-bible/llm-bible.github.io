---
layout: publication
title: 'MIH-TCCT: Mitigating Inconsistent Hallucinations In Llms Via Event-driven Text-code Cyclic Training'
authors: Xinxin You, Xien Liu, Qixin Sun, Huan Zhang, Kaiyin Zhou, Shaohui Liu, Guoping Hu, Shijin Wang, Si Liu, Ji Wu
conference: "Arxiv"
year: 2025
bibkey: you2025mih
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.08904'}
tags: ['RAG', 'Tools', 'Training Techniques']
---
Recent methodologies utilizing synthetic datasets have aimed to address
inconsistent hallucinations in large language models (LLMs); however,these
approaches are primarily tailored to specific tasks, limiting their
generalizability. Inspired by the strong performance of code-trained models in
logic-intensive domains, we propose a novel framework that leverages
event-based text to generate corresponding code and employs cyclic training to
transfer the logical consistency of code to natural language effectively. Our
method significantly reduces inconsistent hallucinations across three leading
LLMs and two categories of natural language tasks while maintaining overall
performance. This framework effectively alleviates hallucinations without
necessitating adaptation to downstream tasks, demonstrating generality and
providing new perspectives to tackle the challenge of inconsistent
hallucinations.
