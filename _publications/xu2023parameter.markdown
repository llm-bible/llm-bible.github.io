---
layout: publication
title: 'Parameter-efficient Fine-tuning Methods For Pretrained Language Models: A
  Critical Review And Assessment'
authors: Lingling Xu, Haoran Xie, Si-zhao Joe Qin, Xiaohui Tao, Fu Lee Wang
conference: Arxiv
year: 2023
citations: 25
bibkey: xu2023parameter
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2312.12148'}]
tags: [Fine-Tuning, Survey Paper, Transformer, Applications, Efficiency and Optimization]
---
With the continuous growth in the number of parameters of transformer-based
pretrained language models (PLMs), particularly the emergence of large language
models (LLMs) with billions of parameters, many natural language processing
(NLP) tasks have demonstrated remarkable success. However, the enormous size
and computational demands of these models pose significant challenges for
adapting them to specific downstream tasks, especially in environments with
limited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers
an effective solution by reducing the number of fine-tuning parameters and
memory usage while achieving comparable performance to full fine-tuning. The
demands for fine-tuning PLMs, especially LLMs, have led to a surge in the
development of PEFT methods, as depicted in Fig. 1. In this paper, we present a
comprehensive and systematic review of PEFT methods for PLMs. We summarize
these PEFT methods, discuss their applications, and outline future directions.
Furthermore, we conduct experiments using several representative PEFT methods
to better understand their effectiveness in parameter efficiency and memory
efficiency. By offering insights into the latest advancements and practical
applications, this survey serves as an invaluable resource for researchers and
practitioners seeking to navigate the challenges and opportunities presented by
PEFT in the context of PLMs.