---
layout: publication
title: LLM4GEN Leveraging Semantic Representation Of Llms For Text45;to45;image Generation
authors: Liu Mushui, Ma Yuhang, Zhen Yang, Dan Jun, Yu Yunlong, Zhao Zeng, Hu Zhipeng, Liu Bai, Fan Changjie
conference: "Arxiv"
year: 2024
bibkey: liu2024leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.00737"}
tags: ['Applications', 'Merging', 'Prompting', 'RAG', 'Tools']
---
Diffusion models have exhibited substantial success in text45;to45;image generation. However they often encounter challenges when dealing with complex and dense prompts involving multiple objects attribute binding and long descriptions. In this paper we propose a novel framework called textbf123;LLM4GEN125; which enhances the semantic understanding of text45;to45;image diffusion models by leveraging the representation of Large Language Models (LLMs). It can be seamlessly incorporated into various diffusion models as a plug45;and45;play component. A specially designed Cross45;Adapter Module (CAM) integrates the original text features of text45;to45;image models with LLM features thereby enhancing text45;to45;image generation. Additionally to facilitate and correct entity45;attribute relationships in text prompts we develop an entity45;guided regularization loss to further improve generation performance. We also introduce DensePrompts which contains 7000 dense prompts to provide a comprehensive evaluation for the text45;to45;image generation task. Experiments indicate that LLM4GEN significantly improves the semantic alignment of SD1.5 and SDXL demonstrating increases of 9.6937; and 12.9037; in color on T2I45;CompBench respectively. Moreover it surpasses existing models in terms of sample quality image45;text alignment and human evaluation.
