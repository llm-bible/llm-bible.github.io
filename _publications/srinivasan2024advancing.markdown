---
layout: publication
title: 'Advancing NLP Security By Leveraging Llms As Adversarial Engines'
authors: Sudarshan Srinivasan, Maria Mahbub, Amir Sadovnik
conference: "Arxiv"
year: 2024
bibkey: srinivasan2024advancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.18215"}
tags: ['RAG', 'Security', 'Applications', 'Model Architecture']
---
This position paper proposes a novel approach to advancing NLP security by
leveraging Large Language Models (LLMs) as engines for generating diverse
adversarial attacks. Building upon recent work demonstrating LLMs'
effectiveness in creating word-level adversarial examples, we argue for
expanding this concept to encompass a broader range of attack types, including
adversarial patches, universal perturbations, and targeted attacks. We posit
that LLMs' sophisticated language understanding and generation capabilities can
produce more effective, semantically coherent, and human-like adversarial
examples across various domains and classifier architectures. This paradigm
shift in adversarial NLP has far-reaching implications, potentially enhancing
model robustness, uncovering new vulnerabilities, and driving innovation in
defense mechanisms. By exploring this new frontier, we aim to contribute to the
development of more secure, reliable, and trustworthy NLP systems for critical
applications.
