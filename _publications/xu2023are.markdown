---
layout: publication
title: 'Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation And Beyond'
authors: Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, Erik Cambria
conference: "Arxiv"
year: 2023
bibkey: xu2023are
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.09841"}
tags: ['Interpretability and Explainability', 'Ethics and Bias', 'Reinforcement Learning']
---
Logical reasoning consistently plays a fundamental and significant role in
the domains of knowledge engineering and artificial intelligence. Recently,
Large Language Models (LLMs) have emerged as a noteworthy innovation in natural
language processing (NLP). However, the question of whether LLMs can
effectively address the task of logical reasoning, which requires gradual
cognitive inference similar to human intelligence, remains unanswered. To this
end, we aim to bridge this gap and provide comprehensive evaluations in this
paper. Firstly, to offer systematic evaluations, we select fifteen typical
logical reasoning datasets and organize them into deductive, inductive,
abductive and mixed-form reasoning settings. Considering the comprehensiveness
of evaluations, we include 3 early-era representative LLMs and 4 trending LLMs.
Secondly, different from previous evaluations relying only on simple metrics
(e.g., *accuracy*), we propose fine-level evaluations in objective and
subjective manners, covering both answers and explanations, including
*answer correctness*, *explain correctness*, *explain
completeness* and *explain redundancy*. Additionally, to uncover the
logical flaws of LLMs, problematic cases will be attributed to five error types
from two dimensions, i.e., *evidence selection process* and
*reasoning process*. Thirdly, to avoid the influences of knowledge bias
and concentrate purely on benchmarking the logical reasoning capability of
LLMs, we propose a new dataset with neutral content. Based on the in-depth
evaluations, this paper finally forms a general evaluation scheme of logical
reasoning capability from six dimensions (i.e., *Correct*,
*Rigorous*, *Self-aware*, *Active*, *Oriented* and *No
hallucination*). It reflects the pros and cons of LLMs and gives guiding
directions for future works.
