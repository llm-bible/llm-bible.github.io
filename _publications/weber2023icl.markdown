---
layout: publication
title: The ICL Consistency Test
authors: Weber Lucas, Bruni Elia, Hupkes Dieuwke
conference: "Arxiv"
year: 2023
bibkey: weber2023icl
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.04945"}
tags: ['Prompting']
---
Just like the previous generation of task45;tuned models large language models (LLMs) that are adapted to tasks via prompt45;based methods like in45;context45;learning (ICL) perform well in some setups but not in others. This lack of consistency in prompt45;based learning hints at a lack of robust generalisation. We here introduce the ICL consistency test 45;45; a contribution to the GenBench collaborative benchmark task (CBT) 45;45; which evaluates how consistent a model makes predictions across many different setups while using the same data. The test is based on different established natural language inference tasks. We provide preprocessed data constituting 96 different setups and a metric that estimates model consistency across these setups. The metric is provided on a fine45;grained level to understand what properties of a setup render predictions unstable and on an aggregated level to compare overall model consistency. We conduct an empirical analysis of eight state45;of45;the45;art models and our consistency metric reveals how all tested LLMs lack robust generalisation.
