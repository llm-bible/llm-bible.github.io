---
layout: publication
title: 'An Overview Of Temporal Commonsense Reasoning And Acquisition'
authors: Wenzel Georg, Jatowt Adam
conference: "Arxiv"
year: 2023
bibkey: wenzel2023overview
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.00002"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
Temporal commonsense reasoning refers to the ability to understand the
typical temporal context of phrases, actions, and events, and use it to reason
over problems requiring such knowledge. This trait is essential in temporal
natural language processing tasks, with possible applications such as timeline
summarization, temporal question answering, and temporal natural language
inference. Recent research on the performance of large language models suggests
that, although they are adept at generating syntactically correct sentences and
solving classification tasks, they often take shortcuts in their reasoning and
fall prey to simple linguistic traps. This article provides an overview of
research in the domain of temporal commonsense reasoning, particularly focusing
on enhancing language model performance through a variety of augmentations and
their evaluation across a growing number of datasets. However, these augmented
models still struggle to approach human performance on reasoning tasks over
temporal common sense properties, such as the typical occurrence times,
orderings, or durations of events. We further emphasize the need for careful
interpretation of research to guard against overpromising evaluation results in
light of the shallow reasoning present in transformers. This can be achieved by
appropriately preparing datasets and suitable evaluation metrics.
