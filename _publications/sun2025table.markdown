---
layout: publication
title: 'Table As Thought: Exploring Structured Thoughts In LLM Reasoning'
authors: Zhenjie Sun, Naihao Deng, Haofei Yu, Jiaxuan You
conference: "Arxiv"
year: 2025
bibkey: sun2025table
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.02152"}
tags: ['Prompting', 'Fine-Tuning', 'Tools']
---
Large language models' reasoning abilities benefit from methods that organize
their thought processes, such as chain-of-thought prompting, which employs a
sequential structure to guide the reasoning process step-by-step. However,
existing approaches focus primarily on organizing the sequence of thoughts,
leaving structure in individual thought steps underexplored. To address this
gap, we propose Table as Thought, a framework inspired by cognitive
neuroscience theories on human thought. Table as Thought organizes reasoning
within a tabular schema, where rows represent sequential thought steps and
columns capture critical constraints and contextual information to enhance
reasoning. The reasoning process iteratively populates the table until
self-verification ensures completeness and correctness. Our experiments show
that Table as Thought excels in planning tasks and demonstrates a strong
potential for enhancing LLM performance in mathematical reasoning compared to
unstructured thought baselines. This work provides a novel exploration of
refining thought representation within LLMs, paving the way for advancements in
reasoning and AI cognition.
