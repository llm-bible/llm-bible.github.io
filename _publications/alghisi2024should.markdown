---
layout: publication
title: Should We Fine45;tune Or RAG Evaluating Different Techniques To Adapt Llms For Dialogue
authors: Alghisi Simone, Rizzoli Massimo, Roccabruna Gabriel, Mousavi Seyed Mahed, Riccardi Giuseppe
conference: "Arxiv"
year: 2024
bibkey: alghisi2024should
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.06399"}
tags: ['Applications', 'Interpretability And Explainability', 'RAG']
---
We study the limitations of Large Language Models (LLMs) for the task of response generation in human45;machine dialogue. Several techniques have been proposed in the literature for different dialogue types (e.g. Open45;Domain). However the evaluations of these techniques have been limited in terms of base LLMs dialogue types and evaluation metrics. In this work we extensively analyze different LLM adaptation techniques when applied to different dialogue types. We have selected two base LLMs Llama45;2 and Mistral and four dialogue types Open45;Domain Knowledge45;Grounded Task45;Oriented and Question Answering. We evaluate the performance of in45;context learning and fine45;tuning techniques across datasets selected for each dialogue type. We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval45;Augmented Generation (RAG) and gold knowledge. We adopt consistent evaluation and explainability criteria for automatic metrics and human evaluation protocols. Our analysis shows that there is no universal best45;technique for adapting large language models as the efficacy of each technique depends on both the base LLM and the specific type of dialogue. Last but not least the assessment of the best adaptation technique should include human evaluation to avoid false expectations and outcomes derived from automatic metrics.
