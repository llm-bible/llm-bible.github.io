---
layout: publication
title: Democratizing Llms An Exploration Of Cost45;performance Trade45;offs In Self45;refined Open45;source Models
authors: Shashidhar Sumuk, Chinta Abhinav, Sahai Vaibhav, Wang Zhenhailong, Ji Heng
conference: "Arxiv"
year: 2023
bibkey: shashidhar2023democratizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.07611"}
tags: ['Applications', 'Fine Tuning', 'GPT', 'Model Architecture', 'RAG']
---
The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. High45;performing open45;source alternatives are crucial for information45;sensitive and high45;volume applications but often lag behind in performance. To address this gap we propose (1) A untargeted variant of iterative self45;critique and self45;refinement devoid of external influence. (2) A novel ranking metric 45; Performance Refinement and Inference Cost Score (PeRFICS) 45; to find the optimal model for a given task considering refined performance and cost. Our experiments show that SoTA open source models of varying sizes from 7B 45; 65B on average improve 8.237; from their baseline performance. Strikingly even models with extremely small memory footprints such as Vicuna45;7B show a 11.7437; improvement overall and up to a 25.3937; improvement in high45;creativity open ended tasks on the Vicuna benchmark. Vicuna45;13B takes it a step further and outperforms ChatGPT post45;refinement. This work has profound implications for resource45;constrained and information45;sensitive environments seeking to leverage LLMs without incurring prohibitive costs compromising on performance and privacy. The domain45;agnostic self45;refinement process coupled with our novel ranking metric facilitates informed decision45;making in model selection thereby reducing costs and democratizing access to high45;performing language models as evidenced by case studies.
