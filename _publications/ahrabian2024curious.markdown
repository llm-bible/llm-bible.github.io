---
layout: publication
title: The Curious Case Of Nonverbal Abstract Reasoning With Multi45;modal Large Language Models
authors: Ahrabian Kian, Sourati Zhivar, Sun Kexuan, Zhang Jiarui, Jiang Yifan, Morstatter Fred, Pujara Jay
conference: "Arxiv"
year: 2024
bibkey: ahrabian2024curious
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.12117"}
  - {name: "Code", url: "https://github.com/usc&#45;isi&#45;i2/isi&#45;mmlm&#45;rpm"}
tags: ['Applications', 'Has Code', 'Masked Language Model', 'Pretraining Methods', 'Prompting']
---
While large language models (LLMs) are still being adopted to new domains and utilized in novel applications we are experiencing an influx of the new generation of foundation models namely multi45;modal large language models (MLLMs). These models integrate verbal and visual information opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However despite the revolutionizing prospect of MLLMs our understanding of their reasoning abilities is limited. In this study we assess the nonverbal abstract reasoning abilities of open45;source and closed45;source MLLMs using variations of Ravens Progressive Matrices. Our experiments reveal the challenging nature of such problems for MLLMs while showcasing the immense gap between open45;source and closed45;source models. We also uncover critical shortcomings of visual and textual perceptions subjecting the models to low45;performance ceilings. Finally to improve MLLMs performance we experiment with different methods such as Chain45;of45;Thought prompting leading to a significant (up to 10037;) boost in performance. Our code and datasets are available at https://github.com/usc&#45;isi&#45;i2/isi&#45;mmlm&#45;rpm.
