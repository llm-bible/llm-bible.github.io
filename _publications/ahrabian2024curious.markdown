---
layout: publication
title: 'The Curious Case Of Nonverbal Abstract Reasoning With Multi-modal Large Language Models'
authors: Ahrabian Kian, Sourati Zhivar, Sun Kexuan, Zhang Jiarui, Jiang Yifan, Morstatter Fred, Pujara Jay
conference: "Arxiv"
year: 2024
bibkey: ahrabian2024curious
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.12117"}
  - {name: "Code", url: "https://github.com/usc-isi-i2/isi-mmlm-rpm"}
tags: ['Applications', 'Has Code', 'Masked Language Model', 'Multimodal Models', 'Prompting']
---
'While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven''s Progressive Matrices. Our experiments reveal the challenging nature of such problems for MLLMs while showcasing the immense gap between open-source and closed-source models. We also uncover critical shortcomings of visual and textual perceptions, subjecting the models to low-performance ceilings. Finally, to improve MLLMs'' performance, we experiment with different methods, such as Chain-of-Thought prompting, leading to a significant (up to 100&#37;) boost in performance. Our code and datasets are available at https://github.com/usc-isi-i2/isi-mmlm-rpm.'
