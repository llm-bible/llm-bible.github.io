---
layout: publication
title: SA45;MDKIF A Scalable And Adaptable Medical Domain Knowledge Injection Framework For Large Language Models
authors: Xu Tianhan, Hu Zhe, Chen Ling, Li Bin
conference: "Arxiv"
year: 2024
bibkey: xu2024sa
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.00474"}
tags: ['Fine Tuning', 'Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study we present SA45;MDKIF a scalable and adaptable framework that aims to inject medical knowledge into general45;purpose LLMs through instruction tuning thereby enabling adaptability for various downstream tasks. SA45;MDKIF consists of two stages skill training and skill adaptation. In the first stage we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage we train the skill router using task45;specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA45;MDKIF improves performance by 1045;2037; compared to the original LLMs. Notably this improvement is particularly pronounced for unseen medical tasks showing an improvement of up to 3037;.
