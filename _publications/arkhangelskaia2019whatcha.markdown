---
layout: publication
title: Whatcha Lookin'' At? Deeplifting Bert''s Attention In Question Answering
authors: Arkhangelskaia Ekaterina, Dutta Sourav
conference: "Arxiv"
year: 2019
bibkey: arkhangelskaia2019whatcha
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1910.06431"}
tags: ['Applications', 'Attention Mechanism', 'BERT', 'Model Architecture', 'RAG']
---
There has been great success recently in tackling challenging NLP tasks by neural networks which have been pre-trained and fine-tuned on large amounts of task data. In this paper we investigate one such model BERT for question-answering with the aim to analyze why it is able to achieve significantly better results than other models. We run DeepLIFT on the model predictions and test the outcomes to monitor shift in the attention values for input. We also cluster the results to analyze any possible patterns similar to human reasoning depending on the kind of input paragraph and question the model is trying to answer.
