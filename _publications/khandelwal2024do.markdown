---
layout: publication
title: 'Do Moral Judgment And Reasoning Capability Of Llms Change With Language? A Study Using The Multilingual Defining Issues Test'
authors: Khandelwal Aditi, Agarwal Utkarsh, Tanmay Kumar, Choudhury Monojit
conference: "Arxiv"
year: 2024
bibkey: khandelwal2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.02135"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
This paper explores the moral judgment and moral reasoning abilities exhibited by Large Language Models (LLMs) across languages through the Defining Issues Test. It is a well known fact that moral judgment depends on the language in which the question is asked. We extend the work of beyond English to 5 new languages (Chinese Hindi Russian Spanish and Swahili) and probe three LLMs -- ChatGPT GPT-4 and Llama2Chat-70B -- that shows substantial multilingual text processing and generation abilities. Our study shows that the moral reasoning ability for all models as indicated by the post-conventional score is substantially inferior for Hindi and Swahili compared to Spanish Russian Chinese and English while there is no clear trend for the performance of the latter four languages. The moral judgments too vary considerably by the language.
