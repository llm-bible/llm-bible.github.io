---
layout: publication
title: 'Shakti-vlms: Scalable Vision-language Models For Enterprise AI'
authors: Syed Abdul Gaffar Shakhadri, Kruthika Kr, Kartik Basavaraj Angadi
conference: "Arxiv"
year: 2025
bibkey: shakhadri2025shakti
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.17092'}
tags: ['Attention Mechanism', 'RAG', 'Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Multimodal Models']
---
We introduce Shakti VLM, a family of vision-language models in the capacity
of 1B and 4B parameters designed to address data efficiency challenges in
multimodal learning. While recent VLMs achieve strong performance through
extensive training data, Shakti models leverage architectural innovations to
attain competitive results with fewer tokens. Key advancements include
QK-Normalization for attention stability, hybrid normalization techniques, and
enhanced positional encoding. A three-stage training strategy further optimizes
learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and
Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR
extraction, and general multimodal reasoning. Our results highlight that high
performance can be achieved through model design and training strategy rather
than sheer data volume, making Shakti an efficient solution for
enterprise-scale multimodal tasks.
