---
layout: publication
title: 'Advancing Student Writing Through Automated Syntax Feedback'
authors: Kamyar Zeinalipour, Mehak Mehak, Fatemeh Parsamotamed, Marco Maggini, Marco Gori
conference: "Arxiv"
year: 2025
bibkey: zeinalipour2025advancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.07740"}
tags: ['Training Techniques', 'Model Architecture', 'Reinforcement Learning', 'RAG', 'GPT', 'Pretraining Methods', 'Fine-Tuning']
---
This study underscores the pivotal role of syntax feedback in augmenting the
syntactic proficiency of students. Recognizing the challenges faced by learners
in mastering syntactic nuances, we introduce a specialized dataset named
Essay-Syntax-Instruct designed to enhance the understanding and application of
English syntax among these students. Leveraging the capabilities of Large
Language Models (LLMs) such as GPT3.5-Turbo, Llama-2-7b-chat-hf,
Llama-2-13b-chat-hf, and Mistral-7B-Instruct-v0.2, this work embarks on a
comprehensive fine-tuning process tailored to the syntax improvement task.
Through meticulous evaluation, we demonstrate that the fine-tuned LLMs exhibit
a marked improvement in addressing syntax-related challenges, thereby serving
as a potent tool for students to identify and rectify their syntactic errors.
The findings not only highlight the effectiveness of the proposed dataset in
elevating the performance of LLMs for syntax enhancement but also illuminate a
promising path for utilizing advanced language models to support language
acquisition efforts. This research contributes to the broader field of language
learning technology by showcasing the potential of LLMs in facilitating the
linguistic development of Students.
