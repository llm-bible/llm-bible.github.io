---
layout: publication
title: 'Recursive Decomposition Of Logical Thoughts: Framework For Superior Reasoning And Knowledge Propagation In Large Language Models'
authors: Kaleem Ullah Qasim, Jiashu Zhang, Tariq Alsahfi, Ateeq Ur Rehman Butt
conference: "Arxiv"
year: 2025
bibkey: qasim2025recursive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.02026"}
tags: ['Prompting', 'Model Architecture', 'GPT', 'Tools']
---
Enhancing the reasoning capabilities of Large Language Models remains a
critical challenge in artificial intelligence. We introduce RDoLT, Recursive
Decomposition of Logical Thought prompting, a novel framework that
significantly boosts LLM reasoning performance. RDoLT is built on three key
innovations: (1) recursively breaking down complex reasoning tasks into
sub-tasks of progressive complexity; (2) employing an advanced selection and
scoring mechanism to identify the most promising reasoning thoughts; and (3)
integrating a knowledge propagation module that mimics human learning by
keeping track of strong and weak thoughts for information propagation. Our
approach was evaluated across multiple benchmarks, including GSM8K, SVAMP,
MultiArith, LastLetterConcatenation, and Gaokao2023 Math. The results
demonstrate that RDoLT consistently outperforms existing state-of-the-art
techniques, achieving a 90.98 percent accuracy on GSM8K with ChatGPT-4,
surpassing state-of-the-art techniques by 6.28 percent. Similar improvements
were observed on other benchmarks, with accuracy gains ranging from 5.5 percent
to 6.75 percent. These findings highlight RDoLT's potential to advance prompt
engineering, offering a more effective and generalizable approach to complex
reasoning tasks.
