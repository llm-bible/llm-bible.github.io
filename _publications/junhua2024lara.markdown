---
layout: publication
title: LARA Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification
authors: Junhua Liu, Keat Tan Yong, Bin Fu
conference: "Arxiv"
year: 2024
bibkey: junhua2024lara
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.16504"}
tags: ['ARXIV', 'Pretraining Methods']
---
Following the significant achievements of large language models (LLMs) researchers have employed in-context learning for text classification tasks. However these studies focused on monolingual single-turn classification tasks. In this paper we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models) designed to enhance accuracy in multi-turn classification tasks across six languages accommodating numerous intents in chatbot interactions. Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism integrated within the architecture of LLMs. This integration allows LARA to dynamically utilize past dialogues and relevant intents thereby improving the understanding of the context. Furthermore our adaptive retrieval techniques bolster the cross-lingual capabilities of LLMs without extensive retraining and fine-tune. Comprehensive experiments demonstrate that LARA achieves state-of-the-art performance on multi-turn intent classification tasks enhancing the average accuracy by 3.67 compared to existing methods.
