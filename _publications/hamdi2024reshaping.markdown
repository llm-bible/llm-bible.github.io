---
layout: publication
title: 'RIRO: Reshaping Inputs, Refining Outputs Unlocking The Potential Of Large Language Models In Data-scarce Contexts'
authors: Ali Hamdi, Hozaifa Kassab, Mohamed Bahaa, Marwa Mohamed
conference: "Arxiv"
year: 2024
bibkey: hamdi2024reshaping
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.15254"}
tags: ['Training Techniques', 'Model Architecture', 'Tools', 'Language Modeling', 'RAG', 'Pretraining Methods', 'Fine-Tuning', 'Prompting', 'Applications']
---
Large language models (LLMs) have significantly advanced natural language
processing, excelling in areas like text generation, summarization, and
question-answering. Despite their capabilities, these models face challenges
when fine-tuned on small, domain-specific datasets, often struggling to
generalize and deliver accurate results with unfamiliar inputs. To tackle this
issue, we introduce RIRO, a novel two-layer architecture designed to improve
performance in data-scarce environments. The first layer leverages advanced
prompt engineering to reformulate inputs, ensuring better alignment with
training data, while the second layer focuses on refining outputs to minimize
inconsistencies. Through fine-tuning models like Phi-2, Falcon 7B, and Falcon
1B, with Phi-2 outperforming the others. Additionally, we introduce a benchmark
using evaluation metrics such as cosine similarity, Levenshtein distance, BLEU
score, ROUGE-1, ROUGE-2, and ROUGE-L. While these advancements improve
performance, challenges like computational demands and overfitting persist,
limiting the potential of LLMs in data-scarce, high-stakes environments such as
healthcare, legal documentation, and software testing.
