---
layout: publication
title: Predicting Issue Types With Sebert
authors: Trautsch Alexander, Herbold Steffen
conference: "Arxiv"
year: 2022
bibkey: trautsch2022predicting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2205.01335"}
tags: ['BERT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Pre45;trained transformer models are the current state45;of45;the45;art for natural language models processing. seBERT is such a model that was developed based on the BERT architecture but trained from scratch with software engineering data. We fine45;tuned this model for the NLBSE challenge for the task of issue type prediction. Our model dominates the baseline fastText for all three issue types in both recall and precisio125; to achieve an overall F145;score of 85.737; which is an increase of 4.137; over the baseline.
