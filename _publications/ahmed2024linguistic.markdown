---
layout: publication
title: Linguistic Intelligence In Large Language Models For Telecommunications
authors: Ahmed Tasnim, Piovesan Nicola, De Domenico Antonio, Choudhury Salimur
conference: "Arxiv"
year: 2024
bibkey: ahmed2024linguistic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.15818"}
tags: ['Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
Large Language Models (LLMs) have emerged as a significant advancement in the field of Natural Language Processing (NLP) demonstrating remarkable capabilities in language generation and other language45;centric tasks. Despite their evaluation across a multitude of analytical and reasoning tasks in various scientific domains a comprehensive exploration of their knowledge and understanding within the realm of natural language tasks in the telecommunications domain is still needed. This study therefore seeks to evaluate the knowledge and understanding capabilities of LLMs within this domain. To achieve this we conduct an exhaustive zero45;shot evaluation of four prominent LLMs45;Llama45;2 Falcon Mistral and Zephyr. These models require fewer resources than ChatGPT making them suitable for resource45;constrained environments. Their performance is compared with state45;of45;the45;art fine45;tuned models. To the best of our knowledge this is the first work to extensively evaluate and compare the understanding of LLMs across multiple language45;centric tasks in this domain. Our evaluation reveals that zero45;shot LLMs can achieve performance levels comparable to the current state45;of45;the45;art fine45;tuned models. This indicates that pretraining on extensive text corpora equips LLMs with a degree of specialization even within the telecommunications domain. We also observe that no single LLM consistently outperforms others and the performance of different LLMs can fluctuate. Although their performance lags behind fine45;tuned models our findings underscore the potential of LLMs as a valuable resource for understanding various aspects of this field that lack large annotated data.
