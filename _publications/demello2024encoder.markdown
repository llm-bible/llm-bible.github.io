---
layout: publication
title: Pelle Encoder45;based Language Models For Brazilian Portuguese Based On Open Data
authors: De Mello Guilherme Lamartine, Finger Marcelo, Serras And Felipe, Carpi Miguel De Mello, Jose Marcos Menon, Domingues Pedro Henrique, Cavalim Paulo
conference: "Arxiv"
year: 2024
bibkey: demello2024encoder
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.19204"}
tags: ['BERT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
In this paper we present PeLLE a family of large language models based on the RoBERTa architecture for Brazilian Portuguese trained on curated open data from the Carolina corpus. Aiming at reproducible results we describe details of the pretraining of the models. We also evaluate PeLLE models against a set of existing multilingual and PT45;BR refined pretrained Transformer45;based LLM encoders contrasting performance of large versus smaller45;but45;curated pretrained models in several downstream tasks. We conclude that several tasks perform better with larger models but some tasks benefit from smaller45;but45;curated data in its pretraining.
