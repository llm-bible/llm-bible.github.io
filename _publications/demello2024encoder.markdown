---
layout: publication
title: 'Pelle: Encoder-based Language Models For Brazilian Portuguese Based On Open Data'
authors: Guilherme Lamartine De Mello, Marcelo Finger, And Felipe Serras, Miguel De Mello Carpi, Marcos Menon Jose, Pedro Henrique Domingues, Paulo Cavalim
conference: "Arxiv"
year: 2024
bibkey: demello2024encoder
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2402.19204'}
tags: ['Transformer', 'Training Techniques', 'BERT', 'Model Architecture', 'Pretraining Methods']
---
In this paper we present PeLLE, a family of large language models based on
the RoBERTa architecture, for Brazilian Portuguese, trained on curated, open
data from the Carolina corpus. Aiming at reproducible results, we describe
details of the pretraining of the models. We also evaluate PeLLE models against
a set of existing multilingual and PT-BR refined pretrained Transformer-based
LLM encoders, contrasting performance of large versus smaller-but-curated
pretrained models in several downstream tasks. We conclude that several tasks
perform better with larger models, but some tasks benefit from
smaller-but-curated data in its pretraining.
