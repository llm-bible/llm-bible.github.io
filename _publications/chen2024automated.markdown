---
layout: publication
title: 'Automated Evaluation Of Large Vision-language Models On Self-driving Corner Cases'
authors: Kai Chen, Yanze Li, Wenhua Zhang, Yanxin Liu, Pengxiang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, Dit-yan Yeung, Huchuan Lu, Xu Jia
conference: "Arxiv"
year: 2024
bibkey: chen2024automated
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.10595"}
tags: ['GPT', 'Model Architecture', 'Attention Mechanism', 'Multimodal Models', 'Prompting']
---
Large Vision-Language Models (LVLMs) have received widespread attention for
advancing the interpretable self-driving. Existing evaluations of LVLMs
primarily focus on multi-faceted capabilities in natural circumstances, lacking
automated and quantifiable assessment for self-driving, let alone the severe
road corner cases. In this work, we propose CODA-LM, the very first benchmark
for the automatic evaluation of LVLMs for self-driving corner cases. We adopt a
hierarchical data structure and prompt powerful LVLMs to analyze complex
driving scenes and generate high-quality pre-annotations for the human
annotators, while for LVLM evaluation, we show that using the text-only large
language models (LLMs) as judges reveals even better alignment with human
preferences than the LVLM judges. Moreover, with our CODA-LM, we build
CODA-VLM, a new driving LVLM surpassing all open-sourced counterparts on
CODA-LM. Our CODA-VLM performs comparably with GPT-4V, even surpassing GPT-4V
by +21.42% on the regional perception task. We hope CODA-LM can become the
catalyst to promote interpretable self-driving empowered by LVLMs.
