---
layout: publication
title: 'Not The Silver Bullet: Llm-enhanced Programming Error Messages Are Ineffective In Practice'
authors: Eddie Antonio Santos, Brett A. Becker
conference: "Arxiv"
year: 2024
bibkey: santos2024not
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.18661"}
tags: ['Interpretability and Explainability', 'GPT', 'Model Architecture']
---
The sudden emergence of large language models (LLMs) such as ChatGPT has had
a disruptive impact throughout the computing education community. LLMs have
been shown to excel at producing correct code to CS1 and CS2 problems, and can
even act as friendly assistants to students learning how to code. Recent work
shows that LLMs demonstrate unequivocally superior results in being able to
explain and resolve compiler error messages -- for decades, one of the most
frustrating parts of learning how to code. However, LLM-generated error message
explanations have only been assessed by expert programmers in artificial
conditions. This work sought to understand how novice programmers resolve
programming error messages (PEMs) in a more realistic scenario. We ran a
within-subjects study with \\(n\\) = 106 participants in which students were tasked
to fix six buggy C programs. For each program, participants were randomly
assigned to fix the problem using either a stock compiler error message, an
expert-handwritten error message, or an error message explanation generated by
GPT-4. Despite promising evidence on synthetic benchmarks, we found that GPT-4
generated error messages outperformed conventional compiler error messages in
only 1 of the 6 tasks, measured by students' time-to-fix each problem.
Handwritten explanations still outperform LLM and conventional error messages,
both on objective and subjective measures.
