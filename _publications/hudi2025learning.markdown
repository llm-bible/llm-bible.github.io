---
layout: publication
title: 'Textgames: Learning To Self-play Text-based Puzzle Games Via Language Model Reasoning'
authors: Frederikus Hudi, Genta Indra Winata, Ruochen Zhang, Alham Fikri Aji
conference: "Arxiv"
year: 2025
bibkey: hudi2025learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.18431"}
tags: ['RAG']
---
Reasoning is a fundamental capability of large language models (LLMs),
enabling them to comprehend, analyze, and solve complex problems. In this
paper, we introduce TextGames, an innovative benchmark specifically crafted to
assess LLMs through demanding text-based games that require advanced skills in
pattern recognition, spatial awareness, arithmetic, and logical reasoning. Our
analysis probes LLMs' performance in both single-turn and multi-turn reasoning,
and their abilities in leveraging feedback to correct subsequent answers
through self-reflection. Our findings reveal that, although LLMs exhibit
proficiency in addressing most easy and medium-level problems, they face
significant challenges with more difficult tasks. In contrast, humans are
capable of solving all tasks when given sufficient time. Moreover, we observe
that LLMs show improved performance in multi-turn predictions through
self-reflection, yet they still struggle with sequencing, counting, and
following complex rules consistently. Additionally, models optimized for
reasoning outperform pre-trained LLMs that prioritize instruction following,
highlighting the crucial role of reasoning skills in addressing highly complex
problems.
