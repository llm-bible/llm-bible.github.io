---
layout: publication
title: Logicot Logical Chain45;of45;thought Instruction45;tuning
authors: Liu Hanmeng, Teng Zhiyang, Cui Leyang, Zhang Chaoli, Zhou Qiji, Zhang Yue
conference: "Arxiv"
year: 2023
bibkey: liu2023logical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.12147"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Transformer']
---
Generative Pre45;trained Transformer 4 (GPT45;4) demonstrates impressive chain45;of45;thought reasoning ability. Recent work on self45;instruction tuning such as Alpaca has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT45;3.5 on general tasks like open45;domain text generation and paraphrasing. However they fall short of helping the model handle complex reasoning tasks. To bridge the gap this paper presents LogiCoT a new instruction45;tuning dataset for Logical Chain45;of45;Thought reasoning with GPT45;4. We elaborate on the process of harvesting instructions for prompting GPT45;4 to generate chain45;of45;thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills.
