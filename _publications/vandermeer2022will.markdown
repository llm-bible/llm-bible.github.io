---
layout: publication
title: Will It Blend Mixing Training Paradigms amp; Prompting For Argument Quality Prediction
authors: Van Der Meer Michiel, Reuver Myrthe, Khurana Urja, Krause Lea, Santamaría Selene Báez
conference: "Arxiv"
year: 2022
bibkey: vandermeer2022will
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2209.08966"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Training Techniques']
---
This paper describes our contributions to the Shared Task of the 9th Workshop on Argument Mining (2022). Our approach uses Large Language Models for the task of Argument Quality Prediction. We perform prompt engineering using GPT45;3 and also investigate the training paradigms multi45;task learning contrastive learning and intermediate45;task training. We find that a mixed prediction setup outperforms single models. Prompting GPT45;3 works best for predicting argument validity and argument novelty is best estimated by a model trained using all three training paradigms.
