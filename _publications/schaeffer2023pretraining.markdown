---
layout: publication
title: Pretraining On The Test Set Is All You Need
authors: Schaeffer Rylan
conference: "Arxiv"
year: 2023
bibkey: schaeffer2023pretraining
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.08632"}
tags: ['Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Inspired by recent work demonstrating the promise of smaller Transformer45;based language models pretrained on carefully curated data we supercharge such approaches by investing heavily in curating a novel high quality non45;synthetic data mixture based solely on evaluation benchmarks. Using our novel dataset mixture consisting of less than 100 thousand tokens we pretrain a 1 million parameter transformer45;based LLM textbf123;phi45;CTNL125; (pronounced fictional) that achieves perfect results across diverse academic benchmarks strictly outperforming all known foundation models. textbf123;phi45;CTNL125; also beats power45;law scaling and exhibits a never45;before45;seen grokking45;like ability to accurately predict downstream evaluation benchmarks canaries.
