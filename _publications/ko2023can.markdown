---
layout: publication
title: Can Language Models Laugh At Youtube Short45;form Videos
authors: Ko Dayoon, Lee Sangho, Kim Gunhee
conference: "Arxiv"
year: 2023
bibkey: ko2023can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.14159"}
tags: ['GPT', 'Interpretability And Explainability', 'Model Architecture', 'Multimodal Models', 'Prompting']
---
As short45;form funny videos on social networks are gaining popularity it becomes demanding for AI models to understand them for better communication with humans. Unfortunately previous video humor datasets target specific domains such as speeches or sitcoms and mostly focus on verbal cues. We curate a user45;generated dataset of 10K multimodal funny videos from YouTube called ExFunTube. Using a video filtering pipeline with GPT45;3.5 we verify both verbal and visual elements contributing to humor. After filtering we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also we develop a zero45;shot video45;to45;text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores rationale quality experiments and human evaluations we show that our prompting significantly improves LLMs ability for humor explanation.
