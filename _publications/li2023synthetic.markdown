---
layout: publication
title: 'Synthetic Data Generation With Large Language Models For Text Classification:
  Potential And Limitations'
authors: Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ming Yin
conference: Arxiv
year: 2023
citations: 34
bibkey: li2023synthetic
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2310.07849'}]
tags: [Reinforcement Learning]
---
The collection and curation of high-quality training data is crucial for
developing text classification models with superior performance, but it is
often associated with significant costs and time investment. Researchers have
recently explored using large language models (LLMs) to generate synthetic
datasets as an alternative approach. However, the effectiveness of the
LLM-generated synthetic data in supporting model training is inconsistent
across different classification tasks. To better understand factors that
moderate the effectiveness of the LLM-generated synthetic data, in this study,
we look into how the performance of models trained on these synthetic data may
vary with the subjectivity of classification. Our results indicate that
subjectivity, at both the task level and instance level, is negatively
associated with the performance of the model trained on synthetic data. We
conclude by discussing the implications of our work on the potential and
limitations of leveraging LLM for synthetic data generation.