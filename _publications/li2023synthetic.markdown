---
layout: publication
title: Synthetic Data Generation With Large Language Models For Text Classification&#58; Potential And Limitations
authors: Li Zhuoyan, Zhu Hangxiao, Lu Zhuoran, Yin Ming
conference: "Arxiv"
year: 2023
bibkey: li2023synthetic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.07849"}
tags: ['Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
The collection and curation of high-quality training data is crucial for developing text classification models with superior performance but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data in this study we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity at both the task level and instance level is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.
