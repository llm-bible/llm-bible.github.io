---
layout: publication
title: Long Time No See! Open-domain Conversation With Long-term Persona Memory
authors: Xinchao Xu et al.
conference: Arxiv
year: 2022
citations: 18
bibkey: xu2022long
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2203.05797'}]
tags: [Reinforcement Learning]
---
Most of the open-domain dialogue models tend to perform poorly in the setting
of long-term human-bot conversations. The possible reason is that they lack the
capability of understanding and memorizing long-term dialogue history
information. To address this issue, we present a novel task of Long-term Memory
Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a
dialogue generation framework with Long-Term Memory (LTM) mechanism (called
PLATO-LTM). This LTM mechanism enables our system to accurately extract and
continuously update long-term persona memory without requiring multiple-session
dialogue datasets for model training. To our knowledge, this is the first
attempt to conduct real-time dynamic management of persona information of both
parties, including the user and the bot. Results on DuLeMon indicate that
PLATO-LTM can significantly outperform baselines in terms of long-term dialogue
consistency, leading to better dialogue engagingness.