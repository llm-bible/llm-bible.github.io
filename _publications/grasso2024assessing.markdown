---
layout: publication
title: 'Assessing Generative Language Models In Classification Tasks: Performance And Self-evaluation Capabilities In The Environmental And Climate Change Domain'
authors: Francesca Grasso, Stefano Locci
conference: "Arxiv"
year: 2024
bibkey: grasso2024assessing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.17362"}
tags: ['Model Architecture', 'GPT', 'Pretraining Methods', 'BERT', 'Transformer']
---
This paper examines the performance of two Large Language Models (LLMs),
GPT3.5 and Llama2 and one Small Language Model (SLM) Gemma, across three
different classification tasks within the climate change (CC) and environmental
domain. Employing BERT-based models as a baseline, we compare their efficacy
against these transformer-based models. Additionally, we assess the models'
self-evaluation capabilities by analyzing the calibration of verbalized
confidence scores in these text classification tasks. Our findings reveal that
while BERT-based models generally outperform both the LLMs and SLM, the
performance of the large generative models is still noteworthy. Furthermore,
our calibration analysis reveals that although Gemma is well-calibrated in
initial tasks, it thereafter produces inconsistent results; Llama is reasonably
calibrated, and GPT consistently exhibits strong calibration. Through this
research, we aim to contribute to the ongoing discussion on the utility and
effectiveness of generative LMs in addressing some of the planet's most urgent
issues, highlighting their strengths and limitations in the context of ecology
and CC.
