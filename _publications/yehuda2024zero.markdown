---
layout: publication
title: Interrogatellm Zero-resource Hallucination Detection In Llm-generated Answers
authors: Yehuda Yakir, Malkiel Itzik, Barkan Oren, Weill Jonathan, Ronen Royi, Koenigstein Noam
conference: "https://aclanthology.org/"
year: 2024
bibkey: yehuda2024zero
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.02889"}
tags: ['Pretraining Methods', 'Reinforcement Learning', 'Tools']
---
Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations where LLMs invent answers that sound realistic yet drift away from factual truth. In this paper we present a novel method for detecting hallucinations in large language models which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs including Llama-2 we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably we observe up to 8737; hallucinations for Llama-2 in a specific experiment where our method achieves a Balanced Accuracy of 8137; all without relying on external knowledge.
