---
layout: publication
title: 'Anteval: Evaluation Of Social Interaction Competencies In Llm-driven Agents'
authors: Yuanzhi Liang, Linchao Zhu, Yi Yang
conference: "Arxiv"
year: 2024
bibkey: liang2024evaluation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.06509"}
tags: ['Agentic', 'Tools']
---
Large Language Models (LLMs) have demonstrated their ability to replicate
human behaviors across a wide range of scenarios. However, their capability in
handling complex, multi-character social interactions has yet to be fully
explored, primarily due to the absence of robust, quantitative evaluation
methods. This gap has slowed the development of agents proficient in more
nuanced interactions beyond simple exchanges, for example, small talk. To
address this challenge, we introduce the Multi-Agent Interaction Evaluation
Framework (AntEval), encompassing a novel interaction framework and evaluation
methods. The interaction framework aims to foster an complex interaction
environment that bolsters information exchange and intention expression within
social interactions. Furthermore, we introduce evaluation methods, including
two metrics: Information Exchanging Precision (IEP) and Interaction
Expressiveness Gap (IEG), designed for the quantitative and objective
assessment of agents' interaction competencies. Our findings highlight the
utility of these evaluative methods and show significant potential for
improving LLMs' ability to construct agents that interact in a more natural
manner with human-like intricacy.
