---
layout: publication
title: SAIL Search45;augmented Instruction Learning
authors: Luo Hongyin, Chuang Yung-sung, Gong Yuan, Zhang Tianhua, Kim Yoon, Wu Xixin, Fox Danny, Meng Helen, Glass James
conference: "Arxiv"
year: 2023
bibkey: luo2023search
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.15225"}
tags: ['Applications', 'Ethics And Bias', 'Tools', 'Training Techniques']
---
Large language models (LLMs) have been significantly improved by instruction fine45;tuning but still lack transparency and the ability to utilize up45;to45;date knowledge and information. In this work we propose search45;augmented instruction learning (SAIL) which grounds the language generation and instruction following abilities on complex search results generated by in45;house and external search engines. With an instruction tuning corpus we collect search results for each training case from different search APIs and domains and construct a new search45;grounded training set containing textit123;(instruction grounding information response)125; triplets. We then fine45;tune the LLaMA45;7B model on the constructed training set. Since the collected results contain unrelated and disputing languages the model needs to learn to ground on trustworthy search results filter out distracting passages and generate the target response. The search result45;denoising process entails explicit trustworthy information selection and multi45;hop reasoning since the retrieved passages might be informative but not contain the instruction45;following answer. Experiments show that the fine45;tuned SAIL45;7B model has a strong instruction45;following ability and it performs significantly better on transparency45;sensitive tasks including open45;ended question answering and fact checking.
