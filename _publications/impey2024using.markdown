---
layout: publication
title: 'Using Large Language Models For Automated Grading Of Student Writing About Science'
authors: Chris Impey, Matthew Wenger, Nikhil Garuda, Shahriar Golchin, Sarah Stamer
conference: "Arxiv"
year: 2024
bibkey: impey2024using
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.18719"}
tags: ['Model Architecture', 'GPT', 'Tools', 'Reinforcement Learning']
---
Assessing writing in large classes for formal or informal learners presents a
significant challenge. Consequently, most large classes, particularly in
science, rely on objective assessment tools such as multiple-choice quizzes,
which have a single correct answer. The rapid development of AI has introduced
the possibility of using large language models (LLMs) to evaluate student
writing. An experiment was conducted using GPT-4 to determine if machine
learning methods based on LLMs can match or exceed the reliability of
instructor grading in evaluating short writing assignments on topics in
astronomy. The audience consisted of adult learners in three massive open
online courses (MOOCs) offered through Coursera. One course was on astronomy,
the second was on astrobiology, and the third was on the history and philosophy
of astronomy. The results should also be applicable to non-science majors in
university settings, where the content and modes of evaluation are similar. The
data comprised answers from 120 students to 12 questions across the three
courses. GPT-4 was provided with total grades, model answers, and rubrics from
an instructor for all three courses. In addition to evaluating how reliably the
LLM reproduced instructor grades, the LLM was also tasked with generating its
own rubrics. Overall, the LLM was more reliable than peer grading, both in
aggregate and by individual student, and approximately matched instructor
grades for all three online courses. The implication is that LLMs may soon be
used for automated, reliable, and scalable grading of student science writing.
