---
layout: publication
title: 'Seeing Is Understanding: Unlocking Causal Attention Into Modality-mutual Attention For Multimodal Llms'
authors: Wei-yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi
conference: "Arxiv"
year: 2025
bibkey: wang2025seeing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.02597"}
  - {name: "Code", url: "https://github.com/sony/aki"}
tags: ['Multimodal Models', 'Training Techniques', 'Model Architecture', 'Reinforcement Learning', 'RAG', 'Transformer', 'Has Code', 'Attention Mechanism']
---
Recent Multimodal Large Language Models (MLLMs) have demonstrated significant
progress in perceiving and reasoning over multimodal inquiries, ushering in a
new research era for foundation models. However, vision-language misalignment
in MLLMs has emerged as a critical challenge, where the textual responses
generated by these models are not factually aligned with the given text-image
inputs. Existing efforts to address vision-language misalignment have focused
on developing specialized vision-language connectors or leveraging visual
instruction tuning from diverse domains. In this paper, we tackle this issue
from a fundamental yet unexplored perspective by revisiting the core
architecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs
consisting of a causal attention mechanism, which limits the ability of the
earlier modalities (e.g., images) to incorporate information from the latter
modalities (e.g., text). To address this problem, we propose \MapleLeaf AKI, a
novel MLLM that unlocks causal attention into modality-mutual attention (MMA)
to enable image tokens to attend to text tokens. This simple yet effective
design allows AKI to achieve superior performance in 12 multimodal
understanding benchmarks (+7.2% on average) without introducing additional
parameters and increasing training time. Our MMA design is intended to be
generic, allowing for application across various modalities, and scalable to
accommodate diverse multimodal scenarios. The code and model are publicly
available at https://github.com/sony/aki to encourage further advancements in
MLLMs across various directions.
