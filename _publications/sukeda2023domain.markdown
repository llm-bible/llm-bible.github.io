---
layout: publication
title: Jmedloramedical Domain Adaptation On Japanese Large Language Models Using Instruction45;tuning
authors: Sukeda Issey, Suzuki Masahiro, Sakaji Hiroki, Kodera Satoshi
conference: "Arxiv"
year: 2023
bibkey: sukeda2023domain
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.10083"}
tags: ['Applications', 'Fine Tuning', 'GPT', 'Model Architecture']
---
In the ongoing wave of impact driven by large language models (LLMs) like ChatGPT the adaptation of LLMs to medical domain has emerged as a crucial research frontier. Since mainstream LLMs tend to be designed for general45;purpose applications constructing a medical LLM through domain adaptation is a huge challenge. While instruction45;tuning is used to fine45;tune some LLMs its precise roles in domain adaptation remain unknown. Here we show the contribution of LoRA45;based instruction45;tuning to performance in Japanese medical question45;answering tasks. In doing so we employ a multifaceted evaluation for multiple45;choice questions including scoring based on Exact match and Gestalt distance in addition to the conventional accuracy. Our findings suggest that LoRA45;based instruction45;tuning can partially incorporate domain45;specific knowledge into LLMs with larger models demonstrating more pronounced effects. Furthermore our results underscore the potential of adapting English45;centric models for Japanese applications in domain adaptation while also highlighting the persisting limitations of Japanese45;centric models. This initiative represents a pioneering effort in enabling medical institutions to fine45;tune and operate models without relying on external services.
