---
layout: publication
title: Parrot Enhancing Multi45;turn Instruction Following For Large Language Models
authors: Sun Yuchong, Liu Che, Zhou Kun, Huang Jinwen, Song Ruihua, Zhao Wayne Xin, Zhang Fuzheng, Zhang Di, Gai Kun
conference: "Arxiv"
year: 2023
bibkey: sun2023enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.07301"}
tags: ['Efficiency And Optimization', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
Humans often interact with large language models (LLMs) in multi45;turn interaction to obtain desired answers or more information. However most existing studies overlook the multi45;turn instruction following ability of LLMs in terms of training dataset training method and evaluation benchmark. In this paper we introduce Parrot a solution aiming to enhance multi45;turn instruction following for LLMs. First we introduce an efficient but effective method for collecting multi45;turn instructions that feature human45;like queries such as anaphora and ellipsis. Second we propose a context45;aware preference optimization strategy to further enhance LLMs for complex queries in multi45;turn interaction. Moreover to quantitatively evaluate LLMs in multi45;turn instruction following we manually build a multi45;turn benchmark derived from existing ones. Extensive experiments show that Parrot improves current LLMs by up to 7.237; in multi45;turn instruction following. Our dataset and codes will be open45;sourced to facilitate future research.
