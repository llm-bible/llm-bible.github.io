---
layout: publication
title: Augmenting Llms With Knowledge A Survey On Hallucination Prevention
authors: Andriopoulos Konstantinos, Pouwelse Johan
conference: "Arxiv"
year: 2023
bibkey: andriopoulos2023augmenting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.16459"}
tags: ['Fine Tuning', 'Language Modeling', 'Merging', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Survey Paper']
---
Large pre45;trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine45;tuned for downstream natural language processing tasks. Nonetheless their capacity to access and manipulate knowledge with precision remains constrained resulting in performance disparities on knowledge45;intensive tasks when compared to task45;specific architectures. Additionally the challenges of providing provenance for model decisions and maintaining up45;to45;date world knowledge persist as open research frontiers. To address these limitations the integration of pre45;trained models with differentiable access mechanisms to explicit non45;parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources including external knowledge bases and search engines. While adhering to the standard objective of predicting missing tokens these augmented LMs leverage diverse possibly non45;parametric external modules to augment their contextual processing capabilities departing from the conventional language modeling paradigm. Through an exploration of current advancements in augmenting large language models with knowledge this work concludes that this emerging research direction holds the potential to address prevalent issues in traditional LMs such as hallucinations un45;grounded responses and scalability challenges.
