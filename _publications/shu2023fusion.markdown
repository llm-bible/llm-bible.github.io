---
layout: publication
title: Fusion45;eval Integrating Assistant Evaluators With Llms
authors: Shu Lei, Wichers Nevan, Luo Liangchen, Zhu Yun, Liu Yinxiao, Chen Jindong, Meng Lei
conference: "Arxiv"
year: 2023
bibkey: shu2023fusion
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.09204"}
tags: ['Applications', 'Merging', 'RAG', 'Reinforcement Learning']
---
Evaluating natural language systems poses significant challenges particularly in the realms of natural language understanding and high45;level reasoning. In this paper we introduce Fusion45;Eval an innovative approach that leverages Large Language Models (LLMs) to integrate insights from various assistant evaluators. The LLM is given the example to evaluate along with scores from the assistant evaluators. Each of these evaluators specializes in assessing distinct aspects of responses. Fusion45;Eval achieves a 0.962 system45;level Kendall45;Tau correlation with humans on SummEval and a 0.744 turn45;level Spearman correlation on TopicalChat which is significantly higher than baseline methods. These results highlight Fusion45;Evals significant potential in the realm of natural language system evaluation.
