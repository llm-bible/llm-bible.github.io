---
layout: publication
title: 'How To Mitigate Information Loss In Knowledge Graphs For Graphrag: Leveraging Triple Context Restoration And Query-driven Feedback'
authors: Manzong Huang, Chenyang Bu, Yi He, Xindong Wu
conference: "Arxiv"
year: 2025
bibkey: huang2025how
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.15378'}
tags: ['Reinforcement Learning', 'RAG', 'Applications', 'Tools']
---
Knowledge Graph (KG)-augmented Large Language Models (LLMs) have recently
propelled significant advances in complex reasoning tasks, thanks to their
broad domain knowledge and contextual awareness. Unfortunately, current methods
often assume KGs to be complete, which is impractical given the inherent
limitations of KG construction and the potential loss of contextual cues when
converting unstructured text into entity-relation triples. In response, this
paper proposes the Triple Context Restoration and Query-driven Feedback
(TCR-QF) framework, which reconstructs the textual context underlying each
triple to mitigate information loss, while dynamically refining the KG
structure by iteratively incorporating query-relevant missing knowledge.
Experiments on five benchmark question-answering datasets substantiate the
effectiveness of TCR-QF in KG and LLM integration, where itachieves a 29.1%
improvement in Exact Match and a 15.5% improvement in F1 over its
state-of-the-art GraphRAG competitors.
