---
layout: publication
title: 'Can Large Language Models Capture Video Game Engagement?'
authors: David Melhart, Matthew Barthet, Georgios N. Yannakakis
conference: "Arxiv"
year: 2025
bibkey: melhart2025can
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.04379'}
tags: ['Model Architecture', 'Fine-Tuning', 'Prompting', 'Multimodal Models', 'Reinforcement Learning']
---
Can out-of-the-box pretrained Large Language Models (LLMs) detect human
affect successfully when observing a video? To address this question, for the
first time, we evaluate comprehensively the capacity of popular LLMs to
annotate and successfully predict continuous affect annotations of videos when
prompted by a sequence of text and video frames in a multimodal fashion.
Particularly in this paper, we test LLMs' ability to correctly label changes of
in-game engagement in 80 minutes of annotated videogame footage from 20
first-person shooter games of the GameVibe corpus. We run over 2,400
experiments to investigate the impact of LLM architecture, model size, input
modality, prompting strategy, and ground truth processing method on engagement
prediction. Our findings suggest that while LLMs rightfully claim human-like
performance across multiple domains, they generally fall behind capturing
continuous experience annotations provided by humans. We examine some of the
underlying causes for the relatively poor overall performance, highlight the
cases where LLMs exceed expectations, and draw a roadmap for the further
exploration of automated emotion labelling via LLMs.
