---
layout: publication
title: 'Integrating UMLS Knowledge Into Large Language Models For Medical Question Answering'
authors: Rui Yang, Edison Marrese-taylor, Yuhe Ke, Lechao Cheng, Qingyu Chen, Irene Li
conference: "Arxiv"
year: 2023
bibkey: yang2023integrating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.02778"}
tags: ['Tools', 'GPT', 'Applications', 'Ethics and Bias', 'Model Architecture', 'Language Modeling', 'BERT']
---
Large language models (LLMs) have demonstrated powerful text generation
capabilities, bringing unprecedented innovation to the healthcare field. While
LLMs hold immense promise for applications in healthcare, applying them to real
clinical scenarios presents significant challenges, as these models may
generate content that deviates from established medical facts and even exhibit
potential biases. In our research, we develop an augmented LLM framework based
on the Unified Medical Language System (UMLS), aiming to better serve the
healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our
benchmark models, and conduct automatic evaluations using the ROUGE Score and
BERTScore on 104 questions from the LiveQA test set. Additionally, we establish
criteria for physician-evaluation based on four dimensions: Factuality,
Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician
evaluation with 20 questions on the LiveQA test set. Multiple resident
physicians conducted blind reviews to evaluate the generated content, and the
results indicate that this framework effectively enhances the factuality,
completeness, and relevance of generated content. Our research demonstrates the
effectiveness of using UMLS-augmented LLMs and highlights the potential
application value of LLMs in in medical question-answering.
