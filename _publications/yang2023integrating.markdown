---
layout: publication
title: Integrating UMLS Knowledge Into Large Language Models For Medical Question Answering
authors: Yang Rui, Marrese-taylor Edison, Ke Yuhe, Cheng Lechao, Chen Qingyu, Li Irene
conference: "Arxiv"
year: 2023
bibkey: yang2023integrating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.02778"}
tags: ['Applications', 'BERT', 'Ethics And Bias', 'GPT', 'Language Modeling', 'Model Architecture', 'Tools']
---
Large language models (LLMs) have demonstrated powerful text generation capabilities bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare applying them to real clinical scenarios presents significant challenges as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research we develop an augmented LLM framework based on the Unified Medical Language System (UMLS) aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally we establish criteria for physician-evaluation based on four dimensions Factuality Completeness Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conducted blind reviews to evaluate the generated content and the results indicate that this framework effectively enhances the factuality completeness and relevance of generated content. Our research demonstrates the effectiveness of using UMLS-augmented LLMs and highlights the potential application value of LLMs in in medical question-answering.
