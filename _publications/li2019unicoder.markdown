---
layout: publication
title: Unicoder45;vl A Universal Encoder For Vision And Language By Cross45;modal Pre45;training
authors: Li Gen, Duan Nan, Fang Yuejian, Gong Ming, Jiang Daxin, Zhou Ming
conference: "Arxiv"
year: 2019
bibkey: li2019unicoder
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1908.06066"}
tags: ['BERT', 'Language Modeling', 'Masked Language Model', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
We propose Unicoder45;VL a universal encoder that aims to learn joint representations of vision and language in a pre45;training manner. Borrow ideas from cross45;lingual pre45;trained models such as XLM and Unicoder both visual and linguistic contents are fed into a multi45;layer Transformer for the cross45;modal pre45;training where three pre45;trained tasks are employed including Masked Language Modeling (MLM) Masked Object Classification (MOC) and Visual45;linguistic Matching (VLM). The first two tasks learn context45;aware representations for input tokens based on linguistic and visual contents jointly. The last task tries to predict whether an image and a text describe each other. After pretraining on large45;scale image45;caption pairs we transfer Unicoder45;VL to caption45;based image45;text retrieval and visual commonsense reasoning with just one additional output layer. We achieve state45;of45;the45;art or comparable results on both two tasks and show the powerful ability of the cross45;modal pre45;training.
