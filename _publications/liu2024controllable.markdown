---
layout: publication
title: Controllable And Diverse Data Augmentation With Large Language Model For Low45;resource Open45;domain Dialogue Generation
authors: Liu Zhenhua, Zhu Tong, Xiang Jianxiang, Chen Wenliang
conference: "Arxiv"
year: 2024
bibkey: liu2024controllable
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.00361"}
tags: ['Applications', 'Training Techniques']
---
Data augmentation (DA) is crucial to mitigate model training instability and over45;fitting problems in low45;resource open45;domain dialogue generation. However traditional DA methods often neglect semantic data diversity restricting the overall quality. Recently large language models (LLM) have been used for DA to generate diversified dialogues. However they have limited controllability and tend to generate dialogues with a distribution shift compared to the seed dialogues. To maximize the augmentation diversity and address the controllability problem we propose textbf123;S125;ummary45;based textbf123;D125;ialogue textbf123;A125;ugmentation with LLM (SDA). Our approach enhances the controllability of LLM by using dialogue summaries as a planning tool. Based on summaries SDA can generate high45;quality and diverse dialogue data even with a small seed dataset. To evaluate the efficacy of data augmentation methods for open45;domain dialogue we designed a clustering45;based metric to characterize the semantic diversity of the augmented dialogue data. The experimental results show that SDA can augment high45;quality and semantically diverse dialogues given a small seed dataset and an LLM and the augmented data can boost the performance of open45;domain dialogue models.
