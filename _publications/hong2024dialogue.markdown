---
layout: publication
title: 'Dialogue Language Model With Large-scale Persona Data Engineering'
authors: Mengze Hong, Chen Jason Zhang, Chaotao Chen, Rongzhong Lian, Di Jiang
conference: "Arxiv"
year: 2024
bibkey: hong2024dialogue
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.09034"}
tags: ['Pre-Training', 'GPT', 'Applications', 'Ethics and Bias', 'Model Architecture', 'Training Techniques']
---
Maintaining persona consistency is paramount in the application of
open-domain dialogue systems, as exemplified by models like ChatGPT. Despite
significant advancements, the limited scale and diversity of current persona
dialogue datasets remain challenges to achieving robust persona-consistent
dialogue models. In this study, drawing inspiration from the success of
large-scale pre-training, we introduce PPDS, an open-domain persona dialogue
system that employs extensive generative pre-training on a persona dialogue
dataset to enhance persona consistency. Specifically, we present a persona
extraction model designed to autonomously and precisely generate vast persona
dialogue datasets. Additionally, we unveil a pioneering persona augmentation
technique to address the invalid persona bias inherent in the constructed
dataset. Both quantitative and human evaluations consistently highlight the
superior response quality and persona consistency of our proposed model,
underscoring its effectiveness.
