---
layout: publication
title: 'CUNI System For The WMT19 Robustness Task'
authors: Jindřich Helcl, Jindřich Libovický, Martin Popel
conference: "Arxiv"
year: 2019
bibkey: helcl2019cuni
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/1906.09246'}
tags: ['Transformer', 'WMT', 'Security', 'Training Techniques', 'Model Architecture', 'Fine-Tuning', 'Reinforcement Learning', 'Pretraining Methods']
---
We present our submission to the WMT19 Robustness Task. Our baseline system
is the Charles University (CUNI) Transformer system trained for the WMT18
shared task on News Translation. Quantitative results show that the CUNI
Transformer system is already far more robust to noisy input than the
LSTM-based baseline provided by the task organizers. We further improved the
performance of our model by fine-tuning on the in-domain noisy data without
influencing the translation quality on the news domain.
