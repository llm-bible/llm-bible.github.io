---
layout: publication
title: 'Are Large Language Models Fit For Guided Reading?'
authors: Peter Ochieng
conference: "Arxiv"
year: 2023
bibkey: ochieng2023are
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2305.10645'}
tags: ['Ethics and Bias', 'GPT', 'RAG', 'Model Architecture']
---
This paper looks at the ability of large language models to participate in
educational guided reading. We specifically, evaluate their ability to generate
meaningful questions from the input text, generate diverse questions both in
terms of content coverage and difficulty of the questions and evaluate their
ability to recommend part of the text that a student should re-read based on
the student's responses to the questions. Based on our evaluation of ChatGPT
and Bard, we report that,
  1) Large language models are able to generate high quality meaningful
questions that have high correlation with the input text, 2) They generate
diverse question that cover most topics in the input text even though this
ability is significantly degraded as the input text increases, 3)The large
language models are able to generate both low and high cognitive questions even
though they are significantly biased toward low cognitive question, 4) They are
able to effectively summarize responses and extract a portion of text that
should be re-read.
