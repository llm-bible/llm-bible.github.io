---
layout: publication
title: How Do Large Language Models Capture The Ever45;changing World Knowledge A Review Of Recent Advances
authors: Zhang Zihan, Fang Meng, Chen Ling, Namazi-rad Mohammad-reza, Wang Jun
conference: "Arxiv"
year: 2023
bibkey: zhang2023how
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.07343"}
  - {name: "Code", url: "https://github.com/hyintell/awesome&#45;refreshing&#45;llms"}
tags: ['Has Code', 'Pretraining Methods', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
Although large language models (LLMs) are impressive in solving various tasks they can quickly be outdated after deployment. Maintaining their up45;to45;date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning LLMs with the ever45;changing world knowledge without re45;training from scratch. We categorize research works systemically and provide in45;depth comparisons and discussion. We also discuss existing challenges and highlight future directions to facilitate research in this field. We release the paper list at https://github.com/hyintell/awesome&#45;refreshing&#45;llms
