---
layout: publication
title: How Do Large Language Models Capture The Ever-changing World Knowledge A Review Of Recent Advances
authors: Zhang Zihan, Fang Meng, Chen Ling, Namazi-rad Mohammad-reza, Wang Jun
conference: "Arxiv"
year: 2023
bibkey: zhang2023how
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.07343"}
  - {name: "Code", url: "https://github.com/hyintell/awesome-refreshing-llms"}
tags: ['Has Code', 'Pretraining Methods', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
Although large language models (LLMs) are impressive in solving various tasks they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning LLMs with the ever-changing world knowledge without re-training from scratch. We categorize research works systemically and provide in-depth comparisons and discussion. We also discuss existing challenges and highlight future directions to facilitate research in this field. We release the paper list at https://github.com/hyintell/awesome-refreshing-llms"
