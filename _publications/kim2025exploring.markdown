---
layout: publication
title: 'Exploring Persona-dependent LLM Alignment For The Moral Machine Experiment'
authors: Jiseon Kim, Jea Kwon, Luiz Felipe Vecchietti, Alice Oh, Meeyoung Cha
conference: "Arxiv"
year: 2025
bibkey: kim2025exploring
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.10886'}
tags: ['Reinforcement Learning', 'Ethics and Bias', 'Applications']
---
Deploying large language models (LLMs) with agency in real-world applications
raises critical questions about how these models will behave. In particular,
how will their decisions align with humans when faced with moral dilemmas? This
study examines the alignment between LLM-driven decisions and human judgment in
various contexts of the moral machine experiment, including personas reflecting
different sociodemographics. We find that the moral decisions of LLMs vary
substantially by persona, showing greater shifts in moral decisions for
critical tasks than humans. Our data also indicate an interesting partisan
sorting phenomenon, where political persona predominates the direction and
degree of LLM decisions. We discuss the ethical implications and risks
associated with deploying these models in applications that involve moral
decisions.
