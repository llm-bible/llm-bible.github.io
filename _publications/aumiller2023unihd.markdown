---
layout: publication
title: Unihd At TSAR45;2022 Shared Task Is Compute All We Need For Lexical Simplification
authors: Aumiller Dennis, Gertz Michael
conference: "Arxiv"
year: 2023
bibkey: aumiller2023unihd
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2301.01764"}
  - {name: "Code", url: "https://github.com/dennlinger/TSAR&#45;2022&#45;Shared&#45;Task"}
tags: ['Ethics And Bias', 'GPT', 'Has Code', 'Model Architecture', 'Prompting', 'Training Techniques']
---
Previous state45;of45;the45;art models for lexical simplification consist of complex pipelines with several components each of which requires deep technical knowledge and fine45;tuned interaction to achieve its full potential. As an alternative we describe a frustratingly simple pipeline based on prompted GPT45;3 responses beating competing approaches by a wide margin in settings with few training instances. Our best45;performing submission to the English language track of the TSAR45;2022 shared task consists of an ensemble of six different prompt templates with varying context levels. As a late45;breaking result we further detail a language transfer technique that allows simplification in languages other than English. Applied to the Spanish and Portuguese subset we achieve state45;of45;the45;art results with only minor modification to the original prompts. Aside from detailing the implementation and setup we spend the remainder of this work discussing the particularities of prompting and implications for future work. Code for the experiments is available online at https://github.com/dennlinger/TSAR&#45;2022&#45;Shared&#45;Task
