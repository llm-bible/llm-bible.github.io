---
layout: publication
title: 'LARA: Linguistic-adaptive Retrieval-augmentation For Multi-turn Intent Classification'
authors: Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim
conference: "Arxiv"
year: 2024
bibkey: liu2024linguistic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.16504"}
tags: ['Fine-Tuning', 'Tools', 'RAG', 'Model Architecture', 'Training Techniques', 'Pretraining Methods']
---
Multi-turn intent classification is notably challenging due to the complexity
and evolving nature of conversational contexts. This paper introduces LARA, a
Linguistic-Adaptive Retrieval-Augmentation framework to enhance accuracy in
multi-turn classification tasks across six languages, accommodating a large
number of intents in chatbot interactions. LARA combines a fine-tuned smaller
model with a retrieval-augmented mechanism, integrated within the architecture
of LLMs. The integration allows LARA to dynamically utilize past dialogues and
relevant intents, thereby improving the understanding of the context.
Furthermore, our adaptive retrieval techniques bolster the cross-lingual
capabilities of LLMs without extensive retraining and fine-tuning.
Comprehensive experiments demonstrate that LARA achieves state-of-the-art
performance on multi-turn intent classification tasks, enhancing the average
accuracy by 3.67% from state-of-the-art single-turn intent classifiers.
