---
layout: publication
title: 'Jailbreaking With Universal Multi-prompts'
authors: Yu-ling Hsu, Hsuan Su, Shang-tse Chen
conference: "Arxiv"
year: 2025
bibkey: hsu2025jailbreaking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.01154'}
tags: ['Security', 'Training Techniques', 'Applications', 'Tools', 'Prompting']
---
Large language models (LLMs) have seen rapid development in recent years,
revolutionizing various applications and significantly enhancing convenience
and productivity. However, alongside their impressive capabilities, ethical
concerns and new types of attacks, such as jailbreaking, have emerged. While
most prompting techniques focus on optimizing adversarial inputs for individual
cases, resulting in higher computational costs when dealing with large
datasets. Less research has addressed the more general setting of training a
universal attacker that can transfer to unseen tasks. In this paper, we
introduce JUMP, a prompt-based method designed to jailbreak LLMs using
universal multi-prompts. We also adapt our approach for defense, which we term
DUMP. Experimental results demonstrate that our method for optimizing universal
multi-prompts outperforms existing techniques.
