---
layout: publication
title: On The Evaluation Consistency Of Attribution45;based Explanations
authors: Duan Jiarui, Li Haoling, Zhang Haofei, Jiang Hao, Xue Mengqi, Sun Li, Song Mingli, Song Jie
conference: "Arxiv"
year: 2024
bibkey: duan2024evaluation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.19471"}
  - {name: "Code", url: "https://github.com/TreeThree&#45;R/Meta&#45;Rank&#125;"}
tags: ['Attention Mechanism', 'Has Code', 'Interpretability And Explainability', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Attribution45;based explanations are garnering increasing attention recently and have emerged as the predominant approach towards textit123;eXplanable Artificial Intelligence125;~(XAI). However the absence of consistent configurations and systematic investigations in prior literature impedes comprehensive evaluations of existing methodologies. In this work we introduce 123;Meta45;Rank125; an open platform for benchmarking attribution methods in the image domain. Presently Meta45;Rank assesses eight exemplary attribution methods using six renowned model architectures on four diverse datasets employing both the textit123;Most Relevant First125; (MoRF) and textit123;Least Relevant First125; (LeRF) evaluation protocols. Through extensive experimentation our benchmark reveals three insights in attribution evaluation endeavors 1) evaluating attribution methods under disparate settings can yield divergent performance rankings; 2) although inconsistent across numerous cases the performance rankings exhibit remarkable consistency across distinct checkpoints along the same training trajectory; 3) prior attempts at consistent evaluation fare no better than baselines when extended to more heterogeneous models and datasets. Our findings underscore the necessity for future research in this domain to conduct rigorous evaluations encompassing a broader range of models and datasets and to reassess the assumptions underlying the empirical success of different attribution methods. Our code is publicly available at url123;https://github.com/TreeThree&#45;R/Meta&#45;Rank&#125;.
