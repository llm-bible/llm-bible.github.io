---
layout: publication
title: 'Memorization In Attention-only Transformers'
authors: LÃ©o Dana, Muni Sreenivas Pydi, Yann Chevaleyre
conference: "Arxiv"
year: 2024
bibkey: dana2024memorization
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.10115'}
tags: ['Attention Mechanism', 'Transformer', 'Model Architecture', 'Pretraining Methods']
---
Recent research has explored the memorization capacity of multi-head
attention, but these findings are constrained by unrealistic limitations on the
context size. We present a novel proof for language-based Transformers that
extends the current hypothesis to any context size. Our approach improves upon
the state-of-the-art by achieving more effective exact memorization with an
attention layer, while also introducing the concept of approximate memorization
of distributions. Through experimental validation, we demonstrate that our
proposed bounds more accurately reflect the true memorization capacity of
language models, and provide a precise comparison with prior work.
