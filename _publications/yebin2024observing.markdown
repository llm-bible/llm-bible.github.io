---
layout: publication
title: BEAF Observing Before45;after Changes To Evaluate Hallucination In Vision45;language Models
authors: Ye-bin Moon, Hyeon-woo Nam, Choi Wonseok, Oh Tae-hyun
conference: "Arxiv"
year: 2024
bibkey: yebin2024observing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.13442"}
  - {name: "Code", url: "https://beafbench.github.io/&#125;"}
tags: ['Has Code', 'Multimodal Models', 'Reinforcement Learning']
---
Vision language models (VLMs) perceive the world through a combination of a visual encoder and a large language model (LLM). The visual encoder pre45;trained on large45;scale vision45;text datasets provides zero45;shot generalization to visual data and the LLM endows its high reasoning ability to VLMs. It leads VLMs to achieve high performance on wide benchmarks without fine45;tuning exhibiting zero or few45;shot capability. However recent studies show that VLMs are vulnerable to hallucination. This undesirable behavior degrades reliability and credibility thereby making users unable to fully trust the output from VLMs. To enhance trustworthiness and better tackle the hallucination of VLMs we curate a new evaluation dataset called the BEfore45;AFter hallucination dataset (BEAF) and introduce new metrics True Understanding (TU) IGnorance (IG) StuBbornness (SB) and InDecision (ID). Unlike prior works that focus only on constructing questions and answers the key idea of our benchmark is to manipulate visual scene information by image editing models and to design the metrics based on scene changes. This allows us to clearly assess whether VLMs correctly understand a given scene by observing the ability to perceive changes. We also visualize image45;wise object relationship by virtue of our two45;axis view vision and text. Upon evaluating VLMs with our dataset we observed that our metrics reveal different aspects of VLM hallucination that have not been reported before. Project page url123;https://beafbench.github.io/&#125;
