---
layout: publication
title: 'Llamalens: Specialized Multilingual LLM For Analyzing News And Social Media Content'
authors: Mohamed Bayan Kmainasi, Ali Ezzat Shahroor, Maram Hasanain, Sahinur Rahman Laskar, Naeemul Hassan, Firoj Alam
conference: "Arxiv"
year: 2024
bibkey: kmainasi2024specialized
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.15308"}
tags: ['Model Architecture', 'Attention Mechanism', 'Reinforcement Learning']
---
Large Language Models (LLMs) have demonstrated remarkable success as
general-purpose task solvers across various fields. However, their capabilities
remain limited when addressing domain-specific problems, particularly in
downstream NLP tasks. Research has shown that models fine-tuned on
instruction-based downstream NLP datasets outperform those that are not
fine-tuned. While most efforts in this area have primarily focused on
resource-rich languages like English and broad domains, little attention has
been given to multilingual settings and specific domains. To address this gap,
this study focuses on developing a specialized LLM, LlamaLens, for analyzing
news and social media content in a multilingual context. To the best of our
knowledge, this is the first attempt to tackle both domain specificity and
multilinguality, with a particular focus on news and social media. Our
experimental setup includes 18 tasks, represented by 52 datasets covering
Arabic, English, and Hindi. We demonstrate that LlamaLens outperforms the
current state-of-the-art (SOTA) on 23 testing sets, and achieves comparable
performance on 8 sets. We make the models and resources publicly available for
the research community
(https://huggingface.co/collections/QCRI/llamalens-672f7e0604a0498c6a2f0fe9).
