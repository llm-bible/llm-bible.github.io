---
layout: publication
title: Chainforge A Visual Toolkit For Prompt Engineering And LLM Hypothesis Testing
authors: Arawjo Ian, Swoopes Chelse, Vaithilingam Priyan, Wattenberg Martin, Glassman Elena
conference: "Arxiv"
year: 2023
bibkey: arawjo2023visual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.09128"}
tags: ['Applications', 'Fine Tuning', 'Language Modeling', 'Prompting', 'Reinforcement Learning', 'Tools']
---
Evaluating outputs of large language models (LLMs) is challenging requiring making 45;45; and making sense of 45;45; many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs focus on narrow domains or are closed45;source. We present ChainForge an open45;source visual toolkit for prompt engineering and on45;demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks model selection prompt template design and hypothesis testing (e.g. auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in45;lab and interview studies we find that a range of people could use ChainForge to investigate hypotheses that matter to them including in real45;world settings. We identify three modes of prompt engineering and LLM hypothesis testing opportunistic exploration limited evaluation and iterative refinement.
