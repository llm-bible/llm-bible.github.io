---
layout: publication
title: 'A Framework To Assess The Persuasion Risks Large Language Model Chatbots Pose To Democratic Societies'
authors: Zhongren Chen, Joshua Kalla, Quan Le, Shinpei Nakamura-sakai, Jasjeet Sekhon, Ruixiao Wang
conference: "Arxiv"
year: 2025
bibkey: chen2025framework
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.00036'}
tags: ['Reinforcement Learning', 'RAG', 'Tools', 'Survey Paper']
---
In recent years, significant concern has emerged regarding the potential
threat that Large Language Models (LLMs) pose to democratic societies through
their persuasive capabilities. We expand upon existing research by conducting
two survey experiments and a real-world simulation exercise to determine
whether it is more cost effective to persuade a large number of voters using
LLM chatbots compared to standard political campaign practice, taking into
account both the "receive" and "accept" steps in the persuasion process (Zaller
1992). These experiments improve upon previous work by assessing extended
interactions between humans and LLMs (instead of using single-shot
interactions) and by assessing both short- and long-run persuasive effects
(rather than simply asking users to rate the persuasiveness of LLM-produced
content). In two survey experiments (N = 10,417) across three distinct
political domains, we find that while LLMs are about as persuasive as actual
campaign ads once voters are exposed to them, political persuasion in the
real-world depends on both exposure to a persuasive message and its impact
conditional on exposure. Through simulations based on real-world parameters, we
estimate that LLM-based persuasion costs between \\\(48-\\\)74 per persuaded voter
compared to \$100 for traditional campaign methods, when accounting for the
costs of exposure. However, it is currently much easier to scale traditional
campaign persuasion methods than LLM-based persuasion. While LLMs do not
currently appear to have substantially greater potential for large-scale
political persuasion than existing non-LLM methods, this may change as LLM
capabilities continue to improve and it becomes easier to scalably encourage
exposure to persuasive LLMs.
