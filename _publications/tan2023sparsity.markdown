---
layout: publication
title: Sparsity45;guided Holistic Explanation For Llms With Interpretable Inference45;time Intervention
authors: Tan Zhen, Chen Tianlong, Zhang Zhenyu, Liu Huan
conference: "Arxiv"
year: 2023
bibkey: tan2023sparsity
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.15033"}
tags: ['Applications', 'Attention Mechanism', 'Interpretability And Explainability', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
Large Language Models (LLMs) have achieved unprecedented breakthroughs in various natural language processing domains. However the enigmatic black45;box nature of LLMs remains a significant challenge for interpretability hampering transparent and accountable applications. While past approaches such as attention visualization pivotal subnetwork extraction and concept45;based analyses offer some insight they often focus on either local or global explanations within a single dimension occasionally falling short in providing comprehensive clarity. In response we propose a novel methodology anchored in sparsity45;guided techniques aiming to provide a holistic interpretation of LLMs. Our framework termed SparseCBM innovatively integrates sparsity to elucidate three intertwined layers of interpretation input subnetwork and concept levels. In addition the newly introduced dimension of interpretable inference45;time intervention facilitates dynamic adjustments to the model during deployment. Through rigorous empirical evaluations on real45;world datasets we demonstrate that SparseCBM delivers a profound understanding of LLM behaviors setting it apart in both interpreting and ameliorating model inaccuracies. Codes are provided in supplements.
