---
layout: publication
title: Multimodal Contrastive In45;context Learning
authors: Miyanishi Yosuke, Nguyen Minh Le
conference: "Arxiv"
year: 2024
bibkey: miyanishi2024multimodal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.12959"}
tags: ['Ethics And Bias', 'Multimodal Models', 'Pretraining Methods', 'Reinforcement Learning', 'Tools']
---
The rapid growth of Large Language Models (LLMs) usage has highlighted the importance of gradient45;free in45;context learning (ICL). However interpreting their inner workings remains challenging. This paper introduces a novel multimodal contrastive in45;context learning framework to enhance our understanding of ICL in LLMs. First we present a contrastive learning45;based interpretation of ICL in real45;world settings marking the distance of the key45;value representation as the differentiator in ICL. Second we develop an analytical framework to address biases in multimodal input formatting for real45;world datasets. We demonstrate the effectiveness of ICL examples where baseline performance is poor even when they are represented in unseen formats. Lastly we propose an on45;the45;fly approach for ICL (Anchored45;by45;Text ICL) that demonstrates effectiveness in detecting hateful memes a task where typical ICL struggles due to resource limitations. Extensive experiments on multimodal datasets reveal that our approach significantly improves ICL performance across various scenarios such as challenging tasks and resource45;constrained environments. Moreover it provides valuable insights into the mechanisms of in45;context learning in LLMs. Our findings have important implications for developing more interpretable efficient and robust multimodal AI systems especially in challenging tasks and resource45;constrained environments.
