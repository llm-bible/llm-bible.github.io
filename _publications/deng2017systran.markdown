---
layout: publication
title: 'SYSTRAN Purely Neural MT Engines For WMT2017'
authors: Yongchao Deng, Jungi Kim, Guillaume Klein, Catherine Kobus, Natalia Segal, Christophe Servan, Bo Wang, Dakun Zhang, Josep Crego, Jean Senellart
conference: "Arxiv"
year: 2017
bibkey: deng2017systran
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1709.03814"}
tags: ['Model Architecture', 'Attention Mechanism', 'Applications', 'WMT']
---
This paper describes SYSTRAN's systems submitted to the WMT 2017 shared news
translation task for English-German, in both translation directions. Our
systems are built using OpenNMT, an open-source neural machine translation
system, implementing sequence-to-sequence models with LSTM encoder/decoders and
attention. We experimented using monolingual data automatically
back-translated. Our resulting models are further hyper-specialised with an
adaptation technique that finely tunes models according to the evaluation test
sentences.
