---
layout: publication
title: COSMIC Data Efficient Instruction45;tuning For Speech In45;context Learning
authors: Pan Jing, Wu Jian, Gaur Yashesh, Sivasankaran Sunit, Chen Zhuo, Liu Shujie, Li Jinyu
conference: "Arxiv"
year: 2023
bibkey: pan2023data
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.02248"}
tags: ['Ethics And Bias', 'Fine Tuning', 'GPT', 'Merging', 'Model Architecture', 'RAG']
---
We present a cost45;effective method to integrate speech into a large language model (LLM) resulting in a Contextual Speech Model with Instruction45;following/in45;context45;learning Capabilities (COSMIC) multi45;modal LLM. Using GPT45;3.5 we generate Speech Comprehension Test Question45;Answer (SQA) pairs from speech transcriptions for supervised instruction tuning. With under 30 million trainable parameters and only 450 hours of English speech data COSMIC demonstrates emerging capabilities in instruction45;following and in45;context learning. Equipped with such capabilities COSMIC achieves a maximum 33.18 BLEU score in 045;shot EN45;to45;X speech to text translation (S2TT) and a significant boost in the 145;shot setting. Additionally there is an average 25.837; relative Word Error Rate (WER) reduction for 145;shot cross45;domain adaptation. COSMIC exhibits a significant automatic speech recognition (ASR) accuracy gain in contextual biasing tasks due to its instruction45;following capability.
