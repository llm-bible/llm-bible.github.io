---
layout: publication
title: 'MT-LENS: An All-in-one Toolkit For Better Machine Translation Evaluation'
authors: Javier Garc√≠a Gilabert, Carlos Escolano, Audrey Mash, Xixian Liao, Maite Melero
conference: "Arxiv"
year: 2024
bibkey: gilabert2024mt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.11615"}
tags: ['Security', 'Tools', 'Reinforcement Learning', 'Ethics and Bias', 'Applications']
---
We introduce MT-LENS, a framework designed to evaluate Machine Translation
(MT) systems across a variety of tasks, including translation quality, gender
bias detection, added toxicity, and robustness to misspellings. While several
toolkits have become very popular for benchmarking the capabilities of Large
Language Models (LLMs), existing evaluation tools often lack the ability to
thoroughly assess the diverse aspects of MT performance. MT-LENS addresses
these limitations by extending the capabilities of LM-eval-harness for MT,
supporting state-of-the-art datasets and a wide range of evaluation metrics. It
also offers a user-friendly platform to compare systems and analyze
translations with interactive visualizations. MT-LENS aims to broaden access to
evaluation strategies that go beyond traditional translation quality
evaluation, enabling researchers and engineers to better understand the
performance of a NMT model and also easily measure system's biases.
