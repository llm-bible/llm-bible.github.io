---
layout: publication
title: Unsupervised Real45;time Hallucination Detection Based On The Internal States Of Large Language Models
authors: Su Weihang, Wang Changyue, Ai Qingyao, Hu Yiran, Wu Zhijing, Zhou Yujia, Liu Yiqun
conference: "Arxiv"
year: 2024
bibkey: su2024unsupervised
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.06448"}
tags: ['Applications', 'Pretraining Methods', 'RAG', 'Tools', 'Training Techniques']
---
Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post45;processing techniques for hallucination detection which tend to be computationally intensive and limited in effectiveness due to their separation from the LLMs inference process. To overcome these limitations we introduce MIND an unsupervised training framework that leverages the internal states of LLMs for real45;time hallucination detection without requiring manual annotations. Additionally we present HELM a new benchmark for evaluating hallucination detection across multiple LLMs featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state45;of45;the45;art methods in hallucination detection.
