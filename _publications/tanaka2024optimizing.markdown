---
layout: publication
title: 'Optimizing Vision-language Interactions Through Decoder-only Models'
authors: Kaito Tanaka, Benjamin Tan, Brian Wong
conference: "Arxiv"
year: 2024
bibkey: tanaka2024optimizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.10758"}
tags: ['Security', 'Model Architecture', 'Efficiency and Optimization', 'Multimodal Models', 'Merging', 'Transformer', 'Attention Mechanism']
---
Vision-Language Models (VLMs) have emerged as key enablers for multimodal
tasks, but their reliance on separate visual encoders introduces challenges in
efficiency, scalability, and modality alignment. To address these limitations,
we propose MUDAIF (Multimodal Unified Decoder with Adaptive Input Fusion), a
decoder-only vision-language model that seamlessly integrates visual and
textual inputs through a novel Vision-Token Adapter (VTA) and adaptive
co-attention mechanism. By eliminating the need for a visual encoder, MUDAIF
achieves enhanced efficiency, flexibility, and cross-modal understanding.
Trained on a large-scale dataset of 45M image-text pairs, MUDAIF consistently
outperforms state-of-the-art methods across multiple benchmarks, including VQA,
image captioning, and multimodal reasoning tasks. Extensive analyses and human
evaluations demonstrate MUDAIF's robustness, generalization capabilities, and
practical usability, establishing it as a new standard in encoder-free
vision-language models.
