---
layout: publication
title: 'Open Llama2 Model For The Lithuanian Language'
authors: Artūras Nakvosas, Povilas Daniušis, Vytas Mulevičius
conference: "Informatica 2025"
year: 2024
bibkey: nakvosas2024open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.12963"}
  - {name: "Code", url: "https://huggingface.co/neurotechnology"}
tags: ['Training Techniques', 'Survey Paper', 'Reinforcement Learning', 'Pretraining Methods', 'Has Code']
---
In this paper, we propose and describe the first open Llama2 large language
models (LLMs) for the Lithuanian language, including an accompanying
question/answer (Q/A) dataset and translations of popular LLM benchmarks. We
provide a brief review of open regional LLMs and detailed information on the
proposed LLMs and their training process. We also conduct an empirical
evaluation, comparing the perplexities of the proposed LLMs with those of other
modern open LLMs. In addition, benchmarking the proposed LLMs against language
understanding tasks reveals that high-quality pretraining datasets may be
essential for achieving models that perform efficiently on these benchmarks.
The full realisations of the described LLMs are available in the accompanying
open repository~\url\{https://huggingface.co/neurotechnology\}.
