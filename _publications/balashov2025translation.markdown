---
layout: publication
title: 'Translation In The Wild'
authors: Yuri Balashov
conference: "Arxiv"
year: 2025
bibkey: balashov2025translation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.23548"}
tags: ['Training Techniques', 'Few-Shot', 'RAG', 'Pre-Training', 'Applications']
---
Large Language Models (LLMs) excel in translation among other things, demonstrating competitive performance for many language pairs in zero- and few-shot settings. But unlike dedicated neural machine translation models, LLMs are not trained on any translation-related objective. What explains their remarkable translation abilities? Are these abilities grounded in "incidental bilingualism" (Briakou et al. 2023) in training data? Does instruction tuning contribute to it? Are LLMs capable of aligning and leveraging semantically identical or similar monolingual contents from different corners of the internet that are unlikely to fit in a single context window? I offer some reflections on this topic, informed by recent studies and growing user experience. My working hypothesis is that LLMs' translation abilities originate in two different types of pre-training data that may be internalized by the models in different ways. I discuss the prospects for testing the "duality" hypothesis empirically and its implications for reconceptualizing translation, human and machine, in the age of deep learning.
