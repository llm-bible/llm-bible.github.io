---
layout: publication
title: Towards Reinforcement Learning For Pivot45;based Neural Machine Translation With Non45;autoregressive Transformer
authors: Tokarchuk Evgeniia, Rosendahl Jan, Wang Weiyue, Petrushkov Pavel, Lancewicki Tomer, Khadivi Shahram, Ney Hermann
conference: "Arxiv"
year: 2021
bibkey: tokarchuk2021towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2109.13097"}
tags: ['Agentic', 'Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
Pivot45;based neural machine translation (NMT) is commonly used in low45;resource setups especially for translation between non45;English language pairs. It benefits from using high resource source45;pivot and pivot45;target language pairs and an individual system is trained for both sub45;tasks. However these models have no connection during training and the source45;pivot model is not optimized to produce the best translation for the source45;target task. In this work we propose to train a pivot45;based NMT system with the reinforcement learning (RL) approach which has been investigated for various text generation tasks including machine translation (MT). We utilize a non45;autoregressive transformer and present an end45;to45;end pivot45;based integrated model enabling training on source45;target data.
