---
layout: publication
title: 'Can LLM Prompting Serve As A Proxy For Static Analysis In Vulnerability Detection'
authors: Ira Ceka, Feitong Qiao, Anik Dey, Aastha Valecha, Gail Kaiser, Baishakhi Ray
conference: "Arxiv"
year: 2024
bibkey: ceka2024can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.12039"}
tags: ['Prompting', 'Security', 'Fine-Tuning', 'Tools']
---
Despite their remarkable success, large language models (LLMs) have shown
limited ability on applied tasks such as vulnerability detection. We
investigate various prompting strategies for vulnerability detection and, as
part of this exploration, propose a prompting strategy that integrates natural
language descriptions of vulnerabilities with a contrastive chain-of-thought
reasoning approach, augmented using contrastive samples from a synthetic
dataset. Our study highlights the potential of LLMs to detect vulnerabilities
by integrating natural language descriptions, contrastive reasoning, and
synthetic examples into a comprehensive prompting framework. Our results show
that this approach can enhance LLM understanding of vulnerabilities. On a
high-quality vulnerability detection dataset such as SVEN, our prompting
strategies can improve accuracies, F1-scores, and pairwise accuracies by 23%,
11%, and 14%, respectively.
