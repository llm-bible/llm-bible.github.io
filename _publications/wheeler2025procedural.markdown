---
layout: publication
title: 'Procedural Memory Is Not All You Need: Bridging Cognitive Gaps In Llm-based Agents'
authors: Schaun Wheeler, Olivier Jeunen
conference: "Arxiv"
year: 2025
bibkey: wheeler2025procedural
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.03434"}
tags: ['Agentic', 'Applications', 'Language Modeling', 'Model Architecture', 'Reinforcement Learning']
---
Large Language Models (LLMs) represent a landmark achievement in Artificial
Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks
such as text generation, code completion, and conversational coherence. These
capabilities stem from their architecture, which mirrors human procedural
memory -- the brain's ability to automate repetitive, pattern-driven tasks
through practice. However, as LLMs are increasingly deployed in real-world
applications, it becomes impossible to ignore their limitations operating in
complex, unpredictable environments. This paper argues that LLMs, while
transformative, are fundamentally constrained by their reliance on procedural
memory. To create agents capable of navigating ``wicked'' learning environments
-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must
augment LLMs with semantic memory and associative learning systems. By adopting
a modular architecture that decouples these cognitive functions, we can bridge
the gap between narrow procedural expertise and the adaptive intelligence
required for real-world problem-solving.
