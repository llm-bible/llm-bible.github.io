---
layout: publication
title: 'A Generative Framework For Bidirectional Image-report Understanding In Chest Radiography'
authors: Nicholas Evans, Stephen Baker, Miles Reed
conference: "Arxiv"
year: 2025
bibkey: evans2025generative
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.05926'}
tags: ['RAG', 'Tools', 'Applications', 'Training Techniques', 'Fine-Tuning', 'Multimodal Models', 'Reinforcement Learning', 'Tokenization', 'Pretraining Methods']
---
The rapid advancements in large language models (LLMs) have unlocked their
potential for multimodal tasks, where text and visual data are processed
jointly. However, applying LLMs to medical imaging, particularly for chest
X-rays (CXR), poses significant challenges due to the need for precise
visual-textual alignment and the preservation of critical diagnostic details.
In this paper, we propose Multi-Stage Adaptive Vision-Language Tuning (MAViLT),
a novel framework designed to enhance multimodal reasoning and generation for
CXR understanding. MAViLT incorporates a clinical gradient-weighted
tokenization process and a hierarchical fine-tuning strategy, enabling it to
generate accurate radiology reports, synthesize realistic CXRs from text, and
answer vision-based clinical questions. We evaluate MAViLT on two benchmark
datasets, MIMIC-CXR and Indiana University CXR, achieving state-of-the-art
results across all tasks. Human evaluations further validate the clinical
relevance and utility of MAViLT, making it a robust tool for real-world medical
applications. This work demonstrates the feasibility of leveraging LLMs for
multimodal medical imaging while addressing key challenges in vision-language
integration.
