---
layout: publication
title: Probing Script Knowledge From Pre45;trained Models
authors: Jin Zijian, Zhang Xingyu, Yu Mo, Huang Lifu
conference: "Arxiv"
year: 2022
bibkey: jin2022probing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.10176"}
tags: ['BERT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Script knowledge is critical for humans to understand the broad daily tasks and routine activities in the world. Recently researchers have explored the large45;scale pre45;trained language models (PLMs) to perform various script related tasks such as story generation temporal ordering of event future event prediction and so on. However its still not well studied in terms of how well the PLMs capture the script knowledge. To answer this question we design three probing tasks inclusive sub45;event selection starting sub45;event selection and temporal ordering to investigate the capabilities of PLMs with and without fine45;tuning. The three probing tasks can be further used to automatically induce a script for each main event given all the possible sub45;events. Taking BERT as a case study by analyzing its performance on script induction as well as each individual probing task we conclude that the stereotypical temporal knowledge among the sub45;events is well captured in BERT however the inclusive or starting sub45;event knowledge is barely encoded.
