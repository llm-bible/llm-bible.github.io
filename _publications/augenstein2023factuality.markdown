---
layout: publication
title: 'Factuality Challenges In The Era Of Large Language Models'
authors: Augenstein Isabelle, Baldwin Timothy, Cha Meeyoung, Chakraborty Tanmoy, Ciampaglia Giovanni Luca, Corney David, Diresta Renee, Ferrara Emilio, Hale Scott, Halevy Alon, Hovy Eduard, Ji Heng, Menczer Filippo, Miguez Ruben, Nakov Preslav, Scheufele Dietram, Sharma Shivam, Zagni Giovanni
conference: "Arxiv"
year: 2023
bibkey: augenstein2023factuality
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.05189"}
tags: ['Applications', 'Attention Mechanism', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Tools']
---
The emergence of tools based on Large Language Models (LLMs) such as OpenAIs ChatGPT Microsofts Bing Chat and Googles Bard has garnered immense public attention. These incredibly useful natural-sounding tools mark significant advances in natural language generation yet they exhibit a propensity to generate false erroneous or misleading content -- commonly referred to as hallucinations. Moreover LLMs can be exploited for malicious applications such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. In light of these risks we explore the kinds of technological innovations regulatory reforms and AI literacy initiatives needed from fact-checkers news organizations and the broader research and policy communities. By identifying the risks the imminent threats and some viable solutions we seek to shed light on navigating various aspects of veracity in the era of generative AI.
