---
layout: publication
title: 'Real-time Evaluation Models For RAG: Who Detects Hallucinations Best?'
authors: Ashish Sardana
conference: "Arxiv"
year: 2025
bibkey: sardana2025real
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.21157"}
tags: ['RAG', 'Survey Paper', 'Applications']
---
This article surveys Evaluation models to automatically detect hallucinations
in Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark
of their performance across six RAG applications. Methods included in our study
include: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation
Model (HHEM), and the Trustworthy Language Model (TLM). These approaches are
all reference-free, requiring no ground-truth answers/labels to catch incorrect
LLM responses. Our study reveals that, across diverse RAG applications, some of
these approaches consistently detect incorrect RAG responses with high
precision/recall.
