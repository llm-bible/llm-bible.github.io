---
layout: publication
title: Multilingual Llms Are Better Cross45;lingual In45;context Learners With Alignment
authors: Tanwar Eshaan, Dutta Subhabrata, Borthakur Manish, Chakraborty Tanmoy
conference: "Arxiv"
year: 2023
bibkey: tanwar2023multilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.05940"}
tags: ['Prompting']
---
In45;context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL45;enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low45;resource setting. Yet only a handful of past studies have explored ICL in a cross45;lingual setting in which the need for transferring label45;knowledge from a high45;resource language to a low45;resource one is immensely crucial. To bridge the gap we provide the first in45;depth analysis of ICL for cross45;lingual text classification. We find that the prevalent mode of selecting random input45;label pairs to construct the prompt45;context is severely limited in the case of cross45;lingual ICL primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this we propose a novel prompt construction strategy 45;45; Cross45;lingual In45;context Source45;Target Alignment (X45;InSTA). With an injected coherence in the semantics of the input examples and a task45;based alignment across the source and target languages X45;InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross45;lingual pairs.
