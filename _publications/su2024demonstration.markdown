---
layout: publication
title: "Demonstration Augmentation For Zero-shot In-context Learning"
authors: Su Yi, Tai Yunpeng, Ji Yixin, Li Juntao, Yan Bowen, Zhang Min
conference: "Arxiv"
year: 2024
bibkey: su2024demonstration
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.01224"}
tags: ['Applications', 'Few Shot', 'In Context Learning', 'Prompting', 'RAG']
---
Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL) which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However many studies have highlighted that the models performance is sensitive to the choice of demonstrations presenting a significant challenge for practical applications where we lack prior knowledge of user queries. Consequently we need to construct an extensive demonstration pool and incorporate external databases to assist the model leading to considerable time and financial costs. In light of this some recent research has shifted focus towards zero-shot ICL aiming to reduce the models reliance on external information by leveraging their inherent generative capabilities. Despite the effectiveness of these approaches the content generated by the model may be unreliable and the generation process is time-consuming. To address these issues we propose Demonstration Augmentation for In-context Learning (DAIL) which employs the models previously predicted historical samples as demonstrations for subsequent ones. DAIL brings no additional inference cost and does not rely on the models generative capabilities. Our experiments reveal that DAIL can significantly improve the models performance over direct zero-shot inference and can even outperform few-shot ICL without any external information.
