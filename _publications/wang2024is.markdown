---
layout: publication
title: Is A Picture Worth A Thousand Words Delving Into Spatial Reasoning For Vision Language Models
authors: Wang Jiayu, Ming Yifei, Shi Zhenmei, Vineet Vibhav, Wang Xin, Joshi Neel
conference: "Arxiv"
year: 2024
bibkey: wang2024is
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.14852"}
tags: ['Multimodal Models', 'RAG', 'Reinforcement Learning']
---
Large language models (LLMs) and vision45;language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise spatial understanding and reasoning 45;45; a fundamental component of human cognition 45;45; remains under45;explored. We develop novel benchmarks that cover diverse aspects of spatial reasoning such as relationship understanding navigation and counting. We conduct a comprehensive evaluation of competitive language and vision45;language models. Our findings reveal several counter45;intuitive insights that have been overlooked in the literature (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input VLMs often under45;perform compared to their LLM counterparts; (3) When both textual and visual information is available multi45;modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence.
