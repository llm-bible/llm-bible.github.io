---
layout: publication
title: Knowledge Prompting In Pre45;trained Language Model For Natural Language Understanding
authors: Wang Jianing, Huang Wenkang, Shi Qiuhui, Wang Hongbin, Qiu Minghui, Li Xiang, Gao Ming
conference: "Arxiv"
year: 2022
bibkey: wang2022knowledge
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2210.08536"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Prompting', 'RAG', 'Tools']
---
Knowledge45;enhanced Pre45;trained Language Model (PLM) has recently received significant attention which aims to incorporate factual knowledge into PLMs. However most existing methods modify the internal structures of fixed types of PLMs by stacking complicated modules and introduce redundant and irrelevant factual knowledge from knowledge bases (KBs). In this paper to address these problems we introduce a seminal knowledge prompting paradigm and further propose a knowledge45;prompting45;based PLM framework KP45;PLM. This framework can be flexibly combined with existing mainstream PLMs. Specifically we first construct a knowledge sub45;graph from KBs for each context. Then we design multiple continuous prompts rules and transform the knowledge sub45;graph into natural language prompts. To further leverage the factual knowledge from these prompts we propose two novel knowledge45;aware self45;supervised tasks including prompt relevance inspection and masked prompt modeling. Extensive experiments on multiple natural language understanding (NLU) tasks show the superiority of KP45;PLM over other state45;of45;the45;art methods in both full45;resource and low45;resource settings.
