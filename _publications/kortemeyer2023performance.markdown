---
layout: publication
title: Performance Of The Pre45;trained Large Language Model GPT45;4 On Automated Short Answer Grading
authors: Kortemeyer Gerd
conference: "Arxiv"
year: 2023
bibkey: kortemeyer2023performance
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.09338"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
Automated Short Answer Grading (ASAG) has been an active area of machine45;learning research for over a decade. It promises to let educators grade and give feedback on free45;form responses in large45;enrollment courses in spite of limited availability of human graders. Over the years carefully trained models have achieved increasingly higher levels of performance. More recently pre45;trained Large Language Models (LLMs) emerged as a commodity and an intriguing question is how a general45;purpose tool without additional training compares to specialized models. We studied the performance of GPT45;4 on the standard benchmark 245;way and 345;way datasets SciEntsBank and Beetle where in addition to the standard task of grading the alignment of the student answer with a reference answer we also investigated withholding the reference answer. We found that overall the performance of the pre45;trained general45;purpose GPT45;4 LLM is comparable to hand45;engineered models but worse than pre45;trained LLMs that had specialized training.
