---
layout: publication
title: 'How Should The Advent Of Large Language Models Affect The Practice Of Science?'
authors: Binz Marcel, Alaniz Stephan, Roskies Adina, Aczel Balazs, Bergstrom Carl T., Allen Colin, Schad Daniel, Wulff Dirk, West Jevin D., Zhang Qiong, Shiffrin Richard M., Gershman Samuel J., Popov Ven, Bender Emily M., Marelli Marco, Botvinick Matthew M., Akata Zeynep, Schulz Eric
conference: "Arxiv"
year: 2023
bibkey: binz2023how
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.03759"}
tags: ['Attention Mechanism', 'Model Architecture', 'Tools', 'Uncategorized']
---
Large language models (LLMs) are being increasingly incorporated into
scientific workflows. However, we have yet to fully grasp the implications of
this integration. How should the advent of large language models affect the
practice of science? For this opinion piece, we have invited four diverse
groups of scientists to reflect on this query, sharing their perspectives and
engaging in debate. Schulz et al. make the argument that working with LLMs is
not fundamentally different from working with human collaborators, while Bender
et al. argue that LLMs are often misused and over-hyped, and that their
limitations warrant a focus on more specialized, easily interpretable tools.
Marelli et al. emphasize the importance of transparent attribution and
responsible use of LLMs. Finally, Botvinick and Gershman advocate that humans
should retain responsibility for determining the scientific roadmap. To
facilitate the discussion, the four perspectives are complemented with a
response from each group. By putting these different perspectives in
conversation, we aim to bring attention to important considerations within the
academic community regarding the adoption of LLMs and their impact on both
current and future scientific practices.
