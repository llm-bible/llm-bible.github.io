---
layout: publication
title: TOD45;BERT Pre45;trained Natural Language Understanding For Task45;oriented Dialogue
authors: Wu Chien-sheng, Hoi Steven, Socher Richard, Xiong Caiming
conference: "Arxiv"
year: 2020
bibkey: wu2020tod
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2004.06871"}
tags: ['Applications', 'BERT', 'Language Modeling', 'Masked Language Model', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
The underlying difference of linguistic patterns between general text and task45;oriented dialogue makes existing pre45;trained language models less useful in practice. In this work we unify nine human45;human and multi45;turn task45;oriented dialogue datasets for language modeling. To better model dialogue behavior during pre45;training we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre45;trained task45;oriented dialogue BERT (TOD45;BERT) outperforms strong baselines like BERT on four downstream task45;oriented dialogue applications including intention recognition dialogue state tracking dialogue act prediction and response selection. We also show that TOD45;BERT has a stronger few45;shot ability that can mitigate the data scarcity problem for task45;oriented dialogue.
