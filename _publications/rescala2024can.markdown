---
layout: publication
title: 'Can Language Models Recognize Convincing Arguments?'
authors: Rescala Paula, Ribeiro Manoel Horta, Hu Tiancheng, West Robert
conference: "Arxiv"
year: 2024
bibkey: rescala2024can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.00750"}
tags: ['Tools', 'Uncategorized']
---
The remarkable and ever-increasing capabilities of Large Language Models
(LLMs) have raised concerns about their potential misuse for creating
personalized, convincing misinformation and propaganda. To gain insights into
LLMs' persuasive capabilities without directly engaging in experimentation with
humans, we propose studying their performance on the related task of detecting
convincing arguments. We extend a dataset by Durmus & Cardie (2018) with
debates, votes, and user traits and propose tasks measuring LLMs' ability to
(1) distinguish between strong and weak arguments, (2) predict stances based on
beliefs and demographic characteristics, and (3) determine the appeal of an
argument to an individual based on their traits. We show that LLMs perform on
par with humans in these tasks and that combining predictions from different
LLMs yields significant performance gains, even surpassing human performance.
The data and code released with this paper contribute to the crucial ongoing
effort of continuously evaluating and monitoring the rapidly evolving
capabilities and potential impact of LLMs.
