---
layout: publication
title: 'Can Llms Faithfully Generate Their Layperson-understandable ''self''?: A Case Study In High-stakes Domains'
authors: Arion Das, Asutosh Mishra, Amitesh Patel, Soumilya De, V. Gurucharan, Kripabandhu Ghosh
conference: "Arxiv"
year: 2024
bibkey: das2024can
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.07781'}
tags: ['Reinforcement Learning', 'Interpretability', 'Interpretability and Explainability']
---
Large Language Models (LLMs) have significantly impacted nearly every domain
of human knowledge. However, the explainability of these models esp. to
laypersons, which are crucial for instilling trust, have been examined through
various skeptical lenses. In this paper, we introduce a novel notion of LLM
explainability to laypersons, termed \\(\textit\{ReQuesting\}\\), across three
high-priority application domains -- law, health and finance, using multiple
state-of-the-art LLMs. The proposed notion exhibits faithful generation of
explainable layman-understandable algorithms on multiple tasks through high
degree of reproducibility. Furthermore, we observe a notable alignment of the
explainable algorithms with intrinsic reasoning of the LLMs.
