---
layout: publication
title: Document-level Neural Machine Translation With Hierarchical Attention Networks
authors: Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, James Henderson
conference: Arxiv
year: 2018
citations: 41
bibkey: miculicich2018document
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1809.01576'}]
tags: [Attention Mechanism, Model Architecture]
---
Neural Machine Translation (NMT) can be improved by including document-level
contextual information. For this purpose, we propose a hierarchical attention
model to capture the context in a structured and dynamic manner. The model is
integrated in the original NMT architecture as another level of abstraction,
conditioning on the NMT model's own previous hidden states. Experiments show
that hierarchical attention significantly improves the BLEU score over a strong
NMT baseline with the state-of-the-art in context-aware methods, and that both
the encoder and decoder benefit from context in complementary ways.