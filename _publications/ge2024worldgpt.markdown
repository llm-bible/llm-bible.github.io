---
layout: publication
title: WorldGPT Empowering LLM as Multimodal World Model
authors: Ge Zhiqi, Huang Hongzhe, Zhou Mingze, Li Juncheng, Wang Guoming, Tang Siliang, Zhuang Yueting
conference: "Arxiv"
year: 2024
bibkey: ge2024worldgpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.18202"}
  - {name: "Code", url: "https://github.com/DCDmllm/WorldGPT}"}
tags: ['ARXIV', 'Agentic', 'Fine Tuning', 'GPT', 'Has Code', 'LLM', 'Merging', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
World models are progressively being employed across diverse fields extending from basic environment simulation to complex scenario construction. However existing models are mainly trained on domain-specific states and actions and confined to single-modality state representations. In this paper We introduce WorldGPT a generalist world model built upon Multimodal Large Language Model (MLLM). WorldGPT acquires an understanding of world dynamics through analyzing millions of videos across various domains. To further enhance WorldGPTs capability in specialized scenarios and long-term tasks we have integrated it with a novel cognitive architecture that combines memory offloading knowledge retrieval and context reflection. As for evaluation we build WorldNet a multimodal state transition prediction benchmark encompassing varied real-life scenarios. Conducting evaluations on WorldNet directly demonstrates WorldGPTs capability to accurately model state transition patterns affirming its effectiveness in understanding and predicting the dynamics of complex scenarios. We further explore WorldGPTs emerging potential in serving as a world simulator helping multimodal agents generalize to unfamiliar domains through efficiently synthesising multimodal instruction instances which are proved to be as reliable as authentic data for fine-tuning purposes. The project is available on urlhttps://github.com/DCDmllm/WorldGPT}.
