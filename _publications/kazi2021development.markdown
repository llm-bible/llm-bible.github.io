---
layout: publication
title: 'Uquad1.0: Development Of An Urdu Question Answering Training Data For Machine Reading Comprehension'
authors: Kazi Samreen, Khoja Shakeel
conference: "Arxiv"
year: 2021
bibkey: kazi2021development
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2111.01543"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
In recent years, low-resource Machine Reading Comprehension (MRC) has made
significant progress, with models getting remarkable performance on various
language datasets. However, none of these models have been customized for the
Urdu language. This work explores the semi-automated creation of the Urdu
Question Answering Dataset (UQuAD1.0) by combining machine-translated SQuAD
with human-generated samples derived from Wikipedia articles and Urdu RC
worksheets from Cambridge O-level books. UQuAD1.0 is a large-scale Urdu dataset
intended for extractive machine reading comprehension tasks consisting of 49k
question Answers pairs in question, passage, and answer format. In UQuAD1.0,
45000 pairs of QA were generated by machine translation of the original
SQuAD1.0 and approximately 4000 pairs via crowdsourcing. In this study, we used
two types of MRC models: rule-based baseline and advanced Transformer-based
models. However, we have discovered that the latter outperforms the others;
thus, we have decided to concentrate solely on Transformer-based architectures.
Using XLMRoBERTa and multi-lingual BERT, we acquire an F1 score of 0.66 and
0.63, respectively.
