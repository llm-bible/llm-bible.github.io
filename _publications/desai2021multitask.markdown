---
layout: publication
title: 'Multitask Finetuning For Improving Neural Machine Translation In Indian Languages'
authors: Shaily Desai, Atharva Kshirsagar, Manisha Marathe
conference: "Arxiv"
year: 2021
bibkey: desai2021multitask
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2112.01742"}
tags: ['Transformer', 'Applications', 'Language Modeling', 'Model Architecture', 'Training Techniques', 'Pretraining Methods']
---
Transformer based language models have led to impressive results across all
domains in Natural Language Processing. Pretraining these models on language
modeling tasks and finetuning them on downstream tasks such as Text
Classification, Question Answering and Neural Machine Translation has
consistently shown exemplary results. In this work, we propose a Multitask
Finetuning methodology which combines the Bilingual Machine Translation task
with an auxiliary Causal Language Modeling task to improve performance on the
former task on Indian Languages. We conduct an empirical study on three
language pairs, Marathi-Hindi, Marathi-English and Hindi-English, where we
compare the multitask finetuning approach to the standard finetuning approach,
for which we use the mBART50 model. Our study indicates that the multitask
finetuning method could be a better technique than standard finetuning, and
could improve Bilingual Machine Translation across language pairs.
