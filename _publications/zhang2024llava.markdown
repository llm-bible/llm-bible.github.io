---
layout: publication
title: Llava-read: Enhancing Reading Ability Of Multimodal Language Models
authors: Zhang Ruiyi, Zhou Yufan, Chen Jian, Gu Jiuxiang, Chen Changyou, Sun Tong
conference: "Arxiv"
year: 2024
bibkey: zhang2024llava
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.19185"}
tags: ['Fine Tuning', 'Multimodal Models']
---
Large multimodal language models have demonstrated impressive capabilities in understanding and manipulating images. However many of these models struggle with comprehending intensive textual contents embedded within the images primarily due to the limited text recognition and layout understanding ability. To understand the sources of these limitations we perform an exploratory analysis showing the drawbacks of classical visual encoders on visual text understanding. Hence we present LLaVA-Read a multimodal large language model that utilizes dual visual encoders along with a visual text encoder. Our model surpasses existing state-of-the-art models in various text-rich image understanding tasks showcasing enhanced comprehension of textual content within images. Together our research suggests visual text understanding remains an open challenge and an efficient visual text encoder is crucial for future successful multimodal systems.
