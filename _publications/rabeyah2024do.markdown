---
layout: publication
title: 'Do Llms Agree On The Creativity Evaluation Of Alternative Uses?'
authors: Abdullah Al Rabeyah, Fabrício Góes, Marco Volpe, Talles Medeiros
conference: "Arxiv"
year: 2024
bibkey: rabeyah2024do
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.15560'}
tags: ['RAG', 'ACL']
---
This paper investigates whether large language models (LLMs) show agreement
in assessing creativity in responses to the Alternative Uses Test (AUT). While
LLMs are increasingly used to evaluate creative content, previous studies have
primarily focused on a single model assessing responses generated by the same
model or humans. This paper explores whether LLMs can impartially and
accurately evaluate creativity in outputs generated by both themselves and
other models. Using an oracle benchmark set of AUT responses, categorized by
creativity level (common, creative, and highly creative), we experiment with
four state-of-the-art LLMs evaluating these outputs. We test both scoring and
ranking methods and employ two evaluation settings (comprehensive and
segmented) to examine if LLMs agree on the creativity evaluation of alternative
uses. Results reveal high inter-model agreement, with Spearman correlations
averaging above 0.7 across models and reaching over 0.77 with respect to the
oracle, indicating a high level of agreement and validating the reliability of
LLMs in creativity assessment of alternative uses. Notably, models do not
favour their own responses, instead they provide similar creativity assessment
scores or rankings for alternative uses generated by other models. These
findings suggest that LLMs exhibit impartiality and high alignment in
creativity evaluation, offering promising implications for their use in
automated creativity assessment.
