---
layout: publication
title: 'Sparkra: A Retrieval-augmented Knowledge Service System Based On Spark Large Language Model'
authors: Dayong Wu, Jiaqi Li, Baoxin Wang, Honghong Zhao, Siyuan Xue, Yanjie Yang, Zhijun Chang, Rui Zhang, Li Qian, Bo Wang, Shijin Wang, Zhixiong Zhang, Guoping Hu
conference: "Arxiv"
year: 2024
bibkey: wu2024retrieval
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2408.06574'}
tags: ['RAG', 'Training Techniques', 'Fine-Tuning', 'Pre-Training', 'Pretraining Methods']
---
Large language models (LLMs) have shown remarkable achievements across
various language tasks.To enhance the performance of LLMs in scientific
literature services, we developed the scientific literature LLM (SciLit-LLM)
through pre-training and supervised fine-tuning on scientific literature,
building upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge
service system Spark Research Assistant (SparkRA) based on our SciLit-LLM.
SparkRA is accessible online and provides three primary functions: literature
investigation, paper reading, and academic writing. As of July 30, 2024,
SparkRA has garnered over 50,000 registered users, with a total usage count
exceeding 1.3 million.
