---
layout: publication
title: GPT45;3 Models Are Poor Few45;shot Learners In The Biomedical Domain
authors: Moradi Milad, Blagec Kathrin, Haberl Florian, Samwald Matthias
conference: "Arxiv"
year: 2021
bibkey: moradi2021gpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2109.02555"}
tags: ['BERT', 'Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Deep neural language models have set new breakthroughs in many tasks of Natural Language Processing (NLP). Recent work has shown that deep transformer language models (pretrained on large amounts of texts) can achieve high levels of task45;specific few45;shot performance comparable to state45;of45;the45;art models. However the ability of these large language models in few45;shot transfer learning has not yet been explored in the biomedical domain. We investigated the performance of two powerful transformer language models i.e. GPT45;3 and BioBERT in few45;shot settings on various biomedical NLP tasks. The experimental results showed that to a great extent both the models underperform a language model fine45;tuned on the full training data. Although GPT45;3 had already achieved near state45;of45;the45;art results in few45;shot knowledge transfer on open45;domain NLP tasks it could not perform as effectively as BioBERT which is orders of magnitude smaller than GPT45;3. Regarding that BioBERT was already pretrained on large biomedical text corpora our study suggests that language models may largely benefit from in45;domain pretraining in task45;specific few45;shot learning. However in45;domain pretraining seems not to be sufficient; novel pretraining and few45;shot learning strategies are required in the biomedical NLP domain.
