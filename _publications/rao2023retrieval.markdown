---
layout: publication
title: Retrieval45;based Knowledge Augmented Vision Language Pre45;training
authors: Rao Jiahua, Shan Zifei, Liu Longpo, Zhou Yao, Yang Yuedong
conference: "Arxiv"
year: 2023
bibkey: rao2023retrieval
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.13923"}
tags: ['Efficiency And Optimization', 'Multimodal Models', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
With the recent progress in large45;scale vision and language representation learning Vision Language Pre45;training (VLP) models have achieved promising improvements on various multi45;modal downstream tasks. Albeit powerful these models have not fully leveraged world knowledge to their advantage. A key challenge of knowledge45;augmented VLP is the lack of clear connections between knowledge and multi45;modal data. Moreover not all knowledge present in images/texts is useful therefore prior approaches often struggle to effectively integrate knowledge visual and textual information. In this study we propose REtrieval45;based knowledge Augmented Vision Language (REAVL) a novel knowledge45;augmented pre45;training framework to address the above issues. For the first time we introduce a knowledge45;aware self45;supervised learning scheme that efficiently establishes the correspondence between knowledge and multi45;modal data and identifies informative knowledge to improve the modeling of alignment and interactions between visual and textual modalities. By adaptively integrating informative knowledge with visual and textual information REAVL achieves new state45;of45;the45;art performance uniformly on knowledge45;based vision45;language understanding and multi45;modal entity linking tasks as well as competitive results on general vision45;language tasks while only using 0.237; pre45;training data of the best models. Our model shows strong sample efficiency and effective knowledge utilization.
