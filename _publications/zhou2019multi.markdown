---
layout: publication
title: Multi45;task Learning With Language Modeling For Question Generation
authors: Zhou Wenjie, Zhang Minghua, Wu Yunfang
conference: "Arxiv"
year: 2019
bibkey: zhou2019multi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1908.11813"}
tags: ['Attention Mechanism', 'Language Modeling', 'Model Architecture', 'Pretraining Methods']
---
This paper explores the task of answer45;aware questions generation. Based on the attention45;based pointer generator model we propose to incorporate an auxiliary task of language modeling to help question generation in a hierarchical multi45;task learning structure. Our joint45;learning model enables the encoder to learn a better representation of the input sequence which will guide the decoder to generate more coherent and fluent questions. On both SQuAD and MARCO datasets our multi45;task learning model boosts the performance achieving state45;of45;the45;art results. Moreover human evaluation further proves the high quality of our generated questions.
