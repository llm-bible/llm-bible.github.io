---
layout: publication
title: Cross45;lingual Prompting Improving Zero45;shot Chain45;of45;thought Reasoning Across Languages
authors: Qin Libo, Chen Qiguang, Wei Fuxuan, Huang Shijue, Che Wanxiang
conference: "Arxiv"
year: 2023
bibkey: qin2023cross
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.14799"}
tags: ['Attention Mechanism', 'Ethics And Bias', 'Model Architecture', 'Prompting']
---
Chain45;of45;thought (CoT) is capable of eliciting models to explicitly generate reasoning paths thus promoting reasoning accuracy and attracting increasing attention. Specifically zero45;shot CoT achieves remarkable improvements in a wide range of reasoning tasks by simply instructing the LLM with the prompt Lets think step by step!. Despite the success of zero45;shot CoT the existing zero45;shot prompting techniques remain limited to a single language making it challenging to generalize to other languages and hindering global development. In this work we introduce cross45;lingual prompting (CLP) aiming to improve zero45;shot CoT reasoning across languages. Specifically CLP consists of two main components (1) cross45;lingual alignment prompting and (2) task45;specific solver prompting. The cross45;lingual alignment prompting is responsible for aligning representations across different languages whereas the task45;specific solver prompting is used to generate the final chain of thoughts and results for the reasoning task. In addition we further introduce cross45;lingual self45;consistent prompting (CLSP) to ensemble different reasoning paths across languages. Our experimental evaluations on several benchmarks demonstrate that CLP and CLSP significantly outperform the existing prompting methods and achieve state45;of45;the45;art performance. We hope this work will inspire further breakthroughs in cross45;lingual CoT.
