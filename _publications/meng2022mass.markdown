---
layout: publication
title: 'Mass-editing Memory In A Transformer'
authors: Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David Bau
conference: "Arxiv"
year: 2022
bibkey: meng2022mass
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2210.07229"}
  - {name: "Code", url: "https://memit.baulab.info"}
tags: ['Model Architecture', 'GPT', 'Pretraining Methods', 'Transformer', 'Has Code']
---
Recent work has shown exciting promise in updating large language models with
new memories, so as to replace obsolete information or add specialized
knowledge. However, this line of work is predominantly limited to updating
single associations. We develop MEMIT, a method for directly updating a
language model with many memories, demonstrating experimentally that it can
scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B),
exceeding prior work by orders of magnitude. Our code and data are at
https://memit.baulab.info.
