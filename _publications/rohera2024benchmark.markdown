---
layout: publication
title: 'L3cube-indicquest: A Benchmark Question Answering Dataset For Evaluating Knowledge Of Llms In Indic Context'
authors: Pritika Rohera, Chaitrali Ginimav, Akanksha Salunke, Gayatri Sawant, Raviraj Joshi
conference: "Arxiv"
year: 2024
bibkey: rohera2024benchmark
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.08706"}
  - {name: "Code", url: "https://github.com/l3cube-pune/indic-nlp"}
tags: ['Has Code', 'Applications']
---
Large Language Models (LLMs) have made significant progress in incorporating
Indic languages within multilingual models. However, it is crucial to
quantitatively assess whether these languages perform comparably to globally
dominant ones, such as English. Currently, there is a lack of benchmark
datasets specifically designed to evaluate the regional knowledge of LLMs in
various Indic languages. In this paper, we present the L3Cube-IndicQuest, a
gold-standard factual question-answering benchmark dataset designed to evaluate
how well multilingual LLMs capture regional knowledge across various Indic
languages. The dataset contains 200 question-answer pairs, each for English and
19 Indic languages, covering five domains specific to the Indic region. We aim
for this dataset to serve as a benchmark, providing ground truth for evaluating
the performance of LLMs in understanding and representing knowledge relevant to
the Indian context. The IndicQuest can be used for both reference-based
evaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at
https://github.com/l3cube-pune/indic-nlp .
