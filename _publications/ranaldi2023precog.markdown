---
layout: publication
title: PreCog Exploring the Relation between Memorization and Performance in Pre-trained Language Models
authors: Ranaldi Leonardo, Ruzzetti Elena Sofia, Zanzotto Fabio Massimo
conference: ""
year: 2023
bibkey: ranaldi2023precog
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.04673"}
tags: ['BERT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
Pre-trained Language Models such as BERT are impressive machines with the ability to memorize possibly generalized learning examples. We present here a small focused contribution to the analysis of the interplay between memorization and performance of BERT in downstream tasks. We propose PreCog a measure for evaluating memorization from pre-training and we analyze its correlation with the BERTs performance. Our experiments show that highly memorized examples are better classified suggesting memorization is an essential key to success for BERT.
