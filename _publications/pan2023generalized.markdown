---
layout: publication
title: Kwaiagents Generalized Information-seeking Agent System With Large Language Models
authors: Pan Haojie, Zhai Zepeng, Yuan Hao, Lv Yaojia, Fu Ruiji, Liu Ming, Wang Zhongyuan, Qin Bing
conference: "Arxiv"
year: 2023
bibkey: pan2023generalized
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.04889"}
tags: ['Agent', 'Agentic', 'GPT', 'Model Architecture', 'Reinforcement Learning', 'Tools']
---
Driven by curiosity humans have continually sought to explore and understand the world around them leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains humans excel in critical thinking planning reflection and harnessing available tools to interact with and interpret the world enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper we introduce KwaiAgents a generalized information-seeking agent system based on LLMs. Within KwaiAgents we propose an agent system that employs LLMs as its cognitive core which is capable of understanding a users query behavior guidelines and referencing external documents. The agent can also update and retrieve information from its internal memory plan and execute actions using a time-aware search-browse toolkit and ultimately provide a comprehensive response. We further investigate the systems performance when powered by LLMs less advanced than GPT-4 and introduce the Meta-Agent Tuning (MAT) framework designed to ensure even an open-sourced 7B or 13B model performs well among many agent systems. We exploit both benchmark and human evaluations to systematically validate these capabilities. Extensive experiments show the superiority of our agent system compared to other autonomous agents and highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.
