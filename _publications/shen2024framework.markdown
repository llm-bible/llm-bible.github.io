---
layout: publication
title: 'Valuecompass: A Framework For Measuring Contextual Value Alignment Between Human And Llms'
authors: Hua Shen, Tiffany Knearem, Reshmi Ghosh, Yu-ju Yang, Nicholas Clark, Tanushree Mitra, Yun Huang
conference: "Arxiv"
year: 2024
bibkey: shen2024framework
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.09586"}
tags: ['Responsible AI', 'Security', 'Survey Paper', 'Tools', 'Reinforcement Learning', 'Ethics and Bias']
---
As AI systems become more advanced, ensuring their alignment with a diverse
range of individuals and societal values becomes increasingly critical. But how
can we capture fundamental human values and assess the degree to which AI
systems align with them? We introduce ValueCompass, a framework of fundamental
values, grounded in psychological theory and a systematic review, to identify
and evaluate human-AI alignment. We apply ValueCompass to measure the value
alignment of humans and large language models (LLMs) across four real-world
scenarios: collaborative writing, education, public sectors, and healthcare.
Our findings reveal concerning misalignments between humans and LLMs, such as
humans frequently endorse values like "National Security" which were largely
rejected by LLMs. We also observe that values differ across scenarios,
highlighting the need for context-aware AI alignment strategies. This work
provides valuable insights into the design space of human-AI alignment, laying
the foundations for developing AI systems that responsibly reflect societal
values and ethics.
