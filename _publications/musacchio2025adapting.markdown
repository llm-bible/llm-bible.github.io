---
layout: publication
title: 'Xvlm2vec: Adapting Lvlm-based Embedding Models To Multilinguality Using Self-knowledge Distillation'
authors: Elio Musacchio, Lucia Siciliani, Pierpaolo Basile, Giovanni Semeraro
conference: "Arxiv"
year: 2025
bibkey: musacchio2025adapting
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.09313'}
tags: ['Language Modeling', 'Transformer', 'Efficiency and Optimization', 'Distillation', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods']
---
In the current literature, most embedding models are based on the
encoder-only transformer architecture to extract a dense and meaningful
representation of the given input, which can be a text, an image, and more.
With the recent advances in language modeling thanks to the introduction of
Large Language Models, the possibility of extracting embeddings from these
large and extensively trained models has been explored. However, current
studies focus on textual embeddings in English, which is also the main language
on which these models have been trained. Furthermore, there are very few models
that consider multimodal and multilingual input. In light of this, we propose
an adaptation methodology for Large Vision-Language Models trained on English
language data to improve their performance in extracting multilingual and
multimodal embeddings. Finally, we design and introduce a benchmark to evaluate
the effectiveness of multilingual and multimodal embedding models.
