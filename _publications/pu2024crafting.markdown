---
layout: publication
title: Crafting A Good Prompt Or Providing Exemplary Dialogues A Study Of In45;context Learning For Persona45;based Dialogue Generation
authors: Pu Jiashu, Wan Yajing, Zhang Yuru, Chen Jing, Cheng Ling, Shao Qian, Chang Yongzhu, Lv Tangjie, Zhang Rongsheng
conference: "Arxiv"
year: 2024
bibkey: pu2024crafting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.09954"}
tags: ['Applications', 'Interpretability And Explainability', 'Prompting']
---
Previous in45;context learning (ICL) research has focused on tasks such as classification machine translation text2table etc. while studies on whether ICL can improve human45;like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona45;based dialogue generation conducting extensive experiments on high45;quality real human Chinese dialogue datasets. From experimental results we draw three conclusions 1) adjusting prompt instructions is the most direct effective and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results possibly due to the greater diversity and the amount of effective information; counter45;intuitively retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi45;turn associations and single45;turn semantics in the demos increasing the number of demos still improves dialogue performance proving that LLMs can learn from corrupted dialogue demos. Previous explanations of the ICL mechanism such as n45;gram induction head cannot fully account for this phenomenon.
