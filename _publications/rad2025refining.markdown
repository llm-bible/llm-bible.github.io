---
layout: publication
title: 'Refining Input Guardrails: Enhancing Llm-as-a-judge Efficiency Through Chain-of-thought Fine-tuning And Alignment'
authors: Melissa Kazemi Rad, Huy Nghiem, Andy Luo, Sahil Wadhwa, Mohammad Sorower, Stephen Rawls
conference: "Arxiv"
year: 2025
bibkey: rad2025refining
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.13080'}
tags: ['Agentic', 'RAG', 'Efficiency and Optimization', 'Security', 'Tools', 'Applications', 'Training Techniques', 'Fine-Tuning', 'Survey Paper', 'Responsible AI', 'Pretraining Methods']
---
Large Language Models (LLMs) have demonstrated powerful capabilities that
render them valuable in different applications, including conversational AI
products. It is paramount to ensure the security and reliability of these
products by mitigating their vulnerabilities towards malicious user
interactions, which can lead to the exposure of great risks and reputational
repercussions. In this work, we present a comprehensive study on the efficacy
of fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs
that serve as input moderation guardrails. We systematically explore various
tuning methods by leveraging a small set of training data to adapt these models
as proxy defense mechanisms to detect malicious inputs and provide a reasoning
for their verdicts, thereby preventing the exploitation of conversational
agents. We rigorously evaluate the efficacy and robustness of different tuning
strategies to generalize across diverse adversarial and malicious query types.
Our experimental results outline the potential of alignment processes tailored
to a varied range of harmful input queries, even with constrained data
resources. These techniques significantly enhance the safety of conversational
AI systems and provide a feasible framework for deploying more secure and
trustworthy AI-driven interactions.
