---
layout: publication
title: 'Multilingual Large Language Models: A Systematic Survey'
authors: Shaolin Zhu, Supryadi, Shaoyang Xu, Haoran Sun, Leiyu Pan, Menglong Cui, Jiangcun Du, Renren Jin, Ant√≥nio Branco, Deyi Xiong
conference: "Arxiv"
year: 2024
bibkey: zhu2024multilingual
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.11072'}
  - {name: "Code", url: 'https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers'}
tags: ['Has Code', 'Interpretability and Explainability', 'Ethics and Bias', 'Applications', 'Model Architecture', 'Training Techniques', 'Survey Paper', 'Reinforcement Learning', 'Pre-Training', 'Responsible AI']
---
This paper provides a comprehensive survey of the latest research on
multilingual large language models (MLLMs). MLLMs not only are able to
understand and generate language across linguistic boundaries, but also
represent an important advancement in artificial intelligence. We first discuss
the architecture and pre-training objectives of MLLMs, highlighting the key
components and methodologies that contribute to their multilingual
capabilities. We then discuss the construction of multilingual pre-training and
alignment datasets, underscoring the importance of data quality and diversity
in enhancing MLLM performance. An important focus of this survey is on the
evaluation of MLLMs. We present a detailed taxonomy and roadmap covering the
assessment of MLLMs' cross-lingual knowledge, reasoning, alignment with human
values, safety, interpretability and specialized applications. Specifically, we
extensively discuss multilingual evaluation benchmarks and datasets, and
explore the use of LLMs themselves as multilingual evaluators. To enhance MLLMs
from black to white boxes, we also address the interpretability of multilingual
capabilities, cross-lingual transfer and language bias within these models.
Finally, we provide a comprehensive review of real-world applications of MLLMs
across diverse domains, including biology, medicine, computer science,
mathematics and law. We showcase how these models have driven innovation and
improvements in these specialized fields while also highlighting the challenges
and opportunities in deploying MLLMs within diverse language communities and
application scenarios. We listed the paper related in this survey and publicly
available at https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers.
