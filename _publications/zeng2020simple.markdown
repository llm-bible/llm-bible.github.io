---
layout: publication
title: 'A Simple And Efficient Multi-task Learning Approach For Conditioned Dialogue Generation'
authors: Yan Zeng, Jian-yun Nie
conference: "Arxiv"
year: 2020
bibkey: zeng2020simple
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2010.11140"}
tags: ['Model Architecture', 'RAG', 'Transformer', 'Pretraining Methods']
---
Conditioned dialogue generation suffers from the scarcity of labeled
responses. In this work, we exploit labeled non-dialogue text data related to
the condition, which are much easier to collect. We propose a multi-task
learning approach to leverage both labeled dialogue and text data. The 3 tasks
jointly optimize the same pre-trained Transformer -- conditioned dialogue
generation task on the labeled dialogue data, conditioned language encoding
task and conditioned language generation task on the labeled text data.
Experimental results show that our approach outperforms the state-of-the-art
models by leveraging the labeled texts, and it also obtains larger improvement
in performance comparing to the previous methods to leverage text data.
