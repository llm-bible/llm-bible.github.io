---
layout: publication
title: Recent Advances Of Foundation Language Models45;based Continual Learning A Survey
authors: Yang Yutao, Zhou Jie, Ding Xuanwen, Huai Tianyu, Liu Shunyu, Chen Qin, He Liang, Xie Yuan
conference: "Arxiv"
year: 2024
bibkey: yang2024recent
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.18653"}
tags: ['Applications', 'Fine Tuning', 'Survey Paper', 'Training Techniques']
---
Recently foundation language models (LMs) have marked significant achievements in the domains of natural language processing (NLP) and computer vision (CV). Unlike traditional neural network models foundation LMs obtain a great ability for transfer learning by acquiring rich commonsense knowledge through pre45;training on extensive unsupervised datasets with a vast number of parameters. However they still can not emulate human45;like continuous learning due to catastrophic forgetting. Consequently various continual learning (CL)45;based methodologies have been developed to refine LMs enabling them to adapt to new tasks without forgetting previous knowledge. However a systematic taxonomy of existing approaches and a comparison of their performance are still lacking which is the gap that our survey aims to fill. We delve into a comprehensive review summarization and classification of the existing literature on CL45;based approaches applied to foundation language models such as pre45;trained language models (PLMs) large language models (LLMs) and vision45;language models (VLMs). We divide these studies into offline CL and online CL which consist of traditional methods parameter45;efficient45;based methods instruction tuning45;based methods and continual pre45;training methods. Offline CL encompasses domain45;incremental learning task45;incremental learning and class45;incremental learning while online CL is subdivided into hard task boundary and blurry task boundary settings. Additionally we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs45;based continual learning.
