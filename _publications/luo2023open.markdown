---
layout: publication
title: Biomedgpt Open Multimodal Generative Pre45;trained Transformer For Biomedicine
authors: Luo Yizhen, Zhang Jiahuan, Fan Siqi, Yang Kai, Wu Yushuai, Qiao Mu, Nie Zaiqing
conference: "Arxiv"
year: 2023
bibkey: luo2023open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.09442"}
  - {name: "Code", url: "https://github.com/PharMolix/OpenBioMed&#125;"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
Foundation models (FMs) have exhibited remarkable performance across a wide range of downstream tasks in many domains. Nevertheless general45;purpose FMs often face challenges when confronted with domain45;specific problems due to their limited access to the proprietary training data in a particular domain. In biomedicine there are various biological modalities such as molecules proteins and cells which are encoded by the language of life and exhibit significant modality gaps with human natural language. In this paper we introduce BioMedGPT an open multimodal generative pre45;trained transformer (GPT) for biomedicine to bridge the gap between the language of life and human natural language. BioMedGPT allows users to easily communicate with diverse biological modalities through free text which is the first of its kind. BioMedGPT aligns different biological modalities with natural language via a large generative language model namely BioMedGPT45;LM. We publish BioMedGPT45;10B which unifies the feature spaces of molecules proteins and natural language via encoding and alignment. Through fine45;tuning BioMedGPT45;10B outperforms or is on par with human and significantly larger general45;purpose foundation models on the biomedical QA task. It also demonstrates promising performance in the molecule QA and protein QA tasks which could greatly accelerate the discovery of new drugs and therapeutic targets. In addition BioMedGPT45;LM45;7B is the first large generative language model based on Llama2 in the biomedical domain therefore is commercial friendly. Both BioMedGPT45;10B and BioMedGPT45;LM45;7B are open45;sourced to the research community. In addition we publish the datasets that are meticulously curated for the alignment of multi45;modalities i.e. PubChemQA and UniProtQA. All the models codes and datasets are available at url123;https://github.com/PharMolix/OpenBioMed&#125;.
