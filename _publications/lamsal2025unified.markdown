---
layout: publication
title: 'Langformers: Unified NLP Pipelines For Language Models'
authors: Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera
conference: "Arxiv"
year: 2025
bibkey: lamsal2025unified
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.09170"}
  - {name: "Code", url: "https://langformers.com"}
tags: ['Transformer', 'Agentic', 'Efficiency and Optimization', 'Tools', 'RAG', 'Model Architecture', 'Reinforcement Learning', 'Masked Language Model', 'Training Techniques', 'Has Code', 'Pretraining Methods', 'BERT', 'Distillation']
---
Transformer-based language models have revolutionized the field of natural
language processing (NLP). However, using these models often involves
navigating multiple frameworks and tools, as well as writing repetitive
boilerplate code. This complexity can discourage non-programmers and beginners,
and even slow down prototyping for experienced developers. To address these
challenges, we introduce Langformers, an open-source Python library designed to
streamline NLP pipelines through a unified, factory-based interface for large
language model (LLM) and masked language model (MLM) tasks. Langformers
integrates conversational AI, MLM pretraining, text classification, sentence
embedding/reranking, data labelling, semantic search, and knowledge
distillation into a cohesive API, supporting popular platforms such as Hugging
Face and Ollama. Key innovations include: (1) task-specific factories that
abstract training, inference, and deployment complexities; (2) built-in memory
and streaming for conversational agents; and (3) lightweight, modular design
that prioritizes ease of use. Documentation: https://langformers.com
