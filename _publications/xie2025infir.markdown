---
layout: publication
title: 'Infir : Crafting Effective Small Language Models And Multimodal Small Language Models In Reasoning'
authors: Congkai Xie, Shuo Cai, Wenjun Wang, Pengxiang Li, Zhijie Sang, Kejing Yang, Yiming Zhang, Zhen Li, Guanghao Zhu, Zeyu Liu, Yang Yu, Yuhang Liu, Su Lu, Baoyi He, Qi Zhou, Xiaotian Han, Jianbo Yuan, Shengyu Zhang, Fei Wu, Hongxia Yang
conference: "Arxiv"
year: 2025
bibkey: xie2025infir
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.11573'}
tags: ['Multimodal Models', 'Training Techniques']
---
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have made significant advancements in reasoning capabilities. However, they
still face challenges such as high computational demands and privacy concerns.
This paper focuses on developing efficient Small Language Models (SLMs) and
Multimodal Small Language Models (MSLMs) that retain competitive reasoning
abilities. We introduce a novel training pipeline that enhances reasoning
capabilities and facilitates deployment on edge devices, achieving
state-of-the-art performance while minimizing development costs. \InfR~ aims to
advance AI systems by improving reasoning, reducing adoption barriers, and
addressing privacy concerns through smaller model sizes. Resources are
available at https://github. com/Reallm-Labs/InfiR.
