---
layout: publication
title: FAC^2E Better Understanding Large Language Model Capabilities By Dissociating Language And Cognition
authors: Wang Xiaoqiang, Liu Bang, Wu Lingfei
conference: "Arxiv"
year: 2024
bibkey: wang2024better
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.00126"}
tags: ['Tools']
---
Large language models (LLMs) are primarily evaluated by overall performance on various text understanding and generation tasks. However such a paradigm fails to comprehensively differentiate the fine45;grained language and cognitive skills rendering the lack of sufficient interpretation to LLMs capabilities. In this paper we present FAC^2E a framework for Fine45;grAined and Cognition45;grounded LLMs Capability Evaluation. Specifically we formulate LLMs evaluation in a multi45;dimensional and explainable manner by dissociating the language45;related capabilities and the cognition45;related ones. Besides through extracting the intermediate reasoning from LLMs we further break down the process of applying a specific capability into three sub45;steps recalling relevant knowledge utilizing knowledge and solving problems. Finally FAC^2E evaluates each sub45;step of each fine45;grained capability providing a two45;faceted diagnosis for LLMs. Utilizing FAC^2E we identify a common shortfall in knowledge utilization among models and propose a straightforward knowledge45;enhanced method to mitigate this issue. Our results not only showcase promising performance enhancements but also highlight a direction for future LLM advancements.
