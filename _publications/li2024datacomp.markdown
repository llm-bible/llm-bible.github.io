---
layout: publication
title: Datacomp45;lm In Search Of The Next Generation Of Training Sets For Language Models
authors: Li Jeffrey, Fang Alex, Smyrnis Georgios, Ivgi Maor, Jordan Matt, Gadre Samir, Bansal Hritik, Guha Etash, Keh Sedrick, Arora Kushal, Garg Saurabh, Xin Rui, Muennighoff Niklas, Heckel Reinhard, Mercat Jean, Chen Mayee, Gururangan Suchin, Wortsman Mitchell, Albalak Alon, Bitton Yonatan, Nezhurina Marianna, Abbas Amro, Hsieh Cheng-yu, Ghosh Dhruba, Gardner Josh, Kilian Maciej, Zhang Hanlin, Shao Rulin, Pratt Sarah, Sanyal Sunny, Ilharco Gabriel, Daras Giannis, Marathe Kalyani, Gokaslan Aaron, Zhang Jieyu, Chandu Khyathi, Nguyen Thao, Vasiljevic Igor, Kakade Sham, Song Shuran, Sanghavi Sujay, Faghri Fartash, Oh Sewoong, Zettlemoyer Luke, Lo Kyle, El-nouby Alaaeldin, Pouransari Hadi, Toshev Alexander, Wang Stephanie, Groeneveld Dirk, Soldaini Luca, Koh Pang Wei, Jitsev Jenia, Kollar Thomas, Dimakis Alexandros G., Carmon Yair, Dave Achal, Schmidt Ludwig, Shankar Vaishaal
conference: "Arxiv"
year: 2024
bibkey: li2024datacomp
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.11794"}
tags: ['Applications', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
We introduce DataComp for Language Models (DCLM) a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM we provide a standardized corpus of 240T tokens extracted from Common Crawl effective pretraining recipes based on the OpenLM framework and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication filtering and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM we conduct extensive experiments and find that model45;based filtering is key to assembling a high45;quality training set. The resulting dataset DCLM45;Baseline enables training a 7B parameter language model from scratch to 6437; 545;shot accuracy on MMLU with 2.6T training tokens. Compared to MAP45;Neo the previous state45;of45;the45;art in open45;data language models DCLM45;Baseline represents a 6.6 percentage point improvement on MMLU while being trained with 4037; less compute. Our baseline model is also comparable to Mistral45;7B45;v0.3 and Llama 3 8B on MMLU (6337; amp; 6637;) and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6x less compute than Llama 3 8B. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation.
