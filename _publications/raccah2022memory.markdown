---
layout: publication
title: 'Memory In Humans And Deep Language Models: Linking Hypotheses For Model Augmentation'
authors: Omri Raccah, Phoebe Chen, Ted L. Willke, David Poeppel, Vy A. Vo
conference: "Arxiv"
year: 2022
bibkey: raccah2022memory
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2210.01869'}
tags: ['Attention Mechanism', 'Transformer', 'Model Architecture', 'Pretraining Methods']
---
The computational complexity of the self-attention mechanism in Transformer
models significantly limits their ability to generalize over long temporal
durations. Memory-augmentation, or the explicit storing of past information in
external memory for subsequent predictions, has become a constructive avenue
for mitigating this limitation. We argue that memory-augmented Transformers can
benefit substantially from considering insights from the memory literature in
humans. We detail an approach for integrating evidence from the human memory
system through the specification of cross-domain linking hypotheses. We then
provide an empirical demonstration to evaluate the use of surprisal as a
linking hypothesis, and further identify the limitations of this approach to
inform future research.
