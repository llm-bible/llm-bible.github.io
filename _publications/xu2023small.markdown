---
layout: publication
title: 'Small Models Are Valuable Plug-ins For Large Language Models'
authors: Xu Canwen, Xu Yichong, Wang Shuohang, Liu Yang, Zhu Chenguang, Mcauley Julian
conference: "Arxiv"
year: 2023
bibkey: xu2023small
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.08848"}
tags: ['GPT', 'In Context Learning', 'Interpretability And Explainability', 'Model Architecture', 'Prompting']
---
Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their
weights are often publicly unavailable and their immense sizes make the models
difficult to be tuned with common hardware. As a result, effectively tuning
these models with large-scale supervised data can be challenging. As an
alternative, In-Context Learning (ICL) can only use a small number of
supervised examples due to context length limits. In this paper, we propose
Super In-Context Learning (SuperICL) which allows black-box LLMs to work with
locally fine-tuned smaller models, resulting in superior performance on
supervised tasks. Our experiments demonstrate that SuperICL can improve
performance beyond state-of-the-art fine-tuned models while addressing the
instability problem of in-context learning. Furthermore, SuperICL can enhance
the capabilities of smaller models, such as multilinguality and
interpretability.
