---
layout: publication
title: 'Evaluating Large Language Model Capability In Vietnamese Fact-checking Data Generation'
authors: Long Truong To, Hung Tuan Le, Dat Van-thanh Nguyen, Manh Trong Nguyen, Tri Thien Nguyen, Tin Van Huynh, Kiet Van Nguyen
conference: "Arxiv"
year: 2024
bibkey: to2024evaluating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.05641'}
tags: ['Fine-Tuning', 'Prompting', 'Training Techniques', 'Pretraining Methods']
---
Large Language Models (LLMs), with gradually improving reading comprehension
and reasoning capabilities, are being applied to a range of complex language
tasks, including the automatic generation of language data for various
purposes. However, research on applying LLMs for automatic data generation in
low-resource languages like Vietnamese is still underdeveloped and lacks
comprehensive evaluation. In this paper, we explore the use of LLMs for
automatic data generation for the Vietnamese fact-checking task, which faces
significant data limitations. Specifically, we focus on fact-checking data
where claims are synthesized from multiple evidence sentences to assess the
information synthesis capabilities of LLMs. We develop an automatic data
construction process using simple prompt techniques on LLMs and explore several
methods to improve the quality of the generated data. To evaluate the quality
of the data generated by LLMs, we conduct both manual quality assessments and
performance evaluations using language models. Experimental results and manual
evaluations illustrate that while the quality of the generated data has
significantly improved through fine-tuning techniques, LLMs still cannot match
the data quality produced by humans.
