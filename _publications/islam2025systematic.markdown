---
layout: publication
title: 'Systematic Weight Evaluation For Pruning Large Language Models: Enhancing Performance And Sustainability'
authors: Ashhadul Islam, Samir Brahim Belhaouari, Amine Bermak
conference: "Arxiv"
year: 2025
bibkey: islam2025systematic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.17071"}
tags: ['Efficiency and Optimization', 'GPT', 'Pruning', 'Model Architecture', 'Training Techniques', 'Multimodal Models']
---
The exponential growth of large language models (LLMs) like ChatGPT has
revolutionized artificial intelligence, offering unprecedented capabilities in
natural language processing. However, the extensive computational resources
required for training these models have significant environmental implications,
including high carbon emissions, energy consumption, and water usage. This
research presents a novel approach to LLM pruning, focusing on the systematic
evaluation of individual weight importance throughout the training process. By
monitoring parameter evolution over time, we propose a method that effectively
reduces model size without compromising performance. Extensive experiments with
both a scaled-down LLM and a large multimodal model reveal that moderate
pruning enhances efficiency and reduces loss, while excessive pruning
drastically deteriorates model performance. These findings highlight the
critical need for optimized AI models to ensure sustainable development,
balancing technological advancement with environmental responsibility.
