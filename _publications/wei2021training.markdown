---
layout: publication
title: Training Multilingual Pre45;trained Language Model With Byte45;level Subwords
authors: Wei Junqiu, Liu Qun, Guo Yinpeng, Jiang Xin
conference: "Arxiv"
year: 2021
bibkey: wei2021training
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2101.09469"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
The pre45;trained language models have achieved great successes in various natural language understanding (NLU) tasks due to its capacity to capture the deep contextualized information in text by pre45;training on large45;scale corpora. One of the fundamental components in pre45;trained language models is the vocabulary especially for training multilingual models on many different languages. In the technical report we present our practices on training multilingual pre45;trained language models with BBPE Byte45;Level BPE (i.e. Byte Pair Encoding). In the experiment we adopted the architecture of NEZHA as the underlying pre45;trained language model and the results show that NEZHA trained with byte45;level subwords consistently outperforms Google multilingual BERT and vanilla NEZHA by a notable margin in several multilingual NLU tasks. We release the source code of our byte45;level vocabulary building tools and the multilingual pre45;trained language models.
