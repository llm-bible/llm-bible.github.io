---
layout: publication
title: 'Do Language Models Exhibit The Same Cognitive Biases In Problem Solving As Human Learners?'
authors: Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan Cotterell, Bernhard Sch√∂lkopf, Abulhair Saparov, Mrinmaya Sachan
conference: "Arxiv"
year: 2024
bibkey: opedal2024do
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2401.18070'}
tags: ['Ethics and Bias', 'Fine-Tuning', 'Survey Paper']
---
There is increasing interest in employing large language models (LLMs) as
cognitive models. For such purposes, it is central to understand which
properties of human cognition are well-modeled by LLMs, and which are not. In
this work, we study the biases of LLMs in relation to those known in children
when solving arithmetic word problems. Surveying the learning science
literature, we posit that the problem-solving process can be split into three
distinct steps: text comprehension, solution planning and solution execution.
We construct tests for each one in order to understand whether current LLMs
display the same cognitive biases as children in these steps. We generate a
novel set of word problems for each of these tests, using a neuro-symbolic
approach that enables fine-grained control over the problem features. We find
evidence that LLMs, with and without instruction-tuning, exhibit human-like
biases in both the text-comprehension and the solution-planning steps of the
solving process, but not in the final step, in which the arithmetic expressions
are executed to obtain the answer.
