---
layout: publication
title: Instruction-following Evaluation For Large Language Models
authors: Zhou Jeffrey, Lu Tianjian, Mishra Swaroop, Brahma Siddhartha, Basu Sujoy, Luan Yi, Zhou Denny, Hou Le
conference: "Arxiv"
year: 2023
bibkey: zhou2023instruction
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.07911"}
  - {name: "Code", url: "https://github.com/google-research/google-research/tree/master/instruction\_following\_eval"}
tags: ['Ethics And Bias', 'Has Code', 'Prompting']
---
One core capability of Large Language Models (LLMs) is to follow natural language instructions. However the evaluation of such abilities is not standardized Human evaluations are expensive slow and not objectively reproducible while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of verifiable instructions such as write in more than 400 words and mention the keyword of AI at least 3 times. We identified 25 types of those verifiable instructions and constructed around 500 prompts with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction\_following\_eval
