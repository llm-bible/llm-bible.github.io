---
layout: publication
title: 'Large Linguistic Models: Investigating Llms'' Metalinguistic Abilities'
authors: Gašper Beguš, Maksymilian Dąbkowski, Ryan Rhodes
conference: "Arxiv"
year: 2023
bibkey: beguš2023large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.00948"}
tags: ['Interpretability and Explainability', 'Prompting']
---
The performance of large language models (LLMs) has recently improved to the point where models can perform well on many language tasks. We show here that--for the first time--the models can also generate valid metalinguistic analyses of language data. We outline a research program where the behavioral interpretability of LLMs on these tasks is tested via prompting. LLMs are trained primarily on text--as such, evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. We show that OpenAI's (2024) o1 vastly outperforms other models on tasks involving drawing syntactic trees and phonological generalization. We speculate that OpenAI o1's unique advantage over other models may result from the model's chain-of-thought mechanism, which mimics the structure of human reasoning used in complex cognitive tasks, such as linguistic analysis.
