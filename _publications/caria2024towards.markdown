---
layout: publication
title: 'Towards Predictive Communication With Brain-computer Interfaces Integrating Large Language Models'
authors: Andrea Caria
conference: "Arxiv"
year: 2024
bibkey: caria2024towards
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.07355'}
tags: ['Large-Scale Training', 'Transformer', 'Model Architecture', 'Training Techniques', 'Tools', 'Fine-Tuning', 'GPT', 'Reinforcement Learning', 'Pre-Training', 'Pretraining Methods']
---
This perspective article aims at providing an outline of the state of the art
and future developments towards the integration of cutting-edge predictive
language models with BCI. A synthetic overview of early and more recent
linguistic models, from natural language processing (NLP) models to recent LLM,
that to a varying extent improved predictive writing systems, is first
provided. Second, a summary of previous BCI implementations integrating
language models is presented. The few preliminary studies investigating the
possible combination of LLM with BCI spellers to efficiently support fast
communication and control are then described. Finally, current challenges and
limitations towards the full integration of LLM with BCI systems are discussed.
Recent investigations suggest that the combination of LLM with BCI might
drastically improve human-computer interaction in patients with motor or
language disorders as well as in healthy individuals. In particular, the
pretrained autoregressive transformer models, such as GPT, that capitalize from
parallelization, learning through pre-training and fine-tuning, promise a
substantial improvement of BCI for communication with respect to previous
systems incorporating simpler language models. Indeed, among various models,
the GPT-2 was shown to represent an excellent candidate for its integration
into BCI although testing was only perfomed on simulated conversations and not
on real BCI scenarios. Prospectively, the full integration of LLM with advanced
BCI systems might lead to a big leap forward towards fast, efficient and
user-adaptive neurotechnology.
