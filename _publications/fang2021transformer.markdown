---
layout: publication
title: Transformer45;based Conditional Variational Autoencoder For Controllable Story Generation
authors: Fang Le, Zeng Tao, Liu Chaochun, Bo Liefeng, Dong Wen, Chen Changyou
conference: "Arxiv"
year: 2021
bibkey: fang2021transformer
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2101.00828"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
We investigate large45;scale latent variable models (LVMs) for neural story generation 45;45; an under45;explored application for open45;domain long text 45;45; with objectives in two threads generation effectiveness and controllability. LVMs especially the variational autoencoder (VAE) have achieved both effective and controllable generation through exploiting flexible distributional latent representations. Recently Transformers and its variants have achieved remarkable effectiveness without explicit latent representation learning thus lack satisfying controllability in generation. In this paper we advocate to revive latent variable modeling essentially the power of representation learning in the era of Transformers to enhance controllability without hurting state45;of45;the45;art generation effectiveness. Specifically we integrate latent representation vectors with a Transformer45;based pre45;trained architecture to build conditional variational autoencoder (CVAE). Model components such as encoder decoder and the variational posterior are all built on top of pre45;trained language models 45;45; GPT2 specifically in this paper. Experiments demonstrate state45;of45;the45;art conditional generation ability of our model as well as its excellent representation learning capability and controllability.
