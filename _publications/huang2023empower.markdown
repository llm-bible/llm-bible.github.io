---
layout: publication
title: Vtimellm Empower LLM To Grasp Video Moments
authors: Huang Bin, Wang Xin, Chen Hong, Song Zihan, Zhu Wenwu
conference: "Arxiv"
year: 2023
bibkey: huang2023empower
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.18445"}
tags: ['Ethics And Bias', 'Training Techniques']
---
Large language models (LLMs) have shown remarkable text understanding capabilities which have been extended as Video LLMs to handle video data for comprehending visual details. However existing Video LLMs can only provide a coarse description of the entire video failing to capture the precise start and end time boundary of specific events. In this paper we solve this issue via proposing VTimeLLM a novel Video LLM designed for fine45;grained video moment understanding and reasoning with respect to time boundary. Specifically our VTimeLLM adopts a boundary45;aware three45;stage training strategy which respectively utilizes image45;text pairs for feature alignment multiple45;event videos to increase temporal45;boundary awareness and high45;quality video45;instruction tuning to further improve temporal understanding ability as well as align with human intents. Extensive experiments demonstrate that in fine45;grained time45;related comprehension tasks for videos such as Temporal Video Grounding and Dense Video Captioning VTimeLLM significantly outperforms existing Video LLMs. Besides benefits from the fine45;grained temporal understanding of the videos further enable VTimeLLM to beat existing Video LLMs in video dialogue benchmark showing its superior cross45;modal understanding and reasoning abilities.
