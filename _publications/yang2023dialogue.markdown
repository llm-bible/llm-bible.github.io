---
layout: publication
title: Refgpt Dialogue Generation Of GPT By GPT And For GPT
authors: Yang Dongjie, Yuan Ruifeng, Fan Yuantao, Yang Yifei, Wang Zili, Wang Shusen, Zhao Hai
conference: "Arxiv"
year: 2023
bibkey: yang2023dialogue
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.14994"}
  - {name: "Code", url: "https://github.com/mutonix/RefGPT"}
tags: ['Applications', 'GPT', 'Has Code', 'Model Architecture', 'RAG']
---
Large Language Models (LLMs) have attained the impressive capability to resolve a wide range of NLP tasks by fine45;tuning high45;quality instruction data. However collecting human45;written data of high quality especially multi45;turn dialogues is expensive and unattainable for most people. Though previous studies have used powerful LLMs to generate the dialogues automatically they all suffer from generating untruthful dialogues because of the model hallucination. Therefore we propose a method called RefGPT to generate enormous truthful and customized dialogues without worrying about factual errors caused by the model hallucination. RefGPT solves the model hallucination in dialogue generation by restricting the LLMs to leverage the given reference instead of reciting their own knowledge to generate dialogues. Additionally RefGPT adds detailed controls on every utterance to enable high customization capability which previous studies have ignored. On the basis of RefGPT we also propose two high45;quality dialogue datasets generated by GPT45;4 namely RefGPT45;Fact and RefGPT45;Code. RefGPT45;Fact is a dataset with 100k multi45;turn dialogues based on factual knowledge and RefGPT45;Code has 76k multi45;turn dialogues covering a wide range of coding scenarios. Our code and datasets are released in https://github.com/mutonix/RefGPT.
