---
layout: publication
title: 'When Prompting Fails To Sway: Inertia In Moral And Value Judgments Of Large Language Models'
authors: Bruce W. Lee, Yeongheon Lee, Hyunsoo Cho
conference: "Arxiv"
year: 2024
bibkey: lee2024when
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2408.09049'}
tags: ['Fairness', 'Applications', 'Prompting', 'Bias Mitigation', 'Ethics and Bias']
---
Large Language Models (LLMs) exhibit non-deterministic behavior, and
prompting has emerged as a primary method for steering their outputs toward
desired directions. One popular strategy involves assigning a specific
"persona" to the model to induce more varied and context-sensitive responses,
akin to the diversity found in human perspectives. However, contrary to the
expectation that persona-based prompting would yield a wide range of opinions,
our experiments demonstrate that LLMs maintain consistent value orientations.
In particular, we observe a persistent inertia in their responses, where
certain moral and value dimensions, especially harm avoidance and fairness,
remain distinctly skewed in one direction despite varied persona settings. To
investigate this phenomenon systematically, use role-play at scale, which
combines randomized, diverse persona prompts with a macroscopic trend analysis
of model outputs. Our findings highlight the strong internal biases and value
preferences in LLMs, underscoring the need for careful scrutiny and potential
adjustment of these models to ensure balanced and equitable applications.
