---
layout: publication
title: Tree45;to45;sequence Attentional Neural Machine Translation
authors: Eriguchi Akiko, Hashimoto Kazuma, Tsuruoka Yoshimasa
conference: "Arxiv"
year: 2016
bibkey: eriguchi2016tree
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1603.06075"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Transformer']
---
Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end45;to45;end syntactic NMT model extending a sequence45;to45;sequence model with the source45;side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT15 English45;to45;Japanese dataset demonstrate that our proposed model considerably outperforms sequence45;to45;sequence attentional NMT models and compares favorably with the state45;of45;the45;art tree45;to45;string SMT system.
