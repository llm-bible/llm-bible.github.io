---
layout: publication
title: 'MAUVE: Measuring The Gap Between Neural Text And Human Text Using Divergence
  Frontiers'
authors: Krishna Pillutla et al.
conference: Arxiv
year: 2021
citations: 87
bibkey: pillutla2021measuring
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2102.01454'}]
tags: [Language Modeling]
---
As major progress is made in open-ended text generation, measuring how close
machine-generated text is to human language remains a critical open problem. We
introduce MAUVE, a comparison measure for open-ended text generation, which
directly compares the learnt distribution from a text generation model to the
distribution of human-written text using divergence frontiers. MAUVE scales up
to modern text generation models by computing information divergences in a
quantized embedding space. Through an extensive empirical study on three
open-ended generation tasks, we find that MAUVE identifies known properties of
generated text, scales naturally with model size, and correlates with human
judgments, with fewer restrictions than existing distributional evaluation
metrics.