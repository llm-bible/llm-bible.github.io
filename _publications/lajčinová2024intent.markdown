---
layout: publication
title: 'Intent Classification For Bank Chatbots Through LLM Fine-tuning'
authors: Bibiána Lajčinová, Patrik Valábek, Michal Spišiak
conference: "Arxiv"
year: 2024
bibkey: lajčinová2024intent
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.04925'}
tags: ['Training Techniques', 'BERT', 'Model Architecture', 'Fine-Tuning', 'Pretraining Methods']
---
This study evaluates the application of large language models (LLMs) for
intent classification within a chatbot with predetermined responses designed
for banking industry websites. Specifically, the research examines the
effectiveness of fine-tuning SlovakBERT compared to employing multilingual
generative models, such as Llama 8b instruct and Gemma 7b instruct, in both
their pre-trained and fine-tuned versions. The findings indicate that
SlovakBERT outperforms the other models in terms of in-scope accuracy and
out-of-scope false positive rate, establishing it as the benchmark for this
application.
