---
layout: publication
title: 'Explingo: Explaining AI Predictions Using Large Language Models'
authors: Alexandra Zytek, Sara Pido, Sarah Alnegheimish, Laure Berti-equille, Kalyan Veeramachaneni
conference: "Arxiv"
year: 2024
bibkey: zytek2024explaining
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.05145"}
tags: ['Interpretability and Explainability', 'Applications', 'Reinforcement Learning']
---
Explanations of machine learning (ML) model predictions generated by
Explainable AI (XAI) techniques such as SHAP are essential for people using ML
outputs for decision-making. We explore the potential of Large Language Models
(LLMs) to transform these explanations into human-readable, narrative formats
that align with natural communication. We address two key research questions:
(1) Can LLMs reliably transform traditional explanations into high-quality
narratives? and (2) How can we effectively evaluate the quality of narrative
explanations? To answer these questions, we introduce Explingo, which consists
of two LLM-based subsystems, a Narrator and Grader. The Narrator takes in ML
explanations and transforms them into natural-language descriptions. The Grader
scores these narratives on a set of metrics including accuracy, completeness,
fluency, and conciseness.
  Our experiments demonstrate that LLMs can generate high-quality narratives
that achieve high scores across all metrics, particularly when guided by a
small number of human-labeled and bootstrapped examples. We also identified
areas that remain challenging, in particular for effectively scoring narratives
in complex domains. The findings from this work have been integrated into an
open-source tool that makes narrative explanations available for further
applications.
