---
layout: publication
title: Leveraging Large Language Models For Sequential Recommendation
authors: Harte Jesse, Zorgdrager Wouter, Louridas Panos, Katsifodimos Asterios, Jannach Dietmar, Fragkoulis Marios
conference: "Arxiv"
year: 2023
bibkey: harte2023leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.09261"}
tags: ['Applications', 'Attention Mechanism', 'BERT', 'Ethics And Bias', 'Model Architecture', 'RAG']
---
Sequential recommendation problems have received increasing attention in research during the past few years leading to the inception of a large variety of algorithmic approaches. In this work we explore how large language models (LLMs) which are nowadays introducing disruptive effects in many AI45;based applications can be used to build or improve sequential recommendation approaches. Specifically we devise and evaluate three approaches to leverage the power of LLMs in different ways. Our results from experiments on two datasets show that initializing the state45;of45;the45;art sequential recommendation model BERT4Rec with embeddings obtained from an LLM improves NDCG by 1545;2037; compared to the vanilla BERT4Rec model. Furthermore we find that a simple approach that leverages LLM embeddings for producing recommendations can provide competitive performance by highlighting semantically related items. We publicly share the code and data of our experiments to ensure reproducibility.
