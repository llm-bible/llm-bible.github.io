---
layout: publication
title: IT5 Text45;to45;text Pretraining For Italian Language Understanding And Generation
authors: Sarti Gabriele, Nissim Malvina
conference: "Proceedings of LREC-COLING"
year: 2022
bibkey: sarti2022text
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2203.03759"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
We introduce IT5 the first family of encoder45;decoder transformer models pretrained specifically on Italian. We document and perform a thorough cleaning procedure for a large Italian corpus and use it to pretrain four IT5 model sizes. We then introduce the ItaGen benchmark which includes a broad range of natural language understanding and generation tasks for Italian and use it to evaluate the performance of IT5 models and multilingual baselines. We find monolingual IT5 models to provide the best scale45;to45;performance ratio across tested models consistently outperforming their multilingual counterparts and setting a new state45;of45;the45;art for Italian language generation.
