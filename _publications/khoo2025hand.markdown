---
layout: publication
title: 'Minorbench: A Hand-built Benchmark For Content-based Risks For Children'
authors: Shaun Khoo, Gabriel Chua, Rachel Shong
conference: "Arxiv"
year: 2025
bibkey: khoo2025hand
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.10242"}
tags: ['Responsible AI', 'Tools', 'Reinforcement Learning', 'Ethics and Bias', 'Prompting']
---
Large Language Models (LLMs) are rapidly entering children's lives - through
parent-driven adoption, schools, and peer networks - yet current AI ethics and
safety research do not adequately address content-related risks specific to
minors. In this paper, we highlight these gaps with a real-world case study of
an LLM-based chatbot deployed in a middle school setting, revealing how
students used and sometimes misused the system. Building on these findings, we
propose a new taxonomy of content-based risks for minors and introduce
MinorBench, an open-source benchmark designed to evaluate LLMs on their ability
to refuse unsafe or inappropriate queries from children. We evaluate six
prominent LLMs under different system prompts, demonstrating substantial
variability in their child-safety compliance. Our results inform practical
steps for more robust, child-focused safety mechanisms and underscore the
urgency of tailoring AI systems to safeguard young users.
