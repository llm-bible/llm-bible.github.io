---
layout: publication
title: 'Is Depth All You Need? An Exploration Of Iterative Reasoning In Llms'
authors: Zongqian Wu, Tianyu Li, Baoduo Xu, Jiaying Yang, Mengmeng Zhan, Xiaofeng Zhu, Lei Feng
conference: "Arxiv"
year: 2025
bibkey: wu2025is
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.10858'}
  - {name: "Code", url: 'https://github.com/zongqianwu/breadth'}
tags: ['Fine-Tuning', 'Has Code']
---
Deep iterative chain-of-thought (CoT) reasoning enables LLMs to tackle
complex tasks by progressively activating relevant pre-trained knowledge.
However, it faces challenges in ensuring continual improvement and determining
a stopping criterion. In this paper, we investigate whether the relevant
knowledge that contributes directly to solving the given question can be
activated from the initial reasoning path, thus circumventing the need for
iterative refinement. Our experiments reveal that increasing the diversity of
initial reasoning paths can achieve comparable or superior performance, a
concept we term \textit\{breadth reasoning\}. However, existing breadth reasoning
approaches, such as self-consistency, offer limited diversity. To address this
limitation, we propose a simple yet effective method that enhances reasoning
breadth by integrating contextual exploration with reduced sampling randomness.
Extensive experiments demonstrate that our approach significantly outperforms
deep iterative reasoning. Our code is provided in
https://github.com/zongqianwu/breadth.
