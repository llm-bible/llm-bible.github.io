---
layout: publication
title: Humans In Humans Out On GPT Converging Toward Common Sense In Both Success And Failure
authors: Koralus Philipp, Wang-ma≈õcianica Vincent
conference: "Arxiv"
year: 2023
bibkey: koralus2023humans
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2303.17276"}
tags: ['GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Training Techniques']
---
Increase in computational scale and fine45;tuning has seen a dramatic improvement in the quality of outputs of large language models (LLMs) like GPT. Given that both GPT45;3 and GPT45;4 were trained on large quantities of human45;generated text we might ask to what extent their outputs reflect patterns of human thinking both for correct and incorrect cases. The Erotetic Theory of Reason (ETR) provides a symbolic generative model of both human success and failure in thinking across propositional quantified and probabilistic reasoning as well as decision45;making. We presented GPT45;3 GPT45;3.5 and GPT45;4 with 61 central inference and judgment problems from a recent book45;length presentation of ETR consisting of experimentally verified data45;points on human judgment and extrapolated data45;points predicted by ETR with correct inference patterns as well as fallacies and framing effects (the ETR61 benchmark). ETR61 includes classics like Wasons card task illusory inferences the decoy effect and opportunity45;cost neglect among others. GPT45;3 showed evidence of ETR45;predicted outputs for 5937; of these examples rising to 7737; in GPT45;3.5 and 7537; in GPT45;4. Remarkably the production of human45;like fallacious judgments increased from 1837; in GPT45;3 to 3337; in GPT45;3.5 and 3437; in GPT45;4. This suggests that larger and more advanced LLMs may develop a tendency toward more human45;like mistakes as relevant thought patterns are inherent in human45;produced training data. According to ETR the same fundamental patterns are involved both in successful and unsuccessful ordinary reasoning so that the bad cases could paradoxically be learned from the good cases. We further present preliminary evidence that ETR45;inspired prompt engineering could reduce instances of these mistakes.
