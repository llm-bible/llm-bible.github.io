---
layout: publication
title: 'LLM Gesticulator: Leveraging Large Language Models For Scalable And Controllable Co-speech Gesture Synthesis'
authors: Haozhou Pang, Tianwei Ding, Lanshan He, Ming Tao, Lu Zhang, Qi Gan
conference: "Arxiv"
year: 2024
bibkey: pang2024llm
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.10851'}
tags: ['RAG', 'Prompting', 'Tools']
---
In this work, we present LLM Gesticulator, an LLM-based audio-driven
co-speech gesture generation framework that synthesizes full-body animations
that are rhythmically aligned with the input audio while exhibiting natural
movements and editability. Compared to previous work, our model demonstrates
substantial scalability. As the size of the backbone LLM model increases, our
framework shows proportional improvements in evaluation metrics (a.k.a. scaling
law). Our method also exhibits strong controllability where the content, style
of the generated gestures can be controlled by text prompt. To the best of our
knowledge, LLM gesticulator is the first work that use LLM on the co-speech
generation task. Evaluation with existing objective metrics and user studies
indicate that our framework outperforms prior works.
