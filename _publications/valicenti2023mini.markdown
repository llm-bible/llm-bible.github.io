---
layout: publication
title: Mini-GPTs Efficient Large Language Models through Contextual Pruning
authors: Valicenti Tim, Vidal Justice, Patnaik Ritik
conference: "Arxiv"
year: 2023
bibkey: valicenti2023mini
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.12682"}
tags: ['Applications', 'Efficiency And Optimization', 'Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Pruning', 'Quantization', 'Training Techniques']
---
In AI research the optimization of Large Language Models (LLMs) remains a significant challenge crucial for advancing the fields practical applications and sustainability. Building upon the foundational work of Professor Song Hans lab at MIT this paper introduces a novel approach in developing Mini-GPTs via contextual pruning. Our methodology strategically prunes the computational architecture of traditional LLMs like Phi-1.5 focusing on retaining core functionalities while drastically reducing model sizes. We employ the technique across diverse and complex datasets including US law Medical Qamp;A Skyrim dialogue English-Taiwanese translation and Economics articles. The results underscore the efficiency and effectiveness of contextual pruning not merely as a theoretical concept but as a practical tool in developing domain-specific resource-efficient LLMs. Contextual pruning is a promising method for building domain-specific LLMs and this research is a building block towards future development with more hardware compute refined fine-tuning and quantization.
