---
layout: publication
title: Mini45;gpts Efficient Large Language Models Through Contextual Pruning
authors: Valicenti Tim, Vidal Justice, Patnaik Ritik
conference: "Arxiv"
year: 2023
bibkey: valicenti2023mini
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.12682"}
tags: ['Applications', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Pruning', 'Quantization']
---
In AI research the optimization of Large Language Models (LLMs) remains a significant challenge crucial for advancing the fields practical applications and sustainability. Building upon the foundational work of Professor Song Hans lab at MIT this paper introduces a novel approach in developing Mini45;GPTs via contextual pruning. Our methodology strategically prunes the computational architecture of traditional LLMs like Phi45;1.5 focusing on retaining core functionalities while drastically reducing model sizes. We employ the technique across diverse and complex datasets including US law Medical Qamp;A Skyrim dialogue English45;Taiwanese translation and Economics articles. The results underscore the efficiency and effectiveness of contextual pruning not merely as a theoretical concept but as a practical tool in developing domain45;specific resource45;efficient LLMs. Contextual pruning is a promising method for building domain45;specific LLMs and this research is a building block towards future development with more hardware compute refined fine45;tuning and quantization.
