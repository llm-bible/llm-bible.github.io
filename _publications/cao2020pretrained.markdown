---
layout: publication
title: 'Pretrained Language Models For Dialogue Generation With Multiple Input Sources'
authors: Yu Cao, Wei Bi, Meng Fang, Dacheng Tao
conference: "Arxiv"
year: 2020
bibkey: cao2020pretrained
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2010.07576"}
tags: ['GPT', 'Applications', 'RAG', 'Model Architecture', 'Merging', 'Attention Mechanism']
---
Large-scale pretrained language models have achieved outstanding performance
on natural language understanding tasks. However, it is still under
investigating how to apply them to dialogue generation tasks, especially those
with responses conditioned on multiple sources. Previous work simply
concatenates all input sources or averages information from different input
sources. In this work, we study dialogue models with multiple input sources
adapted from the pretrained language model GPT2. We explore various methods to
fuse multiple separate attention information corresponding to different
sources. Our experimental results show that proper fusion methods deliver
higher relevance with dialogue history than simple fusion baselines.
