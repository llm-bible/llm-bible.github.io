---
layout: publication
title: TRUE Re45;evaluating Factual Consistency Evaluation
authors: Honovich Or, Aharoni Roee, Herzig Jonathan, Taitelbaum Hagai, Kukliansy Doron, Cohen Vered, Scialom Thomas, Szpektor Idan, Hassidim Avinatan, Matias Yossi
conference: "Arxiv"
year: 2022
bibkey: honovich2022re
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.04991"}
tags: ['Applications', 'Attention Mechanism', 'Language Modeling', 'Model Architecture', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
Grounded text generation systems often generate text that contains factual inconsistencies hindering their real45;world applicability. Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles filtering inconsistent outputs and augmenting training data. While attracting increasing attention such evaluation metrics are usually developed and evaluated in silo for a single task or dataset slowing their adoption. Moreover previous meta45;evaluation protocols focused on system45;level correlations with human annotations which leave the example45;level accuracy of such metrics unclear. In this work we introduce TRUE a comprehensive survey and assessment of factual consistency metrics on a standardized collection of existing texts from diverse tasks manually annotated for factual consistency. Our standardization enables an example45;level meta45;evaluation protocol that is more actionable and interpretable than previously reported correlations yielding clearer quality measures. Across diverse state45;of45;the45;art metrics and 11 datasets we find that large45;scale NLI and question generation45;and45;answering45;based approaches achieve strong and complementary results. We recommend those methods as a starting point for model and metric developers and hope TRUE will foster progress towards even better evaluation methods.
