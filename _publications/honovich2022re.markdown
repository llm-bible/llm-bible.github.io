---
layout: publication
title: 'TRUE: Re-evaluating Factual Consistency Evaluation'
authors: Honovich Or, Aharoni Roee, Herzig Jonathan, Taitelbaum Hagai, Kukliansy Doron, Cohen Vered, Scialom Thomas, Szpektor Idan, Hassidim Avinatan, Matias Yossi
conference: "Arxiv"
year: 2022
bibkey: honovich2022re
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.04991"}
tags: ['Applications', 'Attention Mechanism', 'Language Modeling', 'Model Architecture', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
Grounded text generation systems often generate text that contains factual
inconsistencies, hindering their real-world applicability. Automatic factual
consistency evaluation may help alleviate this limitation by accelerating
evaluation cycles, filtering inconsistent outputs and augmenting training data.
While attracting increasing attention, such evaluation metrics are usually
developed and evaluated in silo for a single task or dataset, slowing their
adoption. Moreover, previous meta-evaluation protocols focused on system-level
correlations with human annotations, which leave the example-level accuracy of
such metrics unclear. In this work, we introduce TRUE: a comprehensive survey
and assessment of factual consistency metrics on a standardized collection of
existing texts from diverse tasks, manually annotated for factual consistency.
Our standardization enables an example-level meta-evaluation protocol that is
more actionable and interpretable than previously reported correlations,
yielding clearer quality measures. Across diverse state-of-the-art metrics and
11 datasets we find that large-scale NLI and question
generation-and-answering-based approaches achieve strong and complementary
results. We recommend those methods as a starting point for model and metric
developers, and hope TRUE will foster progress towards even better evaluation
methods.
