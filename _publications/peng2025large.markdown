---
layout: publication
title: 'Large Language Model-informed Feature Discovery Improves Prediction And Interpretation Of Credibility Perceptions Of Visual Content'
authors: Yilang Peng, Sijia Qian, Yingdan Lu, Cuihua Shen
conference: "Arxiv"
year: 2025
bibkey: peng2025large
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2504.10878'}
tags: ['RAG', 'Model Architecture', 'Tools', 'GPT', 'Prompting', 'Multimodal Models']
---
In today's visually dominated social media landscape, predicting the
perceived credibility of visual content and understanding what drives human
judgment are crucial for countering misinformation. However, these tasks are
challenging due to the diversity and richness of visual features. We introduce
a Large Language Model (LLM)-informed feature discovery framework that
leverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and
explain its reasoning. We extract and quantify interpretable features using
targeted prompts and integrate them into machine learning models to improve
credibility predictions. We tested this approach on 4,191 visual social media
posts across eight topics in science, health, and politics, using credibility
ratings from 5,355 crowdsourced workers. Our method outperformed zero-shot
GPT-based predictions by 13 percent in R2, and revealed key features like
information concreteness and image format. We discuss the implications for
misinformation mitigation, visual credibility, and the role of LLMs in social
science.
