---
layout: publication
title: The Effect Of Sampling Temperature On Problem Solving In Large Language Models
authors: Renze Matthew, Guven Erhan
conference: "Arxiv"
year: 2024
bibkey: renze2024effect
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.05201"}
  - {name: "Code", url: "https://github.com/matthewrenze/jhu&#45;llm&#45;temperature"}
tags: ['Has Code', 'Pretraining Methods', 'Prompting']
---
In this research study we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem45;solving tasks. We created a multiple45;choice question45;and45;answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then we used nine popular LLMs with five prompt45;engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem45;solving tasks. In addition these results appear to generalize across LLMs prompt45;engineering techniques and problem domains. All code data and supplemental materials are available on GitHub at https://github.com/matthewrenze/jhu&#45;llm&#45;temperature
