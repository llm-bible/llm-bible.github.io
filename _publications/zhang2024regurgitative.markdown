---
layout: publication
title: Regurgitative Training The Value Of Real Data In Training Large Language Models
authors: Zhang Jinghui, Qiao Dandan, Yang Mochen, Wei Qiang
conference: "Arxiv"
year: 2024
bibkey: zhang2024regurgitative
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.12835"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
What happens if we train a new Large Language Model (LLM) using data that are at least partially generated by other LLMs The explosive success of LLMs means that a substantial amount of content online will be generated by LLMs rather than humans which will inevitably enter the training datasets of next45;generation LLMs. We evaluate the implications of such regurgitative training on LLM performance. Through fine45;tuning GPT45;3.5 with data generated either by itself or by other LLMs in a machine translation task we find strong evidence that regurgitative training clearly handicaps the performance of LLMs. The same performance loss of regurgitative training is observed on transformer models that we train from scratch. We find suggestive evidence that the performance disadvantage of regurgitative training can be attributed to at least two mechanisms (1) higher error rates and (2) lower lexical diversity in LLM45;generated data as compared to real data. Based on these mechanisms we propose and evaluate three different strategies to mitigate the performance loss of regurgitative training. First we devise data45;driven metrics to gauge the quality of each LLM45;generated data instance and then carry out an ordered training process where high45;quality data are added before low45;quality ones. Second we combine data generated by multiple different LLMs (as an attempt to increase lexical diversity). Third we train an AI detection classifier to differentiate between LLM45; and human45;generated data and include LLM45;generated data in the order of resemblance to human45;generated data. All three strategies can improve the performance of regurgitative training to some extent but are not always able to fully close the gap from training with real data. Our results highlight the value of real human45;generated data in training LLMs which cannot be easily substituted by synthetic LLM45;generated data.
