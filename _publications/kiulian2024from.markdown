---
layout: publication
title: 'From English-centric To Effective Bilingual: Llms With Custom Tokenizers For Underrepresented Languages'
authors: Artur Kiulian, Anton Polishko, Mykola Khandoga, Yevhen Kostiuk, Guillermo Gabrielli, Łukasz Gagała, Fadi Zaraket, Qusai Abu Obaida, Hrishikesh Garud, Wendy Wing Yee Mak, Dmytro Chaplynskyi, Selma Belhadj Amor, Grigol Peradze
conference: "Arxiv"
year: 2024
bibkey: kiulian2024from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.18836"}
tags: ['Training Techniques', 'Fairness', 'Reinforcement Learning', 'Bias Mitigation', 'Ethics and Bias']
---
In this paper, we propose a model-agnostic cost-effective approach to
developing bilingual base large language models (LLMs) to support English and
any target language. The method includes vocabulary expansion, initialization
of new embeddings, model training and evaluation. We performed our experiments
with three languages, each using a non-Latin script - Ukrainian, Arabic, and
Georgian.
  Our approach demonstrates improved language performance while reducing
computational costs. It mitigates the disproportionate penalization of
underrepresented languages, promoting fairness and minimizing adverse phenomena
such as code-switching and broken grammar. Additionally, we introduce new
metrics to evaluate language quality, revealing that vocabulary size
significantly impacts the quality of generated text.
