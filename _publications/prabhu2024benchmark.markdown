---
layout: publication
title: 'DEXTER: A Benchmark For Open-domain Complex Question Answering Using Llms'
authors: Prabhu Venktesh V. Deepali, Anand Avishek
conference: "Arxiv"
year: 2024
bibkey: prabhu2024benchmark
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.17158"}
  - {name: "Code", url: "https://github.com/VenkteshV/DEXTER"}
tags: ['Applications', 'Has Code']
---
"Open-domain complex Question Answering (QA) is a difficult task with challenges in evidence retrieval and reasoning. The complexity of such questions could stem from questions being compositional, hybrid evidence, or ambiguity in questions. While retrieval performance for classical QA tasks is well explored, their capabilities for heterogeneous complex retrieval tasks, especially in an open-domain setting, and the impact on downstream QA performance, are relatively unexplored. To address this, in this work, we propose a benchmark composing diverse complex QA tasks and provide a toolkit to evaluate state-of-the-art pre-trained dense and sparse retrieval models in an open-domain setting. We observe that late interaction models and surprisingly lexical models like BM25 perform well compared to other pre-trained dense retrieval models. In addition, since context-based reasoning is critical for solving complex QA tasks, we also evaluate the reasoning capabilities of LLMs and the impact of retrieval performance on their reasoning capabilities. Through experiments, we observe that much progress is to be made in retrieval for complex QA to improve downstream QA performance. Our software and related data can be accessed at https://github.com/VenkteshV/DEXTER"
