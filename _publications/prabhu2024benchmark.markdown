---
layout: publication
title: DEXTER A Benchmark For Open45;domain Complex Question Answering Using Llms
authors: Prabhu Venktesh V. Deepali, Anand Avishek
conference: "Arxiv"
year: 2024
bibkey: prabhu2024benchmark
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.17158"}
  - {name: "Code", url: "https://github.com/VenkteshV/DEXTER"}
tags: ['Applications', 'Has Code']
---
Open45;domain complex Question Answering (QA) is a difficult task with challenges in evidence retrieval and reasoning. The complexity of such questions could stem from questions being compositional hybrid evidence or ambiguity in questions. While retrieval performance for classical QA tasks is well explored their capabilities for heterogeneous complex retrieval tasks especially in an open45;domain setting and the impact on downstream QA performance are relatively unexplored. To address this in this work we propose a benchmark composing diverse complex QA tasks and provide a toolkit to evaluate state45;of45;the45;art pre45;trained dense and sparse retrieval models in an open45;domain setting. We observe that late interaction models and surprisingly lexical models like BM25 perform well compared to other pre45;trained dense retrieval models. In addition since context45;based reasoning is critical for solving complex QA tasks we also evaluate the reasoning capabilities of LLMs and the impact of retrieval performance on their reasoning capabilities. Through experiments we observe that much progress is to be made in retrieval for complex QA to improve downstream QA performance. Our software and related data can be accessed at https://github.com/VenkteshV/DEXTER
