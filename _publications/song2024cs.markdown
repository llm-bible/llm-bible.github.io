---
layout: publication
title: 'Cs-bench: A Comprehensive Benchmark For Large Language Models Towards Computer Science Mastery'
authors: Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, Weihao Zeng, Yejie Wang, Zhuoma Gongque, Jianing Yu, Qiuna Tan, Weiran Xu
conference: "Arxiv"
year: 2024
bibkey: song2024cs
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.08587"}
  - {name: "Code", url: "https://github.com/csbench/csbench"}
tags: ['Applications', 'Has Code', 'Reinforcement Learning']
---
Large language models (LLMs) have demonstrated significant potential in
advancing various fields of research and society. However, the current
community of LLMs overly focuses on benchmarks for analyzing specific
foundational skills (e.g. mathematics and code generation), neglecting an
all-round evaluation of the computer science field. To bridge this gap, we
introduce CS-Bench, the first multilingual (English, Chinese, French, German)
benchmark dedicated to evaluating the performance of LLMs in computer science.
CS-Bench comprises approximately 10K meticulously curated test samples,
covering 26 subfields across 4 key areas of computer science, encompassing
various task forms and divisions of knowledge and reasoning. Utilizing
CS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs,
revealing the relationship between CS performance and model scales. We also
quantitatively analyze the reasons for failures in existing LLMs and highlight
directions for improvements, including knowledge supplementation and
CS-specific reasoning. Further cross-capability experiments show a high
correlation between LLMs' capabilities in computer science and their abilities
in mathematics and coding. Moreover, expert LLMs specialized in mathematics and
coding also demonstrate strong performances in several CS subfields. Looking
ahead, we envision CS-Bench serving as a cornerstone for LLM applications in
the CS field and paving new avenues in assessing LLMs' diverse reasoning
capabilities. The CS-Bench data and evaluation code are available at
https://github.com/csbench/csbench.
