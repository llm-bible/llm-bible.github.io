---
layout: publication
title: Learning To Prompt In The Classroom To Understand AI Limits: A Pilot Study
authors: Theophilou Emily, Koyuturk Cansu, Yavari Mona, Bursic Sathya, Donabauer Gregor, Telari Alessia, Testa Alessia, Boiano Raffaele, Hernandez-leo Davinia, Ruskov Martin, Taibi Davide, Gabbiadini Alessandro, Ognibene Dimitri
conference: "Arxiv"
year: 2023
bibkey: theophilou2023learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.01540"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'Prompting', 'RAG']
---
Artificial intelligences (AI) progress holds great promise in tackling pressing societal concerns such as health and climate. Large Language Models (LLM) and the derived chatbots like ChatGPT have highly improved the natural language processing capabilities of AI systems allowing them to process an unprecedented amount of unstructured data. However the ensuing excitement has led to negative sentiments even as AI methods demonstrate remarkable contributions (e.g. in health and genetics). A key factor contributing to this sentiment is the misleading perception that LLMs can effortlessly provide solutions across domains ignoring their limitations such as hallucinations and reasoning constraints. Acknowledging AI fallibility is crucial to address the impact of dogmatic overconfidence in possibly erroneous suggestions generated by LLMs. At the same time it can reduce fear and other negative attitudes toward AI. This necessitates comprehensive AI literacy interventions that educate the public about LLM constraints and effective usage techniques i.e prompting strategies. With this aim a pilot educational intervention was performed in a high school with 21 students. It involved presenting high-level concepts about intelligence AI and LLMs followed by practical exercises involving ChatGPT in creating natural educational conversations and applying established prompting strategies. Encouraging preliminary results emerged including high appreciation of the activity improved interaction quality with the LLM reduced negative AI sentiments and a better grasp of limitations specifically unreliability limited understanding of commands leading to unsatisfactory responses and limited presentation flexibility. Our aim is to explore AI acceptance factors and refine this approach for more controlled future studies.
