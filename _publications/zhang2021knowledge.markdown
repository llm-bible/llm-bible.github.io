---
layout: publication
title: A Knowledge45;grounded Dialog System Based On Pre45;trained Language Models
authors: Zhang Weijie, Chen Jiaoxuan, Wu Haipang, Wan Sanhui, Li Gongfeng
conference: "Arxiv"
year: 2021
bibkey: zhang2021knowledge
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2106.14444"}
tags: ['Efficiency And Optimization', 'Fine Tuning', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Tools', 'Transformer']
---
We present a knowledge45;grounded dialog system developed for the ninth Dialog System Technology Challenge (DSTC9) Track 1 45; Beyond Domain APIs Task45;oriented Conversational Modeling with Unstructured Knowledge Access. We leverage transfer learning with existing language models to accomplish the tasks in this challenge track. Specifically we divided the task into four sub45;tasks and fine45;tuned several Transformer models on each of the sub45;tasks. We made additional changes that yielded gains in both performance and efficiency including the combination of the model with traditional entity45;matching techniques and the addition of a pointer network to the output layer of the language model.
