---
layout: publication
title: 'A Systematic Analysis Of Large Language Models As Soft Reasoners: The Case Of Syllogistic Inferences'
authors: Bertolazzi Leonardo, Gatt Albert, Bernardi Raffaella
conference: "Arxiv"
year: 2024
bibkey: bertolazzi2024systematic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.11341"}
tags: ['Ethics And Bias', 'Fine Tuning', 'In Context Learning', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Training Techniques']
---
The reasoning abilities of Large Language Models (LLMs) are becoming a central focus of study in NLP. In this paper we consider the case of syllogistic reasoning an area of deductive reasoning studied extensively in logic and cognitive psychology. Previous research has shown that pre-trained LLMs exhibit reasoning biases such as () avoid answering that () display human-like difficulties and struggle with multi-step reasoning. We contribute to this research line by systematically investigating the effects of chain-of-thought reasoning in-context learning (ICL) and supervised fine-tuning (SFT) on syllogistic reasoning considering syllogisms with conclusions that support or violate world knowledge as well as ones with multiple premises. Crucially we go beyond the standard focus on accuracy with an in-depth analysis of the conclusions generated by the models. Our results suggest that the behavior of pre-trained LLMs can be explained by heuristics studied in cognitive science and that both ICL and SFT improve model performance on valid inferences although only the latter mitigates most reasoning biases without harming model consistency.
