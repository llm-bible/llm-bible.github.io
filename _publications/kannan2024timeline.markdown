---
layout: publication
title: 'A Timeline And Analysis For Representation Plasticity In Large Language Models'
authors: Akshat Kannan
conference: "Arxiv"
year: 2024
bibkey: kannan2024timeline
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.06225"}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Reinforcement Learning', 'Ethics and Bias', 'Pretraining Methods', 'Interpretability', 'Fine-Tuning']
---
The ability to steer AI behavior is crucial to preventing its long term
dangerous and catastrophic potential. Representation Engineering (RepE) has
emerged as a novel, powerful method to steer internal model behaviors, such as
"honesty", at a top-down level. Understanding the steering of representations
should thus be placed at the forefront of alignment initiatives. Unfortunately,
current efforts to understand plasticity at this level are highly neglected.
This paper aims to bridge the knowledge gap and understand how LLM
representation stability, specifically for the concept of "honesty", and model
plasticity evolve by applying steering vectors extracted at different
fine-tuning stages, revealing differing magnitudes of shifts in model behavior.
The findings are pivotal, showing that while early steering exhibits high
plasticity, later stages have a surprisingly responsive critical window. This
pattern is observed across different model architectures, signaling that there
is a general pattern of model plasticity that can be used for effective
intervention. These insights greatly contribute to the field of AI
transparency, addressing a pressing lack of efficiency limiting our ability to
effectively steer model behavior.
