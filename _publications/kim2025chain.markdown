---
layout: publication
title: 'Chain-of-zoom: Extreme Super-resolution Via Scale Autoregression And Preference Alignment'
authors: Bryan Sangwoo Kim, Jeongsol Kim, Jong Chul Ye
conference: "Arxiv"
year: 2025
bibkey: kim2025chain
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.18600'}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Tools', 'GPT', 'Merging', 'Prompting', 'Multimodal Models', 'Reinforcement Learning', 'Pretraining Methods']
---
Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic VLM, aligning text guidance towards human preference. Experiments show that a standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement with high perceptual quality and fidelity. Project Page: https://bryanswkim.github.io/chain-of-zoom/ .
