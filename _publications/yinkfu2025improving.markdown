---
layout: publication
title: 'Improving QA Efficiency With Distilbert: Fine-tuning And Inference On Mobile Intel Cpus'
authors: Ngeyen Yinkfu
conference: "Arxiv"
year: 2025
bibkey: yinkfu2025improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.22937"}
tags: ['Fine-Tuning', 'Transformer', 'Efficiency and Optimization', 'Applications', 'RAG', 'Model Architecture', 'Training Techniques', 'Pretraining Methods', 'BERT']
---
This study presents an efficient transformer-based question-answering (QA) model optimized for deployment on a 13th Gen Intel i7-1355U CPU, using the Stanford Question Answering Dataset (SQuAD) v1.1. Leveraging exploratory data analysis, data augmentation, and fine-tuning of a DistilBERT architecture, the model achieves a validation F1 score of 0.6536 with an average inference time of 0.1208 seconds per question. Compared to a rule-based baseline (F1: 0.3124) and full BERT-based models, our approach offers a favorable trade-off between accuracy and computational efficiency. This makes it well-suited for real-time applications on resource-constrained systems. The study includes systematic evaluation of data augmentation strategies and hyperparameter configurations, providing practical insights into optimizing transformer models for CPU-based inference.
