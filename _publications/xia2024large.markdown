---
layout: publication
title: 'Large Language Models Can Understanding Depth From Monocular Images'
authors: Zhongyi Xia, Tianzhao Wu
conference: "Arxiv"
year: 2024
bibkey: xia2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.01133"}
tags: ['Tools', 'Applications', 'Model Architecture', 'Reinforcement Learning', 'Multimodal Models', 'Prompting']
---
Monocular depth estimation is a critical function in computer vision
applications. This paper shows that large language models (LLMs) can
effectively interpret depth with minimal supervision, using efficient resource
utilization and a consistent neural network architecture. We introduce LLM-MDE,
a multimodal framework that deciphers depth through language comprehension.
Specifically, LLM-MDE employs two main strategies to enhance the pretrained
LLM's capability for depth estimation: cross-modal reprogramming and an
adaptive prompt estimation module. These strategies align vision
representations with text prototypes and automatically generate prompts based
on monocular images, respectively. Comprehensive experiments on real-world MDE
datasets confirm the effectiveness and superiority of LLM-MDE, which excels in
few-/zero-shot tasks while minimizing resource use. The source code is
available.
