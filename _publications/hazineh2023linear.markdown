---
layout: publication
title: Linear Latent World Models In Simple Transformers A Case Study On Othello45;gpt
authors: Hazineh Dean S., Zhang Zechen, Chiu Jeffery
conference: "Arxiv"
year: 2023
bibkey: hazineh2023linear
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.07582"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
Foundation models exhibit significant capabilities in decision45;making and logical deductions. Nonetheless a continuing discourse persists regarding their genuine understanding of the world as opposed to mere stochastic mimicry. This paper meticulously examines a simple transformer trained for Othello extending prior research to enhance comprehension of the emergent world model of Othello45;GPT. The investigation reveals that Othello45;GPT encapsulates a linear representation of opposing pieces a factor that causally steers its decision45;making process. This paper further elucidates the interplay between the linear world representation and causal decision45;making and their dependence on layer depth and model complexity. We have made the code public.
