---
layout: publication
title: Pangu-coder2 Boosting Large Language Models For Code With Ranking Feedback
authors: Shen Bo, Zhang Jiaxin, Chen Taihong, Zan Daoguang, Geng Bing, Fu An, Zeng Muhan, Yu Ailun, Ji Jichuan, Zhao Jingyang, Guo Yuenan, Wang Qianxiang
conference: "Arxiv"
year: 2023
bibkey: shen2023pangu
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.14936"}
tags: ['Agentic', 'Applications', 'Fine Tuning', 'Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs such as supervised fine-tuning instruction tuning reinforcement learning etc. In this paper we propose a novel RRTF (Rank Responses to align Testamp;Teacher Feedback) framework which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework we present PanGu-Coder2 which achieves 62.2037; pass35;64;1 on the OpenAI HumanEval benchmark. Furthermore through an extensive evaluation on CoderEval and LeetCode benchmarks we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.
