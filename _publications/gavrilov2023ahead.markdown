---
layout: publication
title: Ahead45;of45;time P45;tuning
authors: Gavrilov Daniil, Balagansky Nikita
conference: "Arxiv"
year: 2023
bibkey: gavrilov2023ahead
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.10835"}
tags: ['Applications', 'BERT', 'Ethics And Bias', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
In this paper we propose Ahead45;of45;Time (AoT) P45;Tuning a novel parameter45;efficient fine45;tuning method for pre45;trained Language Models (LMs) that adds input45;dependent bias before each Transformer layer. We evaluate AoT P45;Tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa models showing that it outperforms BitFit and is comparable or better than other baseline methods for efficient fine45;tuning. Additionally we assess the inference overhead of AoT P45;Tuning and demonstrate that it introduces negligible overhead compared to established baseline methods. Our method enables multi45;task inference with a single backbone LM making it a practical solution for real45;world applications.
