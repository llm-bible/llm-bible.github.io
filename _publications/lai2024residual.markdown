---
layout: publication
title: Residual45;based Language Models Are Free Boosters For Biomedical Imaging
authors: Lai Zhixin, Wu Jing, Chen Suiyao, Zhou Yucheng, Hovakimyan Naira
conference: "Arxiv"
year: 2024
bibkey: lai2024residual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.17343"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Tools', 'Transformer']
---
In this study we uncover the unexpected efficacy of residual45;based large language models (LLMs) as part of encoders for biomedical imaging tasks a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block extracted from pre45;trained LLMs as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi45;modal vision45;language frameworks which typically hinge on language45;driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications including both 2D and 3D visual classification tasks serving as plug45;and45;play boosters. More interestingly as a byproduct we found that the proposed framework achieved superior performance setting new state45;of45;the45;art results on extensive standardized datasets in MedMNIST45;2D and 3D. Through this work we aim to open new avenues for employing LLMs in biomedical imaging and enriching the understanding of their potential in this specialized domain.
