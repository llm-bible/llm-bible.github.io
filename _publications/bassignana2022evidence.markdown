---
layout: publication
title: Evidence Intuition Transferability Estimation For Encoder Selection
authors: Bassignana Elisa, MÃ¼ller-eberstein Max, Zhang Mike, Plank Barbara
conference: "Arxiv"
year: 2022
bibkey: bassignana2022evidence
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2210.11255"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Survey Paper']
---
With the increase in availability of large pre45;trained language models (LMs) in Natural Language Processing (NLP) it becomes critical to assess their fit for a specific target task a priori 45; as fine45;tuning the entire space of available LMs is computationally prohibitive and unsustainable. However encoder transferability estimation has received little to no attention in NLP. In this paper we propose to generate quantitative evidence to predict which LM out of a pool of models will perform best on a target task without having to fine45;tune all candidates. We provide a comprehensive study on LM ranking for 10 NLP tasks spanning the two fundamental problem types of classification and structured prediction. We adopt the state45;of45;the45;art Logarithm of Maximum Evidence (LogME) measure from Computer Vision (CV) and find that it positively correlates with final LM performance in 9437; of the setups. In the first study of its kind we further compare transferability measures with the de facto standard of human practitioner ranking finding that evidence from quantitative metrics is more robust than pure intuition and can help identify unexpected LM candidates.
