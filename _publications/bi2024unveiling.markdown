---
layout: publication
title: 'Unveiling Visual Perception In Language Models: An Attention Head Analysis Approach'
authors: Jing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, Chenliang Xu
conference: "Arxiv"
year: 2024
bibkey: bi2024unveiling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.18108"}
tags: ['Attention Mechanism', 'Multimodal Models', 'Model Architecture']
---
Recent advancements in Multimodal Large Language Models (MLLMs) have
demonstrated remarkable progress in visual understanding. This impressive leap
raises a compelling question: how can language models, initially trained solely
on linguistic data, effectively interpret and process visual content? This
paper aims to address this question with systematic investigation across 4
model families and 4 model scales, uncovering a unique class of attention heads
that focus specifically on visual content. Our analysis reveals a strong
correlation between the behavior of these attention heads, the distribution of
attention weights, and their concentration on visual tokens within the input.
These findings enhance our understanding of how LLMs adapt to multimodal tasks,
demonstrating their potential to bridge the gap between textual and visual
understanding. This work paves the way for the development of AI systems
capable of engaging with diverse modalities.
