---
layout: publication
title: Rankclip Ranking45;consistent Language45;image Pretraining
authors: Zhang Yiming, Zhao Zhuokai, Chen Zhaorun, Feng Zhili, Ding Zenghui, Sun Yining
conference: "Arxiv"
year: 2024
bibkey: zhang2024ranking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.09387"}
tags: ['Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Self45;supervised contrastive learning models such as CLIP have set new benchmarks for vision45;language models in many downstream tasks. However their dependency on rigid one45;to45;one mappings overlooks the complex and often multifaceted relationships between and within texts and images. To this end we introduce RANKCLIP a novel pretraining method that extends beyond the rigid one45;to45;one matching framework of CLIP and its variants. By extending the traditional pair45;wise loss to list45;wise and leveraging both in45;modal and cross45;modal ranking consistency RANKCLIP improves the alignment process enabling it to capture the nuanced many45;to45;many relationships between and within each modality. Through comprehensive experiments we demonstrate the effectiveness of RANKCLIP in various downstream tasks notably achieving significant gains in zero45;shot classifications over state45;of45;the45;art methods underscoring the importance of this enhanced learning process.
