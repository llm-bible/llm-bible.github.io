---
layout: publication
title: Benchmarking Large Language Models For Math Reasoning Tasks
authors: Seßler Kathrin, Rong Yao, Gözlüklü Emek, Kasneci Enkelejda
conference: "Arxiv"
year: 2024
bibkey: seßler2024benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.10839"}
tags: ['Applications', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
The use of Large Language Models (LLMs) in mathematical reasoning has become a cornerstone of related research demonstrating the intelligence of these models and enabling potential practical applications through their advanced performance such as in educational settings. Despite the variety of datasets and in45;context learning algorithms designed to improve the ability of LLMs to automate mathematical problem solving the lack of comprehensive benchmarking across different datasets makes it complicated to select an appropriate model for specific tasks. In this project we present a benchmark that fairly compares seven state45;of45;the45;art in45;context learning algorithms for mathematical problem solving across five widely used mathematical datasets on four powerful foundation models. Furthermore we explore the trade45;off between efficiency and performance highlighting the practical applications of LLMs for mathematical reasoning. Our results indicate that larger foundation models like GPT45;4o and LLaMA 345;70B can solve mathematical reasoning independently from the concrete prompting strategy while for smaller models the in45;context learning approach significantly influences the performance. Moreover the optimal prompt depends on the chosen foundation model. We open45;source our benchmark code to support the integration of additional models in future research.
