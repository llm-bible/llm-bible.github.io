---
layout: publication
title: 'Benchmarking Large Language Models For Calculus Problem-solving: A Comparative Analysis'
authors: In Hak Moon
conference: "Arxiv"
year: 2025
bibkey: moon2025benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.13187"}
tags: ['Tools', 'GPT', 'Efficiency and Optimization', 'Applications', 'Model Architecture']
---
This study presents a comprehensive evaluation of five leading large language
models (LLMs) - Chat GPT 4o, Copilot Pro, Gemini Advanced, Claude Pro, and Meta
AI - on their performance in solving calculus differentiation problems. The
investigation assessed these models across 13 fundamental problem types,
employing a systematic cross-evaluation framework where each model solved
problems generated by all models. Results revealed significant performance
disparities, with Chat GPT 4o achieving the highest success rate (94.71%),
followed by Claude Pro (85.74%), Gemini Advanced (84.42%), Copilot Pro
(76.30%), and Meta AI (56.75%). All models excelled at procedural
differentiation tasks but showed varying limitations with conceptual
understanding and algebraic manipulation. Notably, problems involving
increasing/decreasing intervals and optimization word problems proved most
challenging across all models. The cross-evaluation matrix revealed that Claude
Pro generated the most difficult problems, suggesting distinct capabilities
between problem generation and problem-solving. These findings have significant
implications for educational applications, highlighting both the potential and
limitations of LLMs as calculus learning tools. While they demonstrate
impressive procedural capabilities, their conceptual understanding remains
limited compared to human mathematical reasoning, emphasizing the continued
importance of human instruction for developing deeper mathematical
comprehension.
