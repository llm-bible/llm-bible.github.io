---
layout: publication
title: 'Challenges In Ensuring AI Safety In Deepseek-r1 Models: The Shortcomings Of Reinforcement Learning Strategies'
authors: Manojkumar Parmar, Yuvaraj Govindarajulu
conference: "Arxiv"
year: 2025
bibkey: parmar2025challenges
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.17030"}
tags: ['Fine-Tuning', 'Responsible AI', 'Agentic', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods']
---
Large Language Models (LLMs) have achieved remarkable progress in reasoning,
alignment, and task-specific performance. However, ensuring harmlessness in
these systems remains a critical challenge, particularly in advanced models
like DeepSeek-R1. This paper examines the limitations of Reinforcement Learning
(RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and
compares it with Supervised Fine-Tuning (SFT). While RL improves reasoning
capabilities, it faces challenges such as reward hacking, generalization
failures, language mixing, and high computational costs. We propose hybrid
training approaches combining RL and SFT to achieve robust harmlessness
reduction. Usage recommendations and future directions for deploying
DeepSeek-R1 responsibly are also presented.
