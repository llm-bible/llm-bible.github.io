---
layout: publication
title: Visualwebbench How Far Have Multimodal Llms Evolved In Web Page Understanding And Grounding
authors: Liu Junpeng, Song Yifan, Lin Bill Yuchen, Lam Wai, Neubig Graham, Li Yuanzhi, Yue Xiang
conference: "Arxiv"
year: 2024
bibkey: liu2024how
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.05955"}
tags: ['Agentic', 'Applications', 'GPT', 'Model Architecture', 'Multimodal Models']
---
Multimodal Large Language models (MLLMs) have shown promise in web45;related tasks but evaluating their performance in the web domain remains a challenge due to the lack of comprehensive benchmarks. Existing benchmarks are either designed for general multimodal tasks failing to capture the unique characteristics of web pages or focus on end45;to45;end web agent tasks unable to measure fine45;grained abilities such as OCR understanding and grounding. In this paper we introduce bench123;125; a multimodal benchmark designed to assess the capabilities of MLLMs across a variety of web tasks. bench123;125; consists of seven tasks and comprises 1.5K human45;curated instances from 139 real websites covering 87 sub45;domains. We evaluate 14 open45;source MLLMs Gemini Pro Claude45;3 series and GPT45;4V(ision) on bench123;125; revealing significant challenges and performance gaps. Further analysis highlights the limitations of current MLLMs including inadequate grounding in text45;rich environments and subpar performance with low45;resolution image inputs. We believe bench123;125; will serve as a valuable resource for the research community and contribute to the creation of more powerful and versatile MLLMs for web45;related applications.
