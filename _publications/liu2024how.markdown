---
layout: publication
title: How Do Large Language Models Navigate Conflicts Between Honesty And Helpfulness
authors: Liu Ryan, Sumers Theodore R., Dasgupta Ishita, Griffiths Thomas L.
conference: "Arxiv"
year: 2024
bibkey: liu2024how
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.07282"}
tags: ['Agentic', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
In day-to-day communication people often approximate the truth - for example rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs To address this question we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listeners decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can to a degree be steered by zero-shot prompting.
