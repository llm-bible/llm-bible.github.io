---
layout: publication
title: 'Jawaher: A Multidialectal Dataset Of Arabic Proverbs For LLM Benchmarking'
authors: Samar M. Magdy, Sang Yun Kwon, Fakhraddin Alwajih, Safaa Abdelfadil, Shady Shehata, Muhammad Abdul-mageed
conference: "Arxiv"
year: 2025
bibkey: magdy2025multidialectal
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.00231'}
tags: ['Agentic', 'Interpretability and Explainability', 'Efficiency and Optimization', 'Training Techniques', 'Fine-Tuning', 'Reinforcement Learning', 'Ethics and Bias', 'Pretraining Methods']
---
Recent advancements in instruction fine-tuning, alignment methods such as
reinforcement learning from human feedback (RLHF), and optimization techniques
like direct preference optimization (DPO) have significantly enhanced the
adaptability of large language models (LLMs) to user preferences. However,
despite these innovations, many LLMs continue to exhibit biases toward Western,
Anglo-centric, or American cultures, with performance on English data
consistently surpassing that of other languages. This reveals a persistent
cultural gap in LLMs, which complicates their ability to accurately process
culturally rich and diverse figurative language such as proverbs. To address
this, we introduce Jawaher, a benchmark designed to assess LLMs' capacity to
comprehend and interpret Arabic proverbs. Jawaher includes proverbs from
various Arabic dialects, along with idiomatic translations and explanations.
Through extensive evaluations of both open- and closed-source models, we find
that while LLMs can generate idiomatically accurate translations, they struggle
with producing culturally nuanced and contextually relevant explanations. These
findings highlight the need for ongoing model refinement and dataset expansion
to bridge the cultural gap in figurative language processing.
