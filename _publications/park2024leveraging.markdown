---
layout: publication
title: Leveraging Large Language Models (llms) To Support Collaborative Human-ai Online Risk Data Annotation
authors: Park Jinkyung, Wisniewski Pamela, Singh Vivek
conference: "Arxiv"
year: 2024
bibkey: park2024leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.07926"}
tags: ['Applications', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Tools']
---
In this position paper we discuss the potential for leveraging LLMs as interactive research tools to facilitate collaboration between human coders and AI to effectively annotate online risk data at scale. Collaborative human-AI labeling is a promising approach to annotating large-scale and complex data for various tasks. Yet tools and methods to support effective human-AI collaboration for data annotation are under-studied. This gap is pertinent because co-labeling tasks need to support a two-way interactive discussion that can add nuance and context particularly in the context of online risk which is highly subjective and contextualized. Therefore we provide some of the early benefits and challenges of using LLMs-based tools for risk annotation and suggest future directions for the HCI research community to leverage LLMs as research tools to facilitate human-AI collaboration in contextualized online data annotation. Our research interests align very well with the purposes of the LLMs as Research Tools workshop to identify ongoing applications and challenges of using LLMs to work with data in HCI research. We anticipate learning valuable insights from organizers and participants into how LLMs can help reshape the HCI communitys methods for working with data.
