---
layout: publication
title: TMT A Transformer45;based Modal Translator For Improving Multimodal Sequence Representations In Audio Visual Scene45;aware Dialog
authors: Li Wubo, Jiang Dongwei, Zou Wei, Li Xiangang
conference: "Arxiv"
year: 2020
bibkey: li2020transformer
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2010.10839"}
tags: ['Applications', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Transformer']
---
Audio Visual Scene45;aware Dialog (AVSD) is a task to generate responses when discussing about a given video. The previous state45;of45;the45;art model shows superior performance for this task using Transformer45;based architecture. However there remain some limitations in learning better representation of modalities. Inspired by Neural Machine Translation (NMT) we propose the Transformer45;based Modal Translator (TMT) to learn the representations of the source modal sequence by translating the source modal sequence to the related target modal sequence in a supervised manner. Based on Multimodal Transformer Networks (MTN) we apply TMT to video and dialog proposing MTN45;TMT for the video45;grounded dialog system. On the AVSD track of the Dialog System Technology Challenge 7 MTN45;TMT outperforms the MTN and other submission models in both Video and Text task and Text Only task. Compared with MTN MTN45;TMT improves all metrics especially achieving relative improvement up to 14.137; on CIDEr. Index Terms multimodal learning audio45;visual scene45;aware dialog neural machine translation multi45;task learning
