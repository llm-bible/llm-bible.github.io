---
layout: publication
title: 'Data Augmentation Using Large Language Models: Data Perspectives, Learning Paradigms And Challenges'
authors: Ding Bosheng, Qin Chengwei, Zhao Ruochen, Luo Tianze, Li Xinze, Chen Guizhen, Xia Wenhan, Hu Junjie, Luu Anh Tuan, Joty Shafiq
conference: "Arxiv"
year: 2024
bibkey: ding2024data
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.02990"}
tags: ['Fine Tuning', 'Multimodal Models', 'Reinforcement Learning', 'Survey Paper', 'Tools', 'Training Techniques']
---
In the rapidly evolving field of large language models (LLMs) data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of LLMs on DA particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From both data and learning perspectives we examine various strategies that utilize LLMs for data augmentation including a novel exploration of learning paradigms where LLM-generated data is used for diverse forms of further training. Additionally this paper highlights the primary open challenges faced in this domain ranging from controllable data augmentation to multi-modal data augmentation. This survey highlights a paradigm shift introduced by LLMs in DA and aims to serve as a comprehensive guide for researchers and practitioners.
