---
layout: publication
title: 'Adaptive In-conversation Team Building For Language Model Agents'
authors: Linxin Song, Jiale Liu, Jieyu Zhang, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, Chi Wang
conference: "Arxiv"
year: 2024
bibkey: song2024adaptive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.19425"}
tags: ['Fine-Tuning', 'Agentic', 'RAG', 'Reinforcement Learning', 'Agent', 'Prompting']
---
Leveraging multiple large language model (LLM) agents has shown to be a
promising approach for tackling complex tasks, while the effective design of
multiple agents for a particular application remains an art. It is thus
intriguing to answer a critical question: Given a task, how can we build a team
of LLM agents to solve it effectively? Our new adaptive team-building paradigm
offers a flexible solution, realized through a novel agent design named Captain
Agent. It dynamically forms and manages teams for each step of a task-solving
process, utilizing nested group conversations and reflection to ensure diverse
expertise and prevent stereotypical outputs, allowing for a flexible yet
structured approach to problem-solving. A comprehensive evaluation across six
real-world scenarios demonstrates that Captain Agent significantly outperforms
existing multi-agent methods with 21.94% improvement in average accuracy,
providing outstanding performance without requiring task-specific prompt
engineering. Our exploration of different backbone LLM and cost analysis
further shows that Captain Agent can improve the conversation quality of weak
LLM and achieve competitive performance with extremely low cost, which
illuminates the application of multi-agent systems.
