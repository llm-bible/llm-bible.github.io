---
layout: publication
title: 'Multilingual European Language Models: Benchmarking Approaches And Challenges'
authors: Fabio Barth, Georg Rehm
conference: "Arxiv"
year: 2025
bibkey: barth2025multilingual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.12895"}
tags: ['Ethics and Bias', 'Applications']
---
The breakthrough of generative large language models (LLMs) that can solve
different tasks through chat interaction has led to a significant increase in
the use of general benchmarks to assess the quality or performance of these
models beyond individual applications. There is also a need for better methods
to evaluate and also to compare models due to the ever increasing number of new
models published. However, most of the established benchmarks revolve around
the English language. This paper analyses the benefits and limitations of
current evaluation datasets, focusing on multilingual European benchmarks. We
analyse seven multilingual benchmarks and identify four major challenges.
Furthermore, we discuss potential solutions to enhance translation quality and
mitigate cultural biases, including human-in-the-loop verification and
iterative translation ranking. Our analysis highlights the need for culturally
aware and rigorously validated benchmarks to assess the reasoning and
question-answering capabilities of multilingual LLMs accurately.
