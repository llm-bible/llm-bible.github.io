---
layout: publication
title: Considers45;the45;human Evaluation Framework Rethinking Human Evaluation For Generative Large Language Models
authors: Elangovan Aparna, Liu Ling, Xu Lei, Bodapati Sravan, Roth Dan
conference: "Arxiv"
year: 2024
bibkey: elangovan2024considers
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.18638"}
tags: ['Ethics And Bias', 'Tools']
---
In this position paper we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable. The conclusions from these evaluations thus must consider factors such as usability aesthetics and cognitive biases. We highlight how cognitive biases can conflate fluent information and truthfulness and how cognitive uncertainty affects the reliability of rating scores such as Likert. Furthermore the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models 45;45; which requires effective test sets. The scalability of human evaluation is also crucial to wider adoption. Hence to design an effective human evaluation system in the age of generative NLP we propose the ConSiDERS45;The45;Human evaluation framework consisting of 6 pillars 45;45; Consistency Scoring Criteria Differentiating User Experience Responsible and Scalability.
