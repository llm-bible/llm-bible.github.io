---
layout: publication
title: A Transformer With Stack Attention
authors: Li Jiaoda, White Jennifer C., Sachan Mrinmaya, Cotterell Ryan
conference: "Arxiv"
year: 2024
bibkey: li2024transformer
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.04515"}
tags: ['Attention Mechanism', 'Interpretability And Explainability', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Natural languages are believed to be (mildly) context45;sensitive. Despite underpinning remarkably capable large language models transformers are unable to model many context45;free language tasks. In an attempt to address this limitation in the modeling power of transformer45;based language models we propose augmenting them with a differentiable stack45;based attention mechanism. Our stack45;based attention mechanism can be incorporated into any transformer45;based language model and adds a level of interpretability to the model. We show that the addition of our stack45;based attention mechanism enables the transformer to model some but not all deterministic context45;free languages.
