---
layout: publication
title: Prompting Language Models For Linguistic Structure
authors: Blevins Terra, Gonen Hila, Zettlemoyer Luke
conference: "Arxiv"
year: 2022
bibkey: blevins2022prompting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2211.07830"}
tags: ['GPT', 'Pretraining Methods', 'Prompting', 'Training Techniques']
---
Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks it remains an open question how much this ability comes from generalizable linguistic understanding versus surface45;level lexical patterns. To test this we present a structured prompting approach for linguistic structured prediction tasks allowing us to perform zero45; and few45;shot sequence tagging with autoregressive PLMs. We evaluate this approach on part45;of45;speech tagging named entity recognition and sentence chunking demonstrating strong few45;shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in45;context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.
