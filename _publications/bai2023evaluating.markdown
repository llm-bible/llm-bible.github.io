---
layout: publication
title: Kgquiz Evaluating The Generalization Of Encoded Knowledge In Large Language Models
authors: Bai Yuyang, Feng Shangbin, Balachandran Vidhisha, Tan Zhaoxuan, Lou Shiqi, He Tianxing, Tsvetkov Yulia
conference: "Arxiv"
year: 2023
bibkey: bai2023evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.09725"}
tags: ['Applications', 'Fine Tuning', 'Reinforcement Learning', 'Tools']
---
Large language models (LLMs) demonstrate remarkable performance on knowledge45;intensive tasks suggesting that real45;world knowledge is encoded in their model parameters. However besides explorations on a few probing tasks in limited knowledge domains it is not well understood how to evaluate LLMs knowledge systematically and how well their knowledge abilities generalize across a spectrum of knowledge domains and progressively complex task formats. To this end we propose KGQuiz a knowledge45;intensive benchmark to comprehensively investigate the knowledge generalization abilities of LLMs. KGQuiz is a scalable framework constructed from triplet45;based knowledge which covers three knowledge domains and consists of five tasks with increasing complexity true45;or45;false multiple45;choice QA blank filling factual editing and open45;ended knowledge generation. To gain a better understanding of LLMs knowledge abilities and their generalization we evaluate 10 open45;source and black45;box LLMs on the KGQuiz benchmark across the five knowledge45;intensive tasks and knowledge domains. Extensive experiments demonstrate that LLMs achieve impressive performance in straightforward knowledge QA tasks while settings and contexts requiring more complex reasoning or employing domain45;specific facts still present significant challenges. We envision KGQuiz as a testbed to analyze such nuanced variations in performance across domains and task formats and ultimately to understand evaluate and improve LLMs knowledge abilities across a wide spectrum of knowledge domains and tasks.
