---
layout: publication
title: MR45;BEN A Comprehensive Meta45;reasoning Benchmark For Large Language Models
authors: Zeng Zhongshen, Liu Yinhong, Wan Yingjia, Li Jingyao, Chen Pengguang, Dai Jianbo, Yao Yuxuan, Xu Rongwu, Qi Zehan, Zhao Wanru, Shen Linling, Lu Jianqiao, Tan Haochen, Chen Yukang, Zhang Hao, Shi Zhan, Wang Bailin, Guo Zhijiang, Jia Jiaya
conference: "Arxiv"
year: 2024
bibkey: zeng2024mr
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.13975"}
  - {name: "Code", url: "https://randolph&#45;zeng.github.io/Mr&#45;Ben.github.io/"}
tags: ['GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Large language models (LLMs) have shown increasing capability in problem45;solving and decision45;making largely based on the step45;by45;step chain45;of45;thought reasoning processes. However it has been increasingly challenging to evaluate the reasoning capability of LLMs. Concretely existing outcome45;based benchmarks begin to saturate and become less sufficient to monitor the progress. To this end we present a process45;based benchmark MR45;BEN that demands a meta reasoning skill where LMs are asked to locate and analyse potential errors in automatically generated reasoning steps. MR45;BEN is a comprehensive benchmark comprising 5975 questions collected from human experts covering various subjects such as physics chemistry logic coding and more. Through our designed metrics for assessing meta45;reasoning on this benchmark we identify interesting limitations and weaknesses of current LLMs (open45;source and closed45;source models). For example open45;source models are seemingly comparable to GPT45;4 on outcome45;based benchmarks but they lag far behind on our benchmark revealing the underlying reasoning capability gap between them. Our dataset and codes are available on https://randolph&#45;zeng.github.io/Mr&#45;Ben.github.io/.
