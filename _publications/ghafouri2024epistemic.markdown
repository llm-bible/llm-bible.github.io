---
layout: publication
title: 'Epistemic Integrity In Large Language Models'
authors: Bijean Ghafouri, Shahrad Mohammadzadeh, James Zhou, Pratheeksha Nair, Jacob-junqi Tian, Mayank Goel, Reihaneh Rabbany, Jean-fran√ßois Godbout, Kellin Pelrine
conference: "Arxiv"
year: 2024
bibkey: ghafouri2024epistemic
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.06528'}
tags: ['Tools']
---
Large language models are increasingly relied upon as sources of information,
but their propensity for generating false or misleading statements with high
confidence poses risks for users and society. In this paper, we confront the
critical problem of epistemic miscalibration \\(\unicode\{x2013\}\\) where a model's
linguistic assertiveness fails to reflect its true internal certainty. We
introduce a new human-labeled dataset and a novel method for measuring the
linguistic assertiveness of Large Language Models (LLMs) which cuts error rates
by over 50% relative to previous benchmarks. Validated across multiple
datasets, our method reveals a stark misalignment between how confidently
models linguistically present information and their actual accuracy. Further
human evaluations confirm the severity of this miscalibration. This evidence
underscores the urgent risk of the overstated certainty LLMs hold which may
mislead users on a massive scale. Our framework provides a crucial step forward
in diagnosing this miscalibration, offering a path towards correcting it and
more trustworthy AI across domains.
