---
layout: publication
title: 'Capturing Greater Context For Question Generation'
authors: Tuan Luu Anh, Shah Darsh J, Barzilay Regina
conference: "Arxiv"
year: 2019
bibkey: tuan2019capturing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1910.10274"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Transformer']
---
Automatic question generation can benefit many applications ranging from dialogue systems to reading comprehension. While questions are often asked with respect to long documents there are many challenges with modeling such long documents. Many existing techniques generate questions by effectively looking at one sentence at a time leading to questions that are easy and not reflective of the human process of question generation. Our goal is to incorporate interactions across multiple sentences to generate realistic questions for long documents. In order to link a broad document context to the target answer we represent the relevant context via a multi-stage attention mechanism which forms the foundation of a sequence to sequence model. We outperform state-of-the-art methods on question generation on three question-answering datasets -- SQuAD MS MARCO and NewsQA.
