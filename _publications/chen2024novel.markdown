---
layout: publication
title: M^3cot A Novel Benchmark For Multi-domain Multi-step Multi-modal Chain-of-thought
authors: Chen Qiguang, Qin Libo, Zhang Jin, Chen Zhi, Xu Xiao, Che Wanxiang
conference: "Arxiv"
year: 2024
bibkey: chen2024novel
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.16473"}
tags: ['Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'RAG']
---
Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning which gains increasing attention. Nevertheless the current MCoT benchmark still faces some challenges (1) absence of visual modal reasoning (2) single-step visual modal reasoning and (3) Domain missing thereby hindering the development of MCoT. Motivated by this we introduce a novel benchmark (M^3CoT) to address the above challenges advancing the multi-domain multi-step and multi-modal CoT. Additionally we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition we highlight that the current VLLMs still struggle to correctly reason in M^3CoT and there remains a large gap between existing VLLMs and human performance in M^3CoT despite their superior results on previous MCoT benchmarks. To our knowledge we take the first meaningful step toward the multi-domain multi-step and multi-modal scenario in MCoT. We hope that M^3CoT can serve as a valuable resource providing a pioneering foundation in multi-domain multi-step multi-modal chain-of-thought research.
