---
layout: publication
title: 'Beyond Numeric Awards: In-context Dueling Bandits With LLM Agents'
authors: Fanzeng Xia, Hao Liu, Yisong Yue, Tongxin Li
conference: "Arxiv"
year: 2024
bibkey: xia2024beyond
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2407.01887'}
tags: ['Attention Mechanism', 'Agentic', 'Transformer', 'Security', 'Model Architecture', 'Training Techniques', 'Tools', 'GPT', 'Prompting', 'Reinforcement Learning', 'Pretraining Methods']
---
In-context reinforcement learning (ICRL) is a frontier paradigm for solving
reinforcement learning problems in the foundation model era. While ICRL
capabilities have been demonstrated in transformers through task-specific
training, the potential of Large Language Models (LLMs) out-of-the-box remains
largely unexplored. Recent findings highlight that LLMs often face challenges
when dealing with numerical contexts, and limited attention has been paid to
evaluating their performance through preference feedback generated by the
environment. This paper is the first to investigate LLMs as in-context
decision-makers under the problem of Dueling Bandits (DB), a stateless
preference-based reinforcement learning setting that extends the classic
Multi-Armed Bandit (MAB) model by querying for preference feedback. We compare
GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine
well-established DB algorithms. Our results reveal that our top-performing LLM,
GPT-4 Turbo, has the zero-shot relative decision-making ability to achieve
surprisingly low weak regret across all the DB environment instances by quickly
including the best arm in duels. However, an optimality gap exists between LLMs
and classic DB algorithms in terms of strong regret. LLMs struggle to converge
and consistently exploit even when explicitly prompted to do so, and are
sensitive to prompt variations. To bridge this gap, we propose an agentic flow
framework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates
off-the-shelf DB algorithms with LLM agents through fine-grained adaptive
interplay. We show that LEAD has theoretical guarantees inherited from classic
DB algorithms on both weak and strong regret. We validate its efficacy and
robustness even with noisy and adversarial prompts. The design of our framework
sheds light on how to enhance the trustworthiness of LLMs used for in-context
decision-making.
