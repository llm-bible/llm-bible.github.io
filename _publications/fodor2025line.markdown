---
layout: publication
title: 'Line Goes Up? Inherent Limitations Of Benchmarks For Evaluating Large Language Models'
authors: James Fodor
conference: "Arxiv"
year: 2025
bibkey: fodor2025line
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.14318"}
tags: ['Interpretability and Explainability', 'Security', 'Tools', 'Reinforcement Learning']
---
Large language models (LLMs) regularly demonstrate new and impressive
performance on a wide range of language, knowledge, and reasoning benchmarks.
Such rapid progress has led many commentators to argue that LLM general
cognitive capabilities have likewise rapidly improved, with the implication
that such models are becoming progressively more capable on various real-world
tasks. Here I summarise theoretical and empirical considerations to challenge
this narrative. I argue that inherent limitations with the benchmarking
paradigm, along with specific limitations of existing benchmarks, render
benchmark performance highly unsuitable as a metric for generalisable
competence over cognitive tasks. I also contend that alternative methods for
assessing LLM capabilities, including adversarial stimuli and interpretability
techniques, have shown that LLMs do not have robust competence in many language
and reasoning tasks, and often fail to learn representations which facilitate
generalisable inferences. I conclude that benchmark performance should not be
used as a reliable indicator of general LLM cognitive capabilities.
