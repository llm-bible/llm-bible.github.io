---
layout: publication
title: Language Model45;in45;the45;loop Data Optimal Approach To Learn45;to45;recommend Actions In Text Games
authors: Sudhakar Arjun Vaithilingam, Parthasarathi Prasanna, Rajendran Janarthanan, Chandar Sarath
conference: "Arxiv"
year: 2023
bibkey: sudhakar2023language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.07687"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'RAG']
---
Large Language Models (LLMs) have demonstrated superior performance in language understanding benchmarks. CALM a popular approach leverages linguistic priors of LLMs 45;45; GPT45;2 45;45; for action candidate recommendations to improve the performance in text games in Jericho without environment45;provided actions. However CALM adapts GPT45;2 with annotated human gameplays and keeps the LLM fixed during the learning of the text based games. In this work we explore and evaluate updating LLM used for candidate recommendation during the learning of the text based game as well to mitigate the reliance on the human annotated gameplays which are costly to acquire. We observe that by updating the LLM during learning using carefully selected in45;game transitions we can reduce the dependency on using human annotated game plays for fine45;tuning the LLMs. We conducted further analysis to study the transferability of the updated LLMs and observed that transferring in45;game trained models to other games did not result in a consistent transfer.
