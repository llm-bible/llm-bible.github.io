---
layout: publication
title: Injecting Salespersons Dialogue Strategies In Large Language Models With Chain45;of45;thought Reasoning
authors: Chang Wen-yu, Chen Yun-nung
conference: "Arxiv"
year: 2024
bibkey: chang2024injecting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.18564"}
tags: ['Agentic', 'Applications', 'Prompting', 'RAG', 'Reinforcement Learning']
---
Recent research in dialogue systems and corpora has focused on two main categories task45;oriented (TOD) and open45;domain (chit45;chat) dialogues. TOD systems help users accomplish specific tasks while open45;domain systems aim to create engaging conversations. However in real45;world scenarios user intents are often revealed during interactions. A recent study introduced SalesBot which simulates dialogues transitioning from chit45;chat to task45;oriented scenarios to train sales agents. Unfortunately the initial data lacked smooth transitions and coherent long45;turn dialogues resulting in poor naturalness in sales45;customer interactions. To address these issues this paper presents SalesBot 2.0 an improved dataset. It leverages commonsense knowledge from large language models (LLMs) through strategic prompting. Additionally we introduce a novel model called SalesAgent trained on salespersons interactions using chain45;of45;thought (CoT) reasoning. This model excels in transitioning topics understanding user intents and selecting appropriate strategies. Experiments using diverse user simulations validate the effectiveness of our method in controlling dialogue strategies in LLMs. Furthermore SalesBot 2.0 enhances coherence and reduces aggression facilitating better model learning for sales45;customer interactions.
