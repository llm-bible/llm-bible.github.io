---
layout: publication
title: 'Unsupervised Text Representation Learning Via Instruction-tuning For Zero-shot Dense Retrieval'
authors: Qiuhai Zeng, Zimeng Qiu, Dae Yon Hwang, Xin He, William M. Campbell
conference: "Arxiv"
year: 2024
bibkey: zeng2024unsupervised
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2409.16497'}
tags: ['RAG', 'Applications', 'Tools', 'Fine-Tuning', 'Prompting']
---
Dense retrieval systems are commonly used for information retrieval (IR).
They rely on learning text representations through an encoder and usually
require supervised modeling via labelled data which can be costly to obtain or
simply unavailable. In this study, we introduce a novel unsupervised text
representation learning technique via instruction-tuning the pre-trained
encoder-decoder large language models (LLM) under the dual-encoder retrieval
framework. We demonstrate the corpus representation can be augmented by the
representations of relevant synthetic queries generated by the instruct-tuned
LLM founded on the Rao-Blackwell theorem. Furthermore, we effectively align the
query and corpus text representation with self-instructed-tuning. Specifically,
we first prompt an open-box pre-trained LLM to follow defined instructions
(i.e. question generation and keyword summarization) to generate synthetic
queries. Next, we fine-tune the pre-trained LLM with defined instructions and
the generated queries that passed quality check. Finally, we generate synthetic
queries with the instruction-tuned LLM for each corpora and represent each
corpora by weighted averaging the synthetic queries and original corpora
embeddings. We evaluate our proposed method under low-resource settings on
three English and one German retrieval datasets measuring NDCG@10, MRR@100,
Recall@100. We significantly improve the average zero-shot retrieval
performance on all metrics, increasing open-box FLAN-T5 model variations by
[3.34%, 3.50%] in absolute and exceeding three competitive dense retrievers
(i.e. mDPR, T-Systems, mBART-Large), with model of size at least 38% smaller,
by 1.96%, 4.62%, 9.52% absolute on NDCG@10.
