---
layout: publication
title: 'Extracting General-use Transformers For Low-resource Languages Via Knowledge Distillation'
authors: Jan Christian Blaise Cruz, Alham Fikri Aji
conference: "Arxiv"
year: 2025
bibkey: cruz2025extracting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.12660"}
tags: ['Efficiency and Optimization', 'Model Architecture', 'Distillation', 'Pretraining Methods', 'Transformer']
---
In this paper, we propose the use of simple knowledge distillation to produce
smaller and more efficient single-language transformers from Massively
Multilingual Transformers (MMTs) to alleviate tradeoffs associated with the use
of such in low-resource settings. Using Tagalog as a case study, we show that
these smaller single-language models perform on-par with strong baselines in a
variety of benchmark tasks in a much more efficient manner. Furthermore, we
investigate additional steps during the distillation process that improves the
soft-supervision of the target language, and provide a number of analyses and
ablations to show the efficacy of the proposed method.
