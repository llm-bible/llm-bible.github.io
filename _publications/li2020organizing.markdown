---
layout: publication
title: Optimus Organizing Sentences Via Pre45;trained Modeling Of A Latent Space
authors: Li Chunyuan, Gao Xiang, Li Yuan, Peng Baolin, Li Xiujun, Zhang Yizhe, Gao Jianfeng
conference: "Arxiv"
year: 2020
bibkey: li2020organizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2004.04092"}
tags: ['Applications', 'BERT', 'GPT', 'Language Modeling', 'Model Architecture', 'Tools', 'Training Techniques']
---
When trained effectively the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper we propose the first large45;scale language VAE model Optimus. A universal latent embedding space for sentences is first pre45;trained on large text corpus and then fine45;tuned for various language generation and understanding tasks. Compared with GPT45;2 Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT Optimus can generalize better on low45;resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state45;of45;the45;art on VAE language modeling benchmarks. We hope that our first pre45;trained big VAE language model itself and results can help the NLP community renew the interests of deep generative models in the era of large45;scale pre45;training and make these principled methods more practical.
