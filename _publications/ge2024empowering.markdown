---
layout: publication
title: Worldgpt Empowering LLM As Multimodal World Model
authors: Ge Zhiqi, Huang Hongzhe, Zhou Mingze, Li Juncheng, Wang Guoming, Tang Siliang, Zhuang Yueting
conference: "Arxiv"
year: 2024
bibkey: ge2024empowering
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.18202"}
  - {name: "Code", url: "https://github.com/DCDmllm/WorldGPT&#125;"}
tags: ['Agentic', 'GPT', 'Has Code', 'Merging', 'Model Architecture', 'Multimodal Models', 'RAG', 'Reinforcement Learning']
---
World models are progressively being employed across diverse fields extending from basic environment simulation to complex scenario construction. However existing models are mainly trained on domain45;specific states and actions and confined to single45;modality state representations. In this paper We introduce WorldGPT a generalist world model built upon Multimodal Large Language Model (MLLM). WorldGPT acquires an understanding of world dynamics through analyzing millions of videos across various domains. To further enhance WorldGPTs capability in specialized scenarios and long45;term tasks we have integrated it with a novel cognitive architecture that combines memory offloading knowledge retrieval and context reflection. As for evaluation we build WorldNet a multimodal state transition prediction benchmark encompassing varied real45;life scenarios. Conducting evaluations on WorldNet directly demonstrates WorldGPTs capability to accurately model state transition patterns affirming its effectiveness in understanding and predicting the dynamics of complex scenarios. We further explore WorldGPTs emerging potential in serving as a world simulator helping multimodal agents generalize to unfamiliar domains through efficiently synthesising multimodal instruction instances which are proved to be as reliable as authentic data for fine45;tuning purposes. The project is available on url123;https://github.com/DCDmllm/WorldGPT&#125;.
