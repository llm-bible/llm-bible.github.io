---
layout: publication
title: "M$^3$GPT: An Advanced Multimodal, Multitask Framework For Motion Comprehension And Generation"
authors: Luo Mingshuang, Hou Ruibing, Chang Hong, Liu Zimo, Wang Yaowei, Shan Shiguang
conference: "Arxiv"
year: 2024
bibkey: luo2024advanced
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.16273"}
tags: ['Efficiency And Optimization', 'GPT', 'Model Architecture', 'Multimodal Models', 'Quantization', 'Tools']
---
This paper presents M^3GPT an advanced (textbfM)ultimodal (textbfM)ultitask framework for (textbfM)otion comprehension and generation. M^3GPT operates on three fundamental principles. The first focuses on creating a unified representation space for various motion-relevant modalities. We employ discrete vector quantization for multimodal control and generation signals such as text music and motion/dance enabling seamless integration into a large language model (LLM) with a single vocabulary. The second involves modeling model generation directly in the raw motion space. This strategy circumvents the information loss associated with discrete tokenizer resulting in more detailed and comprehensive model generation. Third M^3GPT learns to model the connections and synergies among various motion-relevant tasks. Text the most familiar and well-understood modality for LLMs is utilized as a bridge to establish connections between different motion tasks facilitating mutual reinforcement. To our knowledge M^3GPT is the first model capable of comprehending and generating motions based on multiple signals. Extensive experiments highlight M^3GPTs superior performance across various motion-relevant tasks and its powerful zero-shot generalization capabilities for extremely challenging tasks.
