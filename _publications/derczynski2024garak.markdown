---
layout: publication
title: Garak A Framework For Security Probing Large Language Models
authors: Derczynski Leon, Galinkin Erick, Martin Jeffrey, Majumdar Subho, Inie Nanna
conference: "Arxiv"
year: 2024
bibkey: derczynski2024garak
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.11036"}
tags: ['Applications', 'Fine Tuning', 'Pretraining Methods', 'Security', 'Tools']
---
As Large Language Models (LLMs) are deployed and integrated into thousands of applications the need for scalable evaluation of how models respond to adversarial attacks grows rapidly. However LLM security is a moving target models produce unpredictable output are constantly updated and the potential adversary is highly diverse anyone with access to the internet and a decent command of natural language. Further what constitutes a security weak in one context may not be an issue in a different context; one-fits-all guardrails remain theoretical. In this paper we argue that it is time to rethink what constitutes LLM security and pursue a holistic approach to LLM security evaluation where exploration and discovery of issues are central. To this end this paper introduces garak (Generative AI Red-teaming and Assessment Kit) a framework which can be used to discover and identify vulnerabilities in a target LLM or dialog system. garak probes an LLM in a structured fashion to discover potential vulnerabilities. The outputs of the framework describe a target models weaknesses contribute to an informed discussion of what composes vulnerabilities in unique contexts and can inform alignment and policy discussions for LLM deployment.
