---
layout: publication
title: Prepair Pointwise Reasoning Enhance Pairwise Evaluating For Robust Instruction45;following Assessments
authors: Jeong Hawon, Park Chaehun, Hong Jimin, Choo Jaegul
conference: "Arxiv"
year: 2024
bibkey: jeong2024pointwise
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.12319"}
tags: ['Ethics And Bias', 'Security']
---
Pairwise evaluation using large language models (LLMs) is widely used for evaluating natural language generation (NLG) tasks. However the reliability of LLMs is often compromised by biases such as favoring verbosity and authoritative tone. In the study we focus on the comparison of two LLM45;based evaluation approaches pointwise and pairwise. Our findings demonstrate that pointwise evaluators exhibit more robustness against undesirable preferences. Further analysis reveals that pairwise evaluators can accurately identify the shortcomings of low45;quality outputs even when their judgment is incorrect. These results indicate that LLMs are more severely influenced by their bias in a pairwise evaluation setup. To mitigate this we propose a hybrid method that integrates pointwise reasoning into pairwise evaluation. Experimental results show that our method enhances the robustness of pairwise evaluators against adversarial samples while preserving accuracy on normal samples.
