---
layout: publication
title: Meta45;task Prompting Elicits Embeddings From Large Language Models
authors: Lei Yibin, Wu Di, Zhou Tianyi, Shen Tao, Cao Yu, Tao Chongyang, Yates Andrew
conference: "Arxiv"
year: 2024
bibkey: lei2024meta
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.18458"}
tags: ['Model Architecture', 'Prompting', 'RAG']
---
We introduce a new unsupervised text embedding method Meta45;Task Prompting with Explicit One45;Word Limitation (MetaEOL) for generating high45;quality sentence embeddings from Large Language Models (LLMs) without the need for model fine45;tuning. Leveraging meta45;task prompting MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta45;tasks are versatile embeddings that yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks surpassing contrastive45;trained models. Our findings suggest a new scaling law offering a versatile and resource45;efficient approach for embedding generation across diverse scenarios.
