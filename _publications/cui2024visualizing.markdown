---
layout: publication
title: 'Visualizing Attention Zones In Machine Reading Comprehension Models'
authors: Yiming Cui, Wei-nan Zhang, Ting Liu
conference: "Arxiv"
year: 2024
bibkey: cui2024visualizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.20652"}
tags: ['Transformer', 'Interpretability and Explainability', 'Model Architecture', 'Interpretability', 'Attention Mechanism']
---
The attention mechanism plays an important role in the machine reading
comprehension (MRC) model. Here, we describe a pipeline for building an MRC
model with a pretrained language model and visualizing the effect of each
attention zone in different layers, which can indicate the explainability of
the model. With the presented protocol and accompanying code, researchers can
easily visualize the relevance of each attention zone in the MRC model. This
approach can be generalized to other pretrained language models.
