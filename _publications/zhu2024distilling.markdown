---
layout: publication
title: Distilling Mathematical Reasoning Capabilities Into Small Language Models
authors: Zhu Xunyu, Li Jian, Liu Yong, Ma Can, Wang Weiping
conference: "Arxiv"
year: 2024
bibkey: zhu2024distilling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.11864"}
tags: ['Distillation', 'Efficiency And Optimization', 'Tools']
---
This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub45;billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation45;of45;Thought Distillation (EoTD) a novel technique that encapsulates the reasoning process into equation45;based representations to construct an EoTD dataset for fine45;tuning SLMs. Additionally we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes including Chain45;of45;Thought (CoT) Program45;of45;Thought (PoT) and Equation45;of45;Thought (EoT) and using it for fine45;tuning. Our experimental performance demonstrates that EoTD significantly boosts the reasoning abilities of SLMs while ETD enables these models to achieve state45;of45;the45;art reasoning performance.
