---
layout: publication
title: 'Laida: Linguistics-aware In-context Learning With Data Augmentation For Metaphor Components Identification'
authors: Liu Hongde, He Chenyuan, Meng Feiyang, Niu Changyong, Jia Yuxiang
conference: "Arxiv"
year: 2024
bibkey: liu2024linguistics
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.05404"}
  - {name: "Code", url: "https://github.com/WXLJZ/LaiDA"}
tags: ['Attention Mechanism', 'Fine Tuning', 'GPT', 'Has Code', 'In Context Learning', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Tools', 'Training Techniques']
---
"Metaphor Components Identification (MCI) contributes to enhancing machine understanding of metaphors, thereby advancing downstream natural language processing tasks. However, the complexity, diversity, and dependency on context and background knowledge pose significant challenges for MCI. Large language models (LLMs) offer new avenues for accurate comprehension of complex natural language texts due to their strong semantic analysis and extensive commonsense knowledge. In this research, a new LLM-based framework is proposed, named Linguistics-aware In-context Learning with Data Augmentation (LaiDA). Specifically, ChatGPT and supervised fine-tuning are utilized to tailor a high-quality dataset. LaiDA incorporates a simile dataset for pre-training. A graph attention network encoder generates linguistically rich feature representations to retrieve similar examples. Subsequently, LLM is fine-tuned with prompts that integrate linguistically similar examples. LaiDA ranked 2nd in Subtask 2 of NLPCC2024 Shared Task 9, demonstrating its effectiveness. Code and data are available at https://github.com/WXLJZ/LaiDA."
