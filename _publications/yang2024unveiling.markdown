---
layout: publication
title: Unveiling The Generalization Power Of Fine45;tuned Large Language Models
authors: Yang Haoran, Zhang Yumeng, Xu Jiaqi, Lu Hongyuan, Heng Pheng Ann, Lam Wai
conference: "Arxiv"
year: 2024
bibkey: yang2024unveiling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.09162"}
tags: ['Pretraining Methods']
---
While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities fine45;tuning these models on downstream domain45;specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine45;tuning. However the comprehensive effects of fine45;tuning on the LLMs generalization ability are not fully understood. This paper delves into the differences between original unmodified LLMs and their fine45;tuned variants. Our primary investigation centers on whether fine45;tuning affects the generalization ability intrinsic to LLMs. To elaborate on this we conduct extensive experiments across five distinct language tasks on various datasets. Our main findings reveal that models fine45;tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks. Intriguingly we observe that integrating the in45;context learning strategy during fine45;tuning on generation tasks can enhance the models generalization ability. Through this systematic investigation we aim to contribute valuable insights into the evolving landscape of fine45;tuning practices for LLMs.
