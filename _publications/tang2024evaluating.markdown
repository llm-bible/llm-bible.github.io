---
layout: publication
title: Tofueval Evaluating Hallucinations Of Llms On Topic45;focused Dialogue Summarization
authors: Tang Liyan, Shalyminov Igor, Wong Amy Wing-mei, Burnsky Jon, Vincent Jake W., Yang Yu'an, Singh Siffi, Feng Song, Song Hwanjun, Su Hang, Sun Lijia, Zhang Yi, Mansour Saab, Mckeown Kathleen
conference: "Arxiv"
year: 2024
bibkey: tang2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.13249"}
tags: ['Applications', 'GPT', 'Interpretability And Explainability', 'Model Architecture', 'Reinforcement Learning']
---
Single document news summarization has seen substantial progress on faithfulness in recent years driven by research on the evaluation of factual consistency or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic45;focused dialogue summarization generated by LLMs of varying sizes. We provide binary sentence45;level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain regardless of the models size. On the other hand when LLMs including GPT45;4 serve as binary factual evaluators they perform poorly and can be outperformed by prevailing state45;of45;the45;art specialized factuality evaluation metrics. Finally we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model45;generated summaries and that non45;LLM based metrics can capture all error types better than LLM45;based evaluators.
