---
layout: publication
title: 'Multiversal Views On Language Models'
authors: Reynolds Laria, Mcdonell Kyle
conference: "Arxiv"
year: 2021
bibkey: reynolds2021multiversal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2102.06391"}
tags: ['Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Tools']
---
The virtuosity of language models like GPT-3 opens a new world of possibility for human-AI collaboration in writing. In this paper we present a framework in which generative language models are conceptualized as multiverse generators. This framework also applies to human imagination and is core to how we read and write fiction. We call for exploration into this commonality through new forms of interfaces which allow humans to couple their imagination to AI to write explore and understand non-linear fiction. We discuss the early insights we have gained from actively pursuing this approach by developing and testing a novel multiversal GPT-3-assisted writing interface.
