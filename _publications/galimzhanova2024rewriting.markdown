---
layout: publication
title: 'Rewriting Conversational Utterances With Instructed Large Language Models'
authors: Elnara Galimzhanova, Cristina Ioana Muntean, Franco Maria Nardini, Raffaele Perego, Guido Rocchietti
conference: "2023 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)"
year: 2024
bibkey: galimzhanova2024rewriting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.07797"}
tags: ['Agentic', 'Applications', 'Reinforcement Learning', 'Few-Shot', 'Prompting', 'In-Context Learning']
---
Many recent studies have shown the ability of large language models (LLMs) to
achieve state-of-the-art performance on many NLP tasks, such as question
answering, text summarization, coding, and translation. In some cases, the
results provided by LLMs are on par with those of human experts. These models'
most disruptive innovation is their ability to perform tasks via zero-shot or
few-shot prompting. This capability has been successfully exploited to train
instructed LLMs, where reinforcement learning with human feedback is used to
guide the model to follow the user's requests directly. In this paper, we
investigate the ability of instructed LLMs to improve conversational search
effectiveness by rewriting user questions in a conversational setting. We study
which prompts provide the most informative rewritten utterances that lead to
the best retrieval performance. Reproducible experiments are conducted on
publicly-available TREC CAST datasets. The results show that rewriting
conversational utterances with instructed LLMs achieves significant
improvements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and
11.5% in Recall@500 over state-of-the-art techniques.
