---
layout: publication
title: Hallucination Of Multimodal Large Language Models A Survey
authors: Bai Zechen, Wang Pichao, Xiao Tianjun, He Tong, Han Zongbo, Zhang Zheng, Shou Mike Zheng
conference: "Arxiv"
year: 2024
bibkey: bai2024hallucination
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.18930"}
  - {name: "Code", url: "https://github.com/showlab/Awesome&#45;MLLM&#45;Hallucination"}
tags: ['Applications', 'Attention Mechanism', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Prompting', 'Reinforcement Learning', 'Security', 'Survey Paper', 'TACL']
---
This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs) also known as Large Vision45;Language Models (LVLMs) which have demonstrated significant advancements and remarkable abilities in multimodal tasks. Despite these promising developments MLLMs often generate outputs that are inconsistent with the visual content a challenge known as hallucination which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real45;world applications. This problem has attracted increasing attention prompting efforts to detect and mitigate such inaccuracies. We review recent advances in identifying evaluating and mitigating these hallucinations offering a detailed overview of the underlying causes evaluation benchmarks metrics and strategies developed to address this issue. Additionally we analyze the current challenges and limitations formulating open questions that delineate potential pathways for future research. By drawing the granular classification and landscapes of hallucination causes evaluation benchmarks and mitigation methods this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field. Through our thorough and in45;depth review we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs providing valuable insights and resources for researchers and practitioners alike. Resources are available at https://github.com/showlab/Awesome&#45;MLLM&#45;Hallucination.
