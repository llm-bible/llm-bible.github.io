---
layout: publication
title: 'Chatretriever: Adapting Large Language Models For Generalized And Robust Conversational Dense Retrieval'
authors: Kelong Mao, Chenlong Deng, Haonan Chen, Fengran Mo, Zheng Liu, Tetsuya Sakai, Zhicheng Dou
conference: "Arxiv"
year: 2024
bibkey: mao2024adapting
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2404.13556'}
tags: ['Security', 'Pretraining Methods']
---
Conversational search requires accurate interpretation of user intent from
complex multi-turn contexts. This paper presents ChatRetriever, which inherits
the strong generalization capability of large language models to robustly
represent complex conversational sessions for dense retrieval. To achieve this,
we propose a simple and effective dual-learning approach that adapts LLM for
retrieval via contrastive learning while enhancing the complex session
understanding through masked instruction tuning on high-quality conversational
instruction tuning data. Extensive experiments on five conversational search
benchmarks demonstrate that ChatRetriever substantially outperforms existing
conversational dense retrievers, achieving state-of-the-art performance on par
with LLM-based rewriting approaches. Furthermore, ChatRetriever exhibits
superior robustness in handling diverse conversational contexts. Our work
highlights the potential of adapting LLMs for retrieval with complex inputs
like conversational search sessions and proposes an effective approach to
advance this research direction.
