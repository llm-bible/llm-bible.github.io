---
layout: publication
title: Chatretriever Adapting Large Language Models For Generalized And Robust Conversational Dense Retrieval
authors: Mao Kelong, Deng Chenlong, Chen Haonan, Mo Fengran, Liu Zheng, Sakai Tetsuya, Dou Zhicheng
conference: "Arxiv"
year: 2024
bibkey: mao2024adapting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.13556"}
tags: ['Pretraining Methods', 'Security']
---
Conversational search requires accurate interpretation of user intent from complex multi45;turn contexts. This paper presents ChatRetriever which inherits the strong generalization capability of large language models to robustly represent complex conversational sessions for dense retrieval. To achieve this we propose a simple and effective dual45;learning approach that adapts LLM for retrieval via contrastive learning while enhancing the complex session understanding through masked instruction tuning on high45;quality conversational instruction tuning data. Extensive experiments on five conversational search benchmarks demonstrate that ChatRetriever substantially outperforms existing conversational dense retrievers achieving state45;of45;the45;art performance on par with LLM45;based rewriting approaches. Furthermore ChatRetriever exhibits superior robustness in handling diverse conversational contexts. Our work highlights the potential of adapting LLMs for retrieval with complex inputs like conversational search sessions and proposes an effective approach to advance this research direction.
