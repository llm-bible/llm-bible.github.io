---
layout: publication
title: Toward Automatic Relevance Judgment Using Vision45;45;language Models For Image45;45;text Retrieval Evaluation
authors: Yang Jheng-hong, Lin Jimmy
conference: "Arxiv"
year: 2024
bibkey: yang2024toward
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.01363"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Model Architecture']
---
Vision45;45;Language Models (VLMs) have demonstrated success across diverse applications yet their potential to assist in relevance judgments remains uncertain. This paper assesses the relevance estimation capabilities of VLMs including CLIP LLaVA and GPT45;4V within a large45;scale textit123;ad hoc125; retrieval task tailored for multimedia content creation in a zero45;shot fashion. Preliminary experiments reveal the following (1) Both LLaVA and GPT45;4V encompassing open45;source and closed45;source visual45;instruction45;tuned Large Language Models (LLMs) achieve notable Kendalls τ sim 0.4 when compared to human relevance judgments surpassing the CLIPScore metric. (2) While CLIPScore is strongly preferred LLMs are less biased towards CLIP45;based retrieval systems. (3) GPT45;4Vs score distribution aligns more closely with human judgments than other models achieving a Cohens κ value of around 0.08 which outperforms CLIPScore at approximately 45;0.096. These findings underscore the potential of LLM45;powered VLMs in enhancing relevance judgments.
