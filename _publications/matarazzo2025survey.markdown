---
layout: publication
title: 'A Survey On Large Language Models With Some Insights On Their Capabilities And Limitations'
authors: Andrea Matarazzo, Riccardo Torlone
conference: "Arxiv"
year: 2025
bibkey: matarazzo2025survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.04040"}
tags: ['Training Techniques', 'Model Architecture', 'Survey Paper', 'Tools', 'Reinforcement Learning', 'Language Modeling', 'GPT', 'Pretraining Methods', 'Transformer', 'Pre-Training', 'Applications']
---
The rapid advancement of artificial intelligence, particularly with the
development of Large Language Models (LLMs) built on the transformer
architecture, has redefined the capabilities of natural language processing.
These models now exhibit remarkable performance across various language-related
tasks, such as text generation, question answering, translation, and
summarization, often rivaling human-like comprehension. More intriguingly, LLMs
have demonstrated emergent abilities extending beyond their core functions,
showing proficiency in tasks like commonsense reasoning, code generation, and
arithmetic. This survey paper explores the foundational components, scaling
mechanisms, and architectural strategies that drive these capabilities.
Emphasizing models like GPT and LLaMA, we analyze the impact of exponential
data and computational growth on LLM performance, while also addressing the
trade-offs associated with scaling. We also examine LLM applications across
sectors, such as healthcare, finance, education, and law, highlighting their
adaptability and potential to solve domain-specific challenges. Central to this
work are the questions of how LLMs generalize across diverse tasks, exhibit
planning, and reasoning abilities, and whether these emergent abilities can be
systematically elicited or enhanced. In particular, we provide some insights
into the CoT (Chain of Thought) and PoT (Plan of Thought) abilities within
LLMs, focusing on how pre-training data influences their emergence.
Additionally, we investigate LLM-modulo frameworks that integrate external
systems, allowing LLMs to handle complex, dynamic tasks. By analyzing these
factors, this paper aims to foster the ongoing discussion on the capabilities
and limits of LLMs, promoting their responsible development and application in
novel and increasingly complex environments.
