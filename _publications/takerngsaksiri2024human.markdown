---
layout: publication
title: 'Human-in-the-loop Software Development Agents'
authors: Wannita Takerngsaksiri, Jirat Pasuksmit, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Ruixiong Zhang, Fan Jiang, Jing Li, Evan Cook, Kun Chen, Ming Wu
conference: "Arxiv"
year: 2024
bibkey: takerngsaksiri2024human
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.12924'}
tags: ['Reinforcement Learning', 'Agentic', 'Tools']
---
Recently, Large Language Models (LLMs)-based multi-agent paradigms for
software engineering are introduced to automatically resolve software
development tasks (e.g., from a given issue to source code). However, existing
work is evaluated based on historical benchmark datasets, rarely considers
human feedback at each stage of the automated software development process, and
has not been deployed in practice. In this paper, we introduce a
Human-in-the-loop LLM-based Agents framework (HULA) for software development
that allows software engineers to refine and guide LLMs when generating coding
plans and source code for a given task. We design, implement, and deploy the
HULA framework into Atlassian JIRA for internal uses. Through a multi-stage
evaluation of the HULA framework, Atlassian software engineers perceive that
HULA can minimize the overall development time and effort, especially in
initiating a coding plan and writing code for straightforward tasks. On the
other hand, challenges around code quality remain a concern in some cases. We
draw lessons learned and discuss opportunities for future work, which will pave
the way for the advancement of LLM-based agents in software development.
