---
layout: publication
title: Nanot5 A Pytorch Framework For Pre45;training And Fine45;tuning T545;style Models With Limited Resources
authors: Nawrot Piotr
conference: "Arxiv"
year: 2023
bibkey: nawrot2023pytorch
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.02373"}
tags: ['Efficiency And Optimization', 'Tools', 'Training Techniques']
---
State45;of45;the45;art language models like T5 have revolutionized the NLP landscape but their computational demands hinder a large portion of the research community. To address this challenge we present nanoT5 a specially45;optimized PyTorch framework for efficient pre45;training and fine45;tuning of T5 models. Drawing on insights from optimizer differences and prioritizing efficiency nanoT5 allows a T545;Base model to be pre45;trained on a single GPU in just 16 hours without any loss in performance. With the introduction of this open45;source framework we hope to widen the accessibility to language modelling research and cater to the communitys demand for more user45;friendly T5 (Encoder45;Decoder) implementations. We make our contributions including configurations codebase pre45;training insights and pre45;trained models available to the public.
