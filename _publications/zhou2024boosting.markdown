---
layout: publication
title: 'Assistrag: Boosting The Potential Of Large Language Models With An Intelligent Information Assistant'
authors: Yujia Zhou, Zheng Liu, Zhicheng Dou
conference: "Arxiv"
year: 2024
bibkey: zhou2024boosting
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.06805'}
tags: ['RAG', 'Efficiency and Optimization', 'Training Techniques', 'Applications', 'Tools', 'Fine-Tuning', 'Prompting', 'Pretraining Methods']
---
The emergence of Large Language Models (LLMs) has significantly advanced
natural language processing, but these models often generate factually
incorrect information, known as "hallucination". Initial retrieval-augmented
generation (RAG) methods like the "Retrieve-Read" framework was inadequate for
complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised
Fine-Tuning (SFT) methods improved performance but required frequent retraining
and risked altering foundational LLM capabilities. To cope with these
challenges, we propose Assistant-based Retrieval-Augmented Generation
(AssistRAG), integrating an intelligent information assistant within LLMs. This
assistant manages memory and knowledge through tool usage, action execution,
memory building, and plan specification. Using a two-phase training approach,
Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG
enhances information retrieval and decision-making. Experiments show AssistRAG
significantly outperforms benchmarks, especially benefiting less advanced LLMs,
by providing superior reasoning capabilities and accurate responses.
