---
layout: publication
title: Think Twice Before Trusting Self45;detection For Large Language Models Through Comprehensive Answer Reflection
authors: Li Moxin, Wang Wenjie, Feng Fuli, Zhu Fengbin, Wang Qifan, Chua Tat-seng
conference: "Arxiv"
year: 2024
bibkey: li2024think
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.09972"}
tags: ['Ethics And Bias', 'RAG', 'Tools']
---
Self45;detection for Large Language Model (LLM) seeks to evaluate the LLM output trustability by leveraging LLMs own capabilities alleviating the output hallucination issue. However existing self45;detection approaches only retrospectively evaluate answers generated by LLM typically leading to the over45;trust in incorrectly generated answers. To tackle this limitation we propose a novel self45;detection paradigm that considers the comprehensive answer space beyond LLM45;generated answers. It thoroughly compares the trustability of multiple candidate answers to mitigate the over45;trust in LLM45;generated incorrect answers. Building upon this paradigm we introduce a two45;step framework which firstly instructs LLM to reflect and provide justifications for each candidate answer and then aggregates the justifications for comprehensive target answer evaluation. This framework can be seamlessly integrated with existing approaches for superior self45;detection. Extensive experiments on six datasets spanning three tasks demonstrate the effectiveness of the proposed framework.
