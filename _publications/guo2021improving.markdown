---
layout: publication
title: LICHEE Improving Language Model Pre45;training With Multi45;grained Tokenization
authors: Guo Weidong, Zhao Mingjun, Zhang Lusheng, Niu Di, Luo Jinwen, Liu Zhenhua, Li Zhenyang, Tang Jianbo
conference: "Arxiv"
year: 2021
bibkey: guo2021improving
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2108.00801"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Tokenization', 'Training Techniques']
---
Language model pre45;training based on large corpora has achieved tremendous success in terms of constructing enriched contextual representations and has led to significant performance gains on a diverse range of Natural Language Understanding (NLU) tasks. Despite the success most current pre45;trained language models such as BERT are trained based on single45;grained tokenization usually with fine45;grained characters or sub45;words making it hard for them to learn the precise meaning of coarse45;grained words and phrases. In this paper we propose a simple yet effective pre45;training method named LICHEE to efficiently incorporate multi45;grained information of input text. Our method can be applied to various pre45;trained language models and improve their representation capability. Extensive experiments conducted on CLUE and SuperGLUE demonstrate that our method achieves comprehensive improvements on a wide variety of NLU tasks in both Chinese and English with little extra inference cost incurred and that our best ensemble model achieves the state45;of45;the45;art performance on CLUE benchmark competition.
