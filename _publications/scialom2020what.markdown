---
layout: publication
title: What BERT Sees Cross45;modal Transfer For Visual Question Generation
authors: Scialom Thomas, Bordes Patrick, Dray Paul-alexis, Staiano Jacopo, Gallinari Patrick
conference: "Arxiv"
year: 2020
bibkey: scialom2020what
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2002.10832"}
tags: ['Applications', 'BERT', 'Language Modeling', 'Model Architecture', 'RAG', 'Training Techniques']
---
Pre45;trained language models have recently contributed to significant advances in NLP tasks. Recently multi45;modal versions of BERT have been developed using heavy pre45;training relying on vast corpora of aligned textual and image data primarily applied to classification tasks such as VQA. In this paper we are interested in evaluating the visual capabilities of BERT out45;of45;the45;box by avoiding pre45;training made on supplementary data. We choose to study Visual Question Generation a task of great interest for grounded dialog that enables to study the impact of each modality (as input can be visual and/or textual). Moreover the generation aspect of the task requires an adaptation since BERT is primarily designed as an encoder. We introduce BERT45;gen a BERT45;based architecture for text generation able to leverage on either mono45; or multi45; modal representations. The results reported under different configurations indicate an innate capacity for BERT45;gen to adapt to multi45;modal data and text generation even with few data available avoiding expensive pre45;training. The proposed model obtains substantial improvements over the state45;of45;the45;art on two established VQG datasets.
