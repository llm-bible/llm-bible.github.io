---
layout: publication
title: 'Comparing Human Expertise And Large Language Models Embeddings In Content Validity Assessment Of Personality Tests'
authors: Nicola Milano, Michela Ponticorvo, Davide Marocco
conference: "Arxiv"
year: 2025
bibkey: milano2025comparing
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.12080'}
tags: ['Training Techniques']
---
In this article we explore the application of Large Language Models (LLMs) in
assessing the content validity of psychometric instruments, focusing on the Big
Five Questionnaire (BFQ) and Big Five Inventory (BFI). Content validity, a
cornerstone of test construction, ensures that psychological measures
adequately cover their intended constructs. Using both human expert evaluations
and advanced LLMs, we compared the accuracy of semantic item-construct
alignment. Graduate psychology students employed the Content Validity Ratio
(CVR) to rate test items, forming the human baseline. In parallel,
state-of-the-art LLMs, including multilingual and fine-tuned models, analyzed
item embeddings to predict construct mappings. The results reveal distinct
strengths and limitations of human and AI approaches. Human validators excelled
in aligning the behaviorally rich BFQ items, while LLMs performed better with
the linguistically concise BFI items. Training strategies significantly
influenced LLM performance, with models tailored for lexical relationships
outperforming general-purpose LLMs. Here we highlights the complementary
potential of hybrid validation systems that integrate human expertise and AI
precision. The findings underscore the transformative role of LLMs in
psychological assessment, paving the way for scalable, objective, and robust
test development methodologies.
