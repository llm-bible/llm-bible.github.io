---
layout: publication
title: 'Can Llms Explain Themselves Counterfactually?'
authors: Zahra Dehghanighobadi, Asja Fischer, Muhammad Bilal Zafar
conference: "Arxiv"
year: 2025
bibkey: dehghanighobadi2025can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.18156"}
tags: ['Interpretability and Explainability', 'Efficiency and Optimization', 'Prompting']
---
Explanations are an important tool for gaining insights into the behavior of
ML models, calibrating user trust and ensuring regulatory compliance. Past few
years have seen a flurry of post-hoc methods for generating model explanations,
many of which involve computing model gradients or solving specially designed
optimization problems. However, owing to the remarkable reasoning abilities of
Large Language Model (LLMs), self-explanation, that is, prompting the model to
explain its outputs has recently emerged as a new paradigm. In this work, we
study a specific type of self-explanations, self-generated counterfactual
explanations (SCEs). We design tests for measuring the efficacy of LLMs in
generating SCEs. Analysis over various LLM families, model sizes, temperature
settings, and datasets reveals that LLMs sometimes struggle to generate SCEs.
Even when they do, their prediction often does not agree with their own
counterfactual reasoning.
