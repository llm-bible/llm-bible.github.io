---
layout: publication
title: A Comparison Of Pre45;trained Vision45;and45;language Models For Multimodal Representation Learning Across Medical Images And Reports
authors: Li Yikuan, Wang Hanyin, Luo Yuan
conference: "Arxiv"
year: 2020
bibkey: li2020comparison
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2009.01523"}
tags: ['Applications', 'Attention Mechanism', 'BERT', 'Model Architecture', 'Multimodal Models', 'Transformer']
---
Joint image45;text embedding extracted from medical images and associated contextual reports is the bedrock for most biomedical vision45;and45;language (V+L) tasks including medical visual question answering clinical image45;text retrieval clinical report auto45;generation. In this study we adopt four pre45;trained V+L models LXMERT VisualBERT UNIER and PixelBERT to learn multimodal representation from MIMIC45;CXR radiographs and associated reports. The extrinsic evaluation on OpenI dataset shows that in comparison to the pioneering CNN45;RNN model the joint embedding learned by pre45;trained V+L models demonstrate performance improvement in the thoracic findings classification task. We conduct an ablation study to analyze the contribution of certain model components and validate the advantage of joint embedding over text45;only embedding. We also visualize attention maps to illustrate the attention mechanism of V+L models.
