---
layout: publication
title: 'Autologi: Automated Generation Of Logic Puzzles For Evaluating Reasoning Abilities Of Large Language Models'
authors: Qin Zhu, Fei Huang, Runyu Peng, Keming Lu, Bowen Yu, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang, Junyang Lin
conference: "Arxiv"
year: 2025
bibkey: zhu2025automated
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.16906'}
tags: ['Attention Mechanism', 'Training Techniques', 'Model Architecture']
---
While logical reasoning evaluation of Large Language Models (LLMs) has
attracted significant attention, existing benchmarks predominantly rely on
multiple-choice formats that are vulnerable to random guessing, leading to
overestimated performance and substantial performance fluctuations. To obtain
more accurate assessments of models' reasoning capabilities, we propose an
automated method for synthesizing open-ended logic puzzles, and use it to
develop a bilingual benchmark, AutoLogi. Our approach features program-based
verification and controllable difficulty levels, enabling more reliable
evaluation that better distinguishes models' reasoning abilities. Extensive
evaluation of eight modern LLMs shows that AutoLogi can better reflect true
model capabilities, with performance scores spanning from 35% to 73% compared
to the narrower range of 21% to 37% on the source multiple-choice dataset.
Beyond benchmark creation, this synthesis method can generate high-quality
training data by incorporating program verifiers into the rejection sampling
process, enabling systematic enhancement of LLMs' reasoning capabilities across
diverse datasets.
