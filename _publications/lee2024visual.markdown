---
layout: publication
title: Visual Question Answering Instruction Unlocking Multimodal Large Language Model To Domain45;specific Visual Multitasks
authors: Lee Jusung, Cha Sungguk, Lee Younghyun, Yang Cheoljong
conference: "Arxiv"
year: 2024
bibkey: lee2024visual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.08360"}
tags: ['Applications', 'Model Architecture', 'Multimodal Models']
---
Having revolutionized natural language processing (NLP) applications large language models (LLMs) are expanding into the realm of multimodal inputs. Owing to their ability to interpret images multimodal LLMs (MLLMs) have been primarily used for vision45;language tasks. Currently MLLMs have not yet been extended for domain45;specific visual tasks which require a more explicit understanding of visual information. We developed a method to transform domain45;specific visual and vision45;language datasets into a unified question answering format called Visual Question Answering Instruction (VQA45;IN) thereby extending MLLM to domain45;specific tasks. The VQA45;IN was applied to train multiple MLLM architectures using smaller versions of LLMs (sLLMs). The experimental results indicated that the proposed method achieved a high score metric on domainspecific visual tasks while also maintaining its performance on vision45;language tasks in a multitask manner.
