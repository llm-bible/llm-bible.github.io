---
layout: publication
title: On The Efficacy Of Co45;attention Transformer Layers In Visual Question Answering
authors: Sikarwar Ankur, Kreiman Gabriel
conference: "Arxiv"
year: 2022
bibkey: sikarwar2022efficacy
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2201.03965"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
In recent years multi45;modal transformers have shown significant progress in Vision45;Language tasks such as Visual Question Answering (VQA) outperforming previous architectures by a considerable margin. This improvement in VQA is often attributed to the rich interactions between vision and language streams. In this work we investigate the efficacy of co45;attention transformer layers in helping the network focus on relevant regions while answering the question. We generate visual attention maps using the question45;conditioned image attention scores in these co45;attention layers. We evaluate the effect of the following critical components on visual attention of a state45;of45;the45;art VQA model (i) number of object region proposals (ii) question part of speech (POS) tags (iii) question semantics (iv) number of co45;attention layers and (v) answer accuracy. We compare the neural network attention maps against human attention maps both qualitatively and quantitatively. Our findings indicate that co45;attention transformer modules are crucial in attending to relevant regions of the image given a question. Importantly we observe that the semantic meaning of the question is not what drives visual attention but specific keywords in the question do. Our work sheds light on the function and interpretation of co45;attention transformer layers highlights gaps in current networks and can guide the development of future VQA models and networks that simultaneously process visual and language streams.
