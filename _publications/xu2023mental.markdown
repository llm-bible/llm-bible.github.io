---
layout: publication
title: Mental45;llm Leveraging Large Language Models For Mental Health Prediction Via Online Text Data
authors: Xu Xuhai, Yao Bingsheng, Dong Yuanzhe, Gabriel Saadia, Yu Hong, Hendler James, Ghassemi Marzyeh, Dey Anind K., Wang Dakuo
conference: "Arxiv"
year: 2023
bibkey: xu2023mental
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2307.14385"}
tags: ['Applications', 'Ethics And Bias', 'Fine Tuning', 'GPT', 'Model Architecture', 'Prompting', 'RAG', 'Reinforcement Learning']
---
Advances in large language models (LLMs) have empowered a variety of applications. However there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data including Alpaca Alpaca45;LoRA FLAN45;T5 GPT45;3.5 and GPT45;4. We conduct a broad range of experiments covering zero45;shot prompting few45;shot prompting and instruction fine45;tuning. The results indicate a promising yet limited performance of LLMs with zero45;shot and few45;shot prompt designs for mental health tasks. More importantly our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best45;finetuned models Mental45;Alpaca and Mental45;FLAN45;T5 outperform the best prompt design of GPT45;3.5 (25 and 15 times bigger) by 10.937; on balanced accuracy and the best of GPT45;4 (250 and 150 times bigger) by 4.837;. They further perform on par with the state45;of45;the45;art task45;specific language model. We also conduct an exploratory case study on LLMs capability on mental health reasoning tasks illustrating the promising capability of certain models such as GPT45;4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs capability for mental health tasks. Meanwhile we also emphasize the important limitations before achieving deployability in real45;world mental health settings such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.
