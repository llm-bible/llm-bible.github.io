---
layout: publication
title: Teaching Llama A New Language Through Cross-lingual Knowledge Transfer
authors: Kuulmets Hele-andra, Purason Taido, Luhtaru Agnes, Fishel Mark
conference: "Findings of the Association for Computational Linguistics NAACL"
year: 2024
bibkey: kuulmets2024teaching
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.04042"}
tags: ['Fine Tuning', 'Pretraining Methods', 'RAG', 'Training Techniques']
---
This paper explores cost-efficient methods to adapt pretrained Large Language Models (LLMs) to new lower-resource languages with a specific focus on Estonian. Leveraging the Llama 2 model we investigate the impact of combining cross-lingual instruction-tuning with additional monolingual pretraining. Our results demonstrate that even a relatively small amount of additional monolingual pretraining followed by cross-lingual instruction-tuning significantly enhances results on Estonian. Furthermore we showcase cross-lingual knowledge transfer from high-quality English instructions to Estonian resulting in improvements in commonsense reasoning and multi-turn conversation capabilities. Our best model named (textscLlammas) represents the first open-source instruction-following LLM for Estonian. Additionally we publish Alpaca-est the first general task instruction dataset for Estonia. These contributions mark the initial progress in the direction of developing open-source LLMs for Estonian.
