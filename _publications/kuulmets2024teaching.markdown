---
layout: publication
title: Teaching Llama A New Language Through Cross45;lingual Knowledge Transfer
authors: Kuulmets Hele-andra, Purason Taido, Luhtaru Agnes, Fishel Mark
conference: "Findings of the Association for Computational Linguistics NAACL"
year: 2024
bibkey: kuulmets2024teaching
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.04042"}
tags: ['Pretraining Methods', 'RAG', 'Training Techniques']
---
This paper explores cost45;efficient methods to adapt pretrained Large Language Models (LLMs) to new lower45;resource languages with a specific focus on Estonian. Leveraging the Llama 2 model we investigate the impact of combining cross45;lingual instruction45;tuning with additional monolingual pretraining. Our results demonstrate that even a relatively small amount of additional monolingual pretraining followed by cross45;lingual instruction45;tuning significantly enhances results on Estonian. Furthermore we showcase cross45;lingual knowledge transfer from high45;quality English instructions to Estonian resulting in improvements in commonsense reasoning and multi45;turn conversation capabilities. Our best model named textsc123;Llammas125; represents the first open45;source instruction45;following LLM for Estonian. Additionally we publish Alpaca45;est the first general task instruction dataset for Estonia. These contributions mark the initial progress in the direction of developing open45;source LLMs for Estonian.
