---
layout: publication
title: 'Synthetic Knowledge Ingestion: Towards Knowledge Refinement And Injection For Enhancing Large Language Models'
authors: Jiaxin Zhang, Wendi Cui, Yiran Huang, Kamalika Das, Sricharan Kumar
conference: "Arxiv"
year: 2024
bibkey: zhang2024synthetic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.09629"}
tags: ['Training Techniques', 'Reinforcement Learning', 'RAG', 'Pretraining Methods', 'Fine-Tuning', 'Pre-Training']
---
Large language models (LLMs) are proficient in capturing factual knowledge
across various domains. However, refining their capabilities on previously seen
knowledge or integrating new knowledge from external sources remains a
significant challenge. In this work, we propose a novel synthetic knowledge
ingestion method called Ski, which leverages fine-grained synthesis,
interleaved generation, and assemble augmentation strategies to construct
high-quality data representations from raw knowledge sources. We then integrate
Ski and its variations with three knowledge injection techniques: Retrieval
Augmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual
Pre-training (CPT) to inject and refine knowledge in language models. Extensive
empirical experiments are conducted on various question-answering tasks
spanning finance, biomedicine, and open-generation domains to demonstrate that
Ski significantly outperforms baseline methods by facilitating effective
knowledge injection. We believe that our work is an important step towards
enhancing the factual accuracy of LLM outputs by refining knowledge
representation and injection capabilities.
