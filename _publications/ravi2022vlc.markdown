---
layout: publication
title: VLC45;BERT Visual Question Answering With Contextualized Commonsense Knowledge
authors: Ravi Sahithya, Chinchure Aditya, Sigal Leonid, Liao Renjie, Shwartz Vered
conference: "Arxiv"
year: 2022
bibkey: ravi2022vlc
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2210.13626"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
There has been a growing interest in solving Visual Question Answering (VQA) tasks that require the model to reason beyond the content present in the image. In this work we focus on questions that require commonsense reasoning. In contrast to previous methods which inject knowledge from static knowledge bases we investigate the incorporation of contextualized knowledge using Commonsense Transformer (COMET) an existing knowledge model trained on human45;curated knowledge bases. We propose a method to generate select and encode external commonsense knowledge alongside visual and textual cues in a new pre45;trained Vision45;Language45;Commonsense transformer model VLC45;BERT. Through our evaluation on the knowledge45;intensive OK45;VQA and A45;OKVQA datasets we show that VLC45;BERT is capable of outperforming existing models that utilize static knowledge bases. Furthermore through a detailed analysis we explain which questions benefit and which dont from contextualized commonsense knowledge from COMET.
