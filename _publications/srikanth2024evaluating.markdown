---
layout: publication
title: 'Evaluating The Usability Of Llms In Threat Intelligence Enrichment'
authors: Sanchana Srikanth, Mohammad Hasanuzzaman, Farah Tasnur Meem
conference: "Arxiv"
year: 2024
bibkey: srikanth2024evaluating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2409.15072'}
tags: ['GPT', 'Security', 'Model Architecture', 'Tools']
---
Large Language Models (LLMs) have the potential to significantly enhance
threat intelligence by automating the collection, preprocessing, and analysis
of threat data. However, the usability of these tools is critical to ensure
their effective adoption by security professionals. Despite the advanced
capabilities of LLMs, concerns about their reliability, accuracy, and potential
for generating inaccurate information persist. This study conducts a
comprehensive usability evaluation of five LLMs ChatGPT, Gemini, Cohere,
Copilot, and Meta AI focusing on their user interface design, error handling,
learning curve, performance, and integration with existing tools in threat
intelligence enrichment. Utilizing a heuristic walkthrough and a user study
methodology, we identify key usability issues and offer actionable
recommendations for improvement. Our findings aim to bridge the gap between LLM
functionality and user experience, thereby promoting more efficient and
accurate threat intelligence practices by ensuring these tools are
user-friendly and reliable.
