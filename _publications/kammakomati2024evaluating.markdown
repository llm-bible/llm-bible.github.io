---
layout: publication
title: 'Concodeeval: Evaluating Large Language Models For Code Constraints In Domain-specific Languages'
authors: Mehant Kammakomati, Sameer Pimparkhede, Srikanth Tamilselvam, Prince Kumar, Pushpak Bhattacharyya
conference: "Arxiv"
year: 2024
bibkey: kammakomati2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.03387"}
tags: ['Language Modeling', 'Applications', 'Few-Shot']
---
Recent work shows Large Language Models (LLMs) struggle to understand natural
language constraints for various text generation tasks in zero- and few-shot
settings. While, in the code domain, there is wide usage of constraints in code
format to maintain the integrity of code written in Domain-Specific Languages
(DSLs) like JSON and YAML which are widely used for system-level programming
tasks in enterprises. Given that LLMs are increasingly used for system-level
code tasks, evaluating if they can comprehend these code constraints is
crucial. However, no work has been done to evaluate their controllability over
code constraints. Hence, we introduce ConCodeEval, a first-of-its-kind
benchmark having two novel tasks for code constraints across five
representations. Our findings suggest that language models struggle with code
constraints. Code languages that perform excellently for normal code tasks do
not perform well when the same languages represent fine-grained constraints.
