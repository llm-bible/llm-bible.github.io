---
layout: publication
title: 'Regional Tiny Stories: Using Small Models To Compare Language Learning And Tokenizer Performance'
authors: Nirvan Patil, Malhar Abhay Inamdar, Agnivo Gosai, Guruprasad Pathak, Anish Joshi, Aryan Sagavekar, Anish Joshirao, Raj Dandekar, Rajat Dandekar, Sreedath Panat
conference: "Arxiv"
year: 2025
bibkey: patil2025regional
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.07989"}
tags: ['Training Techniques', 'Tools', 'Tokenization', 'Reinforcement Learning']
---
Small Language Models (SLMs) offer efficient alternatives to LLMs for
specific domains. The 2023 TinyStories study developed an English dataset that
allows SLMs with 1 to 10 million parameters to produce coherent outputs. Our
research expands this framework by translating the original dataset into Indian
languages and creating synthetic data using LLMs. We focus on Hindi, Marathi,
and Bengali, evaluating SLMs for regional language processing and understanding
linguistic complexity. We show that SLMs efficiently process regional languages
with significantly fewer parameters than LLMs, providing a complementary
framework for ``inference based evaluation" of tokenization strategies and
linguistic complexity. Our analysis shows that language-specific tokenizers
outperform general-purpose ones for Indian languages. Empirical validations,
supported by information-theoretic and morphological analyses, provides
fundamental understanding behind the better performance of Hindi models over
Marathi and Bengali. Additionally, we show that synthetic datasets outperform
translated content for training SLMs. Correlation analyses reveal
cross-linguistic patterns and language-specific relationships between
creativity, grammatical precision, and narrative completeness. These findings
advance both the practical application of SLMs to underserved languages and our
theoretical understanding of neural language development.
