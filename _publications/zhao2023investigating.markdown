---
layout: publication
title: Investigating Table45;to45;text Generation Capabilities Of Llms In Real45;world Information Seeking Scenarios
authors: Zhao Yilun, Zhang Haowei, Si Shengyun, Nan Linyong, Tang Xiangru, Cohan Arman
conference: "Arxiv"
year: 2023
bibkey: zhao2023investigating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.14987"}
  - {name: "Code", url: "https://github.com/yale&#45;nlp/LLM&#45;T2T"}
tags: ['Applications', 'Efficiency And Optimization', 'GPT', 'Has Code', 'Language Modeling', 'Model Architecture', 'Reinforcement Learning']
---
Tabular data is prevalent across various industries necessitating significant time and effort for users to understand and manipulate for their information45;seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However the adoption of LLMs in real45;world applications for table information seeking remains underexplored. In this paper we investigate the table45;to45;text capabilities of different LLMs using four datasets within two real45;world information seeking scenarios. These include the LogicNLG and our newly45;constructed LoTNLG datasets for data insight generation along with the FeTaQA and our newly45;constructed F2WTQ datasets for query45;based generation. We structure our investigation around three research questions evaluating the performance of LLMs in table45;to45;text generation automated evaluation and feedback generation respectively. Experimental results indicate that the current high45;performing LLM specifically GPT45;4 can effectively serve as a table45;to45;text generator evaluator and feedback generator facilitating users information seeking purposes in real45;world scenarios. However a significant performance gap still exists between other open45;sourced LLMs (e.g. Tulu and LLaMA45;2) and GPT45;4 models. Our data and code are publicly available at https://github.com/yale&#45;nlp/LLM&#45;T2T.
