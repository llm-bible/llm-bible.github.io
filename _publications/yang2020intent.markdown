---
layout: publication
title: IART Intent45;aware Response Ranking With Transformers In Information45;seeking Conversation Systems
authors: Yang Liu, Qiu Minghui, Qu Chen, Chen Cen, Guo Jiafeng, Zhang Yongfeng, Croft W. Bruce, Chen Haiqing
conference: "Arxiv"
year: 2020
bibkey: yang2020intent
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2002.00571"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Personal assistant systems such as Apple Siri Google Assistant Amazon Alexa and Microsoft Cortana are becoming ever more widely used. Understanding user intent such as clarification questions potential answers and user feedback in information45;seeking conversations is critical for retrieving good responses. In this paper we analyze user intent patterns in information45;seeking conversations and propose an intent45;aware neural response ranking model IART which refers to Intent45;Aware Ranking with Transformers. IART is built on top of the integration of user intent modeling and language representation learning with the Transformer architecture which relies entirely on a self45;attention mechanism instead of recurrent nets. It incorporates intent45;aware utterance attention to derive an importance weighting scheme of utterances in conversation context with the aim of better conversation history understanding. We conduct extensive experiments with three information45;seeking conversation data sets including both standard benchmarks and commercial data. Our proposed model outperforms all baseline methods with respect to a variety of metrics. We also perform case studies and analysis of learned user intent and its impact on response ranking in information45;seeking conversations to provide interpretation of results.
