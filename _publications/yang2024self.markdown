---
layout: publication
title: Self45;distillation Bridges Distribution Gap In Language Model Fine45;tuning
authors: Yang Zhaorui, Pang Tianyu, Feng Haozhe, Wang Han, Chen Wei, Zhu Minfeng, Liu Qian
conference: "Arxiv"
year: 2024
bibkey: yang2024self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.13669"}
  - {name: "Code", url: "https://github.com/sail&#45;sg/sdft"}
tags: ['Distillation', 'Efficiency And Optimization', 'Has Code', 'Reinforcement Learning', 'Responsible AI']
---
The surge in Large Language Models (LLMs) has revolutionized natural language processing but fine45;tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction45;following abilities. In this paper we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem we introduce Self45;Distillation Fine45;Tuning (SDFT) a novel approach that bridges the distribution gap by guiding fine45;tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama45;245;chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine45;tuning. Moreover SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at https://github.com/sail&#45;sg/sdft.
