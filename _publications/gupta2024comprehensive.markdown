---
layout: publication
title: Walledeval A Comprehensive Safety Evaluation Toolkit For Large Language Models
authors: Gupta Prannaya, Yau Le Qi, Low Hao Han, Lee I-shiang, Lim Hugo Maximus, Teoh Yu Xin, Koh Jia Hng, Liew Dar Win, Bhardwaj Rishabh, Bhardwaj Rajat, Poria Soujanya
conference: "Arxiv"
year: 2024
bibkey: gupta2024comprehensive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.03837"}
  - {name: "Code", url: "https://github.com/walledai/walledeval"}
tags: ['Has Code', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Responsible AI', 'Tools']
---
WalledEval is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs). It accommodates a diverse range of models including both open45;weight and API45;based ones and features over 35 safety benchmarks covering areas such as multilingual safety exaggerated safety and prompt injections. The framework supports both LLM and judge benchmarking and incorporates custom mutators to test safety against various text45;style mutations such as future tense and paraphrasing. Additionally WalledEval introduces WalledGuard a new small and performant content moderation tool and two datasets SGXSTest and HIXSTest which serve as benchmarks for assessing the exaggerated safety of LLMs and judges in cultural contexts. We make WalledEval publicly available at https://github.com/walledai/walledeval.
