---
layout: publication
title: Tele45;flm Technical Report
authors: Li Xiang, Yao Yiqun, Jiang Xin, Fang Xuezhi, Wang Chao, Liu Xinzhang, Wang Zihan, Zhao Yu, Wang Xin, Huang Yuyao, Song Shuangyong, Li Yongxiang, Zhang Zheng, Zhao Bo, Sun Aixin, Wang Yequan, He Zhongjiang, Wang Zhongyuan, Li Xuelong, Huang Tiejun
conference: "Arxiv"
year: 2024
bibkey: li2024tele
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.16645"}
tags: ['Applications', 'Language Modeling', 'Pretraining Methods', 'Training Techniques']
---
Large language models (LLMs) have showcased profound capabilities in language understanding and generation facilitating a wide array of applications. However there is a notable paucity of detailed open45;sourced methodologies on efficiently scaling LLMs beyond 50 billion parameters with minimum trial45;and45;error cost and computational resources. In this report we introduce Tele45;FLM (aka FLM45;2) a 52B open45;sourced multilingual large language model that features a stable efficient pre45;training paradigm and enhanced factual judgment capabilities. Tele45;FLM demonstrates superior multilingual language modeling abilities measured by BPB on textual corpus. Besides in both English and Chinese foundation model evaluation it is comparable to strong open45;sourced models that involve larger pre45;training FLOPs such as Llama245;70B and DeepSeek45;67B. In addition to the model weights we share the core designs engineering practices and training details which we expect to benefit both the academic and industrial communities.
