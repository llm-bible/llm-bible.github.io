---
layout: publication
title: Speech45;based Slot Filling Using Large Language Models
authors: Sun Guangzhi, Feng Shutong, Jiang Dongcheng, Zhang Chao, Gašić Milica, Woodland Philip C.
conference: "Arxiv"
year: 2023
bibkey: sun2023speech
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.07418"}
tags: ['GPT', 'Model Architecture', 'Prompting', 'Security']
---
Recently advancements in large language models (LLMs) have shown an unprecedented ability across various language tasks. This paper investigates the potential application of LLMs to slot filling with noisy ASR transcriptions via both in45;context learning and task45;specific fine45;tuning. Dedicated prompt designs and fine45;tuning approaches are proposed to improve the robustness of LLMs for slot filling with noisy ASR transcriptions. Moreover a linearised knowledge injection (LKI) scheme is also proposed to integrate dynamic external knowledge into LLMs. Experiments were performed on SLURP to quantify the performance of LLMs including GPT45;3.545;turbo GPT45;4 LLaMA45;13B and Vicuna45;13B (v1.1 and v1.5) with different ASR error rates. The use of the proposed fine45;tuning together with the LKI scheme for LLaMA45;13B achieved an 8.337; absolute SLU45;F1 improvement compared to the strong Flan45;T545;base baseline system on a limited data setup.
