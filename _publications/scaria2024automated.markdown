---
layout: publication
title: Automated Educational Question Generation At Different Blooms Skill Levels Using Large Language Models Strategies And Evaluation
authors: Scaria Nicy, Chenna Suma Dharani, Subramani Deepak
conference: "Artificial Intelligence in Education. AIED"
year: 2024
bibkey: scaria2024automated
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.04394"}
tags: ['Pretraining Methods', 'Prompting']
---
Developing questions that are pedagogically sound relevant and promote learning is a challenging and time45;consuming task for educators. Modern45;day large language models (LLMs) generate high45;quality content across multiple domains potentially helping educators to develop high45;quality questions. Automated educational question generation (AEQG) is important in scaling online education catering to a diverse student population. Past attempts at AEQG have shown limited abilities to generate questions at higher cognitive levels. In this study we examine the ability of five state45;of45;the45;art LLMs of different sizes to generate diverse and high45;quality questions of different cognitive levels as defined by Blooms taxonomy. We use advanced prompting techniques with varying complexity for AEQG. We conducted expert and LLM45;based evaluations to assess the linguistic and pedagogical relevance and quality of the questions. Our findings suggest that LLms can generate relevant and high45;quality educational questions of different cognitive levels when prompted with adequate information although there is a significant variance in the performance of the five LLms considered. We also show that automated evaluation is not on par with human evaluation.
