---
layout: publication
title: Probing For Bridging Inference In Transformer Language Models
authors: Pandit Onkar, Hou Yufang
conference: "Arxiv"
year: 2021
bibkey: pandit2021probing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2104.09400"}
tags: ['Attention Mechanism', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
We probe pre45;trained transformer language models for bridging inference. We first investigate individual attention heads in BERT and observe that attention heads at higher layers prominently focus on bridging relations in45;comparison with the lower and middle layers also few specific attention heads concentrate consistently on bridging. More importantly we consider language models as a whole in our second approach where bridging anaphora resolution is formulated as a masked token prediction task (Of45;Cloze test). Our formulation produces optimistic results without any fine45;tuning which indicates that pre45;trained language models substantially capture bridging inference. Our further investigation shows that the distance between anaphor45;antecedent and the context provided to language models play an important role in the inference.
