---
layout: publication
title: 'Learning Beyond The Surface: How Far Can Continual Pre-training With Lora Enhance Llms'' Domain-specific Insight Learning?'
authors: Pouya Pezeshkpour, Estevam Hruschka
conference: "Arxiv"
year: 2025
bibkey: pezeshkpour2025learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.17840"}
tags: ['Fine-Tuning', 'Training Techniques', 'Pre-Training']
---
Large Language Models (LLMs) have demonstrated remarkable performance on
various tasks, yet their ability to extract and internalize deeper insights
from domain-specific datasets remains underexplored. In this study, we
investigate how continual pre-training can enhance LLMs' capacity for insight
learning across three distinct forms: declarative, statistical, and
probabilistic insights. Focusing on two critical domains: medicine and finance,
we employ LoRA to train LLMs on two existing datasets. To evaluate each insight
type, we create benchmarks to measure how well continual pre-training helps
models go beyond surface-level knowledge. We also assess the impact of document
modification on capturing insights. The results show that, while continual
pre-training on original documents has a marginal effect, modifying documents
to retain only essential information significantly enhances the
insight-learning capabilities of LLMs.
