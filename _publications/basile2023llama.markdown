---
layout: publication
title: Llamantino Llama 2 Models For Effective Text Generation In Italian Language
authors: Basile Pierpaolo, Musacchio Elio, Polignano Marco, Siciliani Lucia, Fiameni Giuseppe, Semeraro Giovanni
conference: "Arxiv"
year: 2023
bibkey: basile2023llama
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.09993"}
tags: ['Applications', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Transformer']
---
Large Language Models represent state45;of45;the45;art linguistic models designed to equip computers with the ability to comprehend natural language. With its exceptional capacity to capture complex contextual relationships the LLaMA (Large Language Model Meta AI) family represents a novel advancement in the field of natural language processing by releasing foundational models designed to improve the natural language understanding abilities of the transformer architecture thanks to their large amount of trainable parameters (7 13 and 70 billion parameters). In many natural language understanding tasks these models obtain the same performances as private company models such as OpenAI Chat45;GPT with the advantage to make publicly available weights and code for research and commercial uses. In this work we investigate the possibility of Language Adaptation for LLaMA models explicitly focusing on addressing the challenge of Italian Language coverage. Adopting an open science approach we explore various tuning approaches to ensure a high45;quality text generated in Italian suitable for common tasks in this underrepresented language in the original models datasets. We aim to release effective text generation models with strong linguistic properties for many tasks that seem challenging using multilingual or general45;purpose LLMs. By leveraging an open science philosophy this study contributes to Language Adaptation strategies for the Italian language by introducing the novel LLaMAntino family of Italian LLMs.
