---
layout: publication
title: Unpaired Cross45;lingual Image Caption Generation With Self45;supervised Rewards
authors: Song Yuqing, Chen Shizhe, Zhao Yida, Jin Qin
conference: "Arxiv"
year: 2019
bibkey: song2019unpaired
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1908.05407"}
tags: ['Agentic', 'Applications', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Generating image descriptions in different languages is essential to satisfy users worldwide. However it is prohibitively expensive to collect large45;scale paired image45;caption dataset for every target language which is critical for training descent image captioning models. Previous works tackle the unpaired cross45;lingual image captioning problem through a pivot language which is with the help of paired image45;caption data in the pivot language and pivot45;to45;target machine translation models. However such language45;pivoted approach suffers from inaccuracy brought by the pivot45;to45;target translation including disfluency and visual irrelevancy errors. In this paper we propose to generate cross45;lingual image captions with self45;supervised rewards in the reinforcement learning framework to alleviate these two types of errors. We employ self45;supervision from mono45;lingual corpus in the target language to provide fluency reward and propose a multi45;level visual semantic matching model to provide both sentence45;level and concept45;level visual relevancy rewards. We conduct extensive experiments for unpaired cross45;lingual image captioning in both English and Chinese respectively on two widely used image caption corpora. The proposed approach achieves significant performance improvement over state45;of45;the45;art methods.
