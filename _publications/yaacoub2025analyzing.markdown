---
layout: publication
title: 'Analyzing Feedback Mechanisms In Ai-generated Mcqs: Insights Into Readability, Lexical Properties, And Levels Of Challenge'
authors: Antoun Yaacoub, Zainab Assaghir, Lionel Prevost, Jérôme Da-rugna
conference: "Arxiv"
year: 2025
bibkey: yaacoub2025analyzing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.21013"}
tags: ['BERT', 'Attention Mechanism', 'Model Architecture', 'Reinforcement Learning']
---
Artificial Intelligence (AI)-generated feedback in educational settings has
garnered considerable attention due to its potential to enhance learning
outcomes. However, a comprehensive understanding of the linguistic
characteristics of AI-generated feedback, including readability, lexical
richness, and adaptability across varying challenge levels, remains limited.
This study delves into the linguistic and structural attributes of feedback
generated by Google's Gemini 1.5-flash text model for computer science
multiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,
considering three difficulty levels (easy, medium, hard) and three feedback
tones (supportive, neutral, challenging). Key linguistic metrics, such as
length, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,
and lexical density, were computed and examined. A fine-tuned RoBERTa-based
multi-task learning (MTL) model was trained to predict these linguistic
properties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and
0.03 for vocabulary richness. The findings reveal significant interaction
effects between feedback tone and question difficulty, demonstrating the
dynamic adaptation of AI-generated feedback within diverse educational
contexts. These insights contribute to the development of more personalized and
effective AI-driven feedback mechanisms, highlighting the potential for
improved learning outcomes while underscoring the importance of ethical
considerations in their design and deployment.
