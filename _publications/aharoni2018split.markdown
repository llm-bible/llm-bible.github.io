---
layout: publication
title: Split And Rephrase&#58; Better Evaluation And A Stronger Baseline
authors: Aharoni Roee, Goldberg Yoav
conference: "Arxiv"
year: 2018
bibkey: aharoni2018split
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1805.01035"}
tags: ['Pretraining Methods', 'Training Techniques']
---
Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in NLP. We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al. 2017) they suffer from memorization of the training set which contains more than 8937; of the unique simple sentences from the validation and test sets. To aid this we present a new train-development-test data split and neural models augmented with a copy-mechanism outperforming the best reported baseline by 8.68 BLEU and fostering further progress on the task.
