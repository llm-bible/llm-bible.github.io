---
layout: publication
title: Are Neural Open45;domain Dialog Systems Robust To Speech Recognition Errors In The Dialog History An Empirical Study
authors: Gopalakrishnan Karthik, Hedayatnia Behnam, Wang Longshaokan, Liu Yang, Hakkani-tur Dilek
conference: "Arxiv"
year: 2020
bibkey: gopalakrishnan2020are
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2008.07683"}
tags: ['Attention Mechanism', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Security', 'Training Techniques', 'Transformer']
---
Large end45;to45;end neural open45;domain chatbots are becoming increasingly popular. However research on building such chatbots has typically assumed that the user input is written in nature and it is not clear whether these chatbots would seamlessly integrate with automatic speech recognition (ASR) models to serve the speech modality. We aim to bring attention to this important question by empirically studying the effects of various types of synthetic and actual ASR hypotheses in the dialog history on TransferTransfo a state45;of45;the45;art Generative Pre45;trained Transformer (GPT) based neural open45;domain dialog system from the NeurIPS ConvAI2 challenge. We observe that TransferTransfo trained on written data is very sensitive to such hypotheses introduced to the dialog history during inference time. As a baseline mitigation strategy we introduce synthetic ASR hypotheses to the dialog history during training and observe marginal improvements demonstrating the need for further research into techniques to make end45;to45;end open45;domain chatbots fully speech45;robust. To the best of our knowledge this is the first study to evaluate the effects of synthetic and actual ASR hypotheses on a state45;of45;the45;art neural open45;domain dialog system and we hope it promotes speech45;robustness as an evaluation criterion in open45;domain dialog.
