---
layout: publication
title: 'Approximately Aligned Decoding'
authors: Daniel Melcer, Sujan Gonugondla, Pramuditha Perera, Haifeng Qian, Wen-hao Chiang, Yanjun Wang, Nihal Jain, Pranav Garg, Xiaofei Ma, Anoop Deoras
conference: "Arxiv"
year: 2024
bibkey: melcer2024approximately
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.01103'}
tags: ['Efficiency and Optimization']
---
It is common to reject undesired outputs of Large Language Models (LLMs);
however, current methods to do so require an excessive amount of computation,
or severely distort the distribution of outputs. We present a method to balance
the distortion of the output distribution with computational efficiency,
allowing for the generation of long sequences of text with difficult-to-satisfy
constraints, with less amplification of low probability outputs compared to
existing methods. We show through a series of experiments that the
task-specific performance of our method is comparable to methods that do not
distort the output distribution, while being much more computationally
efficient.
