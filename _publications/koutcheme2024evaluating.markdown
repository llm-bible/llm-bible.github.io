---
layout: publication
title: Evaluating Language Models For Generating And Judging Programming Feedback
authors: Koutcheme Charles, Dainese Nicola, Hellas Arto, Sarsa Sami, Leinonen Juho, Ashraf Syed, Denny Paul
conference: "Arxiv"
year: 2024
bibkey: koutcheme2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.04873"}
tags: ['Attention Mechanism', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Pretraining Methods']
---
The emergence of large language models (LLMs) has transformed research and practice in a wide range of domains. Within the computing education research (CER) domain LLMs have received plenty of attention especially in the context of learning programming. Much of the work on LLMs in CER has however focused on applying and evaluating proprietary models. In this article we evaluate the efficiency of open45;source LLMs in generating high45;quality feedback for programming assignments and in judging the quality of the programming feedback contrasting the results against proprietary models. Our evaluations on a dataset of students submissions to Python introductory programming exercises suggest that the state45;of45;the45;art open45;source LLMs (Metas Llama3) are almost on45;par with proprietary models (GPT45;4o) in both the generation and assessment of programming feedback. We further demonstrate the efficiency of smaller LLMs in the tasks and highlight that there are a wide range of LLMs that are accessible even for free for educators and practitioners.
