---
layout: publication
title: 'Evaluating Language Models For Generating And Judging Programming Feedback'
authors: Koutcheme Charles, Dainese Nicola, Hellas Arto, Sarsa Sami, Leinonen Juho, Ashraf Syed, Denny Paul
conference: "Arxiv"
year: 2024
bibkey: koutcheme2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.04873"}
tags: ['Attention Mechanism', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Uncategorized']
---
The emergence of large language models (LLMs) has transformed research and
practice in a wide range of domains. Within the computing education research
(CER) domain, LLMs have received plenty of attention especially in the context
of learning programming. Much of the work on LLMs in CER has however focused on
applying and evaluating proprietary models. In this article, we evaluate the
efficiency of open-source LLMs in generating high-quality feedback for
programming assignments, and in judging the quality of the programming
feedback, contrasting the results against proprietary models. Our evaluations
on a dataset of students' submissions to Python introductory programming
exercises suggest that the state-of-the-art open-source LLMs (Meta's Llama3)
are almost on-par with proprietary models (GPT-4o) in both the generation and
assessment of programming feedback. We further demonstrate the efficiency of
smaller LLMs in the tasks, and highlight that there are a wide range of LLMs
that are accessible even for free for educators and practitioners.
