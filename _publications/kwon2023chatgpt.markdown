---
layout: publication
title: Chatgpt For Arabic Grammatical Error Correction
authors: Kwon Sang Yun, Bhatia Gagan, Nagoud El Moatez Billah, Abdul-mageed Muhammad
conference: "Arxiv"
year: 2023
bibkey: kwon2023chatgpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.04492"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Training Techniques']
---
Recently large language models (LLMs) fine45;tuned to follow human instruction have exhibited significant capabilities in various English NLP tasks. However their performance in grammatical error correction (GEC) tasks particularly in non45;English languages remains significantly unexplored. In this paper we delve into abilities of instruction fine45;tuned LLMs in Arabic GEC a task made complex due to Arabics rich morphology. Our findings suggest that various prompting methods coupled with (in45;context) few45;shot learning demonstrate considerable effectiveness with GPT45;4 achieving up to 65.49 Ftextsubscript123;1125; score under expert prompting (approximately 5 points higher than our established baseline). This highlights the potential of LLMs in low45;resource settings offering a viable approach for generating useful synthetic data for model training. Despite these positive results we find that instruction fine45;tuned models regardless of their size significantly underperform compared to fully fine45;tuned models of significantly smaller sizes. This disparity highlights a substantial room for improvements for LLMs. Inspired by methods from low45;resource machine translation we also develop a method exploiting synthetic data that significantly outperforms previous models on two standard Arabic benchmarks. Our work sets new SoTA for Arabic GEC with 72.1937; and 73.26 F95;123;1125; on the 2014 and 2015 QALB datasets respectively.
