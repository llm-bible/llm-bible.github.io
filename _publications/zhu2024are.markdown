---
layout: publication
title: Are Large Language Models Good Statisticians
authors: Zhu Yizhang, Du Shiyin, Li Boyan, Luo Yuyu, Tang Nan
conference: "Arxiv"
year: 2024
bibkey: zhu2024are
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.07815"}
tags: ['GPT', 'Merging', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics physics and chemistry. Despite their successes the effectiveness of LLMs in handling complex statistical tasks remains systematically under45;explored. To bridge this gap we introduce StatQA a new benchmark designed for statistical analysis tasks. StatQA comprises 11623 examples tailored to evaluate LLMs proficiency in specialized statistical tasks and their applicability assessment capabilities particularly for hypothesis testing methods. We systematically experiment with representative LLMs using various prompting strategies and show that even state45;of45;the45;art models such as GPT45;4o achieve a best performance of only 64.8337; indicating significant room for improvement. Notably while open45;source LLMs (e.g. LLaMA45;3) show limited capability those fine45;tuned ones exhibit marked improvements outperforming all in45;context learning45;based methods (e.g. GPT45;4o). Moreover our comparative human experiments highlight a striking contrast in error types between LLMs and humans LLMs primarily make applicability errors whereas humans mostly make statistical task confusion errors. This divergence highlights distinct areas of proficiency and deficiency suggesting that combining LLM and human expertise could lead to complementary strengths inviting further investigation into their collaborative potential.
