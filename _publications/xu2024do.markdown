---
layout: publication
title: Do Large Language Models Have Compositional Ability An Investigation Into Limitations And Scalability
authors: Xu Zhuoyan, Shi Zhenmei, Liang Yingyu
conference: "Arxiv"
year: 2024
bibkey: xu2024do
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.15720"}
  - {name: "Code", url: "https://github.com/OliverXUZY/LLM\_Compose\}\"}
tags: ['Has Code', 'In Context Learning', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Large language models (LLMs) have emerged as powerful tools for many AI problems and exhibit remarkable in-context learning (ICL) capabilities. Compositional ability solving unseen complex tasks that combine two or more simple tasks is an essential reasoning ability for Artificial General Intelligence. Despite the tremendous success of LLMs how they approach composite tasks especially those not encountered during the pretraining phase remains an open and largely underexplored question. In this study we delve into the ICL capabilities of LLMs on composite tasks with only simple tasks as in-context examples. We develop a test suite of composite tasks including linguistic and logical challenges and perform empirical studies across different LLM families. We observe that models exhibit divergent behaviors (1) For simpler composite tasks that apply distinct mapping mechanisms to different input segments the models demonstrate decent compositional ability while scaling up the model enhances this ability; (2) for more complex composite tasks involving reasoning multiple steps where each step represents one task models typically underperform and scaling up generally provides no improvements. We offer theoretical analysis in a simplified setting explaining that models exhibit compositional capability when the task handles different input parts separately. We believe our work sheds new light on the capabilities of LLMs in solving composite tasks regarding the nature of the tasks and model scale. Our dataset and code are available at (url)https://github.com/OliverXUZY/LLM\_Compose\}\}.
