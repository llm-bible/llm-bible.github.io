---
layout: publication
title: Any45;precision LLM Low45;cost Deployment Of Multiple Different45;sized Llms
authors: Park Yeonhong, Hyun Jake, Cho Sanglyul, Sim Bonggeun, Lee Jae W.
conference: "Arxiv"
year: 2024
bibkey: park2024any
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.10517"}
tags: ['Applications', 'Attention Mechanism', 'Efficiency And Optimization', 'Model Architecture', 'Quantization', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Recently considerable efforts have been directed towards compressing Large Language Models (LLMs) which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus this paper introduces emph123;any45;precision LLM125; extending the concept of any45;precision DNN to LLMs. Addressing challenges in any45;precision LLM we propose a lightweight method for any45;precision quantization of LLMs leveraging a post45;training quantization framework and develop a specialized software engine for its efficient serving. As a result our solution significantly reduces the high costs of deploying multiple different45;sized LLMs by overlaying LLMs quantized to varying bit45;widths such as 3 4 ... n bits into a memory footprint comparable to a single n45;bit LLM. All the supported LLMs with varying bit45;widths demonstrate state45;of45;the45;art model quality and inference throughput proving itself to be a compelling option for deployment of multiple different45;sized LLMs. Our code is open45;sourced and available online.
