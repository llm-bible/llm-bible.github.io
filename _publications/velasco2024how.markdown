---
layout: publication
title: 'How Propense Are Large Language Models At Producing Code Smells? A Benchmarking Study'
authors: Alejandro Velasco, Daniel Rodriguez-cardenas, Luftar Rahman Alif, David N. Palacio, Denys Poshyvanyk
conference: "Arxiv"
year: 2024
bibkey: velasco2024how
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.18989'}
tags: ['Reinforcement Learning', 'Applications', 'Merging']
---
Large Language Models (LLMs) have shown significant potential in automating
software engineering tasks, particularly in code generation. However, current
evaluation benchmarks, which primarily focus on accuracy, fall short in
assessing the quality of the code generated by these models, specifically their
tendency to produce code smells. To address this limitation, we introduce
CodeSmellEval, a benchmark designed to evaluate the propensity of LLMs for
generating code smells. Our benchmark includes a novel metric: Propensity
Smelly Score (PSC), and a curated dataset of method-level code smells:
CodeSmellData. To demonstrate the use of CodeSmellEval, we conducted a case
study with two state-of-the-art LLMs, CodeLlama and Mistral. The results reveal
that both models tend to generate code smells, such as simplifiable-condition
and consider-merging-isinstance. These findings highlight the effectiveness of
our benchmark in evaluating LLMs, providing valuable insights into their
reliability and their propensity to introduce code smells in code generation
tasks.
