---
layout: publication
title: REVEAL Retrieval45;augmented Visual45;language Pre45;training With Multi45;source Multimodal Knowledge Memory
authors: Hu Ziniu, Iscen Ahmet, Sun Chen, Wang Zirui, Chang Kai-wei, Sun Yizhou, Schmid Cordelia, Ross David A., Fathi Alireza
conference: "Arxiv"
year: 2022
bibkey: hu2022retrieval
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2212.05221"}
tags: ['Applications', 'Multimodal Models', 'Reinforcement Learning', 'Training Techniques']
---
In this paper we propose an end45;to45;end Retrieval45;Augmented Visual Language Model (REVEAL) that learns to encode world knowledge into a large45;scale memory and to retrieve from it to answer knowledge45;intensive queries. REVEAL consists of four key components the memory the encoder the retriever and the generator. The large45;scale memory encodes various sources of multimodal world knowledge (e.g. image45;text pairs question answering pairs knowledge graph triplets etc) via a unified encoder. The retriever finds the most relevant knowledge entries in the memory and the generator fuses the retrieved knowledge with the input query to produce the output. A key novelty in our approach is that the memory encoder retriever and generator are all pre45;trained end45;to45;end on a massive amount of data. Furthermore our approach can use a diverse set of multimodal knowledge sources which is shown to result in significant gains. We show that REVEAL achieves state45;of45;the45;art results on visual question answering and image captioning.
