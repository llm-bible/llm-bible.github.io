---
layout: publication
title: Transformers To Learn Hierarchical Contexts In Multiparty Dialogue For Span45;based Question Answering
authors: Li Changmao, Choi Jinho D.
conference: "Arxiv"
year: 2020
bibkey: li2020transformers
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2004.03561"}
tags: ['Applications', 'BERT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue. First three language modeling tasks are used to pre45;train the transformers token45; and utterance45;level language modeling and utterance order prediction that learn both token and utterance embeddings for better understanding in dialogue contexts. Then multi45;task learning between the utterance prediction and the token span prediction is applied to fine45;tune for span45;based question answering (QA). Our approach is evaluated on the FriendsQA dataset and shows improvements of 3.837; and 1.437; over the two state45;of45;the45;art transformer models BERT and RoBERTa respectively.
