---
layout: publication
title: XDBERT Distilling Visual Information To BERT From Cross45;modal Systems To Improve Language Understanding
authors: Hsu Chan-jan, Lee Hung-yi, Tsao Yu
conference: "Arxiv"
year: 2022
bibkey: hsu2022distilling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.07316"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Security', 'Tools', 'Training Techniques', 'Transformer']
---
Transformer45;based models are widely used in natural language understanding (NLU) tasks and multimodal transformers have been effective in visual45;language tasks. This study explores distilling visual information from pretrained multimodal transformers to pretrained language encoders. Our framework is inspired by cross45;modal encoders success in visual45;language tasks while we alter the learning objective to cater to the language45;heavy characteristics of NLU. After training with a small number of extra adapting steps and finetuned the proposed XDBERT (cross45;modal distilled BERT) outperforms pretrained45;BERT in general language understanding evaluation (GLUE) situations with adversarial generations (SWAG) benchmarks and readability benchmarks. We analyze the performance of XDBERT on GLUE to show that the improvement is likely visually grounded.
