---
layout: publication
title: WESE Weak Exploration To Strong Exploitation For LLM Agents
authors: Huang Xu, Liu Weiwen, Chen Xiaolong, Wang Xingmei, Lian Defu, Wang Yasheng, Tang Ruiming, Chen Enhong
conference: "Arxiv"
year: 2024
bibkey: huang2024weak
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.07456"}
tags: ['Agentic', 'Applications', 'Efficiency And Optimization', 'Fine Tuning', 'Prompting', 'Reinforcement Learning']
---
Recently large language models (LLMs) have demonstrated remarkable potential as an intelligent agent. However existing researches mainly focus on enhancing the agents reasoning or decision45;making abilities through well45;designed prompt engineering or task45;specific fine45;tuning ignoring the procedure of exploration and exploitation. When addressing complex tasks within open45;world interactive environments these methods exhibit limitations. Firstly the lack of global information of environments leads to greedy decisions resulting in sub45;optimal solutions. On the other hand irrelevant information acquired from the environment not only adversely introduces noise but also incurs additional cost. This paper proposes a novel approach Weak Exploration to Strong Exploitation (WESE) to enhance LLM agents in solving open45;world interactive tasks. Concretely WESE involves decoupling the exploration and exploitation process employing a cost45;effective weak agent to perform exploration tasks for global knowledge. A knowledge graph45;based strategy is then introduced to store the acquired knowledge and extract task45;relevant knowledge enhancing the stronger agent in success rate and efficiency for the exploitation task. Our approach is flexible enough to incorporate diverse tasks and obtains significant improvements in both success rates and efficiency across four interactive benchmarks.
