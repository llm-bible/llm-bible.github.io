---
layout: publication
title: Growover How Can Llms Adapt To Growing Real45;world Knowledge
authors: Ko Dayoon, Kim Jinyoung, Choi Hahyeon, Kim Gunhee
conference: "Arxiv"
year: 2024
bibkey: ko2024how
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.05606"}
tags: ['Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
In the real world knowledge is constantly evolving which can render existing knowledge45;based datasets outdated. This unreliability highlights the critical need for continuous updates to ensure both accuracy and relevance in knowledge45;intensive tasks. To address this we propose GrowOVER45;QA and GrowOVER45;Dialogue dynamic open45;domain QA and dialogue benchmarks that undergo a continuous cycle of updates keeping pace with the rapid evolution of knowledge. Our research indicates that retrieval45;augmented language models (RaLMs) struggle with knowledge that has not been trained on or recently updated. Consequently we introduce a novel retrieval45;interactive language model framework where the language model evaluates and reflects on its answers for further re45;retrieval. Our exhaustive experiments demonstrate that our training45;free framework significantly improves upon existing methods performing comparably to or even surpassing continuously trained language models.
