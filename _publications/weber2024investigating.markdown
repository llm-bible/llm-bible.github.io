---
layout: publication
title: Investigating Multilingual Instruction45;tuning Do Polyglot Models Demand For Multilingual Instructions
authors: Weber Alexander Arno, Thellmann Klaudia, Ebert Jan, Flores-herr Nicolas, Lehmann Jens, Fromm Michael, Ali Mehdi
conference: "Arxiv"
year: 2024
bibkey: weber2024investigating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.13703"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
The adaption of multilingual pre45;trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit we are the first to conduct an extensive study of the performance of multilingual models on parallel multi45;turn instruction45;tuning benchmarks across a selection of the most45;spoken Indo45;European languages. We systematically examine the effects of language and instruction dataset size on a mid45;sized multilingual LLM by instruction45;tuning it on parallel instruction45;tuning datasets. Our results demonstrate that instruction45;tuning on parallel instead of monolingual corpora benefits cross45;lingual instruction following capabilities by up to 4.637;. Furthermore we show that the Superficial Alignment Hypothesis does not hold in general as the investigated multilingual 7B parameter model presents a counter45;example requiring large45;scale instruction45;tuning datasets. Finally we conduct a human annotation study to understand the alignment between human45;based and GPT45;445;based evaluation within multilingual chat scenarios.
