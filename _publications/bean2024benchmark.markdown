---
layout: publication
title: LINGOLY: A Benchmark Of Olympiad-level Linguistic Reasoning Puzzles In Low-resource And Extinct Languages
authors: Bean Andrew M., Hellsten Simi, Mayne Harry, Magomere Jabez, Chi Ethan A., Chi Ryan, Hale Scott A., Kirk Hannah Rose
conference: "Arxiv"
year: 2024
bibkey: bean2024benchmark
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.06196"}
tags: ['Pretraining Methods', 'Reinforcement Learning']
---
In this paper we present the LingOly benchmark a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages minimising issues of data contamination and contains 1133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging and models perform poorly on the higher difficulty problems. On harder problems even the top model only achieved 38.737; accuracy 24.737; improvement over the no-context baseline. Large closed models typically outperform open models and in general the higher resource the language the better the scores. These results indicate in absence of memorisation true multi-step out-of-domain reasoning remains a challenge for current language models.
