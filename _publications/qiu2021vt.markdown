---
layout: publication
title: VT45;CLIP Enhancing Vision45;language Models With Visual45;guided Texts
authors: Qiu Longtian, Zhang Renrui, Guo Ziyu, Zeng Ziyao, Guo Zilu, Li Yafeng, Zhang Guangnan
conference: "Arxiv"
year: 2021
bibkey: qiu2021vt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2112.02399"}
tags: ['Attention Mechanism', 'Model Architecture', 'Training Techniques', 'Transformer']
---
Contrastive Language45;Image Pre45;training (CLIP) has drawn increasing attention recently for its transferable visual representation learning. However due to the semantic gap within datasets CLIPs pre45;trained image45;text alignment becomes sub45;optimal on downstream tasks which severely harms its transferring performance. To better adapt the cross45;modality embedding space we propose to enhance CLIP via Visual45;guided Texts named VT45;CLIP. Specifically we guide textual features of different categories to adaptively explore informative regions on the image and aggregate visual features by attention mechanisms. In this way the texts become visual45;guided namely more semantically correlated with downstream images which greatly benefits the category45;wise matching process. In few45;shot settings we evaluate our VT45;CLIP on 11 well45;known classification datasets to demonstrate its effectiveness.
