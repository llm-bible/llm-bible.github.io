---
layout: publication
title: 'Introducing Dictalm -- A Large Generative Language Model For Modern Hebrew'
authors: Shaltiel Shmidman, Avi Shmidman, Amir David Nissan Cohen, Moshe Koppel
conference: "Arxiv"
year: 2023
bibkey: shmidman2023introducing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.14568"}
tags: ['Fine-Tuning', 'Training Techniques', 'Pretraining Methods']
---
We present DictaLM, a large-scale language model tailored for Modern Hebrew.
Boasting 7B parameters, this model is predominantly trained on Hebrew-centric
data. As a commitment to promoting research and development in the Hebrew
language, we release both the foundation model and the instruct-tuned model
under a Creative Commons license. Concurrently, we introduce DictaLM-Rab,
another foundation model geared towards Rabbinic/Historical Hebrew. These
foundation models serve as ideal starting points for fine-tuning various
Hebrew-specific tasks, such as instruction, Q&A, sentiment analysis, and more.
This release represents a preliminary step, offering an initial Hebrew LLM
model for the Hebrew NLP community to experiment with.
