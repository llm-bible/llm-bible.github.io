---
layout: publication
title: 'Llasmol: Advancing Large Language Models For Chemistry With A Large-scale, Comprehensive, High-quality Instruction Tuning Dataset'
authors: Yu Botao, Baker Frazier N., Chen Ziqi, Ning Xia, Sun Huan
conference: "Arxiv"
year: 2024
bibkey: yu2024advancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.09391"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques']
---
Chemistry plays a crucial role in many domains such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper however we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks outperforming the most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish this we propose SMolInstruct a large-scale comprehensive and high-quality dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples laying a solid foundation for training and evaluating LLMs for chemistry. Using SMolInstruct we fine-tune a set of open-source LLMs among which we find that Mistral serves as the best base model for chemistry tasks. Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements.
