---
layout: publication
title: Which Kind Is Better In Open45;domain Multi45;turn Dialoghierarchical Or Non45;hierarchical Models An Empirical Study
authors: Lan Tian, Mao Xian-ling, Wei Wei, Huang Heyan
conference: "Arxiv"
year: 2020
bibkey: lan2020which
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2008.02964"}
tags: ['Attention Mechanism', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Transformer']
---
Currently open45;domain generative dialog systems have attracted considerable attention in academia and industry. Despite the success of single45;turn dialog generation multi45;turn dialog generation is still a big challenge. So far there are two kinds of models for open45;domain multi45;turn dialog generation hierarchical and non45;hierarchical models. Recently some works have shown that the hierarchical models are better than non45;hierarchical models under their experimental settings; meanwhile some works also demonstrate the opposite conclusion. Due to the lack of adequate comparisons its not clear which kind of models are better in open45;domain multi45;turn dialog generation. Thus in this paper we will measure systematically nearly all representative hierarchical and non45;hierarchical models over the same experimental settings to check which kind is better. Through extensive experiments we have the following three important conclusions (1) Nearly all hierarchical models are worse than non45;hierarchical models in open45;domain multi45;turn dialog generation except for the HRAN model. Through further analysis the excellent performance of HRAN mainly depends on its word45;level attention mechanism; (2) The performance of other hierarchical models will also obtain a great improvement if integrating the word45;level attention mechanism into these models. The modified hierarchical models even significantly outperform the non45;hierarchical models; (3) The reason why the word45;level attention mechanism is so powerful for hierarchical models is because it can leverage context information more effectively especially the fine45;grained information. Besides we have implemented all of the models and already released the codes.
