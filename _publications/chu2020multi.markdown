---
layout: publication
title: Multi-step Joint-modality Attention Network For Scene-aware Dialogue System
authors: Yun-wei Chu, Kuan-yen Lin, Chao-chun Hsu, Lun-wei Ku
conference: Arxiv
year: 2020
citations: 18
bibkey: chu2020multi
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2001.06206'}]
tags: [Transformer, Multimodal Models]
---
Understanding dynamic scenes and dialogue contexts in order to converse with
users has been challenging for multimodal dialogue systems. The 8-th Dialog
System Technology Challenge (DSTC8) proposed an Audio Visual Scene-Aware Dialog
(AVSD) task, which contains multiple modalities including audio, vision, and
language, to evaluate how dialogue systems understand different modalities and
response to users. In this paper, we proposed a multi-step joint-modality
attention network (JMAN) based on recurrent neural network (RNN) to reason on
videos. Our model performs a multi-step attention mechanism and jointly
considers both visual and textual representations in each reasoning process to
better integrate information from the two different modalities. Compared to the
baseline released by AVSD organizers, our model achieves a relative 12.1% and
22.4% improvement over the baseline on ROUGE-L score and CIDEr score.