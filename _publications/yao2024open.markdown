---
layout: publication
title: Open45;domain Implicit Format Control For Large Language Model Generation
authors: Yao Yiqun, Ma Wenjia, Fang Xuezhi, Jiang Xin, Li Xiang, Meng Xuying, Han Peng, Li Jing, Sun Aixin, Wang Yequan
conference: "Arxiv"
year: 2024
bibkey: yao2024open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.04392"}
  - {name: "Code", url: "https://github.com/cofe&#45;ai/OIFC"}
tags: ['Applications', 'Fine Tuning', 'Has Code', 'RAG', 'Tools']
---
Controlling the format of outputs generated by large language models (LLMs) is a critical functionality in various applications. Current methods typically employ constrained decoding with rule45;based automata or fine45;tuning with manually crafted format instructions both of which struggle with open45;domain format requirements. To address this limitation we introduce a novel framework for controlled generation in LLMs leveraging user45;provided one45;shot QA pairs. This study investigates LLMs capabilities to follow open45;domain one45;shot constraints and replicate the format of the example answers. We observe that this is a non45;trivial problem for current LLMs. We also develop a dataset collection methodology for supervised fine45;tuning that enhances the open45;domain format control of LLMs without degrading output quality as well as a benchmark on which we evaluate both the helpfulness and format correctness of LLM outputs. The resulting datasets named OIFC45;SFT along with the related code will be made publicly available at https://github.com/cofe&#45;ai/OIFC.
