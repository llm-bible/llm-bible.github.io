---
layout: publication
title: Attentionlego An Open45;source Building Block For Spatially45;scalable Large Language Model Accelerator With Processing45;in45;memory Technology
authors: Cong Rongqing, He Wenyang, Li Mingxuan, Luo Bangning, Yang Zebin, Yang Yuchao, Huang Ru, Yan Bonan
conference: "Arxiv"
year: 2024
bibkey: cong2024open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.11459"}
  - {name: "Code", url: "https://bonany.cc/attentionleg"}
tags: ['Agentic', 'Attention Mechanism', 'Has Code', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Transformer']
---
Large language models (LLMs) with Transformer architectures have become phenomenal in natural language processing multimodal generative artificial intelligence and agent45;oriented artificial intelligence. The self45;attention module is the most dominating sub45;structure inside Transformer45;based LLMs. Computation using general45;purpose graphics processing units (GPUs) inflicts reckless demand for I/O bandwidth for transferring intermediate calculation results between memories and processing units. To tackle this challenge this work develops a fully customized vanilla self45;attention accelerator AttentionLego as the basic building block for constructing spatially expandable LLM processors. AttentionLego provides basic implementation with fully45;customized digital logic incorporating Processing45;In45;Memory (PIM) technology. It is based on PIM45;based matrix45;vector multiplication and look45;up table45;based Softmax design. The open45;source code is available online https://bonany.cc/attentionleg.
