---
layout: publication
title: Internvl Scaling Up Vision Foundation Models And Aligning For Generic Visual45;linguistic Tasks
authors: Chen Zhe, Wu Jiannan, Wang Wenhai, Su Weijie, Chen Guo, Xing Sen, Zhong Muyan, Zhang Qinglong, Zhu Xizhou, Lu Lewei, Li Bin, Luo Ping, Lu Tong, Qiao Yu, Dai Jifeng
conference: "Arxiv"
year: 2023
bibkey: chen2023scaling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.14238"}
  - {name: "Code", url: "https://github.com/OpenGVLab/InternVL"}
tags: ['Applications', 'Has Code', 'Multimodal Models']
---
The exponential growth of large language models (LLMs) has opened up numerous possibilities for multimodal AGI systems. However the progress in vision and vision45;language foundation models which are also critical elements of multi45;modal AGI has not kept pace with LLMs. In this work we design a large45;scale vision45;language foundation model (InternVL) which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM using web45;scale image45;text data from various sources. This model can be broadly applied to and achieve state45;of45;the45;art performance on 32 generic visual45;linguistic benchmarks including visual perception tasks such as image45;level or pixel45;level recognition vision45;language tasks such as zero45;shot image/video classification zero45;shot image/video45;text retrieval and link with LLMs to create multi45;modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT45;22B. We hope that our research could contribute to the development of multi45;modal large models. Code and models are available at https://github.com/OpenGVLab/InternVL.
