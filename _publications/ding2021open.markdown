---
layout: publication
title: Openprompt An Open45;source Framework For Prompt45;learning
authors: Ding Ning, Hu Shengding, Zhao Weilin, Chen Yulin, Liu Zhiyuan, Zheng Hai-tao, Sun Maosong
conference: "Arxiv"
year: 2021
bibkey: ding2021open
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2111.01998"}
  - {name: "Code", url: "https://github.com/thunlp/OpenPrompt&#125;&#125;"}
tags: ['Applications', 'Efficiency And Optimization', 'GPT', 'Has Code', 'Language Modeling', 'Pretraining Methods', 'Prompting', 'Reinforcement Learning', 'Tools']
---
Prompt45;learning has become a new paradigm in modern natural language processing which directly adapts pre45;trained language models (PLMs) to cloze45;style prediction autoregressive modeling or sequence to sequence generation resulting in promising performances on various tasks. However no standard implementation framework of prompt45;learning is proposed yet and most existing prompt45;learning codebases often unregulated only provide limited implementations for specific scenarios. Since there are many details such as templating strategy initializing strategy and verbalizing strategy etc. need to be considered in prompt45;learning practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper we present 123;OpenPrompt125; a unified easy45;to45;use toolkit to conduct prompt45;learning over PLMs. OpenPrompt is a research45;friendly framework that is equipped with efficiency modularity and extendibility and its combinability allows the freedom to combine different PLMs task formats and prompting modules in a unified paradigm. Users could expediently deploy prompt45;learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. OpenPrompt is publicly released at 123;url123; https://github.com/thunlp/OpenPrompt&#125;&#125;.
