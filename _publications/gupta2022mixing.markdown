---
layout: publication
title: MALM Mixing Augmented Language Modeling For Zero45;shot Machine Translation
authors: Gupta Kshitij
conference: "Arxiv"
year: 2022
bibkey: gupta2022mixing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2210.00320"}
tags: ['Applications', 'Language Modeling', 'Prompting', 'Training Techniques']
---
Large pre45;trained language models have brought remarkable progress in NLP. Pre45;training and Fine45;tuning have given state45;of45;art performance across tasks in text processing. Data Augmentation techniques have also helped build state45;of45;art models on low or zero resource tasks. Many works in the past have attempted at learning a single massively45;multilingual machine translation model for zero45;shot translation. Although those translation models are producing correct translations the main challenge is those models are producing the wrong languages for zero45;shot translation. This work and its results indicate that prompt conditioned large models do not suffer from off45;target language errors i.e. errors arising due to translation to wrong languages. We empirically demonstrate the effectiveness of self45;supervised pre45;training and data augmentation for zero45;shot multi45;lingual machine translation.
