---
layout: publication
title: 'Can Llms Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence From Humans And GPT-4'
authors: Phanish Puranam, Prothit Sen, Maciej Workiewicz
conference: "Arxiv"
year: 2025
bibkey: puranam2025can
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.00603'}
tags: ['GPT', 'Interpretability', 'Model Architecture']
---
This study investigates whether large language models, specifically GPT4, can
match human capabilities in analogical reasoning within strategic decision
making contexts. Using a novel experimental design involving source to target
matching, we find that GPT4 achieves high recall by retrieving all plausible
analogies but suffers from low precision, frequently applying incorrect
analogies based on superficial similarities. In contrast, human participants
exhibit high precision but low recall, selecting fewer analogies yet with
stronger causal alignment. These findings advance theory by identifying
matching, the evaluative phase of analogical reasoning, as a distinct step that
requires accurate causal mapping beyond simple retrieval. While current LLMs
are proficient in generating candidate analogies, humans maintain a comparative
advantage in recognizing deep structural similarities across domains. Error
analysis reveals that AI errors arise from surface level matching, whereas
human errors stem from misinterpretations of causal structure. Taken together,
the results suggest a productive division of labor in AI assisted
organizational decision making where LLMs may serve as broad analogy
generators, while humans act as critical evaluators, applying the most
contextually appropriate analogies to strategic problems.
