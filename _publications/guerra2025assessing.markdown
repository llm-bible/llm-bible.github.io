---
layout: publication
title: 'Assessing Llms For Front-end Software Architecture Knowledge'
authors: L. P. Franciscatto Guerra, N. Ernst
conference: "Arxiv"
year: 2025
bibkey: guerra2025assessing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.19518"}
tags: ['Model Architecture', 'Tools', 'Reinforcement Learning', 'RAG', 'GPT', 'Applications']
---
Large Language Models (LLMs) have demonstrated significant promise in
automating software development tasks, yet their capabilities with respect to
software design tasks remains largely unclear. This study investigates the
capabilities of an LLM in understanding, reproducing, and generating structures
within the complex VIPER architecture, a design pattern for iOS applications.
We leverage Bloom's taxonomy to develop a comprehensive evaluation framework to
assess the LLM's performance across different cognitive domains such as
remembering, understanding, applying, analyzing, evaluating, and creating.
Experimental results, using ChatGPT 4 Turbo 2024-04-09, reveal that the LLM
excelled in higher-order tasks like evaluating and creating, but faced
challenges with lower-order tasks requiring precise retrieval of architectural
details. These findings highlight both the potential of LLMs to reduce
development costs and the barriers to their effective application in real-world
software design scenarios. This study proposes a benchmark format for assessing
LLM capabilities in software architecture, aiming to contribute toward more
robust and accessible AI-driven development tools.
