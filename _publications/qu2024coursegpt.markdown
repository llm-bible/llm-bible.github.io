---
layout: publication
title: Coursegpt45;zh An Educational Large Language Model Based On Knowledge Distillation Incorporating Prompt Optimization
authors: Qu Zheyan, Yin Lu, Yu Zitong, Wang Wenbo, Zhang Xing
conference: "Arxiv"
year: 2024
bibkey: qu2024coursegpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.04781"}
tags: ['Distillation', 'Efficiency And Optimization', 'GPT', 'Model Architecture', 'Prompting', 'RAG', 'Reinforcement Learning', 'TACL', 'Tools']
---
Large language models (LLMs) have demonstrated astonishing capabilities in natural language processing (NLP) tasks sparking interest in their application to professional domains with higher specialized requirements. However restricted access to closed45;source LLMs via APIs and the difficulty in collecting massive high45;quality datasets pose obstacles to the development of large language models in education fields of various courses. Given these challenges we propose CourseGPT45;zh a course45;oriented education LLM that supports customization and low45;cost deployment. To address the comprehensiveness and diversity requirements of course45;specific corpora we design a high45;quality question45;answering corpus distillation framework incorporating prompt optimization which effectively mines textbook knowledge and enhances its diversity. Moreover considering the alignment of LLM responses with user needs a novel method for discrete prompt optimization based on LLM45;as45;Judge is introduced. During optimization this framework leverages the LLMs ability to reflect on and exploit error feedback and patterns allowing for prompts that meet user needs and preferences while saving response length. Lastly we obtain CourseGPT45;zh based on the open45;source LLM using parameter45;efficient fine45;tuning. Experimental results show that our discrete prompt optimization framework effectively improves the response quality of ChatGPT and CourseGPT45;zh exhibits strong professional capabilities in specialized knowledge question45;answering significantly outperforming comparable open45;source models.
