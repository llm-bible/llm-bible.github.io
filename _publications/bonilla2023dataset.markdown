---
layout: publication
title: Pixlore A Dataset45;driven Approach To Rich Image Captioning
authors: Bonilla Diego
conference: "Arxiv"
year: 2023
bibkey: bonilla2023dataset
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.05349"}
tags: ['Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques', 'Transformer']
---
In the domain of vision45;language integration generating detailed image captions poses a significant challenge due to the lack of a curated and rich dataset. This study introduces PixLore a novel method that leverages Querying Transformers through the fine45;tuning of the BLIP45;2 model using the LoRa method on a standard commercial GPU. Our approach which involves training on a carefully assembled dataset from state45;of45;the45;art Computer Vision models combined and augmented by ChatGPT addresses the question of whether intricate image understanding can be achieved with an ensemble of smaller45;scale models. Comparative evaluations against major models such as GPT45;4 and Google Bard demonstrate that PixLore45;2.7B despite having considerably fewer parameters is rated higher than the existing State45;of45;the45;Art models in over half of the assessments. This research not only presents a groundbreaking approach but also highlights the importance of well45;curated datasets in enhancing the performance of smaller models.
