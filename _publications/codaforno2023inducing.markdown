---
layout: publication
title: 'Inducing Anxiety In Large Language Models Can Induce Bias'
authors: Julian Coda-forno, Kristin Witte, Akshay K. Jagadish, Marcel Binz, Zeynep Akata, Eric Schulz
conference: "Arxiv"
year: 2023
bibkey: codaforno2023inducing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.11111"}
tags: ['Prompting', 'Ethics and Bias', 'Tools']
---
Large language models (LLMs) are transforming research on machine learning
while galvanizing public debates. Understanding not only when these models work
well and succeed but also why they fail and misbehave is of great societal
relevance. We propose to turn the lens of psychiatry, a framework used to
describe and modify maladaptive behavior, to the outputs produced by these
models. We focus on twelve established LLMs and subject them to a questionnaire
commonly used in psychiatry. Our results show that six of the latest LLMs
respond robustly to the anxiety questionnaire, producing comparable anxiety
scores to humans. Moreover, the LLMs' responses can be predictably changed by
using anxiety-inducing prompts. Anxiety-induction not only influences LLMs'
scores on an anxiety questionnaire but also influences their behavior in a
previously-established benchmark measuring biases such as racism and ageism.
Importantly, greater anxiety-inducing text leads to stronger increases in
biases, suggesting that how anxiously a prompt is communicated to large
language models has a strong influence on their behavior in applied settings.
These results demonstrate the usefulness of methods taken from psychiatry for
studying the capable algorithms to which we increasingly delegate authority and
autonomy.
