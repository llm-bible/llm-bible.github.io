---
layout: publication
title: EVALALIGN Supervised Fine45;tuning Multimodal Llms With Human45;aligned Data For Evaluating Text45;to45;image Models
authors: Tan Zhiyu, Yang Xiaomeng, Qin Luozheng, Yang Mengping, Zhang Cheng, Li Hao
conference: "Arxiv"
year: 2024
bibkey: tan2024supervised
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.16562"}
tags: ['Efficiency And Optimization', 'Fine Tuning', 'Multimodal Models', 'RAG', 'Reinforcement Learning']
---
The recent advancements in text45;to45;image generative models have been remarkable. Yet the field suffers from a lack of evaluation metrics that accurately reflect the performance of these models particularly lacking fine45;grained metrics that can guide the optimization of the models. In this paper we propose EvalAlign a metric characterized by its accuracy stability and fine granularity. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) pre45;trained on extensive datasets. We develop evaluation protocols that focus on two key dimensions image faithfulness and text45;image alignment. Each protocol comprises a set of detailed fine45;grained instructions linked to specific scoring options enabling precise manual scoring of the generated images. We Supervised Fine45;Tune (SFT) the MLLM to align closely with human evaluative judgments resulting in a robust evaluation model. Our comprehensive tests across 24 text45;to45;image generation models demonstrate that EvalAlign not only provides superior metric stability but also aligns more closely with human preferences than existing metrics confirming its effectiveness and utility in model assessment.
