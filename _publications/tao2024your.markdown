---
layout: publication
title: 'Your Weak LLM Is Secretly A Strong Teacher For Alignment'
authors: Leitian Tao, Yixuan Li
conference: "Arxiv"
year: 2024
bibkey: tao2024your
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2409.08813'}
tags: ['Tools']
---
The burgeoning capabilities of large language models (LLMs) have underscored
the need for alignment to ensure these models act in accordance with human
values and intentions. Existing alignment frameworks present constraints either
in the form of expensive human effort or high computational costs. This paper
explores a promising middle ground, where we employ a weak LLM that is
significantly less resource-intensive than top-tier models, yet offers more
automation than purely human feedback. We present a systematic study to
evaluate and understand weak LLM's ability to generate feedback for alignment.
Our empirical findings demonstrate that weak LLMs can provide feedback that
rivals or even exceeds that of fully human-annotated data. Our study indicates
a minimized impact of model size on feedback efficacy, shedding light on a
scalable and sustainable alignment strategy. To deepen our understanding of
alignment under weak LLM feedback, we conduct a series of qualitative and
quantitative analyses, offering novel insights into the quality discrepancies
between human feedback vs. weak LLM feedback.
