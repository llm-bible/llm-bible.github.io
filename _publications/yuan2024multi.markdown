---
layout: publication
title: 'A Multi-faceted Evaluation Framework For Assessing Synthetic Data Generated By Large Language Models'
authors: Yefeng Yuan, Yuhong Liu, Liang Cheng
conference: "Arxiv"
year: 2024
bibkey: yuan2024multi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.14445"}
tags: ['Tools', 'GPT', 'Survey Paper', 'Applications', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques']
---
The rapid advancements in generative AI and large language models (LLMs) have
opened up new avenues for producing synthetic data, particularly in the realm
of structured tabular formats, such as product reviews. Despite the potential
benefits, concerns regarding privacy leakage have surfaced, especially when
personal information is utilized in the training datasets. In addition, there
is an absence of a comprehensive evaluation framework capable of quantitatively
measuring the quality of the generated synthetic data and their utility for
downstream tasks. In response to this gap, we introduce SynEval, an open-source
evaluation framework designed to assess the fidelity, utility, and privacy
preservation of synthetically generated tabular data via a suite of diverse
evaluation metrics. We validate the efficacy of our proposed framework -
SynEval - by applying it to synthetic product review data generated by three
state-of-the-art LLMs: ChatGPT, Claude, and Llama. Our experimental findings
illuminate the trade-offs between various evaluation metrics in the context of
synthetic data generation. Furthermore, SynEval stands as a critical instrument
for researchers and practitioners engaged with synthetic tabular data,,
empowering them to judiciously determine the suitability of the generated data
for their specific applications, with an emphasis on upholding user privacy.
