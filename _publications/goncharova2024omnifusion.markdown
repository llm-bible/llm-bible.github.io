---
layout: publication
title: Omnifusion Technical Report
authors: Goncharova Elizaveta, Razzhigaev Anton, Mikhalchuk Matvey, Kurkin Maxim, Abdullaeva Irina, Skripkin Matvey, Oseledets Ivan, Dimitrov Denis, Kuznetsov Andrey
conference: "Arxiv"
year: 2024
bibkey: goncharova2024omnifusion
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.06212"}
  - {name: "Code", url: "https://github.com/AIRI&#45;Institute/OmniFusion"}
tags: ['Has Code', 'Merging', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Last year multimodal architectures served up a revolution in AI45;based approaches and solutions extending the capabilities of large language models (LLM). We propose an textit123;OmniFusion125; model based on a pretrained LLM and adapters for visual modality. We evaluated and compared several architecture design principles for better text and visual data coupling MLP and transformer adapters various CLIP ViT45;based encoders (SigLIP InternVIT etc.) and their fusing approach image encoding method (whole image or tiles encoding) and two 7B LLMs (the proprietary one and open45;source Mistral). Experiments on 8 visual45;language benchmarks show the top score for the best OmniFusion setup in terms of different VQA tasks in comparison with open45;source LLaVA45;like solutions VizWiz Pope MM45;Vet ScienceQA MMBench TextVQA VQAv2 MMMU. We also propose a variety of situations where OmniFusion provides highly45;detailed answers in different domains housekeeping sightseeing culture medicine handwritten and scanned equations recognition etc. Mistral45;based OmniFusion model is an open45;source solution with weights training and inference scripts available at https://github.com/AIRI&#45;Institute/OmniFusion.
