---
layout: publication
title: 'Scaling Laws Across Model Architectures: A Comparative Analysis Of Dense And Moe Models In Large Language Models'
authors: Siqi Wang, Zhengyu Chen, Bei Li, Keqing He, Min Zhang, Jingang Wang
conference: "Arxiv"
year: 2024
bibkey: wang2024scaling
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2410.05661'}
tags: ['Large-Scale Training', 'Efficiency and Optimization', 'Training Techniques', 'Model Architecture', 'Tools', 'Scaling Laws', 'Pre-Training']
---
The scaling of large language models (LLMs) is a critical research area for
the efficiency and effectiveness of model training and deployment. Our work
investigates the transferability and discrepancies of scaling laws between
Dense Models and Mixture of Experts (MoE) models. Through a combination of
theoretical analysis and extensive experiments, including consistent loss
scaling, optimal batch size and learning rate scaling, and resource allocation
strategies scaling, our findings reveal that the power-law scaling framework
also applies to MoE Models, indicating that the fundamental principles
governing the scaling behavior of these models are preserved, even though the
architecture differs. Additionally, MoE Models demonstrate superior
generalization, resulting in lower testing losses with the same training
compute budget compared to Dense Models. These findings indicate the scaling
consistency and transfer generalization capabilities of MoE Models, providing
new insights for optimizing MoE Model training and deployment strategies.
