---
layout: publication
title: Longllava Scaling Multi45;modal Llms To 1000 Images Efficiently Via Hybrid Architecture
authors: Wang Xidong, Song Dingjie, Chen Shunian, Zhang Chen, Wang Benyou
conference: "Arxiv"
year: 2024
bibkey: wang2024scaling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.02889"}
tags: ['Agentic', 'Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
Expanding the long45;context capabilities of Multi45;modal Large Language Models~(MLLMs) is crucial for video understanding high45;resolution image understanding and multi45;modal agents. This involves a series of systematic optimizations including model architecture data construction and training strategy particularly addressing challenges such as textit123;degraded performance with more images125; and textit123;high computational costs125;. In this paper we adapt the model architecture to a hybrid of Mamba and Transformer blocks approach data construction with both temporal and spatial dependencies among multiple images and employ a progressive training strategy. The released model textbf123;LongLLaVA125;~(textbf123;Long125;45;Context textbf123;L125;arge textbf123;L125;anguage textbf123;a125;nd textbf123;V125;ision textbf123;A125;ssistant) is the first hybrid MLLM which achieved a better balance between efficiency and effectiveness. LongLLaVA not only achieves competitive results across various benchmarks but also maintains high throughput and low memory consumption. Especially it could process nearly a thousand images on a single A100 80GB GPU showing promising application prospects for a wide range of tasks.
