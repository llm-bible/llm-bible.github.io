---
layout: publication
title: Images In Language Space Exploring The Suitability Of Large Language Models For Vision amp; Language Tasks
authors: Hakimov Sherzod, Schlangen David
conference: "Arxiv"
year: 2023
bibkey: hakimov2023images
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.13782"}
tags: ['GPT', 'Interpretability And Explainability', 'Model Architecture', 'Multimodal Models']
---
Large language models have demonstrated robust performance on various language tasks using zero45;shot or few45;shot learning paradigms. While being actively researched multimodal models that can additionally handle images as input have yet to catch up in size and generality with language45;only models. In this work we ask whether language45;only models can be utilised for tasks that require visual input 45;45; but also as we argue often require a strong reasoning component. Similar to some recent related work we make visual information accessible to the language model using separate verbalisation models. Specifically we investigate the performance of open45;source open45;access language models against GPT45;3 on five vision45;language tasks when given textually45;encoded visual information. Our results suggest that language models are effective for solving vision45;language tasks even with limited samples. This approach also enhances the interpretability of a models output by providing a means of tracing the output back through the verbalised image content.
