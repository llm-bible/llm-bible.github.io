---
layout: publication
title: Images In Language Space\: Exploring The Suitability Of Large Language Models For Vision & Language Tasks
authors: Hakimov Sherzod, Schlangen David
conference: "Arxiv"
year: 2023
bibkey: hakimov2023images
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.13782"}
tags: ['Few Shot', 'GPT', 'Interpretability And Explainability', 'Model Architecture', 'Multimodal Models']
---
Large language models have demonstrated robust performance on various language tasks using zero-shot or few-shot learning paradigms. While being actively researched multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models. In this work we ask whether language-only models can be utilised for tasks that require visual input -- but also as we argue often require a strong reasoning component. Similar to some recent related work we make visual information accessible to the language model using separate verbalisation models. Specifically we investigate the performance of open-source open-access language models against GPT-3 on five vision-language tasks when given textually-encoded visual information. Our results suggest that language models are effective for solving vision-language tasks even with limited samples. This approach also enhances the interpretability of a models output by providing a means of tracing the output back through the verbalised image content.
