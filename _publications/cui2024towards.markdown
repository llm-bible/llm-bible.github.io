---
layout: publication
title: Phaseevo Towards Unified In45;context Prompt Optimization For Large Language Models
authors: Cui Wendi, Zhang Jiaxin, Li Zhuohang, Sun Hao, Lopez Damien, Das Kamalika, Malin Bradley, Kumar Sricharan
conference: "Arxiv"
year: 2024
bibkey: cui2024towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.11347"}
tags: ['Efficiency And Optimization', 'Pretraining Methods', 'Prompting', 'Tools']
---
Crafting an ideal prompt for Large Language Models (LLMs) is a challenging task that demands significant resources and expert human input. Existing work treats the optimization of prompt instruction and in45;context learning examples as distinct problems leading to sub45;optimal prompt performance. This research addresses this limitation by establishing a unified in45;context prompt optimization framework which aims to achieve joint optimization of the prompt instruction and examples. However formulating such optimization in the discrete and high45;dimensional natural language space introduces challenges in terms of convergence and computational efficiency. To overcome these issues we present PhaseEvo an efficient automatic prompt optimization framework that combines the generative capability of LLMs with the global search proficiency of evolution algorithms. Our framework features a multi45;phase design incorporating innovative LLM45;based mutation operators to enhance search efficiency and accelerate convergence. We conduct an extensive evaluation of our approach across 35 benchmark tasks. The results demonstrate that PhaseEvo significantly outperforms the state45;of45;the45;art baseline methods by a large margin whilst maintaining good efficiency.
