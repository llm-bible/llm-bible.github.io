---
layout: publication
title: Continual Learning For Large Language Models: A Survey
authors: Wu Tongtong, Luo Linhao, Li Yuan-fang, Pan Shirui, Vu Thuy-trang, Haffari Gholamreza
conference: "Arxiv"
year: 2024
bibkey: wu2024continual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.01364"}
tags: ['Pretraining Methods', 'RAG', 'Survey Paper', 'Tools', 'Training Techniques']
---
Large language models (LLMs) are not amenable to frequent re-training due to high training costs arising from their massive scale. However updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs we catalog continue learning techniques in a novel multi-staged categorization scheme involving continual pretraining instruction tuning and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover informed by a discussion of benchmarks and evaluation we identify several challenges and future work directions for this crucial task.
