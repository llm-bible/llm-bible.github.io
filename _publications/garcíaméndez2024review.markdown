---
layout: publication
title: "A Review On The Use Of Large Language Models As Virtual Tutors"
authors: García-méndez Silvia, De Arriba-pérez Francisco, Somoza-lópez María Del Carmen
conference: "Science Education"
year: 2024
bibkey: garcíaméndez2024review
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.11983"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Survey Paper', 'Transformer']
---
Transformer architectures contribute to managing long-term dependencies for Natural Language Processing representing one of the most recent changes in the field. These architectures are the basis of the innovative cutting-edge Large Language Models (LLMs) that have produced a huge buzz in several fields and industrial sectors among the ones education stands out. Accordingly these generative Artificial Intelligence-based solutions have directed the change in techniques and the evolution in educational methods and contents along with network infrastructure towards high-quality learning. Given the popularity of LLMs this review seeks to provide a comprehensive overview of those solutions designed specifically to generate and evaluate educational materials and which involve students and teachers in their design or experimental plan. To the best of our knowledge this is the first review of educational applications (e.g. student assessment) of LLMs. As expected the most common role of these systems is as virtual tutors for automatic question generation. Moreover the most popular models are GTP-3 and BERT. However due to the continuous launch of new generative models new works are expected to be published shortly.
