---
layout: publication
title: 'A Review On The Use Of Large Language Models As Virtual Tutors'
authors: Silvia García-méndez, Francisco De Arriba-pérez, María Del Carmen Somoza-lópez
conference: "Science Education (2024) 1-16"
year: 2024
bibkey: garcíaméndez2024review
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2405.11983'}
tags: ['Transformer', 'BERT', 'Applications', 'Model Architecture', 'Survey Paper', 'Pretraining Methods']
---
Transformer architectures contribute to managing long-term dependencies for
Natural Language Processing, representing one of the most recent changes in the
field. These architectures are the basis of the innovative, cutting-edge Large
Language Models (LLMs) that have produced a huge buzz in several fields and
industrial sectors, among the ones education stands out. Accordingly, these
generative Artificial Intelligence-based solutions have directed the change in
techniques and the evolution in educational methods and contents, along with
network infrastructure, towards high-quality learning. Given the popularity of
LLMs, this review seeks to provide a comprehensive overview of those solutions
designed specifically to generate and evaluate educational materials and which
involve students and teachers in their design or experimental plan. To the best
of our knowledge, this is the first review of educational applications (e.g.,
student assessment) of LLMs. As expected, the most common role of these systems
is as virtual tutors for automatic question generation. Moreover, the most
popular models are GTP-3 and BERT. However, due to the continuous launch of new
generative models, new works are expected to be published shortly.
