---
layout: publication
title: A Large Language Model Approach To Educational Survey Feedback Analysis
authors: Parker Michael J., Anderson Caitlin, Stone Claire, Oh Yearim
conference: "Int J Artif Intell Educ"
year: 2023
bibkey: parker2023large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.17447"}
tags: ['Applications', 'Fine Tuning', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
This paper assesses the potential for the large language models (LLMs) GPT45;4 and GPT45;3.5 to aid in deriving insight from education feedback surveys. Exploration of LLM use cases in education has focused on teaching and learning with less exploration of capabilities in education feedback analysis. Survey analysis in education involves goals such as finding gaps in curricula or evaluating teachers often requiring time45;consuming manual processing of textual responses. LLMs have the potential to provide a flexible means of achieving these goals without specialized machine learning models or fine45;tuning. We demonstrate a versatile approach to such goals by treating them as sequences of natural language processing (NLP) tasks including classification (multi45;label multi45;class and binary) extraction thematic analysis and sentiment analysis each performed by LLM. We apply these workflows to a real45;world dataset of 2500 end45;of45;course survey comments from biomedical science courses and evaluate a zero45;shot approach (i.e. requiring no examples or labeled training data) across all tasks reflecting education settings where labeled data is often scarce. By applying effective prompting practices we achieve human45;level performance on multiple tasks with GPT45;4 enabling workflows necessary to achieve typical goals. We also show the potential of inspecting LLMs chain45;of45;thought (CoT) reasoning for providing insight that may foster confidence in practice. Moreover this study features development of a versatile set of classification categories suitable for various course types (online hybrid or in45;person) and amenable to customization. Our results suggest that LLMs can be used to derive a range of insights from survey text.
