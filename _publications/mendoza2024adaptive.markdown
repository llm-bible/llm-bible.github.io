---
layout: publication
title: 'Adaptive Self-supervised Learning Strategies For Dynamic On-device LLM Personalization'
authors: Rafael Mendoza, Isabella Cruz, Richard Liu, Aarav Deshmukh, David Williams, Jesscia Peng, Rohan Iyer
conference: "Arxiv"
year: 2024
bibkey: mendoza2024adaptive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.16973"}
tags: ['Efficiency and Optimization', 'Training Techniques', 'Tools', 'Reinforcement Learning', 'Pretraining Methods', 'Fine-Tuning', 'Applications']
---
Large language models (LLMs) have revolutionized how we interact with
technology, but their personalization to individual user preferences remains a
significant challenge, particularly in on-device applications. Traditional
methods often depend heavily on labeled datasets and can be resource-intensive.
To address these issues, we present Adaptive Self-Supervised Learning
Strategies (ASLS), which utilizes self-supervised learning techniques to
personalize LLMs dynamically. The framework comprises a user profiling layer
for collecting interaction data and a neural adaptation layer for real-time
model fine-tuning. This innovative approach enables continuous learning from
user feedback, allowing the model to generate responses that align closely with
user-specific contexts. The adaptive mechanisms of ASLS minimize computational
demands and enhance personalization efficiency. Experimental results across
various user scenarios illustrate the superior performance of ASLS in boosting
user engagement and satisfaction, highlighting its potential to redefine LLMs
as highly responsive and context-aware systems on-device.
