---
layout: publication
title: Can Language Models Learn From Explanations In Context
authors: Andrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. Mcclelland, Jane X. Wang, Felix Hill
conference: "Arxiv"
year: 2022
bibkey: k2022can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2204.02329v4"}
tags: ['Interpretability And Explainability', 'Prompting', 'Reinforcement Learning']
---
Language Models (LMs) can perform new tasks by adapting to a few in45;context examples. For humans explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few45;shot examples can help LMs. We annotate questions from 40 challenging tasks with answer explanations and various matched control explanations. We evaluate how different types of explanations instructions and controls affect zero45; and few45;shot performance. We analyze these results using statistical multilevel modeling techniques that account for the nested dependencies among conditions tasks prompts and models. We find that explanations can improve performance 45;45; even without tuning. Furthermore explanations hand45;tuned for performance on a small validation set offer substantially larger benefits and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally even untuned explanations outperform carefully matched controls suggesting that the benefits are due to the link between an example and its explanation rather than lower45;level features. However only large models benefit. In summary explanations can support the in45;context learning of large LMs on challenging tasks.
