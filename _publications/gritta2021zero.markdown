---
layout: publication
title: Xeroalign Zero45;shot Cross45;lingual Transformer Alignment
authors: Gritta Milan, Iacobacci Ignacio
conference: "Arxiv"
year: 2021
bibkey: gritta2021zero
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2105.02472"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Security', 'Training Techniques', 'Transformer']
---
The introduction of pretrained cross45;lingual language models brought decisive improvements to multilingual NLP tasks. However the lack of labelled task data necessitates a variety of methods aiming to close the gap to high45;resource languages. Zero45;shot methods in particular often use translated task data as a training signal to bridge the performance gap between the source and target language(s). We introduce XeroAlign a simple method for task45;specific alignment of cross45;lingual pretrained transformers such as XLM45;R. XeroAlign uses translated task data to encourage the model to generate similar sentence embeddings for different languages. The XeroAligned XLM45;R called XLM45;RA shows strong improvements over the baseline models to achieve state45;of45;the45;art zero45;shot results on three multilingual natural language understanding tasks. XLM45;RAs text classification accuracy exceeds that of XLM45;R trained with labelled data and performs on par with state45;of45;the45;art models on a cross45;lingual adversarial paraphrasing task.
