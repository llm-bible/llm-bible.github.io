---
layout: publication
title: 'Scifive: A Text-to-text Transformer Model For Biomedical Literature'
authors: Long N. Phan, James T. Anibal, Hieu Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, Gr√©goire Altan-bonnet
conference: "Arxiv"
year: 2021
bibkey: phan2021text
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2106.03598"}
tags: ['Fine-Tuning', 'Transformer', 'Applications', 'Model Architecture', 'Reinforcement Learning', 'Language Modeling', 'Pretraining Methods', 'BERT']
---
In this report, we introduce SciFive, a domain-specific T5 model that has
been pre-trained on large biomedical corpora. Our model outperforms the current
SOTA methods (i.e. BERT, BioBERT, Base T5) on tasks in named entity relation,
relation extraction, natural language inference, and question-answering. We
show that text-generation methods have significant potential in a broad array
of biomedical NLP tasks, particularly those requiring longer, more complex
outputs. Our results support the exploration of more difficult text generation
tasks and the development of new methods in this area
