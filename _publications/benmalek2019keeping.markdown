---
layout: publication
title: Keeping Notes Conditional Natural Language Generation With A Scratchpad Mechanism
authors: Benmalek Ryan Y., Khabsa Madian, Desu Suma, Cardie Claire, Banko Michele
conference: "Arxiv"
year: 2019
bibkey: benmalek2019keeping
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1906.05275"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture']
---
We introduce the Scratchpad Mechanism a novel addition to the sequence45;to45;sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers Scratchpad can employ the encoder as a scratchpad memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well45;studied natural language generation tasks 45;45;45; Machine Translation Question Generation and Text Summarization 45;45;45; and obtain state45;of45;the45;art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation) attention visualization (MT) and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.
