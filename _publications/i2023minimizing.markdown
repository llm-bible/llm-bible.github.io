---
layout: publication
title: Minimizing Factual Inconsistency And Hallucination In Large Language Models
authors: I Muneeswaran, Saxena Shreya, Prasad Siva, Prakash M V Sai, Shankar Advaith, V Varun, Vaddina Vishal, Gopalakrishnan Saisubramaniam
conference: "Arxiv"
year: 2023
bibkey: i2023minimizing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.13878"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools']
---
Large Language Models (LLMs) are widely used in critical fields such as healthcare education and finance due to their remarkable proficiency in various language45;related tasks. However LLMs are prone to generating factually incorrect responses or hallucinations which can lead to a loss of credibility and trust among users. To address this issue we propose a multi45;stage framework that generates the rationale first verifies and refines incorrect ones and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer by using this rationale and the references to the context. In this paper we demonstrate its effectiveness in improving the quality of responses to drug45;related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT45;3.545;turbo to be 1445;2537; more faithful and 1645;2237; more accurate on two datasets. Furthermore fine45;tuning samples based on our framework improves the accuracy of smaller open45;access LLMs by 3345;4237; and competes with RAG on commercial models.
