---
layout: publication
title: 'An Empirical Study On Prompt Compression For Large Language Models'
authors: Zheng Zhang, Jinyi Li, Yihuai Lan, Xiang Wang, Hao Wang
conference: "Arxiv"
year: 2025
bibkey: zhang2025empirical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.00019"}
  - {name: "Code", url: "https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression"}
tags: ['Agentic', 'Multimodal Models', 'Reinforcement Learning', 'Has Code', 'Prompting']
---
Prompt engineering enables Large Language Models (LLMs) to perform a variety
of tasks. However, lengthy prompts significantly increase computational
complexity and economic costs. To address this issue, we study six prompt
compression methods for LLMs, aiming to reduce prompt length while maintaining
LLM response quality. In this paper, we present a comprehensive analysis
covering aspects such as generation performance, model hallucinations, efficacy
in multimodal tasks, word omission analysis, and more. We evaluate these
methods across 13 datasets, including news, scientific articles, commonsense
QA, math QA, long-context QA, and VQA datasets. Our experiments reveal that
prompt compression has a greater impact on LLM performance in long contexts
compared to short ones. In the Longbench evaluation, moderate compression even
enhances LLM performance. Our code and data is available at
https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.
