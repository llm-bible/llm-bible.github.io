---
layout: publication
title: Evaluating Students Open45;ended Written Responses With Llms Using The RAG Framework For GPT45;3.5 GPT45;4 Claude45;3 And Mistral45;large
authors: Jauhiainen Jussi S., Guerra Agust√≠n Garagorry
conference: "Arxiv"
year: 2024
bibkey: jauhiainen2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.05444"}
tags: ['GPT', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools']
---
Evaluating open45;ended written examination responses from students is an essential yet time45;intensive task for educators requiring a high degree of effort consistency and precision. Recent developments in Large Language Models (LLMs) present a promising opportunity to balance the need for thorough evaluation with efficient use of educators time. In our study we explore the effectiveness of LLMs ChatGPT45;3.5 ChatGPT45;4 Claude45;3 and Mistral45;Large in assessing university students open45;ended answers to questions made about reference material they have studied. Each model was instructed to evaluate 54 answers repeatedly under two conditions 10 times (1045;shot) with a temperature setting of 0.0 and 10 times with a temperature of 0.5 expecting a total of 1080 evaluations per model and 4320 evaluations across all models. The RAG (Retrieval Augmented Generation) framework was used as the framework to make the LLMs to process the evaluation of the answers. As of spring 2024 our analysis revealed notable variations in consistency and the grading outcomes provided by studied LLMs. There is a need to comprehend strengths and weaknesses of LLMs in educational settings for evaluating open45;ended written responses. Further comparative research is essential to determine the accuracy and cost45;effectiveness of using LLMs for educational assessments.
