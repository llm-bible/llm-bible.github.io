---
layout: publication
title: Exposure Bias Versus Self45;recovery Are Distortions Really Incremental For Autoregressive Text Generation
authors: He Tianxing, Zhang Jingzhao, Zhou Zhiming, Glass James
conference: "EMNLP"
year: 2019
bibkey: he2019exposure
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1905.10617"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Language Modeling', 'Pretraining Methods', 'Training Techniques']
---
Exposure bias has been regarded as a central problem for auto45;regressive language models (LM). It claims that teacher forcing would cause the test45;time generation to be incrementally distorted due to the training45;generation discrepancy. Although a lot of algorithms have been proposed to avoid teacher forcing and therefore alleviate exposure bias there is little work showing how serious the exposure bias problem actually is. In this work we focus on the task of open45;ended language generation propose metrics to quantify the impact of exposure bias in the aspects of quality diversity and consistency. Our key intuition is that if we feed ground45;truth data prefixes (instead of prefixes generated by the model itself) into the model and ask it to continue the generation the performance should become much better because the training45;generation discrepancy in the prefix is removed. Both automatic and human evaluations are conducted in our experiments. On the contrary to the popular belief in exposure bias we find that the the distortion induced by the prefix discrepancy is limited and does not seem to be incremental during the generation. Moreover our analysis reveals an interesting self45;recovery ability of the LM which we hypothesize to be countering the harmful effects from exposure bias.
