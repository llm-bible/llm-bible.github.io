---
layout: publication
title: 'Luxveri At Genai Detection Task 3: Cross-domain Detection Of Ai-generated Text Using Inverse Perplexity-weighted Ensemble Of Fine-tuned Transformer Models'
authors: Md Kamrujjaman Mobin, Md Saiful Islam
conference: "Arxiv"
year: 2025
bibkey: mobin2025luxveri
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.11918'}
tags: ['Transformer', 'Security', 'Model Architecture', 'BERT', 'COLING', 'Pretraining Methods']
---
This paper presents our approach for Task 3 of the GenAI content detection
workshop at COLING-2025, focusing on Cross-Domain Machine-Generated Text (MGT)
Detection. We propose an ensemble of fine-tuned transformer models, enhanced by
inverse perplexity weighting, to improve classification accuracy across diverse
text domains. For Subtask A (Non-Adversarial MGT Detection), we combined a
fine-tuned RoBERTa-base model with an OpenAI detector-integrated RoBERTa-base
model, achieving an aggregate TPR score of 0.826, ranking 10th out of 23
detectors. In Subtask B (Adversarial MGT Detection), our fine-tuned
RoBERTa-base model achieved a TPR score of 0.801, securing 8th out of 22
detectors. Our results demonstrate the effectiveness of inverse
perplexity-based weighting for enhancing generalization and performance in both
non-adversarial and adversarial MGT detection, highlighting the potential for
transformer models in cross-domain AI-generated content detection.
