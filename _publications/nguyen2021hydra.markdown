---
layout: publication
title: 'HYDRA -- Hyper Dependency Representation Attentions'
authors: Ha-thanh Nguyen, Vu Tran, Tran-binh Dang, Minh-quan Bui, Minh-phuong Nguyen, Le-minh Nguyen
conference: "Arxiv"
year: 2021
bibkey: nguyen2021hydra
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2109.05349'}
tags: ['Attention Mechanism', 'Transformer', 'Training Techniques', 'Model Architecture', 'Tools', 'Pretraining Methods']
---
Attention is all we need as long as we have enough data. Even so, it is
sometimes not easy to determine how much data is enough while the models are
becoming larger and larger. In this paper, we propose HYDRA heads, lightweight
pretrained linguistic self-attention heads to inject knowledge into transformer
models without pretraining them again. Our approach is a balanced paradigm
between leaving the models to learn unsupervised and forcing them to conform to
linguistic knowledge rigidly as suggested in previous studies. Our experiment
proves that the approach is not only the boost performance of the model but
also lightweight and architecture friendly. We empirically verify our framework
on benchmark datasets to show the contribution of linguistic knowledge to a
transformer model. This is a promising result for a new approach to
transferring knowledge from linguistic resources into transformer-based models.
