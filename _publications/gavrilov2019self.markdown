---
layout: publication
title: Self45;attentive Model For Headline Generation
authors: Gavrilov Daniil, Kalaidin Pavel, Malykh Valentin
conference: "Arxiv"
year: 2019
bibkey: gavrilov2019self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1901.07786"}
tags: ['Applications', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Headline generation is a special type of text summarization task. While the amount of available training data for this task is almost unlimited it still remains challenging as learning to generate headlines for news articles implies that the model has strong reasoning about natural language. To overcome this issue we applied recent Universal Transformer architecture paired with byte45;pair encoding technique and achieved new state45;of45;the45;art results on the New York Times Annotated corpus with ROUGE45;L F145;score 24.84 and ROUGE45;2 F145;score 13.48. We also present the new RIA corpus and reach ROUGE45;L F145;score 36.81 and ROUGE45;2 F145;score 22.15 on it.
