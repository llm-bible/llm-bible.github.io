---
layout: publication
title: 'Self-attentive Model For Headline Generation'
authors: Daniil Gavrilov, Pavel Kalaidin, Valentin Malykh
conference: "Arxiv"
year: 2019
bibkey: gavrilov2019self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1901.07786"}
tags: ['Transformer', 'Applications', 'Model Architecture', 'Training Techniques', 'Pretraining Methods']
---
Headline generation is a special type of text summarization task. While the
amount of available training data for this task is almost unlimited, it still
remains challenging, as learning to generate headlines for news articles
implies that the model has strong reasoning about natural language. To overcome
this issue, we applied recent Universal Transformer architecture paired with
byte-pair encoding technique and achieved new state-of-the-art results on the
New York Times Annotated corpus with ROUGE-L F1-score 24.84 and ROUGE-2
F1-score 13.48. We also present the new RIA corpus and reach ROUGE-L F1-score
36.81 and ROUGE-2 F1-score 22.15 on it.
