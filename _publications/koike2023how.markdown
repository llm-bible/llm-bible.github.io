---
layout: publication
title: How You Prompt Matters! Even Task-oriented Constraints In Instructions Affect Llm-generated Text Detection
authors: Koike Ryuto, Kaneko Masahiro, Okazaki Naoaki
conference: "Arxiv"
year: 2023
bibkey: koike2023how
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.08369"}
tags: ['Pretraining Methods', 'Prompting']
---
To combat the misuse of Large Language Models (LLMs) many recent studies have presented LLM-generated-text detectors with promising performance. When users instruct LLMs to generate texts the instruction can include different constraints depending on the users need. However most recent studies do not cover such diverse instruction patterns when creating datasets for LLM detection. In this paper we reveal that even task-oriented constraints -- constraints that would naturally be included in an instruction and are not related to detection-evasion -- cause existing powerful detectors to have a large variance in detection performance. We focus on student essay writing as a realistic domain and manually create task-oriented constraints based on several factors for essay quality. Our experiments show that the standard deviation (SD) of current detector performance on texts generated by an instruction with such a constraint is significantly larger (up to an SD of 14.4 F1-score) than that by generating texts multiple times or paraphrasing the instruction. We also observe an overall trend where the constraints can make LLM detection more challenging than without them. Finally our analysis indicates that the high instruction-following ability of LLMs fosters the large impact of such constraints on detection performance.
