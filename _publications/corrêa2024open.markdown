---
layout: publication
title: 'Teenytinyllama: Open-source Tiny Language Models Trained In Brazilian Portuguese'
authors: Nicholas Kluge Corrêa, Sophia Falk, Shiza Fatimah, Aniket Sen, Nythamar De Oliveira
conference: "Machine Learning With Applications 16 100558"
year: 2024
bibkey: corrêa2024open
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2401.16640'}
  - {name: "Code", url: 'https://github.com/Nkluge-correa/TeenyTinyLlama'}
tags: ['Has Code', 'Language Modeling', 'Applications']
---
Large language models (LLMs) have significantly advanced natural language
processing, but their progress has yet to be equal across languages. While most
LLMs are trained in high-resource languages like English, multilingual models
generally underperform monolingual ones. Additionally, aspects of their
multilingual foundation sometimes restrict the byproducts they produce, like
computational demands and licensing regimes. In this study, we document the
development of open-foundation models tailored for use in low-resource
settings, their limitations, and their benefits. This is the TeenyTinyLlama
pair: two compact models for Brazilian Portuguese text generation. We release
them under the permissive Apache 2.0 license on GitHub and Hugging Face for
community use and further development. See
https://github.com/Nkluge-correa/TeenyTinyLlama
