---
layout: publication
title: Leveraging Linguistic Coordination in Reranking N-Best Candidates For End-to-End Response Selection Using BERT
authors: Yu Mingzhi University Of Pittsburgh, Litman Diane University Of Pittsburgh
conference: "Arxiv"
year: 2021
bibkey: yu2021leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2105.13479"}
tags: ['Applications', 'BERT', 'Model Architecture', 'RAG']
---
Retrieval-based dialogue systems select the best response from many candidates. Although many state-of-the-art models have shown promising performance in dialogue response selection tasks there is still quite a gap between R64;1 and R64;10 performance. To address this we propose to leverage linguistic coordination (a phenomenon that individuals tend to develop similar linguistic behaviors in conversation) to rerank the N-best candidates produced by BERT a state-of-the-art pre-trained language model. Our results show an improvement in R64;1 compared to BERT baselines demonstrating the utility of repairing machine-generated outputs by leveraging a linguistic theory.
