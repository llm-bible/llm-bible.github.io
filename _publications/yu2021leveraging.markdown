---
layout: publication
title: 'Leveraging Linguistic Coordination In Reranking N-best Candidates For End-to-end Response Selection Using BERT'
authors: Mingzhi University Of Pittsburgh Yu, Diane University Of Pittsburgh Litman
conference: "Arxiv"
year: 2021
bibkey: yu2021leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2105.13479"}
tags: ['RAG', 'BERT', 'Applications', 'Model Architecture']
---
Retrieval-based dialogue systems select the best response from many
candidates. Although many state-of-the-art models have shown promising
performance in dialogue response selection tasks, there is still quite a gap
between R@1 and R@10 performance. To address this, we propose to leverage
linguistic coordination (a phenomenon that individuals tend to develop similar
linguistic behaviors in conversation) to rerank the N-best candidates produced
by BERT, a state-of-the-art pre-trained language model. Our results show an
improvement in R@1 compared to BERT baselines, demonstrating the utility of
repairing machine-generated outputs by leveraging a linguistic theory.
