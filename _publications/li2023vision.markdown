---
layout: publication
title: Vision-language Instruction Tuning&#58; A Review And Analysis
authors: Li Chen, Ge Yixiao, Li Dian, Shan Ying
conference: "Arxiv"
year: 2023
bibkey: li2023vision
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.08172"}
  - {name: "Code", url: "https://github.com/palchenli/VL-Instruction-Tuning"}
tags: ['Fine Tuning', 'Has Code', 'Multimodal Models', 'Survey Paper', 'Training Techniques']
---
Instruction tuning is a crucial supervised training phase in Large Language Models (LLMs) aiming to enhance the LLMs ability to generalize instruction execution and adapt to user preferences. With the increasing integration of multi-modal data into LLMs there is growing interest in Vision-Language Instruction Tuning (VLIT) which presents more complex characteristics compared to pure text instruction tuning. In this paper we systematically review the latest VLIT settings and corresponding datasets in multi-modal LLMs and provide insights into the intrinsic motivations behind their design. For the first time we offer a detailed multi-perspective categorization for existing VLIT datasets and identify the characteristics that high-quality VLIT data should possess. By incorporating these characteristics as guiding principles into the existing VLIT data construction process we conduct extensive experiments and verify their positive impact on the performance of tuned multi-modal LLMs. Furthermore we discuss the current challenges and future research directions of VLIT providing insights for the continuous development of this field. The code and dataset related to this paper have been open-sourced at https://github.com/palchenli/VL-Instruction-Tuning."
