---
layout: publication
title: 'Vision-language Instruction Tuning: A Review And Analysis'
authors: Chen Li, Yixiao Ge, Dian Li, Ying Shan
conference: "Arxiv"
year: 2023
bibkey: li2023vision
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.08172"}
  - {name: "Code", url: "https://github.com/palchenli/VL-Instruction-Tuning"}
tags: ['Multimodal Models', 'Training Techniques', 'Survey Paper', 'Fine-Tuning', 'Has Code']
---
Instruction tuning is a crucial supervised training phase in Large Language
Models (LLMs), aiming to enhance the LLM's ability to generalize instruction
execution and adapt to user preferences. With the increasing integration of
multi-modal data into LLMs, there is growing interest in Vision-Language
Instruction Tuning (VLIT), which presents more complex characteristics compared
to pure text instruction tuning. In this paper, we systematically review the
latest VLIT settings and corresponding datasets in multi-modal LLMs and provide
insights into the intrinsic motivations behind their design. For the first
time, we offer a detailed multi-perspective categorization for existing VLIT
datasets and identify the characteristics that high-quality VLIT data should
possess. By incorporating these characteristics as guiding principles into the
existing VLIT data construction process, we conduct extensive experiments and
verify their positive impact on the performance of tuned multi-modal LLMs.
Furthermore, we discuss the current challenges and future research directions
of VLIT, providing insights for the continuous development of this field. The
code and dataset related to this paper have been open-sourced at
https://github.com/palchenli/VL-Instruction-Tuning.
