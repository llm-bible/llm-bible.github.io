---
layout: publication
title: 'Exploiting Contextual Knowledge In Llms Through V-usable Information Based Layer Enhancement'
authors: Xiaowei Yuan, Zhao Yang, Ziyang Huang, Yequan Wang, Siqi Fan, Yiming Ju, Jun Zhao, Kang Liu
conference: "Arxiv"
year: 2025
bibkey: yuan2025exploiting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.15630"}
tags: ['RAG', 'Reinforcement Learning']
---
Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks, yet they often struggle with context-faithfulness generations
that properly reflect contextual knowledge. While existing approaches focus on
enhancing the decoding strategies, they ignore the fundamental mechanism of how
contextual information is processed within LLMs' internal states. As a result,
LLMs remain limited in their ability to fully leverage contextual knowledge. In
this paper, we propose Context-aware Layer Enhancement (CaLE), a novel
intervention method that enhances the utilization of contextual knowledge
within LLMs' internal representations. By employing V-usable information
analysis, CaLE strategically amplifies the growth of contextual information at
an optimal layer, thereby enriching representations in the final layer. Our
experiments demonstrate that CaLE effectively improves context-faithful
generation in Question-Answering tasks, particularly in scenarios involving
unknown or conflicting contextual knowledge.
