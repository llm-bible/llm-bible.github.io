---
layout: publication
title: 'Benchmarking The Performance Of Pre-trained Llms Across Urdu NLP Tasks'
authors: Munief Hassan Tahir, Sana Shams, Layba Fiaz, Farah Adeeba, Sarmad Hussain
conference: "Arxiv"
year: 2024
bibkey: tahir2024benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.15453"}
tags: ['RAG', 'GPT', 'Model Architecture']
---
Large Language Models (LLMs) pre-trained on multilingual data have
revolutionized natural language processing research, by transitioning from
languages and task specific model pipelines to a single model adapted on a
variety of tasks. However majority of existing multilingual NLP benchmarks for
LLMs provide evaluation data in only few languages with little linguistic
diversity. In addition these benchmarks lack quality assessment against the
respective state-of the art models. This study presents an in-depth examination
of 7 prominent LLMs: GPT-3.5-turbo, Llama 2-7B-Chat, Llama 3.1-8B, Bloomz 3B,
Bloomz 7B1, Ministral-8B and Whisper (Large, medium and small variant) across
17 tasks using 22 datasets, 13.8 hours of speech, in a zero-shot setting, and
their performance against state-of-the-art (SOTA) models, has been compared and
analyzed. Our experiments show that SOTA models currently outperform
encoder-decoder models in majority of Urdu NLP tasks under zero-shot settings.
However, comparing Llama 3.1-8B over prior version Llama 2-7B-Chat, we can
deduce that with improved language coverage, LLMs can surpass these SOTA
models. Our results emphasize that models with fewer parameters but richer
language-specific data, like Llama 3.1-8B, often outperform larger models with
lower language diversity, such as GPT-3.5, in several tasks.
