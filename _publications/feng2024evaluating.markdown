---
layout: publication
title: 'Sciknoweval: Evaluating Multi-level Scientific Knowledge Of Large Language Models'
authors: Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, Huajun Chen
conference: "Arxiv"
year: 2024
bibkey: feng2024evaluating
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2406.09098'}
  - {name: "Code", url: 'https://scimind.ai/sciknoweval'}
tags: ['Has Code', 'Few-Shot', 'RAG', 'Applications', 'Tools', 'Prompting', 'Reinforcement Learning']
---
Large language models (LLMs) have gained increasing prominence in scientific
research, but there is a lack of comprehensive benchmarks to fully evaluate
their proficiency in understanding and mastering scientific knowledge. To
address this need, we introduce the SciKnowEval benchmark, a novel framework
that systematically evaluates LLMs across five progressive levels of scientific
knowledge: studying extensively, inquiring earnestly, thinking profoundly,
discerning clearly, and practicing assiduously. These levels aim to assess the
breadth and depth of scientific knowledge in LLMs, including memory,
comprehension, reasoning, discernment, and application. Specifically, we first
construct a large-scale evaluation dataset encompassing 70K multi-level
scientific problems and solutions in the domains of biology, chemistry,
physics, and materials science. By leveraging this dataset, we benchmark 26
advanced open-source and proprietary LLMs using zero-shot and few-shot
prompting strategies. The results reveal that despite the state-of-the-art
performance of proprietary LLMs, there is still significant room for
improvement, particularly in addressing scientific reasoning and applications.
We anticipate that SciKnowEval will establish a standard for benchmarking LLMs
in science research and promote the development of stronger scientific LLMs.
The dataset and code are publicly available at https://scimind.ai/sciknoweval .
