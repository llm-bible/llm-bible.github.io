---
layout: publication
title: Clip-guided Vision-language Pre-training For Question Answering In 3D Scenes
authors: Maria Parelli et al.
conference: Arxiv
year: 2023
citations: 20
bibkey: parelli2023clip
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2304.06061'}]
tags: [Pre-Training, Multimodal Models]
---
Training models to apply linguistic knowledge and visual concepts from 2D
images to 3D world understanding is a promising direction that researchers have
only recently started to explore. In this work, we design a novel 3D
pre-training Vision-Language method that helps a model learn semantically
meaningful and transferable 3D scene point cloud representations. We inject the
representational power of the popular CLIP model into our 3D encoder by
aligning the encoded 3D scene features with the corresponding 2D image and text
embeddings produced by CLIP. To assess our model's 3D world reasoning
capability, we evaluate it on the downstream task of 3D Visual Question
Answering. Experimental quantitative and qualitative results show that our
pre-training method outperforms state-of-the-art works in this task and leads
to an interpretable representation of 3D scene features.