---
layout: publication
title: 'Not All Llm-generated Data Are Equal: Rethinking Data Weighting In Text Classification'
authors: Hsun-yu Kuo, Yin-hsiang Liao, Yu-chieh Chao, Wei-yun Ma, Pu-jen Cheng
conference: "Arxiv"
year: 2024
bibkey: kuo2024not
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.21526"}
tags: ['Applications', 'RAG', 'Model Architecture', 'Reinforcement Learning', 'Training Techniques', 'BERT']
---
Synthetic data augmentation via large language models (LLMs) allows
researchers to leverage additional training data, thus enhancing the
performance of downstream tasks, especially when real-world data is scarce.
However, the generated data can deviate from the real-world data, and this
misalignment can bring deficient outcomes while applying the trained model to
applications. Therefore, we proposed efficient weighted-loss approaches to
align synthetic data with real-world distribution by emphasizing high-quality
and diversified data generated by LLMs with using merely a little real-world
data. We empirically assessed the effectiveness of our method on multiple text
classification tasks, and the results showed leveraging our approaches on a
BERT-level model robustly outperformed standard cross-entropy and other data
weighting approaches, providing potential solutions to effectively leveraging
synthetic data from any suitable data generator for model training.
