---
layout: publication
title: 'Repetitions Are Not All Alike: Distinct Mechanisms Sustain Repetition In Language Models'
authors: Mat√©o Mahaut, Francesca Franzon
conference: "Arxiv"
year: 2025
bibkey: mahaut2025repetitions
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.01100"}
tags: ['Model Architecture', 'Reinforcement Learning', 'Language Modeling', 'In-Context Learning', 'Prompting', 'Applications', 'Attention Mechanism']
---
Text generated by language models (LMs) can degrade into repetitive cycles,
where identical word sequences are persistently repeated one after another.
Prior research has typically treated repetition as a unitary phenomenon.
However, repetitive sequences emerge under diverse tasks and contexts, raising
the possibility that it may be driven by multiple underlying factors. Here, we
experimentally explore the hypothesis that repetition in LMs can result from
distinct mechanisms, reflecting different text generation strategies used by
the model. We examine the internal working of LMs under two conditions that
prompt repetition: one in which repeated sequences emerge naturally after
human-written text, and another where repetition is explicitly induced through
an in-context learning (ICL) setup. Our analysis reveals key differences
between the two conditions: the model exhibits varying levels of confidence,
relies on different attention heads, and shows distinct pattens of change in
response to controlled perturbations. These findings suggest that distinct
internal mechanisms can interact to drive repetition, with implications for its
interpretation and mitigation strategies. More broadly, our results highlight
that the same surface behavior in LMs may be sustained by different underlying
processes, acting independently or in combination.
