---
layout: publication
title: 'Argumentative Large Language Models For Explainable And Contestable Claim Verification'
authors: Gabriel Freedman, Adam Dejl, Deniz Gorur, Xiang Yin, Antonio Rago, Francesca Toni
conference: "Proceedings of the AAAI Conference on Artificial Intelligence 39(14) 14930-14939. 2025"
year: 2024
bibkey: freedman2024argumentative
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2405.02079'}
tags: ['Reinforcement Learning', 'Tools', 'Merging']
---
The profusion of knowledge encoded in large language models (LLMs) and their
ability to apply this knowledge zero-shot in a range of settings makes them
promising candidates for use in decision-making. However, they are currently
limited by their inability to provide outputs which can be faithfully explained
and effectively contested to correct mistakes. In this paper, we attempt to
reconcile these strengths and weaknesses by introducing *argumentative
LLMs (ArgLLMs)*, a method for augmenting LLMs with argumentative reasoning.
Concretely, ArgLLMs construct argumentation frameworks, which then serve as the
basis for formal reasoning in support of decision-making. The interpretable
nature of these argumentation frameworks and formal reasoning means that any
decision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'
performance experimentally in comparison with state-of-the-art techniques, in
the context of the decision-making task of claim verification. We also define
novel properties to characterise contestability and assess ArgLLMs formally in
terms of these properties.
