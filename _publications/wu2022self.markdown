---
layout: publication
title: Self45;adaptive In45;context Learning An Information Compression Perspective For In45;context Example Selection And Ordering
authors: Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong
conference: "Arxiv"
year: 2022
bibkey: wu2022self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2212.10375v2"}
  - {name: "Code", url: "https://github.com/Shark&#45;NLP/self&#45;adaptive&#45;ICL"}
tags: ['Has Code', 'Pretraining Methods', 'Tools']
---
Despite the surprising few45;shot performance of in45;context learning (ICL) it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL self45;adaptive in45;context learning. The self45;adaption mechanism is introduced to help each sample find an in45;context example permutation (i.e. selection and ordering) that can derive the correct prediction thus maximizing performance. To validate the effectiveness of self45;adaptive ICL we propose a general select45;then45;rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets our self45;adaptive ICL method achieves a 4037; relative improvement over the common practice setting. Further analysis reveals the enormous potential of self45;adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code is released to facilitate future research in this area https://github.com/Shark&#45;NLP/self&#45;adaptive&#45;ICL
