---
layout: publication
title: Prompting And Evaluating Large Language Models For Proactive Dialogues Clarification Target45;guided And Non45;collaboration
authors: Deng Yang, Liao Lizi, Chen Liang, Wang Hongru, Lei Wenqiang, Chua Tat-seng
conference: "Arxiv"
year: 2023
bibkey: deng2023prompting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.13626"}
tags: ['Agentic', 'Applications', 'GPT', 'Model Architecture', 'Prompting']
---
Conversational systems based on Large Language Models (LLMs) such as ChatGPT show exceptional proficiency in context understanding and response generation. However despite their impressive capabilities they still possess limitations such as providing randomly45;guessed answers to ambiguous queries or failing to refuse users requests both of which are considered aspects of a conversational agents proactivity. This raises the question of whether LLM45;based conversational systems are equipped to handle proactive dialogue problems. In this work we conduct a comprehensive analysis of LLM45;based conversational systems specifically focusing on three aspects of proactive dialogue systems clarification target45;guided and non45;collaborative dialogues. To trigger the proactivity of LLMs we propose the Proactive Chain45;of45;Thought prompting scheme which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM45;based proactive dialogue systems.
