---
layout: publication
title: 'The University Of Cambridge''s Machine Translation Systems For WMT18'
authors: Stahlberg Felix, De Gispert Adria, Byrne Bill
conference: "Arxiv"
year: 2018
bibkey: stahlberg2018university
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1808.09465"}
tags: ['Applications', 'Attention Mechanism', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
The University of Cambridge submission to the WMT18 news translation task
focuses on the combination of diverse models of translation. We compare
recurrent, convolutional, and self-attention-based neural models on
German-English, English-German, and Chinese-English. Our final system combines
all neural models together with a phrase-based SMT system in an MBR-based
scheme. We report small but consistent gains on top of strong Transformer
ensembles.
