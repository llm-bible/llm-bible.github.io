---
layout: publication
title: Hipporag Neurobiologically Inspired Long45;term Memory For Large Language Models
authors: Gutiérrez Bernal Jiménez, Shu Yiheng, Gu Yu, Yasunaga Michihiro, Su Yu
conference: "Arxiv"
year: 2024
bibkey: gutiérrez2024neurobiologically
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.14831"}
  - {name: "Code", url: "https://github.com/OSU&#45;NLP&#45;Group/HippoRAG"}
tags: ['Applications', 'Has Code', 'RAG', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
In order to thrive in hostile and ever45;changing natural environments mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments large language models (LLMs) even with retrieval45;augmented generation (RAG) still struggle to efficiently and effectively integrate a large amount of new experiences after pre45;training. In this work we introduce HippoRAG a novel retrieval framework inspired by the hippocampal indexing theory of human long45;term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs knowledge graphs and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi45;hop question answering and show that our method outperforms the state45;of45;the45;art methods remarkably by up to 2037;. Single45;step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 1045;30 times cheaper and 645;13 times faster and integrating HippoRAG into IRCoT brings further substantial gains. Finally we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU&#45;NLP&#45;Group/HippoRAG.
