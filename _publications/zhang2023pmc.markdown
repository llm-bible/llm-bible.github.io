---
layout: publication
title: PMC45;VQA Visual Instruction Tuning For Medical Visual Question Answering
authors: Zhang Xiaoman, Wu Chaoyi, Zhao Ziheng, Lin Weixiong, Zhang Ya, Wang Yanfeng, Xie Weidi
conference: "Arxiv"
year: 2023
bibkey: zhang2023pmc
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.10415"}
  - {name: "Code", url: "https://paperswithcode.com/paper/pmc&#45;vqa&#45;visual&#45;instruction&#45;tuning&#45;for&#45;medical,"}
tags: ['Applications', 'Has Code', 'RAG', 'Reinforcement Learning']
---
Medical Visual Question Answering (MedVQA) presents a significant opportunity to enhance diagnostic accuracy and healthcare delivery by leveraging artificial intelligence to interpret and answer questions based on medical images. In this study we reframe the problem of MedVQA as a generation task that naturally follows the human45;machine interaction and propose a generative45;based model for medical visual understanding by aligning visual information from a pre45;trained vision encoder with a large language model. We establish a scalable pipeline to construct a large45;scale medical visual question45;answering dataset named PMC45;VQA which contains 227k VQA pairs of 149k images that cover various modalities or diseases. We train the proposed model on PMC45;VQA and then fine45;tune it on multiple public benchmarks e.g. VQA45;RAD SLAKE and Image45;Clef45;2019 significantly outperforming existing MedVQA models in generating relevant accurate free45;form answers. In addition we propose a test set that has undergone manual verification which is significantly more challenging serving to better monitor the development of generative MedVQA methods. To facilitate comprehensive evaluation and comparison we have maintained a leaderboard at https://paperswithcode.com/paper/pmc&#45;vqa&#45;visual&#45;instruction&#45;tuning&#45;for&#45;medical, offering a centralized resource for tracking progress and benchmarking state45;of45;the45;art approaches. The PMC45;VQA dataset emerges as a vital resource for the field of research and the MedVInT presents a significant breakthrough in the area of MedVQA.
