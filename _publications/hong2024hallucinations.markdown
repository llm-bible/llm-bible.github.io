---
layout: publication
title: 'The Hallucinations Leaderboard -- An Open Effort To Measure Hallucinations In Large Language Models'
authors: Giwon Hong, Aryo Pradipta Gema, Rohit Saxena, Xiaotang Du, Ping Nie, Yu Zhao, Laura Perez-beltrachini, Max Ryabinin, Xuanli He, Cl√©mentine Fourrier, Pasquale Minervini
conference: "Arxiv"
year: 2024
bibkey: hong2024hallucinations
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2404.05904'}
tags: ['Applications']
---
Large Language Models (LLMs) have transformed the Natural Language Processing
(NLP) landscape with their remarkable ability to understand and generate
human-like text. However, these models are prone to ``hallucinations'' --
outputs that do not align with factual reality or the input context. This paper
introduces the Hallucinations Leaderboard, an open initiative to quantitatively
measure and compare the tendency of each model to produce hallucinations. The
leaderboard uses a comprehensive set of benchmarks focusing on different
aspects of hallucinations, such as factuality and faithfulness, across various
tasks, including question-answering, summarisation, and reading comprehension.
Our analysis provides insights into the performance of different models,
guiding researchers and practitioners in choosing the most reliable models for
their applications.
