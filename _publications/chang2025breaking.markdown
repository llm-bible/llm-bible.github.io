---
layout: publication
title: 'Breaking The Prompt Wall (I): A Real-world Case Study Of Attacking Chatgpt Via Lightweight Prompt Injection'
authors: Xiangyu Chang, Guang Dai, Hao Di, Haishan Ye
conference: "Arxiv"
year: 2025
bibkey: chang2025breaking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.16125"}
tags: ['Responsible AI', 'Agentic', 'Security', 'Model Architecture', 'Tools', 'Reinforcement Learning', 'GPT', 'Prompting']
---
This report presents a real-world case study demonstrating how prompt
injection can attack large language model platforms such as ChatGPT according
to a proposed injection framework. By providing three real-world examples, we
show how adversarial prompts can be injected via user inputs, web-based
retrieval, and system-level agent instructions. These attacks, though
lightweight and low-cost, can cause persistent and misleading behaviors in LLM
outputs. Our case study reveals that even commercial-grade LLMs remain
vulnerable to subtle manipulations that bypass safety filters and influence
user decisions. \textbf\{More importantly, we stress that this report is not
intended as an attack guide, but as a technical alert. As ethical researchers,
we aim to raise awareness and call upon developers, especially those at OpenAI,
to treat prompt-level security as a critical design priority.
