---
layout: publication
title: JASMINE Arabic GPT Models For Few45;shot Learning
authors: Nagoudi El Moatez Billah, Abdul-mageed Muhammad, Elmadany Abdelrahim, Inciarte Alcides Alcoba, Khondaker Md Tawkat Islam
conference: "Arxiv"
year: 2022
bibkey: nagoudi2022arabic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2212.10755"}
tags: ['Ethics And Bias', 'GPT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Training Techniques', 'Transformer']
---
Scholarship on generative pretraining (GPT) remains acutely Anglocentric leaving serious gaps in our understanding of the whole class of autoregressive models. For example we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic a wide collection of languages and dialectal varieties with more than 400 million population by introducing JASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million45;6.7 billion parameters pretrained on a large and diverse dataset (~ 235 GB of text). We also carefully design and release a comprehensive benchmark for both automated and human evaluation of Arabic autoregressive models with coverage of potential social biases harms and toxicity. Using our novel benchmark we evaluate JASMINE extensively showing powerful performance intrinsically as well as in few45;shot learning on a wide range of NLP tasks. We aim to responsibly release our models and evaluation benchmark with interested researchers along with code for experimenting with them.
