---
layout: publication
title: 'DLPO: Towards A Robust, Efficient, And Generalizable Prompt Optimization Framework From A Deep-learning Perspective'
authors: Dengyun Peng, Yuhang Zhou, Qiguang Chen, Jinhao Liu, Jingjing Chen, Libo Qin
conference: "Arxiv"
year: 2025
bibkey: peng2025towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.13413"}
  - {name: "Code", url: "https://github.com/sfasfaffa/DLPO"}
tags: ['Tools', 'Efficiency and Optimization', 'Security', 'Has Code', 'Prompting']
---
Large Language Models (LLMs) have achieved remarkable success across diverse
tasks, largely driven by well-designed prompts. However, crafting and selecting
such prompts often requires considerable human effort, significantly limiting
its scalability. To mitigate this, recent studies have explored automated
prompt optimization as a promising solution. Despite these efforts, existing
methods still face critical challenges in robustness, efficiency, and
generalization. To systematically address these challenges, we first conduct an
empirical analysis to identify the limitations of current reflection-based
prompt optimization paradigm. Building on these insights, we propose 7
innovative approaches inspired by traditional deep learning paradigms for
prompt optimization (DLPO), seamlessly integrating these concepts into
text-based gradient optimization. Through these advancements, we progressively
tackle the aforementioned challenges and validate our methods through extensive
experimentation. We hope our study not only provides valuable guidance for
future research but also offers a comprehensive understanding of the challenges
and potential solutions in prompt optimization. Our code is available at
https://github.com/sfasfaffa/DLPO.
