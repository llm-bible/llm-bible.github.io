---
layout: publication
title: When Life Gives You Llms, Make LLM-ADE: Large Language Models With Adaptive Data Engineering
authors: Choi Stephen, Gazeley William
conference: "Arxiv"
year: 2024
bibkey: choi2024when
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.13028"}
tags: ['Applications', 'Pretraining Methods', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
This paper presents the LLM-ADE framework a novel methodology for continued pre-training of large language models (LLMs) that addresses the challenges of catastrophic forgetting and double descent. LLM-ADE employs dynamic architectural adjustments including selective block freezing and expansion tailored to specific datasets. This strategy enhances model adaptability to new data while preserving previously acquired knowledge. We demonstrate LLM-ADEs effectiveness on the TinyLlama model across various general knowledge benchmarks showing significant performance improvements without the drawbacks of traditional continuous training methods. This approach promises a more versatile and robust way to keep LLMs current and efficient in real-world applications.
