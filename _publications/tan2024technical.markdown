---
layout: publication
title: 1.545;pints Technical Report Pretraining In Days Not Months 45;45; Your Language Model Thrives On Quality Data
authors: Tan Calvin, Wang Jerome
conference: "Arxiv"
year: 2024
bibkey: tan2024technical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.03506"}
tags: ['Model Architecture', 'Pretraining Methods', 'Survey Paper', 'Training Techniques']
---
This paper presents a compute45;efficient approach to pre45;training a Language Model45;the 1.545;Pints45;in only 9 days while outperforming state45;of45;the45;art models as an instruction45;following assistant.Based on MT45;Bench (a benchmark that emulates human judgments) 1.545;Pints outperforms Apples OpenELM and Microsofts Phi.This is achieved by a carefully curated pre45;training dataset of 57 billion tokens using a mix of automated workflows and manual human review. The selection of the dataset prioritizes content that is considered expository and textbook45;like to aid the model in reasoning and logical deduction culminating in its overall ability as a strong and versatile AI model. In terms of the model architecture we employed a modified Mistral tokenizer alongside a Llama45;2 architecture for wider compatibility. For training we adopted the methodologies used by StableLM TinyLlama and Huggingface Zephyr. 1.545;Pints demonstrates that by focusing on data quality over quantity in LLM training we can significantly reduce training time and resources required. We believe this approach will not only make pre45;training more accessible but also reduce our carbon footprint. Our findings and resources from this research are open45;sourced aiming to facilitate further advancements in the field. The 1.545;Pints model is available in two versions 2K and 16K context windows.
