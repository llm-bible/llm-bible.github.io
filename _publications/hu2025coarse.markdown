---
layout: publication
title: 'Coarse-to-fine Process Reward Modeling For Mathematical Reasoning'
authors: Yulan Hu, Ge Chen, Jinman Zhao, Sheng Ouyang, Yong Liu
conference: "Arxiv"
year: 2025
bibkey: hu2025coarse
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.13622'}
tags: ['Reinforcement Learning', 'RAG', 'Training Techniques']
---
The Process Reward Model (PRM) plays a crucial role in mathematical reasoning
tasks, requiring high-quality supervised process data. However, we observe that
reasoning steps generated by Large Language Models (LLMs) often fail to exhibit
strictly incremental information, leading to redundancy that can hinder
effective reasoning. To address this issue, we propose CFPRM, a simple yet
effective coarse-to-fine strategy. Instead of focusing on the detection of
redundant steps, our approach first establishes a coarse-grained window to
merge adjacent reasoning steps into unified, holistic steps. The window size is
then progressively reduced to extract fine-grained reasoning steps, enabling
data collection at multiple granularities for training. By leveraging this
hierarchical refinement process, CFPRM mitigates redundancy while preserving
essential fine-grained knowledge. Extensive experiments on two reasoning
datasets across three loss criteria validate the CFPRM's effectiveness and
versatility.
