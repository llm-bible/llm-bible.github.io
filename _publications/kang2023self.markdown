---
layout: publication
title: Self45;specialization Uncovering Latent Expertise Within Large Language Models
authors: Kang Junmo, Luo Hongyin, Zhu Yada, Hansen Jacob, Glass James, Cox David, Ritter Alan, Feris Rogerio, Karlinsky Leonid
conference: "Arxiv"
year: 2023
bibkey: kang2023self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.00160"}
tags: ['Efficiency And Optimization', 'RAG', 'Training Techniques']
---
Recent works have demonstrated the effectiveness of self45;alignment in which a large language model is aligned to follow general instructions using instructional data generated from the model itself starting from a handful of human45;written seeds. Instead of general alignment in this work we focus on self45;alignment for expert domain specialization (e.g. biomedicine finance). As a preliminary we quantitively show the marginal effect that generic instruction45;following training has on downstream expert domains performance. To remedy this we propose self45;specialization 45; allowing for effective model specialization while achieving cross45;task generalization by leveraging only a few labeled seeds. Self45;specialization offers a data45; and parameter45;efficient way of carving out an expert model out of a generalist pre45;trained LLM. Exploring a variety of popular open large models as a base for specialization our experimental results in both biomedical and financial domains show that our self45;specialized models outperform their base models by a large margin and even larger models that are generally instruction45;tuned or that have been adapted to the target domain by other means.
