---
layout: publication
title: 'SAFE: A Sparse Autoencoder-based Framework For Robust Query Enrichment And Hallucination Mitigation In Llms'
authors: Samir Abdaljalil, Filippo Pallucchini, Andrea Seveso, Hasan Kurban, Fabio Mercorio, Erchin Serpedin
conference: "Arxiv"
year: 2025
bibkey: abdaljalil2025sparse
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.03032"}
tags: ['RAG', 'Applications', 'Tools', 'Reinforcement Learning']
---
Despite the state-of-the-art performance of Large Language Models (LLMs),
these models often suffer from hallucinations, which can undermine their
performance in critical applications. In this work, we propose SAFE, a novel
method for detecting and mitigating hallucinations by leveraging Sparse
Autoencoders (SAEs). While hallucination detection techniques and SAEs have
been explored independently, their synergistic application in a comprehensive
system, particularly for hallucination-aware query enrichment, has not been
fully investigated. To validate the effectiveness of SAFE, we evaluate it on
two models with available SAEs across three diverse cross-domain datasets
designed to assess hallucination problems. Empirical results demonstrate that
SAFE consistently improves query generation accuracy and mitigates
hallucinations across all datasets, achieving accuracy improvements of up to
29.45%.
