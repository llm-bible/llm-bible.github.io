---
layout: publication
title: Video45;mme The First45;ever Comprehensive Evaluation Benchmark Of Multi45;modal Llms In Video Analysis
authors: Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, Xing Sun
conference: "Arxiv"
year: 2024
bibkey: fu2024video
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2405.21075v2"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
In the quest for artificial general intelligence Multi45;modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs in processing sequential visual data is still insufficiently explored highlighting the absence of a comprehensive high45;quality assessment of their performance. In this paper we introduce Video45;MME the first45;ever full45;spectrum Multi45;Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features 1) Diversity in video types spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension encompassing both short45; medium45; and long45;term videos ranging from 11 seconds to 1 hour for robust contextual dynamics; 3) Breadth in data modalities integrating multi45;modal inputs besides video frames including subtitles and audios to unveil the all45;round capabilities of MLLMs; 4) Quality in annotations utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. 900 videos with a total of 254 hours are manually selected and annotated by repeatedly viewing all the video content resulting in 2700 question45;answer pairs. With Video45;MME we extensively evaluate various state45;of45;the45;art MLLMs including GPT45;4 series and Gemini 1.5 Pro as well as open45;source image models like InternVL45;Chat45;V1.5 and video models like LLaVA45;NeXT45;Video. Our experiments reveal that Gemini 1.5 Pro is the best45;performing commercial model significantly outperforming the open45;source models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi45;modal data. Project Page https://video&#45;mme.github.io
