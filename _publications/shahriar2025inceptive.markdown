---
layout: publication
title: 'Inceptive Transformers: Enhancing Contextual Representations Through Multi-scale Feature Learning Across Domains And Languages'
authors: Asif Shahriar, Rifat Shahriyar, M Saifur Rahman
conference: "Arxiv"
year: 2025
bibkey: shahriar2025inceptive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.20496"}
tags: ['Transformer', 'Efficiency and Optimization', 'Model Architecture', 'Pretraining Methods']
---
Conventional transformer models typically compress the information from all tokens in a sequence into a single \texttt\{[CLS]\} token to represent global context-- an approach that can lead to information loss in tasks requiring localized or hierarchical cues. In this work, we introduce \textit\{Inceptive Transformer\}, a modular and lightweight architecture that enriches transformer-based token representations by integrating a multi-scale feature extraction module inspired by inception networks. Our model is designed to balance local and global dependencies by dynamically weighting tokens based on their relevance to a particular task. Evaluation across a diverse range of tasks including emotion recognition (both English and Bangla), irony detection, disease identification, and anti-COVID vaccine tweets classification shows that our models consistently outperform the baselines by 1% to 14% while maintaining efficiency. These findings highlight the versatility and cross-lingual applicability of our method for enriching transformer-based representations across diverse domains.
