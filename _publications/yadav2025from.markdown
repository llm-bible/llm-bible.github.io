---
layout: publication
title: 'From Idea To Implementation: Evaluating The Influence Of Large Language Models In Software Development -- An Opinion Paper'
authors: Sargam Yadav, Asifa Mehmood Qureshi, Abhishek Kaushik, Shubham Sharma, Roisin Loughran, Subramaniam Kazhuparambil, Andrew Shaw, Mohammed Sabry, Niamh St John Lynch, . Nikhil Singh, Padraic O'hara, Pranay Jaiswal, Roshan Chandru, David Lillis
conference: "Arxiv"
year: 2025
bibkey: yadav2025from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.07450"}
tags: ['Transformer', 'GPT', 'Applications', 'RAG', 'Model Architecture', 'Pretraining Methods', 'BERT']
---
The introduction of transformer architecture was a turning point in Natural
Language Processing (NLP). Models based on the transformer architecture such as
Bidirectional Encoder Representations from Transformers (BERT) and Generative
Pre-Trained Transformer (GPT) have gained widespread popularity in various
applications such as software development and education. The availability of
Large Language Models (LLMs) such as ChatGPT and Bard to the general public has
showcased the tremendous potential of these models and encouraged their
integration into various domains such as software development for tasks such as
code generation, debugging, and documentation generation. In this study,
opinions from 11 experts regarding their experience with LLMs for software
development have been gathered and analysed to draw insights that can guide
successful and responsible integration. The overall opinion of the experts is
positive, with the experts identifying advantages such as increase in
productivity and reduced coding time. Potential concerns and challenges such as
risk of over-dependence and ethical considerations have also been highlighted.
