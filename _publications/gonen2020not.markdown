---
layout: publication
title: Its Not Greek To Mbert Inducing Word45;level Translations From Multilingual BERT
authors: Gonen Hila, Ravfogel Shauli, Elazar Yanai, Goldberg Yoav
conference: "Arxiv"
year: 2020
bibkey: gonen2020not
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2010.08275"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Tools']
---
Recent works have demonstrated that multilingual BERT (mBERT) learns rich cross45;lingual representations that allow for transfer across languages. We study the word45;level translation information embedded in mBERT and present two simple methods that expose remarkable translation capabilities with no fine45;tuning. The results suggest that most of this information is encoded in a non45;linear way while some of it can also be recovered with purely linear tools. As part of our analysis we test the hypothesis that mBERT learns representations which contain both a language45;encoding component and an abstract cross45;lingual component and explicitly identify an empirical language45;identity subspace within mBERT representations.
