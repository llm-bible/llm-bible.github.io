---
layout: publication
title: 'Codejudge: Evaluating Code Generation With Large Language Models'
authors: Weixi Tong, Tianyi Zhang
conference: "Arxiv"
year: 2024
bibkey: tong2024evaluating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.02184"}
  - {name: "Code", url: "https://github.com/VichyTong/CodeJudge"}
tags: ['Model Architecture', 'Tools', 'RAG', 'GPT', 'Has Code', 'Applications']
---
Large Language Models (LLMs) have shown promising performance in code
generation. However, how to reliably evaluate code generated by LLMs remains an
unresolved problem. This paper presents CodeJudge, a code evaluation framework
that leverages LLMs to evaluate the semantic correctness of generated code
without the need for test cases. We investigate different ways to guide the LLM
in performing "slow thinking" to arrive at an in-depth and reliable evaluation.
We experimented with four LLMs as evaluators on four code generation datasets
and five programming languages. The results show that CodeJudge significantly
outperformed existing methods in most settings. Furthermore, compared with a
SOTA GPT-3.5-based code evaluation method, CodeJudge achieved better results
even when using a much smaller model, Llama-3-8B-Instruct. Our code and
datasets are available on GitHub https://github.com/VichyTong/CodeJudge.
