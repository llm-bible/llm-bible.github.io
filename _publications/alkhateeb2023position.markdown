---
layout: publication
title: 'Position Interpolation Improves Alibi Extrapolation'
authors: Al-khateeb Faisal, Dey Nolan, Soboleva Daria, Hestness Joel
conference: "Arxiv"
year: 2023
bibkey: alkhateeb2023position
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.13017"}
tags: ['Applications', 'Attention Mechanism', 'Ethics And Bias', 'Model Architecture']
---
Linear position interpolation helps pre-trained models using rotary position
embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using
linear position interpolation to extend the extrapolation range of models using
Attention with Linear Biases (ALiBi). We find position interpolation
significantly improves extrapolation capability on upstream language modelling
and downstream summarization and retrieval tasks.
