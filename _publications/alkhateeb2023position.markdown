---
layout: publication
title: 'Position Interpolation Improves Alibi Extrapolation'
authors: Faisal Al-khateeb, Nolan Dey, Daria Soboleva, Joel Hestness
conference: "Arxiv"
year: 2023
bibkey: alkhateeb2023position
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.13017"}
tags: ['Ethics and Bias', 'Model Architecture', 'Applications', 'Attention Mechanism']
---
Linear position interpolation helps pre-trained models using rotary position
embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using
linear position interpolation to extend the extrapolation range of models using
Attention with Linear Biases (ALiBi). We find position interpolation
significantly improves extrapolation capability on upstream language modelling
and downstream summarization and retrieval tasks.
