---
layout: publication
title: 'Llm-evaluation Tropes: Perspectives On The Validity Of Llm-evaluations'
authors: Laura Dietz, Oleg Zendel, Peter Bailey, Charles Clarke, Ellese Cotterill, Jeff Dalton, Faegheh Hasibi, Mark Sanderson, Nick Craswell
conference: "Arxiv"
year: 2025
bibkey: dietz2025llm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.19076"}
tags: ['Tools', 'Ethics and Bias', 'Reinforcement Learning']
---
Large Language Models (LLMs) are increasingly used to evaluate information
retrieval (IR) systems, generating relevance judgments traditionally made by
human assessors. Recent empirical studies suggest that LLM-based evaluations
often align with human judgments, leading some to suggest that human judges may
no longer be necessary, while others highlight concerns about judgment
reliability, validity, and long-term impact. As IR systems begin incorporating
LLM-generated signals, evaluation outcomes risk becoming self-reinforcing,
potentially leading to misleading conclusions.
  This paper examines scenarios where LLM-evaluators may falsely indicate
success, particularly when LLM-based judgments influence both system
development and evaluation. We highlight key risks, including bias
reinforcement, reproducibility challenges, and inconsistencies in assessment
methodologies. To address these concerns, we propose tests to quantify adverse
effects, guardrails, and a collaborative framework for constructing reusable
test collections that integrate LLM judgments responsibly. By providing
perspectives from academia and industry, this work aims to establish best
practices for the principled use of LLMs in IR evaluation.
