---
layout: publication
title: Matryoshka45;adaptor Unsupervised And Supervised Tuning For Smaller Embedding Dimensions
authors: Yoon Jinsung, Sinha Raj, Arik Sercan O, Pfister Tomas
conference: "Arxiv"
year: 2024
bibkey: yoon2024matryoshka
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.20243"}
tags: ['Applications', 'Efficiency And Optimization', 'Model Architecture', 'Multimodal Models', 'Reinforcement Learning', 'Tools']
---
Embeddings from Large Language Models (LLMs) have emerged as critical components in various applications particularly for information retrieval. While high45;dimensional embeddings generally demonstrate superior performance as they contain more salient information their practical application is frequently hindered by elevated computational latency and the associated higher cost. To address these challenges we propose Matryoshka45;Adaptor a novel tuning framework designed for the customization of LLM embeddings. Matryoshka45;Adaptor facilitates substantial dimensionality reduction while maintaining comparable performance levels thereby achieving a significant enhancement in computational efficiency and cost45;effectiveness. Our framework directly modifies the embeddings from pre45;trained LLMs which is designed to be seamlessly integrated with any LLM architecture encompassing those accessible exclusively through black45;box APIs. Also it exhibits efficacy in both unsupervised and supervised learning settings. A rigorous evaluation conducted across a diverse corpus of English multilingual and multimodal datasets consistently reveals substantial gains with Matryoshka45;Adaptor. Notably with Google and OpenAI Embedding APIs Matryoshka45;Adaptor achieves a reduction in dimensionality ranging from two45; to twelve45;fold without compromising performance across multiple BEIR datasets.
