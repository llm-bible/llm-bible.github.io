---
layout: publication
title: 'Exploring The Role Of Explicit Temporal Modeling In Multimodal Large Language Models For Video Understanding'
authors: Yun Li, Zhe Liu, Yajing Kong, Guangrui Li, Jiyuan Zhang, Chao Bian, Feng Liu, Lina Yao, Zhenbang Sun
conference: "Arxiv"
year: 2025
bibkey: li2025exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.16786"}
tags: ['Multimodal Models']
---
Applying Multimodal Large Language Models (MLLMs) to video understanding
presents significant challenges due to the need to model temporal relations
across frames. Existing approaches adopt either implicit temporal modeling,
relying solely on the LLM decoder, or explicit temporal modeling, employing
auxiliary temporal encoders. To investigate this debate between the two
paradigms, we propose the Stackable Temporal Encoder (STE). STE enables
flexible explicit temporal modeling with adjustable temporal receptive fields
and token compression ratios. Using STE, we systematically compare implicit and
explicit temporal modeling across dimensions such as overall performance, token
compression effectiveness, and temporal-specific understanding. We also explore
STE's design considerations and broader impacts as a plug-in module and in
image modalities. Our findings emphasize the critical role of explicit temporal
modeling, providing actionable insights to advance video MLLMs.
