---
layout: publication
title: 'Multi-user Chat Assistant (MUCA): A Framework Using Llms To Facilitate Group Conversations'
authors: Manqing Mao, Paishun Ting, Yijian Xiang, Mingyang Xu, Julia Chen, Jianzhe Lin
conference: "Arxiv"
year: 2024
bibkey: mao2024multi
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2401.04883'}
tags: ['Reinforcement Learning', 'Efficiency and Optimization', 'Tools']
---
Recent advancements in large language models (LLMs) have provided a new
avenue for chatbot development. Most existing research, however, has primarily
centered on single-user chatbots that determine "What" to answer. This paper
highlights the complexity of multi-user chatbots, introducing the 3W design
dimensions: "What" to say, "When" to respond, and "Who" to answer.
Additionally, we proposed Multi-User Chat Assistant (MUCA), an LLM-based
framework tailored for group discussions. MUCA consists of three main modules:
Sub-topic Generator, Dialog Analyzer, and Conversational Strategies Arbitrator.
These modules jointly determine suitable response contents, timings, and
appropriate addressees. This paper further proposes an LLM-based Multi-User
Simulator (MUS) to ease MUCA's optimization, enabling faster simulation of
conversations between the chatbot and simulated users, and speeding up MUCA's
early development. In goal-oriented conversations with a small to medium number
of participants, MUCA demonstrates effectiveness in tasks like chiming in at
appropriate timings, generating relevant content, and improving user
engagement, as shown by case studies and user studies.
