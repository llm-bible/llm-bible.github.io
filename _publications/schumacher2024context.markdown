---
layout: publication
title: 'Context Matters: An Empirical Study Of The Impact Of Contextual Information In Temporal Question Answering Systems'
authors: Dan Schumacher, Fatemeh Haji, Tara Grey, Niharika Bandlamudi, Nupoor Karnik, Gagana Uday Kumar, Jason Cho-yu Chiang, Paul Rad, Nishant Vishwamitra, Anthony Rios
conference: "Arxiv"
year: 2024
bibkey: schumacher2024context
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.19538"}
tags: ['Security', 'Training Techniques', 'Applications']
---
Large language models (LLMs) often struggle with temporal reasoning, crucial
for tasks like historical event analysis and time-sensitive information
retrieval. Despite advancements, state-of-the-art models falter in handling
temporal information, especially when faced with irrelevant or noisy contexts.
This paper addresses this gap by empirically examining the robustness of
temporal question-answering (TQA) systems trained on various context types,
including relevant, irrelevant, slightly altered, and no context. Our findings
indicate that training with a mix of these contexts enhances model robustness
and accuracy. Additionally, we show that the position of context relative to
the question significantly impacts performance, with question-first positioning
yielding better results. We introduce two new context-rich TQA datasets,
ContextAQA and ContextTQE, and provide comprehensive evaluations and guidelines
for training robust TQA models. Our work lays the foundation for developing
reliable and context-aware temporal QA systems, with broader implications for
enhancing LLM robustness against diverse and potentially adversarial
information.
