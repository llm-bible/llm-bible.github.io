---
layout: publication
title: Uncertainty45;aware Evaluation For Vision45;language Models
authors: Kostumov Vasily, Nutfullin Bulat, Pilipenko Oleg, Ilyushin Eugene
conference: "Arxiv"
year: 2024
bibkey: kostumov2024uncertainty
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.14418"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Reinforcement Learning']
---
Vision45;Language Models like GPT45;4 LLaVA and CogVLM have surged in popularity recently due to their impressive performance in several vision45;language tasks. Current evaluation methods however overlook an essential component uncertainty which is crucial for a comprehensive assessment of VLMs. Addressing this oversight we present a benchmark incorporating uncertainty quantification into evaluating VLMs. Our analysis spans 20+ VLMs focusing on the multiple45;choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision45;language capabilities. Using conformal prediction as an uncertainty estimation approach we demonstrate that the models uncertainty is not aligned with their accuracy. Specifically we show that models with the highest accuracy may also have the highest uncertainty which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.
