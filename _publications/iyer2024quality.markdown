---
layout: publication
title: Quality Or Quantity On Data Scale And Diversity In Adapting Large Language Models For Low45;resource Translation
authors: Iyer Vivek, Malik Bhavitvya, Stepachev Pavel, Chen Pinzhen, Haddow Barry, Birch Alexandra
conference: "Arxiv"
year: 2024
bibkey: iyer2024quality
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.12780"}
tags: ['Applications', 'Fine Tuning', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
Despite the recent popularity of Large Language Models (LLMs) in Machine Translation (MT) their performance in low45;resource translation still lags significantly behind Neural Machine Translation (NMT) models. In this paper we explore what it would take to adapt LLMs for low45;resource settings. In particular we re45;examine the role of two factors a) the importance and application of parallel data and b) diversity in Supervised Fine45;Tuning (SFT). Recently parallel data has been shown to be less important for MT using LLMs than in previous MT research. Similarly diversity during SFT has been shown to promote significant transfer in LLMs across languages and tasks. However for low45;resource LLM45;MT we show that the opposite is true for both of these considerations a) parallel data is critical during both pretraining and SFT and b) diversity tends to cause interference not transfer. Our experiments conducted with 3 LLMs across 2 low45;resourced language groups 45; indigenous American and North45;East Indian 45; reveal consistent patterns in both cases underscoring the generalizability of our findings. We believe these insights will be valuable for scaling to massively multilingual LLM45;MT models that can effectively serve lower45;resource languages.
