---
layout: publication
title: Large Language Models A Survey
authors: Minaee Shervin, Mikolov Tomas, Nikzad Narjes, Chenaghlu Meysam, Socher Richard, Amatriain Xavier, Gao Jianfeng
conference: "Arxiv"
year: 2024
bibkey: minaee2024large
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.06196"}
tags: ['Attention Mechanism', 'Efficiency And Optimization', 'Fine Tuning', 'GPT', 'Large Scale Training', 'Model Architecture', 'Pretraining Methods', 'Scaling Laws', 'Survey Paper', 'Tools', 'Training Techniques']
---
Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks since the release of ChatGPT in November 2022. LLMs ability of general-purpose language understanding and generation is acquired by training billions of models parameters on massive amounts of text data as predicted by scaling laws citekaplan2020scalinghoffmann2022training. The research area of LLMs while very recent is evolving rapidly in many different ways. In this paper we review some of the most prominent LLMs including three popular LLM families (GPT LLaMA PaLM) and discuss their characteristics contributions and limitations. We also give an overview of techniques developed to build and augment LLMs. We then survey popular datasets prepared for LLM training fine-tuning and evaluation review widely used LLM evaluation metrics and compare the performance of several popular LLMs on a set of representative benchmarks. Finally we conclude the paper by discussing open challenges and future research directions.
