---
layout: publication
title: Review45;feedback45;reason (refer) A Novel Framework For NLG Evaluation And Reasoning
authors: Narsupalli Yaswanth, Chandra Abhranil, Muppirala Sreevatsa, Gupta Manish, Goyal Pawan
conference: "Arxiv"
year: 2024
bibkey: narsupalli2024review
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.12877"}
tags: ['Agentic', 'Ethics And Bias', 'GPT', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools']
---
Assessing the quality of Natural Language Generation (NLG) outputs such as those produced by large language models (LLMs) poses significant challenges. Traditional approaches involve either resource45;intensive human evaluations or automatic metrics which often exhibit a low correlation with human judgment. In this study we propose Review45;Feedback45;Reason (ReFeR) a novel evaluation framework for NLG using LLM agents. We rigorously test ReFeR using two pre45;existing benchmark datasets on diverse NLG tasks. The proposed framework not only enhances the accuracy of NLG evaluation surpassing previous benchmarks by sim2037; but also generates constructive feedback and significantly improves collective reasoning. This feedback is then leveraged for the creation of instruction45;tuning datasets which when used to fine45;tune smaller models like Mistral45;7B makes them extremely good evaluators yielding a better correlation with human evaluations and performance nearly on par with GPT45;3.5. We highlight the effectiveness of our methodology through its application on three reasoning benchmarks where it outperforms most of the state45;of45;the45;art methods and also outperforms the reasoning capabilities of models like GPT45;3.5 Turbo by sim11.6737; and GPT45;4 by sim137; on an average.
