---
layout: publication
title: 'Review-feedback-reason (refer): A Novel Framework For NLG Evaluation And Reasoning'
authors: Narsupalli Yaswanth, Chandra Abhranil, Muppirala Sreevatsa, Gupta Manish, Goyal Pawan
conference: "Arxiv"
year: 2024
bibkey: narsupalli2024review
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.12877"}
tags: ['Agentic', 'Ethics And Bias', 'Fine Tuning', 'GPT', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Survey Paper', 'Tools']
---
'Assessing the quality of Natural Language Generation (NLG) outputs, such as those produced by large language models (LLMs), poses significant challenges. Traditional approaches involve either resource-intensive human evaluations or automatic metrics, which often exhibit a low correlation with human judgment. In this study, we propose Review-Feedback-Reason (ReFeR), a novel evaluation framework for NLG using LLM agents. We rigorously test ReFeR using two pre-existing benchmark datasets on diverse NLG tasks. The proposed framework not only enhances the accuracy of NLG evaluation, surpassing previous benchmarks by \(\sim\)20\&#37;, but also generates constructive feedback and significantly improves collective reasoning. This feedback is then leveraged for the creation of instruction-tuning datasets, which, when used to fine-tune smaller models like Mistral-7B, makes them extremely good evaluators, yielding a better correlation with human evaluations and performance nearly on par with GPT-3.5. We highlight the effectiveness of our methodology through its application on three reasoning benchmarks, where it outperforms most of the state-of-the-art methods, and also outperforms the reasoning capabilities of models like GPT-3.5 Turbo by \(\sim\)11.67\&#37; and GPT-4 by \(\sim\)1\&#37; on an average.'
