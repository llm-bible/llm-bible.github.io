---
layout: publication
title: 'Natural Language To Code Using Transformers'
authors: Uday Kusupati, Venkata Ravi Teja Ailavarapu
conference: "Arxiv"
year: 2022
bibkey: kusupati2022natural
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2202.00367"}
tags: ['Pretraining Methods', 'Model Architecture', 'Transformer', 'Attention Mechanism']
---
We tackle the problem of generating code snippets from natural language
descriptions using the CoNaLa dataset. We use the self-attention based
transformer architecture and show that it performs better than recurrent
attention-based encoder decoder. Furthermore, we develop a modified form of
back translation and use cycle consistent losses to train the model in an
end-to-end fashion. We achieve a BLEU score of 16.99 beating the previously
reported baseline of the CoNaLa challenge.
