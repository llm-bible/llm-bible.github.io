---
layout: publication
title: Natural Language to Code Using Transformers
authors: Kusupati Uday, Ailavarapu Venkata Ravi Teja
conference: "Arxiv"
year: 2022
bibkey: kusupati2022natural
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2202.00367"}
tags: ['ARXIV', 'Model Architecture', 'Transformer']
---
We tackle the problem of generating code snippets from natural language descriptions using the CoNaLa dataset. We use the self-attention based transformer architecture and show that it performs better than recurrent attention-based encoder decoder. Furthermore we develop a modified form of back translation and use cycle consistent losses to train the model in an end-to-end fashion. We achieve a BLEU score of 16.99 beating the previously reported baseline of the CoNaLa challenge.
