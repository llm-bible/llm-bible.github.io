---
layout: publication
title: 'A Survey On Uncertainty Quantification Of Large Language Models: Taxonomy, Open Research Challenges, And Future Directions'
authors: Ola Shorinwa, Zhiting Mei, Justin Lidard, Allen Z. Ren, Anirudha Majumdar
conference: "Arxiv"
year: 2024
bibkey: shorinwa2024survey
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.05563'}
tags: ['Prompting', 'Applications', 'Survey Paper']
---
The remarkable performance of large language models (LLMs) in content
generation, coding, and common-sense reasoning has spurred widespread
integration into many facets of society. However, integration of LLMs raises
valid questions on their reliability and trustworthiness, given their
propensity to generate hallucinations: plausible, factually-incorrect
responses, which are expressed with striking confidence. Previous work has
shown that hallucinations and other non-factual responses generated by LLMs can
be detected by examining the uncertainty of the LLM in its response to the
pertinent prompt, driving significant research efforts devoted to quantifying
the uncertainty of LLMs. This survey seeks to provide an extensive review of
existing uncertainty quantification methods for LLMs, identifying their salient
features, along with their strengths and weaknesses. We present existing
methods within a relevant taxonomy, unifying ostensibly disparate methods to
aid understanding of the state of the art. Furthermore, we highlight
applications of uncertainty quantification methods for LLMs, spanning chatbot
and textual applications to embodied artificial intelligence applications in
robotics. We conclude with open research challenges in uncertainty
quantification of LLMs, seeking to motivate future research.
