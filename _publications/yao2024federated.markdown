---
layout: publication
title: 'Federated Large Language Models: Current Progress And Future Directions'
authors: Yuhang Yao, Jianyi Zhang, Junda Wu, Chengkai Huang, Yu Xia, Tong Yu, Ruiyi Zhang, Sungchul Kim, Ryan Rossi, Ang Li, Lina Yao, Julian Mcauley, Yiran Chen, Carlee Joe-wong
conference: "Arxiv"
year: 2024
bibkey: yao2024federated
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.15723"}
tags: ['Multimodal Models', 'Training Techniques', 'Survey Paper', 'Tools', 'Reinforcement Learning', 'Pretraining Methods', 'Fine-Tuning', 'Prompting', 'Pre-Training', 'Applications']
---
Large language models are rapidly gaining popularity and have been widely
adopted in real-world applications. While the quality of training data is
essential, privacy concerns arise during data collection. Federated learning
offers a solution by allowing multiple clients to collaboratively train LLMs
without sharing local data. However, FL introduces new challenges, such as
model convergence issues due to heterogeneous data and high communication
costs. A comprehensive study is required to address these challenges and guide
future research. This paper surveys Federated learning for LLMs (FedLLM),
highlighting recent advances and future directions. We focus on two key
aspects: fine-tuning and prompt learning in a federated setting, discussing
existing work and associated research challenges. We finally propose potential
research directions for federated LLMs, including pre-training and how LLMs can
further enhance federated learning.
