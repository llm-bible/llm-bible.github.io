---
layout: publication
title: Llms Achieve Adult Human Performance On Higher45;order Theory Of Mind Tasks
authors: Street Winnie, Siy John Oliver, Keeling Geoff, Baranes Adrien, Barnett Benjamin, Mckibben Michael, Kanyere Tatenda, Lentz Alison, Arcas Blaise Aguera Y, Dunbar Robin I. M.
conference: "Arxiv"
year: 2024
bibkey: street2024llms
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.18870"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Pretraining Methods']
---
This paper examines the extent to which large language models (LLMs) have developed higher45;order theory of mind (ToM); the human ability to reason about multiple mental and emotional states in a recursive manner (e.g. I think that you believe that she knows). This paper builds on prior work by introducing a handwritten test suite 45;45; Multi45;Order Theory of Mind Qamp;A 45;45; and using it to compare the performance of five LLMs to a newly gathered adult human benchmark. We find that GPT45;4 and Flan45;PaLM reach adult45;level and near adult45;level performance on ToM tasks overall and that GPT45;4 exceeds adult performance on 6th order inferences. Our results suggest that there is an interplay between model size and finetuning for the realisation of ToM abilities and that the best45;performing LLMs have developed a generalised capacity for ToM. Given the role that higher45;order ToM plays in a wide range of cooperative and competitive human behaviours these findings have significant implications for user45;facing LLM applications.
