---
layout: publication
title: Demystifying Embedding Spaces Using Large Language Models
authors: Tennenholtz Guy, Chow Yinlam, Hsu Chih-wei, Jeong Jihwan, Shani Lior, Tulepbergenov Azamat, Ramachandran Deepak, Mladenov Martin, Boutilier Craig
conference: "Arxiv"
year: 2023
bibkey: tennenholtz2023demystifying
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.04475"}
tags: ['Fine Tuning', 'Interpretability And Explainability', 'Pretraining Methods']
---
Embeddings have become a pivotal means to represent complex multi45;faceted information about entities concepts and relationships in a condensed and useful format. Nevertheless they often preclude direct interpretation. While downstream tasks make use of these compressed representations meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful by employing Large Language Models (LLMs) to directly interact with embeddings 45;45; transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks including enhancing concept activation vectors (CAVs) communicating novel embedded entities and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs.
