---
layout: publication
title: 'Demystifying Embedding Spaces Using Large Language Models'
authors: Guy Tennenholtz, Yinlam Chow, Chih-wei Hsu, Jihwan Jeong, Lior Shani, Azamat Tulepbergenov, Deepak Ramachandran, Martin Mladenov, Craig Boutilier
conference: "Arxiv"
year: 2023
bibkey: tennenholtz2023demystifying
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2310.04475'}
tags: ['Fine-Tuning', 'RecSys', 'Interpretability and Explainability']
---
Embeddings have become a pivotal means to represent complex, multi-faceted
information about entities, concepts, and relationships in a condensed and
useful format. Nevertheless, they often preclude direct interpretation. While
downstream tasks make use of these compressed representations, meaningful
interpretation usually requires visualization using dimensionality reduction or
specialized machine learning interpretability methods. This paper addresses the
challenge of making such embeddings more interpretable and broadly useful, by
employing Large Language Models (LLMs) to directly interact with embeddings --
transforming abstract vectors into understandable narratives. By injecting
embeddings into LLMs, we enable querying and exploration of complex embedding
data. We demonstrate our approach on a variety of diverse tasks, including:
enhancing concept activation vectors (CAVs), communicating novel embedded
entities, and decoding user preferences in recommender systems. Our work
couples the immense information potential of embeddings with the interpretative
power of LLMs.
