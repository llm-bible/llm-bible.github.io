---
layout: publication
title: 'Language Models Hallucinate, But May Excel At Fact Verification'
authors: Guan Jian, Dodge Jesse, Wadden David, Huang Minlie, Peng Hao
conference: "Arxiv"
year: 2023
bibkey: guan2023language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.14564"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Security']
---
Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless LLMs frequently hallucinate resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue revealing that even GPT-3.5 produces factual outputs less than 2537; of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments. Surprisingly FLAN-T5-11B the least factual generator in our study performs the best as a fact verifier even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper we analyze the reliance of these LLMs on high-quality evidence as well as their deficiencies in robustness and generalization ability. Our study presents insights for developing trustworthy generation models.
