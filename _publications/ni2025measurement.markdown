---
layout: publication
title: Measurement Of Llm's Philosophies Of Human Nature
authors: Minheng Ni et al.
conference: Arxiv
year: 2025
citations: 179
bibkey: ni2025measurement
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2504.02304'}, {name: Code,
    url: 'https://github.com/kodenii/M-PHNS'}]
tags: [Ethics and Bias, Prompting]
---
The widespread application of artificial intelligence (AI) in various tasks,
along with frequent reports of conflicts or violations involving AI, has
sparked societal concerns about interactions with AI systems. Based on
Wrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically
validated over decades to effectively assess individuals' attitudes toward
human nature, we design the standardized psychological scale specifically
targeting large language models (LLM), named the Machine-based Philosophies of
Human Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature
across six dimensions, we reveal that current LLMs exhibit a systemic lack of
trust in humans, and there is a significant negative correlation between the
model's intelligence level and its trust in humans. Furthermore, we propose a
mental loop learning framework, which enables LLM to continuously optimize its
value system during virtual interactions by constructing moral scenarios,
thereby improving its attitude toward human nature. Experiments demonstrate
that mental loop learning significantly enhances their trust in humans compared
to persona or instruction prompts. This finding highlights the potential of
human-based psychological assessments for LLM, which can not only diagnose
cognitive biases but also provide a potential solution for ethical learning in
artificial intelligence. We release the M-PHNS evaluation code and data at
https://github.com/kodenii/M-PHNS.