---
layout: publication
title: Benchmarking Chinese Commonsense Reasoning Of Llms From Chinese45;specifics To Reasoning45;memorization Correlations
authors: Sun Jiaxing, Huang Weiquan, Wu Jiang, Gu Chenya, Li Wei, Zhang Songyang, Yan Hang, He Conghui
conference: "Arxiv"
year: 2024
bibkey: sun2024benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.14112"}
  - {name: "Code", url: "https://github.com/opendatalab/CHARM"}
tags: ['Efficiency And Optimization', 'Has Code', 'Prompting', 'Training Techniques']
---
We introduce CHARM the first benchmark for comprehensively and in45;depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese which covers both globally known and Chinese45;specific commonsense. We evaluated 7 English and 12 Chinese45;oriented LLMs on CHARM employing 5 representative prompt strategies for improving LLMs reasoning ability such as Chain45;of45;Thought. Our findings indicate that the LLMs language orientation and the tasks domain influence the effectiveness of the prompt strategy which enriches previous research findings. We built closely45;interconnected reasoning and memorization tasks and found that some LLMs struggle with memorizing Chinese commonsense affecting their reasoning ability while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs memorization45;independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs strengths and weaknesses providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM .
