---
layout: publication
title: Knowledge-grounded Dialogue Generation With Pre-trained Language Models
authors: Xueliang Zhao et al.
conference: Arxiv
year: 2020
citations: 56
bibkey: zhao2020knowledge
additional_links:
- name: Paper
  url: https://arxiv.org/abs/2010.08824
tags:
- RAG
- Pre-Training
---
We study knowledge-grounded dialogue generation with pre-trained language
models. To leverage the redundant external knowledge under capacity constraint,
we propose equipping response generation defined by a pre-trained language
model with a knowledge selection module, and an unsupervised approach to
jointly optimizing knowledge selection and response generation with unlabeled
dialogues. Empirical results on two benchmarks indicate that our model can
significantly outperform state-of-the-art methods in both automatic evaluation
and human judgment.