---
layout: publication
title: Solving Quantitative Reasoning Problems With Language Models
authors: Aitor Lewkowycz et al.
conference: Arxiv
year: 2022
citations: 232
bibkey: lewkowycz2022solving
additional_links:
- name: Paper
  url: https://arxiv.org/abs/2206.14858
tags:
- Pre-Training
- Reinforcement Learning
- Language Modeling
---
Language models have achieved remarkable performance on a wide range of tasks
that require natural language understanding. Nevertheless, state-of-the-art
models have generally struggled with tasks that require quantitative reasoning,
such as solving mathematics, science, and engineering problems at the college
level. To help close this gap, we introduce Minerva, a large language model
pretrained on general natural language data and further trained on technical
content. The model achieves state-of-the-art performance on technical
benchmarks without the use of external tools. We also evaluate our model on
over two hundred undergraduate-level problems in physics, biology, chemistry,
economics, and other sciences that require quantitative reasoning, and find
that the model can correctly answer nearly a third of them.