---
layout: publication
title: 'Better Question-answering Models On A Budget'
authors: Wijeratne Yudhanjaya, Marikar Ishan
conference: "Arxiv"
year: 2023
bibkey: wijeratne2023better
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.12370"}
tags: ['Applications', 'Fine Tuning', 'GPT', 'Model Architecture', 'Prompting']
---
Low-rank adaptation (LoRA) and question-answer datasets from large language models have made it much easier for much smaller models to be finetuned to the point where they display sophisticated conversational abilities. In this paper we present Eluwa a family of LoRA models that use the Stanford Alpaca dataset and massively improve the capabilities of Facebooks OPT 1.3B 2.7B and 6.7B models. We benchmark these models in multiple ways including letting GPT-4 judge their answers to prompts that span general knowledge writing programming and other tasks. We show that smaller models here can be fine-tuned to be as performant as models 3x larger - all for as little as 40 USD in compute.
