---
layout: publication
title: 'Red Teaming Chatgpt Via Jailbreaking: Bias, Robustness, Reliability And Toxicity'
authors: Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing
conference: Arxiv
year: 2023
citations: 139
bibkey: zhuo2023red
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2301.12867'}]
tags: [Applications, Security, Ethics and Bias, GPT]
---
Recent breakthroughs in natural language processing (NLP) have permitted the
synthesis and comprehension of coherent text in an open-ended way, therefore
translating the theoretical algorithms into practical applications. The large
language models (LLMs) have significantly impacted businesses such as report
summarization software and copywriters. Observations indicate, however, that
LLMs may exhibit social prejudice and toxicity, posing ethical and societal
dangers of consequences resulting from irresponsibility. Large-scale benchmarks
for accountable LLMs should consequently be developed. Although several
empirical investigations reveal the existence of a few ethical difficulties in
advanced LLMs, there is little systematic examination and user study of the
risks and harmful behaviors of current LLM usage. To further educate future
efforts on constructing ethical LLMs responsibly, we perform a qualitative
research method called ``red teaming'' on OpenAI's ChatGPT\footnote\{In this
paper, ChatGPT refers to the version released on Dec 15th.\} to better
understand the practical features of ethical dangers in recent LLMs. We analyze
ChatGPT comprehensively from four perspectives: 1) \textit\{Bias\} 2)
\textit\{Reliability\} 3) \textit\{Robustness\} 4) \textit\{Toxicity\}. In accordance
with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample
datasets. We find that a significant number of ethical risks cannot be
addressed by existing benchmarks, and hence illustrate them via additional case
studies. In addition, we examine the implications of our findings on AI ethics
and harmal behaviors of ChatGPT, as well as future problems and practical
design considerations for responsible LLMs. We believe that our findings may
give light on future efforts to determine and mitigate the ethical hazards
posed by machines in LLM applications.