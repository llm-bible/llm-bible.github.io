---
layout: publication
title: Red Teaming Chatgpt Via Jailbreaking Bias Robustness Reliability And Toxicity
authors: Zhuo Terry Yue, Huang Yujin, Chen Chunyang, Xing Zhenchang
conference: "Arxiv"
year: 2023
bibkey: zhuo2023red
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2301.12867"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Model Architecture', 'Responsible AI', 'Security']
---
Recent breakthroughs in natural language processing (NLP) have permitted the synthesis and comprehension of coherent text in an open45;ended way therefore translating the theoretical algorithms into practical applications. The large language models (LLMs) have significantly impacted businesses such as report summarization software and copywriters. Observations indicate however that LLMs may exhibit social prejudice and toxicity posing ethical and societal dangers of consequences resulting from irresponsibility. Large45;scale benchmarks for accountable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difficulties in advanced LLMs there is little systematic examination and user study of the risks and harmful behaviors of current LLM usage. To further educate future efforts on constructing ethical LLMs responsibly we perform a qualitative research method called red teaming on OpenAIs ChatGPTfootnote123;In this paper ChatGPT refers to the version released on Dec 15th.125; to better understand the practical features of ethical dangers in recent LLMs. We analyze ChatGPT comprehensively from four perspectives 1) textit123;Bias125; 2) textit123;Reliability125; 3) textit123;Robustness125; 4) textit123;Toxicity125;. In accordance with our stated viewpoints we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks and hence illustrate them via additional case studies. In addition we examine the implications of our findings on AI ethics and harmal behaviors of ChatGPT as well as future problems and practical design considerations for responsible LLMs. We believe that our findings may give light on future efforts to determine and mitigate the ethical hazards posed by machines in LLM applications.
