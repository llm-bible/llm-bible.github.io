---
layout: publication
title: "Openllm-ro -- Technical Report On Open-source Romanian Llms"
authors: Masala Mihai, Ilie-ablachim Denis C., Corlatescu Dragos, Zavelca Miruna, Leordeanu Marius, Velicu Horia, Popescu Marius, Dascalu Mihai, Rebedea Traian
conference: "Arxiv"
year: 2024
bibkey: masala2024openllm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.07703"}
tags: ['Pretraining Methods', 'Training Techniques']
---
In recent years Large Language Models (LLMs) have achieved almost human-like performance on various tasks. While some LLMs have been trained on multilingual data most of the training data is in English. Hence their performance in English greatly exceeds their performance in other languages. This document presents our approach to training and evaluating the first foundational and chat LLM specialized for Romanian.
