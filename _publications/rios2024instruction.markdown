---
layout: publication
title: Instruction-tuned Large Language Models For Machine Translation In The Medical Domain
authors: Rios Miguel
conference: "Arxiv"
year: 2024
bibkey: rios2024instruction
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.16440"}
tags: ['Applications', 'Fine Tuning', 'Pretraining Methods', 'Training Techniques']
---
Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However in specialised domains (e.g. medical) LLMs have shown lower performance compared to standard neural machine translation models. The consistency in the machine translation of terminology is crucial for users researchers and translators in specialised domains. In this study we compare the performance between baseline LLMs and instruction-tuned LLMs in the medical domain. In addition we introduce terminology from specialised medical dictionaries into the instruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs significantly outperform the baseline models with automatic metrics.
