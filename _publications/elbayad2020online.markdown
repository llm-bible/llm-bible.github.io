---
layout: publication
title: 'Online Versus Offline NMT Quality: An In-depth Analysis On English-german And German-english'
authors: Maha Elbayad, Michael Ustaszewski, Emmanuelle Esperan√ßa-rodier, Francis Brunet Manquat, Jakob Verbeek, Laurent Besacier
conference: "Arxiv"
year: 2020
bibkey: elbayad2020online
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2006.00814'}
tags: ['Attention Mechanism', 'Transformer', 'Model Architecture', 'Applications', 'Reinforcement Learning', 'Pretraining Methods']
---
We conduct in this work an evaluation study comparing offline and online
neural machine translation architectures. Two sequence-to-sequence models:
convolutional Pervasive Attention (Elbayad et al. 2018) and attention-based
Transformer (Vaswani et al. 2017) are considered. We investigate, for both
architectures, the impact of online decoding constraints on the translation
quality through a carefully designed human evaluation on English-German and
German-English language pairs, the latter being particularly sensitive to
latency constraints. The evaluation results allow us to identify the strengths
and shortcomings of each model when we shift to the online setup.
