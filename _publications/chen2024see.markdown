---
layout: publication
title: See What Llms Cannot Answer A Self45;challenge Framework For Uncovering LLM Weaknesses
authors: Chen Yulong, Liu Yang, Yan Jianhao, Bai Xuefeng, Zhong Ming, Yang Yinghao, Yang Ziyi, Zhu Chenguang, Zhang Yue
conference: "Arxiv"
year: 2024
bibkey: chen2024see
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.08978"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Tools']
---
The impressive performance of Large Language Models (LLMs) has consistently surpassed numerous human45;designed benchmarks presenting new challenges in assessing the shortcomings of LLMs. Designing tasks and finding LLMs limitations are becoming increasingly important. In this paper we investigate the question of whether an LLM can discover its own limitations from the errors it makes. To this end we propose a Self45;Challenge evaluation framework with human45;in45;the45;loop. Starting from seed instances that GPT45;4 fails to answer we prompt GPT45;4 to summarize error patterns that can be used to generate new instances and incorporate human feedback on them to refine these patterns for generating more challenging data iteratively. We end up with 8 diverse patterns such as text manipulation and questions with assumptions. We then build a benchmark SC45;G4 consisting of 1835 instances generated by GPT45;4 using these patterns with human45;annotated gold responses. The SC45;G4 serves as a challenging benchmark that allows for a detailed assessment of LLMs abilities. Our results show that only 44.9637; of instances in SC45;G4 can be answered correctly by GPT45;4. Interestingly our pilot study indicates that these error patterns also challenge other LLMs such as Claude45;3 and Llama45;3 and cannot be fully resolved through fine45;tuning. Our work takes the first step to demonstrate that LLMs can autonomously identify their inherent flaws and provide insights for future dynamic and automatic evaluation.
