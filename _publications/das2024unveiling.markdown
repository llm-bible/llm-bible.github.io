---
layout: publication
title: 'Unveiling And Mitigating Bias In Large Language Model Recommendations: A Path To Fairness'
authors: Anindya Bijoy Das, Shahnewaz Karim Sakib
conference: "Arxiv"
year: 2024
bibkey: das2024unveiling
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.10825"}
tags: ['Training Techniques', 'Fairness', 'Model Architecture', 'RAG', 'GPT', 'Bias Mitigation', 'Ethics and Bias', 'Prompting']
---
excel in delivering comprehensive suggestions by deeply analyzing content and
user behavior. However, they often inherit biases from skewed training data,
favoring mainstream content while underrepresenting diverse or non-traditional
options. This study explores the interplay between bias and LLM-based
recommendation systems, focusing on music, song, and book recommendations
across diverse demographic and cultural groups. This paper analyzes bias in
LLM-based recommendation systems across multiple models (GPT, LLaMA, and
Gemini), revealing its deep and pervasive impact on outcomes. Intersecting
identities and contextual factors, like socioeconomic status, further amplify
biases, complicating fair recommendations across diverse groups. Our findings
reveal that bias in these systems is deeply ingrained, yet even simple
interventions like prompt engineering can significantly reduce it. We further
propose a retrieval-augmented generation strategy to mitigate bias more
effectively. Numerical experiments validate these strategies, demonstrating
both the pervasive nature of bias and the impact of the proposed solutions.
