---
layout: publication
title: Cogview2 Faster And Better Text45;to45;image Generation Via Hierarchical Transformers
authors: Ding Ming, Zheng Wendi, Hong Wenyi, Tang Jie
conference: "Arxiv"
year: 2022
bibkey: ding2022faster
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2204.14217"}
tags: ['Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
The development of the transformer45;based text45;to45;image models are impeded by its slow generation and complexity for high45;resolution images. In this work we put forward a solution based on hierarchical transformers and local parallel auto45;regressive generation. We pretrain a 6B45;parameter transformer with a simple and flexible self45;supervised task Cross45;modal general language model (CogLM) and finetune it for fast super45;resolution. The new text45;to45;image system CogView2 shows very competitive generation compared to concurrent state45;of45;the45;art DALL45;E45;2 and naturally supports interactive text45;guided editing on images.
