---
layout: publication
title: 'The Representation And Recall Of Interwoven Structured Knowledge In Llms: A Geometric And Layered Analysis'
authors: Ge Lei, Samuel J. Cooper
conference: "Arxiv"
year: 2025
bibkey: lei2025representation
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.10871"}
tags: ['Model Architecture', 'Reinforcement Learning', 'Pretraining Methods', 'Transformer', 'Interpretability and Explainability', 'Prompting']
---
This study investigates how large language models (LLMs) represent and recall
multi-associated attributes across transformer layers. We show that
intermediate layers encode factual knowledge by superimposing related
attributes in overlapping spaces, along with effective recall even when
attributes are not explicitly prompted. In contrast, later layers refine
linguistic patterns and progressively separate attribute representations,
optimizing task-specific outputs while appropriately narrowing attribute
recall. We identify diverse encoding patterns including, for the first time,
the observation of 3D spiral structures when exploring information related to
the periodic table of elements. Our findings reveal a dynamic transition in
attribute representations across layers, contributing to mechanistic
interpretability and providing insights for understanding how LLMs handle
complex, interrelated knowledge.
