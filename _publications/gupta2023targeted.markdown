---
layout: publication
title: Targen Targeted Data Generation With Large Language Models
authors: Gupta Himanshu, Scaria Kevin, Anantheswaran Ujjwala, Verma Shreyas, Parmar Mihir, Sawant Saurabh Arjun, Baral Chitta, Mishra Swaroop
conference: "Arxiv"
year: 2023
bibkey: gupta2023targeted
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.17876"}
tags: ['Ethics And Bias', 'Prompting', 'Tools', 'Training Techniques']
---
The rapid advancement of large language models (LLMs) has sparked interest in data synthesis techniques aiming to generate diverse and high45;quality synthetic datasets. However these synthetic datasets often suffer from a lack of diversity and added noise. In this paper we present TarGEN a multi45;step prompting strategy for generating high45;quality synthetic datasets utilizing a LLM. An advantage of TarGEN is its seedless nature; it does not require specific task instances broadening its applicability beyond task replication. We augment TarGEN with a method known as self45;correction empowering LLMs to rectify inaccurately labeled instances during dataset creation ensuring reliable labels. To assess our techniques effectiveness we emulate 8 tasks from the SuperGLUE benchmark and finetune various language models including encoder45;only encoder45;decoder and decoder45;only models on both synthetic and original training sets. Evaluation on the original test set reveals that models trained on datasets generated by TarGEN perform approximately 145;237; points better than those trained on original datasets (82.8437; via syn. vs. 81.1237; on og. using Flan45;T5). When incorporating instruction tuning the performance increases to 84.5437; on synthetic data vs. 81.4937; on original data by Flan45;T5. A comprehensive analysis of the synthetic dataset compared to the original dataset reveals that the synthetic dataset demonstrates similar or higher levels of dataset complexity and diversity. Furthermore the synthetic dataset displays a bias level that aligns closely with the original dataset. Finally when pre45;finetuned on our synthetic SuperGLUE dataset T545;3B yields impressive results on the OpenLLM leaderboard surpassing the model trained on the Self45;Instruct dataset by 4.1437; points. We hope that TarGEN can be helpful for quality data generation and reducing the human efforts to create complex benchmarks.
