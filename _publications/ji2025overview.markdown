---
layout: publication
title: 'An Overview Of Large Language Models For Statisticians'
authors: Wenlong Ji, Weizhe Yuan, Emily Getzen, Kyunghyun Cho, Michael I. Jordan, Song Mei, Jason E Weston, Weijie J. Su, Jing Xu, Linjun Zhang
conference: "Arxiv"
year: 2025
bibkey: ji2025overview
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.17814'}
tags: ['Language Modeling', 'Interpretability and Explainability', 'Fairness', 'Model Architecture', 'Applications', 'Tools', 'Merging', 'Bias Mitigation', 'Reinforcement Learning', 'Ethics and Bias', 'Interpretability']
---
Large Language Models (LLMs) have emerged as transformative tools in
artificial intelligence (AI), exhibiting remarkable capabilities across diverse
tasks such as text generation, reasoning, and decision-making. While their
success has primarily been driven by advances in computational power and deep
learning architectures, emerging problems -- in areas such as uncertainty
quantification, decision-making, causal inference, and distribution shift --
require a deeper engagement with the field of statistics. This paper explores
potential areas where statisticians can make important contributions to the
development of LLMs, particularly those that aim to engender trustworthiness
and transparency for human users. Thus, we focus on issues such as uncertainty
quantification, interpretability, fairness, privacy, watermarking and model
adaptation. We also consider possible roles for LLMs in statistical analysis.
By bridging AI and statistics, we aim to foster a deeper collaboration that
advances both the theoretical foundations and practical applications of LLMs,
ultimately shaping their role in addressing complex societal challenges.
