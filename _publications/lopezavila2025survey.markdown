---
layout: publication
title: 'A Survey On Large Language Models In Multimodal Recommender Systems'
authors: Alejo Lopez-avila, Jinhua Du
conference: "Arxiv"
year: 2025
bibkey: lopezavila2025survey
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2505.09777'}
tags: ['Tools', 'Training Techniques', 'Merging', 'Fine-Tuning', 'RecSys', 'Prompting', 'Multimodal Models', 'Survey Paper', 'Reinforcement Learning', 'In-Context Learning', 'Pretraining Methods']
---
Multimodal recommender systems (MRS) integrate heterogeneous user and item data, such as text, images, and structured information, to enhance recommendation performance. The emergence of large language models (LLMs) introduces new opportunities for MRS by enabling semantic reasoning, in-context learning, and dynamic input handling. Compared to earlier pre-trained language models (PLMs), LLMs offer greater flexibility and generalisation capabilities but also introduce challenges related to scalability and model accessibility. This survey presents a comprehensive review of recent work at the intersection of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and data adaptation techniques. We propose a novel taxonomy to characterise integration patterns, identify transferable techniques from related recommendation domains, provide an overview of evaluation metrics and datasets, and point to possible future directions. We aim to clarify the emerging role of LLMs in multimodal recommendation and support future research in this rapidly evolving field.
