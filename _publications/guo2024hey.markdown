---
layout: publication
title: 'Hey GPT, Can You Be More Racist? Analysis From Crowdsourced Attempts To Elicit Biased Content From Generative AI'
authors: Hangzhi Guo, Pranav Narayanan Venkit, Eunchae Jang, Mukund Srinath, Wenbo Zhang, Bonam Mingole, Vipul Gupta, Kush R. Varshney, S. Shyam Sundar, Amulya Yadav
conference: "Arxiv"
year: 2024
bibkey: guo2024hey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.15467"}
tags: ['Model Architecture', 'Tools', 'GPT', 'Ethics and Bias', 'Prompting', 'Applications']
---
The widespread adoption of large language models (LLMs) and generative AI
(GenAI) tools across diverse applications has amplified the importance of
addressing societal biases inherent within these technologies. While the NLP
community has extensively studied LLM bias, research investigating how
non-expert users perceive and interact with biases from these systems remains
limited. As these technologies become increasingly prevalent, understanding
this question is crucial to inform model developers in their efforts to
mitigate bias. To address this gap, this work presents the findings from a
university-level competition, which challenged participants to design prompts
for eliciting biased outputs from GenAI tools. We quantitatively and
qualitatively analyze the competition submissions and identify a diverse set of
biases in GenAI and strategies employed by participants to induce bias in
GenAI. Our finding provides unique insights into how non-expert users perceive
and interact with biases from GenAI tools.
