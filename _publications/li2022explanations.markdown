---
layout: publication
title: 'Explanations From Large Language Models Make Small Reasoners Better'
authors: Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen, Xifeng Yan
conference: "Arxiv"
year: 2022
bibkey: li2022explanations
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2210.06726'}
tags: ['Interpretability and Explainability', 'RAG', 'Training Techniques', 'Model Architecture', 'Tools', 'GPT', 'Prompting', 'In-Context Learning']
---
Integrating free-text explanations to in-context learning of large language
models (LLM) is shown to elicit strong reasoning capabilities along with
reasonable explanations. In this paper, we consider the problem of leveraging
the explanations generated by LLM to improve the training of small reasoners,
which are more favorable in real-production deployment due to their low cost.
We systematically explore three explanation generation approaches from LLM and
utilize a multi-task learning framework to facilitate small models to acquire
strong reasoning power together with explanation generation capabilities.
Experiments on multiple reasoning tasks show that our method can consistently
and significantly outperform finetuning baselines across different settings,
and even perform better than finetuning/prompting a 60x larger GPT-3 (175B)
model by up to 9.5% in accuracy. As a side benefit, human evaluation further
shows that our method can generate high-quality explanations to justify its
predictions, moving towards the goal of explainable AI.
