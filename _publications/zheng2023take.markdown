---
layout: publication
title: 'Take A Step Back: Evoking Reasoning Via Abstraction In Large Language Models'
authors: Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-tze Cheng, Ed H. Chi, Quoc V Le, Denny Zhou
conference: "Arxiv"
year: 2023
bibkey: zheng2023take
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.06117"}
tags: ['GPT', 'Prompting', 'Model Architecture']
---
We present Step-Back Prompting, a simple prompting technique that enables
LLMs to do abstractions to derive high-level concepts and first principles from
instances containing specific details. Using the concepts and principles to
guide reasoning, LLMs significantly improve their abilities in following a
correct reasoning path towards the solution. We conduct experiments of
Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe
substantial performance gains on various challenging reasoning-intensive tasks
including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back
Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%
and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.
