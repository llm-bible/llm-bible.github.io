---
layout: publication
title: Take A Step Back Evoking Reasoning Via Abstraction In Large Language Models
authors: Zheng Huaixiu Steven, Mishra Swaroop, Chen Xinyun, Cheng Heng-tze, Chi Ed H., Le Quoc V, Zhou Denny
conference: "Arxiv"
year: 2023
bibkey: zheng2023take
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.06117"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Prompting']
---
We present Step45;Back Prompting a simple prompting technique that enables LLMs to do abstractions to derive high45;level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step45;Back Prompting with PaLM45;2L GPT45;4 and Llama245;70B models and observe substantial performance gains on various challenging reasoning45;intensive tasks including STEM Knowledge QA and Multi45;Hop Reasoning. For instance Step45;Back Prompting improves PaLM45;2L performance on MMLU (Physics and Chemistry) by 737; and 1137; respectively TimeQA by 2737; and MuSiQue by 737;.
