---
layout: publication
title: Self45;refinement Of Language Models From External Proxy Metrics Feedback
authors: Ramji Keshav, Lee Young-suk, Astudillo Ram√≥n Fernandez, Sultan Md Arafat, Naseem Tahira, Munawar Asim, Florian Radu, Roukos Salim
conference: "Arxiv"
year: 2024
bibkey: ramji2024self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.00827"}
tags: ['Agentic', 'Applications', 'RAG']
---
It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response. In document45;grounded response generation for example agent responses are expected to be relevant to a users query while also being grounded in a given document. In this paper we introduce Proxy Metric45;based Self45;Refinement (ProMiSe) which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback yielding an overall better final response. ProMiSe leverages feedback on response quality through principle45;specific proxy metrics and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan45;T545;XXL and Llama45;245;13B45;Chat to evaluate its performance on document45;grounded question answering datasets MultiDoc2Dial and QuAC demonstrating that self45;refinement improves response quality. We further show that fine45;tuning Llama45;245;13B45;Chat on the synthetic dialogue data generated by ProMiSe yields significant performance improvements over the zero45;shot baseline as well as a supervised fine45;tuned model on human annotated data.
