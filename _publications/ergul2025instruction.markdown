---
layout: publication
title: 'Instruction-based Fine-tuning Of Open-source Llms For Predicting Customer Purchase Behaviors'
authors: Halil Ibrahim Ergul, Selim Balcisoy, Burcin Bozkaya
conference: "Arxiv"
year: 2025
bibkey: ergul2025instruction
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.15724"}
tags: ['Pretraining Methods', 'Training Techniques', 'Fine-Tuning', 'Model Architecture']
---
In this study, the performance of various predictive models, including
probabilistic baseline, CNN, LSTM, and finetuned LLMs, in forecasting merchant
categories from financial transaction data have been evaluated. Utilizing
datasets from Bank A for training and Bank B for testing, the superior
predictive capabilities of the fine-tuned Mistral Instruct model, which was
trained using customer data converted into natural language format have been
demonstrated. The methodology of this study involves instruction fine-tuning
Mistral via LoRA (LowRank Adaptation of Large Language Models) to adapt its
vast pre-trained knowledge to the specific domain of financial transactions.
The Mistral model significantly outperforms traditional sequential models,
achieving higher F1 scores in the three key merchant categories of bank
transaction data (grocery, clothing, and gas stations) that is crucial for
targeted marketing campaigns. This performance is attributed to the model's
enhanced semantic understanding and adaptability which enables it to better
manage minority classes and predict transaction categories with greater
accuracy. These findings highlight the potential of LLMs in predicting human
behavior.
