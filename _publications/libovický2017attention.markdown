---
layout: publication
title: Attention Strategies For Multi45;source Sequence45;to45;sequence Learning
authors: Libovický Jindřich, Helcl Jindřich
conference: "Arxiv"
year: 2017
bibkey: libovický2017attention
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1704.06567"}
tags: ['Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Transformer']
---
Modeling attention in neural multi45;source sequence45;to45;sequence learning remains a relatively unexplored area despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence flat and hierarchical. We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post45;editing tasks. We show that the proposed methods achieve competitive results on both tasks.
