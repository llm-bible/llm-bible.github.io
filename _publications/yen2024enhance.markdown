---
layout: publication
title: 'Enhance The Robustness Of Text-centric Multimodal Alignments'
authors: Ting-yu Yen, Yun-da Tsai, Keng-te Liao, Shou-de Lin
conference: "Arxiv"
year: 2024
bibkey: yen2024enhance
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.05036"}
tags: ['Security', 'Multimodal Models', 'Reinforcement Learning', 'RAG', 'Prompting', 'Applications']
---
Converting different modalities into general text, serving as input prompts
for large language models (LLMs), is a common method to align multimodal models
when there is limited pairwise data. This text-centric approach leverages the
unique properties of text as a modality space, transforming diverse inputs into
a unified textual representation. This enables downstream models to effectively
interpret various modal inputs. This study assesses the quality and robustness
of multimodal representations in the presence of missing entries, noise, or
absent modalities, revealing that current text-centric alignment methods
compromise downstream robustness. To address this issue, we propose a new
text-centric approach that achieves superior robustness compared to previous
methods across various modalities in different settings. Our findings highlight
the potential of this approach to enhance the robustness and adaptability of
multimodal representations, offering a promising solution for dynamic and
real-world applications.
