---
layout: publication
title: Lawgpt A Chinese Legal Knowledge45;enhanced Large Language Model
authors: Zhou Zhi, Shi Jiang-xin, Song Peng-xiao, Yang Xiao-wen, Jin Yi-xuan, Guo Lan-zhe, Li Yu-feng
conference: "Arxiv"
year: 2024
bibkey: zhou2024chinese
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.04614"}
  - {name: "Code", url: "https://github.com/pengxiao&#45;song/LaWGPT"}
tags: ['Applications', 'GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
Large language models (LLMs) including both proprietary and open45;source models have showcased remarkable capabilities in addressing a wide range of downstream tasks. Nonetheless when it comes to practical Chinese legal tasks these models fail to meet the actual requirements. Proprietary models do not ensure data privacy for sensitive legal cases while open45;source models demonstrate unsatisfactory performance due to their lack of legal knowledge. To address this problem we introduce LawGPT the first open45;source model specifically designed for Chinese legal applications. LawGPT comprises two key components legal45;oriented pre45;training and legal supervised fine45;tuning. Specifically we employ large45;scale Chinese legal documents for legal45;oriented pre45;training to incorporate legal domain knowledge. To further improve the models performance on downstream legal tasks we create a knowledge45;driven instruction dataset for legal supervised fine45;tuning. Our experimental results demonstrate that LawGPT outperforms the open45;source LLaMA 7B model. Our code and resources are publicly available at https://github.com/pengxiao&#45;song/LaWGPT and have received 5.7K stars on GitHub.
