---
layout: publication
title: Toolink Linking Toolkit Creation And Using Through Chain45;of45;solving On Open45;source Model
authors: Qian Cheng, Xiong Chenyan, Liu Zhenghao, Liu Zhiyuan
conference: "Arxiv"
year: 2023
bibkey: qian2023linking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.05155"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'RAG', 'Reinforcement Learning', 'Security', 'Tools']
---
Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools but their closed45;source nature and high inference costs pose limitations on their adaptability necessitating a valid method that leverages smaller open45;sourced models. In this paper we introduce Toolink a comprehensive framework that performs task45;solving by first creating a toolkit and then integrating the planning and calling of tools through a chain45;of45;solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the models creativity and CoS ability on ChatGPT. Subsequently we curate CoS45;GPT a chain45;of45;solving dataset designed for tool45;using and finetune the LLaMA45;7B model. It results in LLaMA45;CoS a powerful open45;source model with advanced tool45;planning and tool45;calling capabilities. Evaluation of diverse tasks from BIG45;bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses the chain45;of45;thought approach. Further studies highlight the generalization of LLaMA45;CoS to unseen tasks and showcase its capability in using toolkits not explicitly tailored for the target task affirming its robustness in real45;world scenarios.
