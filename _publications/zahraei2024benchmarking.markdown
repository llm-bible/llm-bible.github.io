---
layout: publication
title: 'Turingq: Benchmarking AI Comprehension In Theory Of Computation'
authors: Pardis Sadat Zahraei, Ehsaneddin Asgari
conference: "Arxiv"
year: 2024
bibkey: zahraei2024benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.06547"}
tags: ['Fine-Tuning', 'GPT', 'Model Architecture', 'Training Techniques', 'Pretraining Methods', 'Prompting']
---
We present TuringQ, the first benchmark designed to evaluate the reasoning
capabilities of large language models (LLMs) in the theory of computation.
TuringQ consists of 4,006 undergraduate and graduate-level question-answer
pairs, categorized into four difficulty levels and covering seven core
theoretical areas. We evaluate several open-source LLMs, as well as GPT-4,
using Chain of Thought prompting and expert human assessment. Additionally, we
propose an automated LLM-based evaluation system that demonstrates competitive
accuracy when compared to human evaluation. Fine-tuning a Llama3-8B model on
TuringQ shows measurable improvements in reasoning ability and out-of-domain
tasks such as algebra. TuringQ serves as both a benchmark and a resource for
enhancing LLM performance in complex computational reasoning tasks. Our
analysis offers insights into LLM capabilities and advances in AI comprehension
of theoretical computer science.
