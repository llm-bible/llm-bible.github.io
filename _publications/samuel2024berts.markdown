---
layout: publication
title: 'Berts Are Generative In-context Learners'
authors: David Samuel
conference: "Arxiv"
year: 2024
bibkey: samuel2024berts
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.04823"}
tags: ['Training Techniques', 'Model Architecture', 'Reinforcement Learning', 'GPT', 'Pretraining Methods', 'BERT', 'Prompting', 'In-Context Learning']
---
While in-context learning is commonly associated with causal language models,
such as GPT, we demonstrate that this capability also 'emerges' in masked
language models. Through an embarrassingly simple inference technique, we
enable an existing masked model, DeBERTa, to perform generative tasks without
additional training or architectural changes. Our evaluation reveals that the
masked and causal language models behave very differently, as they clearly
outperform each other on different categories of tasks. These complementary
strengths suggest that the field's focus on causal models for in-context
learning may be limiting - both architectures can develop these capabilities,
but with distinct advantages; pointing toward promising hybrid approaches that
combine the strengths of both objectives.
