---
layout: publication
title: 'PLATO-XL: Exploring The Large-scale Pre-training Of Dialogue Generation'
authors: Bao Siqi, He Huang, Wang Fan, Wu Hua, Wang Haifeng, Wu Wenquan, Wu Zhihua, Guo Zhen, Lu Hua, Huang Xinxian, Tian Xin, Xu Xinchao, Lin Yingzhan, Niu Zheng-yu
conference: "Arxiv"
year: 2021
bibkey: bao2021plato
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2109.09519"}
tags: ['Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
To explore the limit of dialogue generation pre-training we present the models of PLATO-XL with up to 11 billion parameters trained on both Chinese and English social media conversations. To train such large models we adopt the architecture of unified transformer with high computation and parameter efficiency. In addition we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks verifying its potential as a foundation model of conversational AI.
