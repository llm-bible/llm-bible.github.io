---
layout: publication
title: PLATO45;XL Exploring The Large45;scale Pre45;training Of Dialogue Generation
authors: Bao Siqi, He Huang, Wang Fan, Wu Hua, Wang Haifeng, Wu Wenquan, Wu Zhihua, Guo Zhen, Lu Hua, Huang Xinxian, Tian Xin, Xu Xinchao, Lin Yingzhan, Niu Zheng-yu
conference: "Arxiv"
year: 2021
bibkey: bao2021plato
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2109.09519"}
tags: ['Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
To explore the limit of dialogue generation pre45;training we present the models of PLATO45;XL with up to 11 billion parameters trained on both Chinese and English social media conversations. To train such large models we adopt the architecture of unified transformer with high computation and parameter efficiency. In addition we carry out multi45;party aware pre45;training to better distinguish the characteristic information in social media conversations. With such designs PLATO45;XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO45;XL on other conversational tasks such as knowledge grounded dialogue and task45;oriented conversation. The experimental results indicate that PLATO45;XL obtains state45;of45;the45;art results across multiple conversational tasks verifying its potential as a foundation model of conversational AI.
