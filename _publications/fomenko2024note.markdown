---
layout: publication
title: A Note on LoRA
authors: Fomenko Vlad, Yu Han, Lee Jongho, Hsieh Stanley, Chen Weizhu
conference: "Arxiv"
year: 2024
bibkey: fomenko2024note
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.05086"}
tags: ['Fine Tuning', 'Pretraining Methods']
---
LoRA (Low-Rank Adaptation) has emerged as a preferred method for efficiently adapting Large Language Models (LLMs) with remarkable simplicity and efficacy. This note extends the original LoRA paper by offering new perspectives that were not initially discussed and presents a series of insights for deploying LoRA at scale. Without introducing new experiments we aim to improve the understanding and application of LoRA.
