---
layout: publication
title: Delucionqa Detecting Hallucinations In Domain45;specific Question Answering
authors: Sadat Mobashir, Zhou Zhengyu, Lange Lukas, Araki Jun, Gundroo Arsalan, Wang Bingqing, Menon Rakesh R, Parvez Md Rizwan, Feng Zhe
conference: "Arxiv"
year: 2023
bibkey: sadat2023detecting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.05200"}
tags: ['Applications', 'RAG']
---
Hallucination is a well45;known phenomenon in text generated by large language models (LLMs). The existence of hallucinatory responses is found in almost all application scenarios e.g. summarization question45;answering (QA) etc. For applications requiring high reliability (e.g. customer45;facing assistants) the potential existence of hallucination in LLM45;generated text is a critical problem. The amount of hallucination can be reduced by leveraging information retrieval to provide relevant background information to the LLM. However LLMs can still generate hallucinatory content for various reasons (e.g. prioritizing its parametric knowledge over the context failure to capture the relevant information from the context etc.). Detecting hallucinations through automated methods is thus paramount. To facilitate research in this direction we introduce a sophisticated dataset DelucionQA that captures hallucinations made by retrieval45;augmented LLMs for a domain45;specific QA task. Furthermore we propose a set of hallucination detection methods to serve as baselines for future works from the research community. Analysis and case study are also provided to share valuable insights on hallucination phenomena in the target scenario.
