---
layout: publication
title: 'Lifelong Learning Of Large Language Model Based Agents: A Roadmap'
authors: Junhao Zheng, Chengming Shi, Xidi Cai, Qiuke Li, Duzhen Zhang, Chenxing Li, Dong Yu, Qianli Ma
conference: "Arxiv"
year: 2025
bibkey: zheng2025lifelong
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2501.07278'}
  - {name: "Code", url: 'https://github.com/qianlima-lab/awesome-lifelong-llm-agent'}
tags: ['Agentic', 'Has Code', 'Merging', 'Multimodal Models', 'Survey Paper', 'Reinforcement Learning']
---
Lifelong learning, also known as continual or incremental learning, is a
crucial component for advancing Artificial General Intelligence (AGI) by
enabling systems to continuously adapt in dynamic environments. While large
language models (LLMs) have demonstrated impressive capabilities in natural
language processing, existing LLM agents are typically designed for static
systems and lack the ability to adapt over time in response to new challenges.
This survey is the first to systematically summarize the potential techniques
for incorporating lifelong learning into LLM-based agents. We categorize the
core components of these agents into three modules: the perception module for
multimodal input integration, the memory module for storing and retrieving
evolving knowledge, and the action module for grounded interactions with the
dynamic environment. We highlight how these pillars collectively enable
continuous adaptation, mitigate catastrophic forgetting, and improve long-term
performance. This survey provides a roadmap for researchers and practitioners
working to develop lifelong learning capabilities in LLM agents, offering
insights into emerging trends, evaluation metrics, and application scenarios.
Relevant literature and resources are available at \href\{this
url\}\{https://github.com/qianlima-lab/awesome-lifelong-llm-agent\}.
