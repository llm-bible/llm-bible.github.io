---
layout: publication
title: Bitfit Simple Parameter45;efficient Fine45;tuning For Transformer45;based Masked Language45;models
authors: Zaken Elad Ben, Ravfogel Shauli, Goldberg Yoav
conference: "Arxiv"
year: 2021
bibkey: zaken2021simple
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2106.10199"}
tags: ['BERT', 'Ethics And Bias', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
We introduce BitFit a sparse45;finetuning method where only the bias45;terms of the model (or a subset of them) are being modified. We show that with small45;to45;medium training data applying BitFit on pre45;trained BERT models is competitive with (and sometimes better than) fine45;tuning the entire model. For larger data the method is competitive with other sparse fine45;tuning methods. Besides their practical utility these findings are relevant for the question of understanding the commonly45;used process of finetuning they support the hypothesis that finetuning is mainly about exposing knowledge induced by language45;modeling training rather than learning new task45;specific linguistic knowledge.
