---
layout: publication
title: Argue Attribute45;guided Prompt Tuning For Vision45;language Models
authors: Tian Xinyu, Zou Shu, Yang Zhaoyuan, Zhang Jing
conference: "Arxiv"
year: 2023
bibkey: tian2023attribute
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.16494"}
tags: ['Pretraining Methods', 'Prompting', 'RAG']
---
Although soft prompt tuning is effective in efficiently adapting Vision45;Language (Vamp;L) models for downstream tasks it shows limitations in dealing with distribution shifts. We address this issue with Attribute45;Guided Prompt Tuning (ArGue) making three key contributions. 1) In contrast to the conventional approach of directly appending soft prompts preceding class names we align the model with primitive visual attributes generated by Large Language Models (LLMs). We posit that a models ability to express high confidence in these attributes signifies its capacity to discern the correct class rationales. 2) We introduce attribute sampling to eliminate disadvantageous attributes thus only semantically meaningful attributes are preserved. 3) We propose negative prompting explicitly enumerating class45;agnostic attributes to activate spurious correlations and encourage the model to generate highly orthogonal probability distributions in relation to these negative features. In experiments our method significantly outperforms current state45;of45;the45;art prompt tuning methods on both novel class prediction and out45;of45;distribution generalization tasks.
