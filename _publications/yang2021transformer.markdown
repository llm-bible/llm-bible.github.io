---
layout: publication
title: 'Transformer-based Korean Pretrained Language Models: A Survey On Three Years Of Progress'
authors: Kichang Yang
conference: "Arxiv"
year: 2021
bibkey: yang2021transformer
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2112.03014"}
tags: ['Transformer', 'GPT', 'Survey Paper', 'Model Architecture', 'Attention Mechanism', 'Pretraining Methods', 'BERT']
---
With the advent of Transformer, which was used in translation models in 2017,
attention-based architectures began to attract attention. Furthermore, after
the emergence of BERT, which strengthened the NLU-specific encoder part, which
is a part of the Transformer, and the GPT architecture, which strengthened the
NLG-specific decoder part, various methodologies, data, and models for learning
the Pretrained Language Model began to appear. Furthermore, in the past three
years, various Pretrained Language Models specialized for Korean have appeared.
In this paper, we intend to numerically and qualitatively compare and analyze
various Korean PLMs released to the public.
