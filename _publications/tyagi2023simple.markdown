---
layout: publication
title: Simple Is Better And Large Is Not Enough Towards Ensembling Of Foundational Language Models
authors: Tyagi Nancy, Shiri Aidin, Sarkar Surjodeep, Umrawal Abhishek Kumar, Gaur Manas
conference: "Arxiv"
year: 2023
bibkey: tyagi2023simple
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.12272"}
tags: ['Agentic', 'Attention Mechanism', 'BERT', 'Model Architecture', 'Reinforcement Learning']
---
Foundational Language Models (FLMs) have advanced natural language processing (NLP) research. Current researchers are developing larger FLMs (e.g. XLNet T5) to enable contextualized language representation classification and generation. While developing larger FLMs has been of significant advantage it is also a liability concerning hallucination and predictive uncertainty. Fundamentally larger FLMs are built on the same foundations as smaller FLMs (e.g. BERT); hence one must recognize the potential of smaller FLMs which can be realized through an ensemble. In the current research we perform a reality check on FLMs and their ensemble on benchmark and real45;world datasets. We hypothesize that the ensembling of FLMs can influence the individualistic attention of FLMs and unravel the strength of coordination and cooperation of different FLMs. We utilize BERT and define three other ensemble techniques 123;Shallow Semi and Deep125; wherein the Deep45;Ensemble introduces a knowledge45;guided reinforcement learning approach. We discovered that the suggested Deep45;Ensemble BERT outperforms its large variation i.e. BERTlarge by a factor of many times using datasets that show the usefulness of NLP in sensitive fields such as mental health.
