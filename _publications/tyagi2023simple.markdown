---
layout: publication
title: 'Simple Is Better And Large Is Not Enough: Towards Ensembling Of Foundational Language Models'
authors: Tyagi Nancy, Shiri Aidin, Sarkar Surjodeep, Umrawal Abhishek Kumar, Gaur Manas
conference: "Arxiv"
year: 2023
bibkey: tyagi2023simple
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2308.12272"}
tags: ['Agentic', 'Attention Mechanism', 'BERT', 'Model Architecture', 'Reinforcement Learning']
---
Foundational Language Models (FLMs) have advanced natural language processing
(NLP) research. Current researchers are developing larger FLMs (e.g., XLNet,
T5) to enable contextualized language representation, classification, and
generation. While developing larger FLMs has been of significant advantage, it
is also a liability concerning hallucination and predictive uncertainty.
Fundamentally, larger FLMs are built on the same foundations as smaller FLMs
(e.g., BERT); hence, one must recognize the potential of smaller FLMs which can
be realized through an ensemble. In the current research, we perform a reality
check on FLMs and their ensemble on benchmark and real-world datasets. We
hypothesize that the ensembling of FLMs can influence the individualistic
attention of FLMs and unravel the strength of coordination and cooperation of
different FLMs. We utilize BERT and define three other ensemble techniques:
\{Shallow, Semi, and Deep\}, wherein the Deep-Ensemble introduces a
knowledge-guided reinforcement learning approach. We discovered that the
suggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by
a factor of many times using datasets that show the usefulness of NLP in
sensitive fields, such as mental health.
