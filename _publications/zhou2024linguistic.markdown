---
layout: publication
title: 'Linguistic Minimal Pairs Elicit Linguistic Similarity In Large Language Models'
authors: Xinyu Zhou, Delong Chen, Samuel Cahyawijaya, Xufeng Duan, Zhenguang G. Cai
conference: "Arxiv"
year: 2024
bibkey: zhou2024linguistic
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2409.12435'}
  - {name: "Code", url: 'https://github.com/ChenDelong1999/Linguistic-Similarity'}
tags: ['RAG', 'Has Code', 'Training Techniques']
---
We introduce a novel analysis that leverages linguistic minimal pairs to
probe the internal linguistic representations of Large Language Models (LLMs).
By measuring the similarity between LLM activation differences across minimal
pairs, we quantify the and gain insight into the linguistic knowledge captured
by LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairs
in three languages, reveal properties of linguistic similarity from four key
aspects: consistency across LLMs, relation to theoretical categorizations,
dependency to semantic context, and cross-lingual alignment of relevant
phenomena. Our findings suggest that 1) linguistic similarity is significantly
influenced by training data exposure, leading to higher cross-LLM agreement in
higher-resource languages. 2) Linguistic similarity strongly aligns with
fine-grained theoretical linguistic categories but weakly with broader ones. 3)
Linguistic similarity shows a weak correlation with semantic similarity,
showing its context-dependent nature. 4) LLMs exhibit limited cross-lingual
alignment in their understanding of relevant linguistic phenomena. This work
demonstrates the potential of minimal pairs as a window into the neural
representations of language in LLMs, shedding light on the relationship between
LLMs and linguistic theory. Codes and data are available at
https://github.com/ChenDelong1999/Linguistic-Similarity
