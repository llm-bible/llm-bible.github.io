---
layout: publication
title: 'The Quest For Visual Understanding: A Journey Through The Evolution Of Visual Question Answering'
authors: Anupam Pandey, Deepjyoti Bodo, Arpan Phukan, Asif Ekbal
conference: "Arxiv"
year: 2025
bibkey: pandey2025quest
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.07109"}
tags: ['Transformer', 'Pre-Training', 'Tools', 'Survey Paper', 'Ethics and Bias', 'Applications', 'Interpretability and Explainability', 'Model Architecture', 'Reinforcement Learning', 'Merging', 'Training Techniques', 'Attention Mechanism', 'Pretraining Methods', 'Multimodal Models']
---
Visual Question Answering (VQA) is an interdisciplinary field that bridges
the gap between computer vision (CV) and natural language processing(NLP),
enabling Artificial Intelligence(AI) systems to answer questions about images.
Since its inception in 2015, VQA has rapidly evolved, driven by advances in
deep learning, attention mechanisms, and transformer-based models. This survey
traces the journey of VQA from its early days, through major breakthroughs,
such as attention mechanisms, compositional reasoning, and the rise of
vision-language pre-training methods. We highlight key models, datasets, and
techniques that shaped the development of VQA systems, emphasizing the pivotal
role of transformer architectures and multimodal pre-training in driving recent
progress. Additionally, we explore specialized applications of VQA in domains
like healthcare and discuss ongoing challenges, such as dataset bias, model
interpretability, and the need for common-sense reasoning. Lastly, we discuss
the emerging trends in large multimodal language models and the integration of
external knowledge, offering insights into the future directions of VQA. This
paper aims to provide a comprehensive overview of the evolution of VQA,
highlighting both its current state and potential advancements.
