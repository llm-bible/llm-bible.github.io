---
layout: publication
title: 'On The Impact Of Fine-tuning On Chain-of-thought Reasoning'
authors: Elita Lobo, Chirag Agarwal, Himabindu Lakkaraju
conference: "Arxiv"
year: 2024
bibkey: lobo2024impact
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2411.15382"}
tags: ['Fine-Tuning', 'Responsible AI', 'Agentic', 'Tools', 'Applications', 'RAG', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods']
---
Large language models have emerged as powerful tools for general
intelligence, showcasing advanced natural language processing capabilities that
find applications across diverse domains. Despite their impressive performance,
recent studies have highlighted the potential for significant enhancements in
LLMs' task-specific performance through fine-tuning strategies like
Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning
(SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works
have shown that while fine-tuning offers significant performance gains, it also
leads to challenges such as catastrophic forgetting and privacy and safety
risks. To this end, there has been little to no work in \textit\{understanding
the impact of fine-tuning on the reasoning capabilities of LLMs\}. Our research
investigates the effect of fine-tuning on the reasoning abilities of LLMs,
addressing critical questions regarding the impact of task-specific fine-tuning
on overall reasoning capabilities, the influence of fine-tuning on
Chain-of-Thought (CoT) reasoning performance, and the implications for the
faithfulness of CoT reasonings. By exploring these dimensions, our study shows
the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness
of CoT reasoning, on average across four datasets, decreases, highlighting
potential shifts in internal mechanisms of the LLMs resulting from fine-tuning
processes.
