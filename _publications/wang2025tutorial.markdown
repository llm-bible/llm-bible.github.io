---
layout: publication
title: 'A Tutorial On LLM Reasoning: Relevant Methods Behind Chatgpt O1'
authors: Jun Wang
conference: "Arxiv"
year: 2025
bibkey: wang2025tutorial
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.10867'}
tags: ['Agentic', 'Model Architecture', 'Tools', 'Training Techniques', 'GPT', 'Reinforcement Learning', 'Pretraining Methods']
---
OpenAI o1 has shown that applying reinforcement learning to integrate
reasoning steps directly during inference can significantly improve a model's
reasoning capabilities. This result is exciting as the field transitions from
the conventional autoregressive method of generating answers to a more
deliberate approach that models the slow-thinking process through step-by-step
reasoning training. Reinforcement learning plays a key role in both the model's
training and decoding processes. In this article, we present a comprehensive
formulation of reasoning problems and investigate the use of both model-based
and model-free approaches to better support this slow-thinking framework.
