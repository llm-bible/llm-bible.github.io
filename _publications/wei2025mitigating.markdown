---
layout: publication
title: 'Mitigating Gender Bias Via Fostering Exploratory Thinking In Llms'
authors: Kangda Wei, Hasnat Md Abdullah, Ruihong Huang
conference: "Arxiv"
year: 2025
bibkey: wei2025mitigating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.17217"}
tags: ['Fine-Tuning', 'Tools', 'Efficiency and Optimization', 'Ethics and Bias', 'Reinforcement Learning', 'Prompting']
---
Large Language Models (LLMs) often exhibit gender bias, resulting in unequal treatment of male and female subjects across different contexts. To address this issue, we propose a novel data generation framework that fosters exploratory thinking in LLMs. Our approach prompts models to generate story pairs featuring male and female protagonists in structurally identical, morally ambiguous scenarios, then elicits and compares their moral judgments. When inconsistencies arise, the model is guided to produce balanced, gender-neutral judgments. These story-judgment pairs are used to fine-tune or optimize the models via Direct Preference Optimization (DPO). Experimental results show that our method significantly reduces gender bias while preserving or even enhancing general model capabilities. We will release the code and generated data.
