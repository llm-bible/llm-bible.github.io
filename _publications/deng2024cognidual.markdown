---
layout: publication
title: 'Cognidual Framework: Self-training Large Language Models Within A Dual-system Theoretical Framework For Improving Cognitive Tasks'
authors: Deng Yongxin, Qiu Xihe, Tan Xiaoyu, Qu Chao, Pan Jing, Cheng Yuan, Xu Yinghui, Chu Wei
conference: "Arxiv"
year: 2024
bibkey: deng2024cognidual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.03381"}
tags: ['Attention Mechanism', 'Model Architecture', 'Tools', 'Training Techniques']
---
Cognitive psychology investigates perception, attention, memory, language, problem-solving, decision-making, and reasoning. Kahneman's dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive System 1 and the deliberative, rational System 2. Recent advancements have positioned large language Models (LLMs) as formidable tools nearing human-level proficiency in various cognitive tasks. Nonetheless, the presence of a dual-system framework analogous to human cognition in LLMs remains unexplored. This study introduces the \textbf\{CogniDual Framework for LLMs\} (CFLLMs), designed to assess whether LLMs can, through self-training, evolve from deliberate deduction to intuitive responses, thereby emulating the human process of acquiring and mastering new information. Our findings reveal the cognitive mechanisms behind LLMs' response generation, enhancing our understanding of their capabilities in cognitive psychology. Practically, self-trained models can provide faster responses to certain queries, reducing computational demands during inference.
