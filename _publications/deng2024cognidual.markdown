---
layout: publication
title: 'Cognidual Framework: Self-training Large Language Models Within A Dual-system Theoretical Framework For Improving Cognitive Tasks'
authors: Yongxin Deng, Xihe Qiu, Xiaoyu Tan, Chao Qu, Jing Pan, Yuan Cheng, Yinghui Xu, Wei Chu
conference: "Arxiv"
year: 2024
bibkey: deng2024cognidual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.03381"}
tags: ['Training Techniques', 'Tools', 'Attention Mechanism', 'Model Architecture']
---
Cognitive psychology investigates perception, attention, memory, language,
problem-solving, decision-making, and reasoning. Kahneman's dual-system theory
elucidates the human decision-making process, distinguishing between the rapid,
intuitive System 1 and the deliberative, rational System 2. Recent advancements
have positioned large language Models (LLMs) as formidable tools nearing
human-level proficiency in various cognitive tasks. Nonetheless, the presence
of a dual-system framework analogous to human cognition in LLMs remains
unexplored. This study introduces the \textbf\{CogniDual Framework for LLMs\}
(CFLLMs), designed to assess whether LLMs can, through self-training, evolve
from deliberate deduction to intuitive responses, thereby emulating the human
process of acquiring and mastering new information. Our findings reveal the
cognitive mechanisms behind LLMs' response generation, enhancing our
understanding of their capabilities in cognitive psychology. Practically,
self-trained models can provide faster responses to certain queries, reducing
computational demands during inference.
