---
layout: publication
title: Cognidual Framework Self45;training Large Language Models Within A Dual45;system Theoretical Framework For Improving Cognitive Tasks
authors: Deng Yongxin, Qiu Xihe, Tan Xiaoyu, Qu Chao, Pan Jing, Cheng Yuan, Xu Yinghui, Chu Wei
conference: "Arxiv"
year: 2024
bibkey: deng2024cognidual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.03381"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Tools', 'Training Techniques']
---
Cognitive psychology investigates perception attention memory language problem45;solving decision45;making and reasoning. Kahnemans dual45;system theory elucidates the human decision45;making process distinguishing between the rapid intuitive System 1 and the deliberative rational System 2. Recent advancements have positioned large language Models (LLMs) as formidable tools nearing human45;level proficiency in various cognitive tasks. Nonetheless the presence of a dual45;system framework analogous to human cognition in LLMs remains unexplored. This study introduces the textbf123;CogniDual Framework for LLMs125; (CFLLMs) designed to assess whether LLMs can through self45;training evolve from deliberate deduction to intuitive responses thereby emulating the human process of acquiring and mastering new information. Our findings reveal the cognitive mechanisms behind LLMs response generation enhancing our understanding of their capabilities in cognitive psychology. Practically self45;trained models can provide faster responses to certain queries reducing computational demands during inference.
