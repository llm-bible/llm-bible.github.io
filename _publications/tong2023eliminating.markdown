---
layout: publication
title: Eliminating Reasoning Via Inferring With Planning A New Framework To Guide Llms Non45;linear Thinking
authors: Tong Yongqi, Wang Yifan, Li Dawei, Wang Sizhe, Lin Zi, Han Simeng, Shang Jingbo
conference: "Arxiv"
year: 2023
bibkey: tong2023eliminating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.12342"}
tags: ['Prompting', 'Reinforcement Learning', 'Tools']
---
Chain45;of45;Thought(CoT) prompting and its variants explore equipping large language models (LLMs) with high45;level reasoning abilities by emulating human45;like linear cognition and logic. However the human mind is complicated and mixed with both linear and nonlinear thinking. In this work we propose textbf123;I125;nferential textbf123;E125;xclusion textbf123;P125;rompting (IEP) a novel prompting that combines the principles of elimination and inference in order to guide LLMs to think non45;linearly. IEP guides LLMs to plan and then utilize Natural Language Inference (NLI) to deduce each possible solutions entailment relation with context commonsense or facts therefore yielding a broader perspective by thinking back for inferring. This forward planning and backward eliminating process allows IEP to better simulate the complex human thinking processes compared to other CoT45;based methods which only reflect linear cognitive processes. We conducted a series of empirical studies and have corroborated that IEP consistently outperforms CoT across various tasks. Additionally we observe that integrating IEP and CoT further improves the LLMs performance on certain tasks highlighting the necessity of equipping LLMs with mixed logic processes. Moreover to better evaluate comprehensive features inherent in human logic we introduce textbf123;M125;ental45;textbf123;A125;bility textbf123;R125;easoning textbf123;B125;enchmark (MARB). The benchmark comprises six novel subtasks with a total of 9115 questions among which 1685 are developed with hand45;crafted rationale references. We believe both textsc123;IEP125; and textsc123;MARB125; can serve as a promising direction for unveiling LLMs logic and verbal reasoning abilities and drive further advancements. textsc123;MARB125; will be available at ~texttt123;anonymity link125; soon.
