---
layout: publication
title: 'Realsafe-r1: Safety-aligned Deepseek-r1 Without Compromising Reasoning Capability'
authors: Yichi Zhang, Zihao Zeng, Dongbai Li, Yao Huang, Zhijie Deng, Yinpeng Dong
conference: "Arxiv"
year: 2025
bibkey: zhang2025realsafe
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.10081"}
tags: ['Responsible AI', 'Tools', 'Applications', 'Security', 'Training Techniques']
---
Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been
rapidly progressing and achieving breakthrough performance on complex reasoning
tasks such as mathematics and coding. However, the open-source R1 models have
raised safety concerns in wide applications, such as the tendency to comply
with malicious queries, which greatly impacts the utility of these powerful
models in their applications. In this paper, we introduce RealSafe-R1 as
safety-aligned versions of DeepSeek-R1 distilled models. To train these models,
we construct a dataset of 15k safety-aware reasoning trajectories generated by
DeepSeek-R1, under explicit instructions for expected refusal behavior. Both
quantitative experiments and qualitative case studies demonstrate the models'
improvements, which are shown in their safety guardrails against both harmful
queries and jailbreak attacks. Importantly, unlike prior safety alignment
efforts that often compromise reasoning performance, our method preserves the
models' reasoning capabilities by maintaining the training data within the
original distribution of generation. Model weights of RealSafe-R1 are
open-source at https://huggingface.co/RealSafe.
