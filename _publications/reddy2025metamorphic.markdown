---
layout: publication
title: 'Metamorphic Testing For Fairness Evaluation In Large Language Models: Identifying Intersectional Bias In Llama And GPT'
authors: Harishwar Reddy, Madhusudan Srinivasan, Upulee Kanewala
conference: "Arxiv"
year: 2025
bibkey: reddy2025metamorphic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2504.07982"}
tags: ['Security', 'Training Techniques', 'Fairness', 'Model Architecture', 'Reinforcement Learning', 'GPT', 'Bias Mitigation', 'Ethics and Bias', 'Applications']
---
Large Language Models (LLMs) have made significant strides in Natural
Language Processing but remain vulnerable to fairness-related issues, often
reflecting biases inherent in their training data. These biases pose risks,
particularly when LLMs are deployed in sensitive areas such as healthcare,
finance, and law. This paper introduces a metamorphic testing approach to
systematically identify fairness bugs in LLMs. We define and apply a set of
fairness-oriented metamorphic relations (MRs) to assess the LLaMA and GPT
model, a state-of-the-art LLM, across diverse demographic inputs. Our
methodology includes generating source and follow-up test cases for each MR and
analyzing model responses for fairness violations. The results demonstrate the
effectiveness of MT in exposing bias patterns, especially in relation to tone
and sentiment, and highlight specific intersections of sensitive attributes
that frequently reveal fairness faults. This research improves fairness testing
in LLMs, providing a structured approach to detect and mitigate biases and
improve model robustness in fairness-sensitive applications.
