---
layout: publication
title: Jdocqa Japanese Document Question Answering Dataset For Generative Language Models
authors: Onami Eri, Kurita Shuhei, Miyanishi Taiki, Watanabe Taro
conference: "Arxiv"
year: 2024
bibkey: onami2024japanese
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2403.19454"}
tags: ['Applications', 'Multimodal Models']
---
Document question answering is a task of question answering on given documents such as reports slides pamphlets and websites and it is a truly demanding task as paper and electronic forms of documents are so common in our society. This is known as a quite challenging task because it requires not only text understanding but also understanding of figures and tables and hence visual question answering (VQA) methods are often examined in addition to textual approaches. We introduce Japanese Document Question Answering (JDocQA) a large45;scale document45;based QA dataset essentially requiring both visual and textual information to answer questions which comprises 5504 documents in PDF format and annotated 11600 question45;and45;answer instances in Japanese. Each QA instance includes references to the document pages and bounding boxes for the answer clues. We incorporate multiple categories of questions and unanswerable questions from the document for realistic question45;answering applications. We empirically evaluate the effectiveness of our dataset with text45;based large language models (LLMs) and multimodal models. Incorporating unanswerable questions in finetuning may contribute to harnessing the so45;called hallucination generation.
