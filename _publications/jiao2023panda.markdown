---
layout: publication
title: Panda LLM Training Data And Evaluation For Open45;sourced Chinese Instruction45;following Large Language Models
authors: Jiao Fangkai, Ding Bosheng, Luo Tianze, Mo Zhanfeng
conference: "Arxiv"
year: 2023
bibkey: jiao2023panda
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.03025"}
tags: ['Pretraining Methods', 'Training Techniques']
---
This project focuses on enhancing open45;source large language models through instruction45;tuning and providing comprehensive evaluations of their performance. We explore how various training data factors such as quantity quality and linguistic distribution influence the performance of instruction45;tuned models trained on publicly accessible high45;quality instruction datasets for both English and Chinese languages. Our goal is to supplement evaluation with quantitative analyses providing valuable insights for the continued advancement of open45;source chat models. Our model data and code are publicly available for others to use and build upon.
