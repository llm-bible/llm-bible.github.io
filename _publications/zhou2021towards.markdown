---
layout: publication
title: LAFITE Towards Language45;free Training For Text45;to45;image Generation
authors: Zhou Yufan, Zhang Ruiyi, Chen Changyou, Li Chunyuan, Tensmeyer Chris, Yu Tong, Gu Jiuxiang, Xu Jinhui, Sun Tong
conference: "Arxiv"
year: 2021
bibkey: zhou2021towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2111.13792"}
tags: ['Applications', 'RAG', 'Reinforcement Learning', 'Training Techniques']
---
One of the major challenges in training text45;to45;image generation models is the need of a large number of high45;quality image45;text pairs. While image samples are often easily accessible the associated text descriptions typically require careful human captioning which is particularly time45; and cost45;consuming. In this paper we propose the first work to train text45;to45;image generation models without any text data. Our method leverages the well45;aligned multi45;modal semantic space of the powerful pre45;trained CLIP model the requirement of text45;conditioning is seamlessly alleviated via generating text features from image features. Extensive experiments are conducted to illustrate the effectiveness of the proposed method. We obtain state45;of45;the45;art results in the standard text45;to45;image generation tasks. Importantly the proposed language45;free model outperforms most existing models trained with full image45;text pairs. Furthermore our method can be applied in fine45;tuning pre45;trained models which saves both training time and cost in training text45;to45;image generation models. Our pre45;trained model obtains competitive results in zero45;shot text45;to45;image generation on the MS45;COCO dataset yet with around only 137; of the model size and training data size relative to the recently proposed large DALL45;E model.
