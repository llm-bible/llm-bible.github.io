---
layout: publication
title: Wechat Neural Machine Translation Systems For WMT21
authors: Zeng Xianfeng, Liu Yijin, Li Ernan, Ran Qiu, Meng Fandong, Li Peng, Xu Jinan, Zhou Jie
conference: "Arxiv"
year: 2021
bibkey: zeng2021wechat
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2108.02401"}
tags: ['Applications', 'Distillation', 'Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
This paper introduces WeChat AIs participation in WMT 2021 shared news translation task on English45;Chinese English45;Japanese Japanese45;English and English45;German. Our systems are based on the Transformer (Vaswani et al. 2017) with several novel and effective variants. In our experiments we employ data filtering large45;scale synthetic data generation (i.e. back45;translation knowledge distillation forward45;translation iterative in45;domain knowledge transfer) advanced finetuning approaches and boosted Self45;BLEU based model ensemble. Our constrained systems achieve 36.9 46.9 27.8 and 31.3 case45;sensitive BLEU scores on English45;Chinese English45;Japanese Japanese45;English and English45;German respectively. The BLEU scores of English45;Chinese English45;Japanese and Japanese45;English are the highest among all submissions and that of English45;German is the highest among all constrained submissions.
