---
layout: publication
title: Socratic Models Composing Zero45;shot Multimodal Reasoning With Language
authors: Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Pete Florence
conference: "Arxiv"
year: 2022
bibkey: zeng2022socratic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2204.00598v2"}
tags: ['Applications', 'Multimodal Models', 'Prompting', 'RAG', 'Reinforcement Learning', 'Tools']
---
Large pretrained (e.g. foundation) models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic they may only barely overlap. For example visual45;language models (VLMs) are trained on Internet45;scale image captions but large language models (LMs) are further trained on Internet45;scale text with no images (e.g. spreadsheets SAT questions code). As a result these models store different forms of commonsense knowledge across different domains. In this work we show that this diversity is symbiotic and can be leveraged through Socratic Models (SMs) a modular framework in which multiple pretrained models may be composed zero45;shot i.e. via multimodal45;informed prompting to exchange information with each other and capture new multimodal capabilities without requiring finetuning. With minimal engineering SMs are not only competitive with state45;of45;the45;art zero45;shot image captioning and video45;to45;text retrieval but also enable new applications such as (i) answering free45;form questions about egocentric video (ii) engaging in multimodal assistive dialogue with people (e.g. for cooking recipes) by interfacing with external APIs and databases (e.g. web search) and (iii) robot perception and planning.
