---
layout: publication
title: 'Safer Conversational AI As A Source Of User Delight'
authors: Xiaoding Lu, Aleksey Korshuk, Zongyi Liu, William Beauchamp, Chai Research
conference: "Arxiv"
year: 2023
bibkey: lu2023safer
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.09865"}
tags: ['Responsible AI', 'Tools', 'Reinforcement Learning', 'RAG', 'Ethics and Bias']
---
This work explores the impact of moderation on users' enjoyment of
conversational AI systems. While recent advancements in Large Language Models
(LLMs) have led to highly capable conversational AIs that are increasingly
deployed in real-world settings, there is a growing concern over AI safety and
the need to moderate systems to encourage safe language and prevent harm.
However, some users argue that current approaches to moderation limit the
technology, compromise free expression, and limit the value delivered by the
technology. This study takes an unbiased stance and shows that moderation does
not necessarily detract from user enjoyment. Heavy handed moderation does seem
to have a nefarious effect, but models that are moderated to be safer can lead
to a better user experience. By deploying various conversational AIs in the
Chai platform, the study finds that user retention can increase with a level of
moderation and safe system design. These results demonstrate the importance of
appropriately defining safety in models in a way that is both responsible and
focused on serving users.
