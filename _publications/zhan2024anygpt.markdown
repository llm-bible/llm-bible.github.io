---
layout: publication
title: AnyGPT Unified Multimodal LLM with Discrete Sequence Modeling
authors: Zhan Jun, Dai Junqi, Ye Jiasheng, Zhou Yunhua, Zhang Dong, Liu Zhigeng, Zhang Xin, Yuan Ruibin, Zhang Ge, Li Linyang, Yan Hang, Fu Jie, Gui Tao, Sun Tianxiang, Jiang Yugang, Qiu Xipeng
conference: "Arxiv"
year: 2024
bibkey: zhan2024anygpt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.12226"}
tags: ['LLM', 'Multimodal Models', 'Arxiv']
---
We introduce AnyGPT an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities including speech text images and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead it relies exclusively on data-level preprocessing facilitating the seamless integration of new modalities into LLMs akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/
