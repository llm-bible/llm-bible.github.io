---
layout: publication
title: Freely Long-Thinking Transformer (FraiLT)
authors: Tabak Akbay
conference: "Arxiv"
year: 2024
bibkey: tabak2024freely
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.11626"}
tags: ['Model Architecture', 'Transformer', 'Arxiv']
---
Freely Long-Thinking Transformer (FraiLT) is an improved transformer model designed to enhance processing capabilities without scaling up size. It utilizes a recursive approach iterating over a subset of layers multiple times and introduces iteration encodings to maintain awareness across these cycles. Iteration encoding allows FraiLT to achieve the interpretive depth of larger models in a compact form. When evaluated on a synthetic story dataset FraiLT outperformed larger models showcasing its ability to deliver high-quality performance while reducing memory demands. This model represents a step forward towards more efficient and accessible language models.
