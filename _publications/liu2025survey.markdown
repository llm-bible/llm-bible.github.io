---
layout: publication
title: 'A Survey On Transformer Context Extension: Approaches And Evaluation'
authors: Yijun Liu, Jinzheng Yu, Yang Xu, Zhongyang Li, Qingfu Zhu
conference: "Arxiv"
year: 2025
bibkey: liu2025survey
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.13299'}
tags: ['Attention Mechanism', 'Transformer', 'Model Architecture', 'Survey Paper', 'Reinforcement Learning', 'Pretraining Methods']
---
Large language models (LLMs) based on Transformer have been widely applied in
the filed of natural language processing (NLP), demonstrating strong
performance, particularly in handling short text tasks. However, when it comes
to long context scenarios, the performance of LLMs degrades due to some
challenges. To alleviate this phenomenon, there is a number of work proposed
recently. In this survey, we first list the challenges of applying pre-trained
LLMs to process long contexts. Then systematically review the approaches
related to long context and propose our taxonomy categorizing them into four
main types: positional encoding, context compression, retrieval augmented, and
attention pattern. In addition to the approaches, we focus on the evaluation of
long context, organizing relevant data, tasks, and metrics based on existing
long context benchmarks. Finally, we summarize unresolved issues in the long
context domain and put forward our views on future developments.
