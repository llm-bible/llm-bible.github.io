---
layout: publication
title: 'Chemdfm: Dialogue Foundation Model For Chemistry'
authors: Zhao Zihan, Ma Da, Chen Lu, Sun Liangtai, Li Zihao, Xu Hongshen, Zhu Zichen, Zhu Su, Fan Shuai, Shen Guodong, Chen Xin, Yu Kai
conference: "Arxiv"
year: 2024
bibkey: zhao2024dialogue
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.14818"}
tags: ['Efficiency And Optimization', 'GPT', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning']
---
Large language models (LLMs) have established great success in the general domain of natural language processing. Their emerging task generalization and free-form dialogue capabilities can greatly help to design Chemical General Intelligence (CGI) to assist real-world research in chemistry. However, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry. To this end, we develop ChemDFM, the first LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions as well as various data from the general domain. Therefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities. Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs. Moreover, ChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite the significant size difference. Further qualitative evaluations demonstrate the efficiency and effectiveness of ChemDFM in real-world research scenarios. We will open-source the ChemDFM model soon.
