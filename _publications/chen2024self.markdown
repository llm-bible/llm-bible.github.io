---
layout: publication
title: Self45;cognition In Large Language Models An Exploratory Study
authors: Chen Dongping, Shi Jiawen, Wan Yao, Zhou Pan, Gong Neil Zhenqiang, Sun Lichao
conference: "Arxiv"
year: 2024
bibkey: chen2024self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.01505"}
tags: ['Applications', 'Fine Tuning', 'Prompting', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
While Large Language Models (LLMs) have achieved remarkable success across various applications they also raise concerns regarding self45;cognition. In this paper we perform a pioneering study to explore self45;cognition in LLMs. Specifically we first construct a pool of self45;cognition instruction prompts to evaluate where an LLM exhibits self45;cognition and four well45;designed principles to quantify LLMs self45;cognition. Our study reveals that 4 of the 48 models on Chatbot Arena45;45;specifically Command R Claude345;Opus Llama45;345;70b45;Instruct and Reka45;core45;45;demonstrate some level of detectable self45;cognition. We observe a positive correlation between model size training data quality and self45;cognition level. Additionally we also explore the utility and trustworthiness of LLM in the self45;cognition state revealing that the self45;cognition state enhances some specific tasks such as creative writing and exaggeration. We believe that our work can serve as an inspiration for further research to study the self45;cognition in LLMs.
