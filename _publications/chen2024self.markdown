---
layout: publication
title: 'Self-cognition In Large Language Models: An Exploratory Study'
authors: Chen Dongping, Shi Jiawen, Wan Yao, Zhou Pan, Gong Neil Zhenqiang, Sun Lichao
conference: "Arxiv"
year: 2024
bibkey: chen2024self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.01505"}
tags: ['Applications', 'Fine Tuning', 'Prompting', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
While Large Language Models (LLMs) have achieved remarkable success across various applications they also raise concerns regarding self-cognition. In this paper we perform a pioneering study to explore self-cognition in LLMs. Specifically we first construct a pool of self-cognition instruction prompts to evaluate where an LLM exhibits self-cognition and four well-designed principles to quantify LLMs self-cognition. Our study reveals that 4 of the 48 models on Chatbot Arena--specifically Command R Claude3-Opus Llama-3-70b-Instruct and Reka-core--demonstrate some level of detectable self-cognition. We observe a positive correlation between model size training data quality and self-cognition level. Additionally we also explore the utility and trustworthiness of LLM in the self-cognition state revealing that the self-cognition state enhances some specific tasks such as creative writing and exaggeration. We believe that our work can serve as an inspiration for further research to study the self-cognition in LLMs.
