---
layout: publication
title: 'Influence Of Solution Efficiency And Valence Of Instruction On Additive And Subtractive Solution Strategies In Humans And GPT-4'
authors: Lydia Uhler, Verena Jordan, JÃ¼rgen Buder, Markus Huff, Frank Papenmeier
conference: "Arxiv"
year: 2024
bibkey: uhler2024influence
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.16692"}
tags: ['Efficiency and Optimization', 'GPT', 'Applications', 'Ethics and Bias', 'Model Architecture', 'Reinforcement Learning', 'Interpretability']
---
Generative artificial intelligences, particularly large language models
(LLMs), play an increasingly prominent role in human decision-making contexts,
necessitating transparency about their capabilities. While prior studies have
shown addition biases in humans (Adams et al., 2021) and OpenAI's GPT-3 (Winter
et al., 2023), this study extends the research by comparing human and GPT-4
problem-solving across both spatial and linguistic tasks, with variations in
solution efficiency and valence of task instruction. Four preregistered
experiments with 588 participants from the U.S. and 680 GPT-4 iterations
revealed a stronger tendency towards additive transformations in GPT-4 than in
humans. Human participants were less likely to use additive strategies when
subtraction was relatively more efficient than when addition and subtraction
were equally efficient. GPT-4 exhibited the opposite behavior, with a strong
addition bias when subtraction was more efficient. In terms of valence of task
instruction, GPT-4's use of additive strategies increased when instructed to
"improve" (positive) rather than "edit" (neutral). These findings demonstrate
that biases in human problem-solving are amplified in GPT-4, and that LLM
behavior differs from human efficiency-based strategies. This highlights the
limitations of LLMs and the need for caution when using them in real-world
applications.
