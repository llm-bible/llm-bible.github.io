---
layout: publication
title: 'Toward Cultural Interpretability: A Linguistic Anthropological Framework For Describing And Evaluating Large Language Models (llms)'
authors: Graham M. Jones, Shai Satran, Arvind Satyanarayan
conference: "Arxiv"
year: 2024
bibkey: jones2024toward
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2411.05200'}
tags: ['Attention Mechanism', 'Interpretability and Explainability', 'Model Architecture', 'Tools']
---
This article proposes a new integration of linguistic anthropology and
machine learning (ML) around convergent interests in both the underpinnings of
language and making language technologies more socially responsible. While
linguistic anthropology focuses on interpreting the cultural basis for human
language use, the ML field of interpretability is concerned with uncovering the
patterns that Large Language Models (LLMs) learn from human verbal behavior.
Through the analysis of a conversation between a human user and an LLM-powered
chatbot, we demonstrate the theoretical feasibility of a new, conjoint field of
inquiry, cultural interpretability (CI). By focusing attention on the
communicative competence involved in the way human users and AI chatbots
co-produce meaning in the articulatory interface of human-computer interaction,
CI emphasizes how the dynamic relationship between language and culture makes
contextually sensitive, open-ended conversation possible. We suggest that, by
examining how LLMs internally "represent" relationships between language and
culture, CI can: (1) provide insight into long-standing linguistic
anthropological questions about the patterning of those relationships; and (2)
aid model developers and interface designers in improving value alignment
between language models and stylistically diverse speakers and culturally
diverse speech communities. Our discussion proposes three critical research
axes: relativity, variation, and indexicality.
