---
layout: publication
title: Efficient Language Adaptive Pre45;training Extending State45;of45;the45;art Large Language Models For Polish
authors: Ruciński Szymon
conference: "Arxiv"
year: 2024
bibkey: ruciński2024efficient
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.09759"}
tags: ['Efficiency And Optimization', 'Pretraining Methods', 'Training Techniques']
---
This study explores the potential of fine45;tuning foundational English Large Language Models (LLMs) for generating Polish text. The first step involves Language Adaptive Pre45;training (LAPT) on a high45;quality dataset of 3.11 GB consisting of 276 million Polish tokens. The LAPT is followed by additional fine45;tuning aimed at solving nine KLEJ challenges. Our trained model Curie45;7B45;v1 not only generates Polish text with the lowest perplexity of 3.02 among decoder45;based Polish models but also closely rivals the performance of the best Polish encoder45;decoder models with a less than 237; gap on 8 out of 9 tasks. Curie45;7B45;v1 used approximately 245;337; of a typical dataset size to learn Polish. The LAPT was completed in less than five days using a consumer GPU highlighting the methods efficiency. The proficiency of the model in Polish was significantly enhanced demonstrating the viability of this approach for adding new languages to existing LLMs by training just 1.237; of its parameters. To contribute to the communitys collaborative progress the model has been released as open45;source.
