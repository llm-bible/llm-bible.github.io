---
layout: publication
title: Multi45;modal Understanding And Generation For Medical Images And Text Via Vision45;language Pre45;training
authors: Moon Jong Hak, Lee Hyungyung, Shin Woncheol, Kim Young-hak, Choi Edward
conference: "IEEE Journal of Biomedical and Health Informatics"
year: 2021
bibkey: moon2021multi
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2105.11333"}
  - {name: "Code", url: "https://github.com/SuperSupermoon/MedViLL"}
tags: ['Applications', 'Attention Mechanism', 'BERT', 'Has Code', 'Model Architecture', 'Training Techniques']
---
Recently a number of studies demonstrated impressive performance on diverse vision45;language multi45;modal tasks such as image captioning and visual question answering by extending the BERT architecture with multi45;modal pre45;training objectives. In this work we explore a broad set of multi45;modal representation learning tasks in the medical domain specifically using radiology images and the unstructured report. We propose Medical Vision Language Learner (MedViLL) which adopts a BERT45;based architecture combined with a novel multi45;modal attention masking scheme to maximize generalization performance for both vision45;language understanding tasks (diagnosis classification medical image45;report retrieval medical visual question answering) and vision45;language generation task (radiology report generation). By statistically and rigorously evaluating the proposed model on four downstream tasks with three radiographic image45;report datasets (MIMIC45;CXR Open45;I and VQA45;RAD) we empirically demonstrate the superior downstream task performance of MedViLL against various baselines including task45;specific architectures. The source code is publicly available at https://github.com/SuperSupermoon/MedViLL
