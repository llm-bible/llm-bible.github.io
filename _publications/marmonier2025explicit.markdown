---
layout: publication
title: 'Explicit Learning And The LLM In Machine Translation'
authors: Malik Marmonier, Rachel Bawden, Beno√Æt Sagot
conference: "Arxiv"
year: 2025
bibkey: marmonier2025explicit
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.09454"}
tags: ['Fine-Tuning', 'Applications', 'Interpretability and Explainability', 'Training Techniques', 'Pretraining Methods']
---
This study explores an LLM's ability to learn new languages using explanations found in a grammar book\\(\unicode\{x2014\}\\)a process we term "explicit learning." To rigorously assess this ability, we design controlled translation experiments between English and constructed languages generated\\(\unicode\{x2014\}\\)by specific cryptographic means\\(\unicode\{x2014\}\\)out of Latin or French. Contrary to previous studies, our results demonstrate that LLMs do possess a measurable capacity for explicit learning. This ability, however, diminishes as the complexity of the linguistic phenomena to be learned increases. Supervised fine-tuning on ad hoc chains of thought significantly enhances LLM performance but struggles to generalize to typologically novel or more complex linguistic features. These findings point to the need for more diverse training sets and alternative fine-tuning strategies to further improve explicit learning by LLMs, benefiting low-resource languages typically described in grammar books but lacking extensive corpora.
