---
layout: publication
title: Towards Democratizing Multilingual Large Language Models For Medicine Through A Two45;stage Instruction Fine45;tuning Approach
authors: Zhou Meng, Parmar Surajsinh, Bhatti Anubhav
conference: "Arxiv"
year: 2024
bibkey: zhou2024towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.05732"}
  - {name: "Code", url: "https://github.com/SpassMed/Med&#45;Llama3&#125;"}
tags: ['Efficiency And Optimization', 'Has Code', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
Open45;source multilingual medical large language models (LLMs) have the potential to serve linguistically diverse populations across different regions. Adapting generic LLMs for healthcare often requires continual pretraining but this approach is computationally expensive and sometimes impractical. Instruction fine45;tuning on a specific task may not always guarantee optimal performance due to the lack of broader domain knowledge that the model needs to understand and reason effectively in diverse scenarios. To address these challenges we introduce two multilingual instruction fine45;tuning datasets MMed45;IFT and MMed45;IFT45;MC containing over 200k high45;quality medical samples in six languages. We propose a two45;stage training paradigm the first stage injects general medical knowledge using MMed45;IFT while the second stage fine45;tunes task45;specific multiple45;choice questions with MMed45;IFT45;MC. Our method achieves competitive results on both English and multilingual benchmarks striking a balance between computational efficiency and performance. We plan to make our dataset and model weights public at url123;https://github.com/SpassMed/Med&#45;Llama3&#125; in the future.
