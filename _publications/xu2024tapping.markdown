---
layout: publication
title: 'Tapping The Potential Of Large Language Models As Recommender Systems: A Comprehensive Framework And Empirical Analysis'
authors: Lanling Xu, Junjie Zhang, Bingqian Li, Jinpeng Wang, Sheng Chen, Wayne Xin Zhao, Ji-rong Wen
conference: "Arxiv"
year: 2024
bibkey: xu2024tapping
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2401.04997'}
tags: ['GPT', 'Tools', 'Applications', 'Model Architecture', 'RecSys', 'Prompting']
---
Recently, Large Language Models~(LLMs) such as ChatGPT have showcased
remarkable abilities in solving general tasks, demonstrating the potential for
applications in recommender systems. To assess how effectively LLMs can be used
in recommendation tasks, our study primarily focuses on employing LLMs as
recommender systems through prompting engineering. We propose a general
framework for utilizing LLMs in recommendation tasks, focusing on the
capabilities of LLMs as recommenders. To conduct our analysis, we formalize the
input of LLMs for recommendation into natural language prompts with two key
aspects, and explain how our framework can be generalized to various
recommendation scenarios. As for the use of LLMs as recommenders, we analyze
the impact of public availability, tuning strategies, model architecture,
parameter scale, and context length on recommendation results based on the
classification of LLMs. As for prompt engineering, we further analyze the
impact of four important components of prompts, \ie task descriptions, user
interest modeling, candidate items construction and prompting strategies. In
each section, we first define and categorize concepts in line with the existing
literature. Then, we propose inspiring research questions followed by detailed
experiments on two public datasets, in order to systematically analyze the
impact of different factors on performance. Based on our empirical analysis, we
finally summarize promising directions to shed lights on future research.
