---
layout: publication
title: Better Neural Machine Translation By Extracting Linguistic Information From BERT
authors: Shavarani Hassan S., Sarkar Anoop
conference: "Arxiv"
year: 2021
bibkey: shavarani2021better
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2104.02831"}
tags: ['Applications', 'BERT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
Adding linguistic information (syntax or semantics) to neural machine translation (NMT) has mostly focused on using point estimates from pre45;trained models. Directly using the capacity of massive pre45;trained contextual word embedding models such as BERT (Devlin et al. 2019) has been marginally useful in NMT because effective fine45;tuning is difficult to obtain for NMT without making training brittle and unreliable. We augment NMT by extracting dense fine45;tuned vector45;based linguistic information from BERT instead of using point estimates. Experimental results show that our method of incorporating linguistic information helps NMT to generalize better in a variety of training contexts and is no more difficult to train than conventional Transformer45;based NMT.
