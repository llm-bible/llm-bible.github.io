---
layout: publication
title: Enhancing Pre45;trained Generative Language Models With Question Attended Span Extraction On Machine Reading Comprehension
authors: Ai Lin, Hui Zheng, Liu Zizhou, Hirschberg Julia
conference: "Arxiv"
year: 2024
bibkey: ai2024enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.17991"}
tags: ['BERT', 'GPT', 'Model Architecture', 'Pretraining Methods', 'RAG']
---
Machine Reading Comprehension (MRC) poses a significant challenge in the field of Natural Language Processing (NLP). While mainstream MRC methods predominantly leverage extractive strategies using encoder45;only models such as BERT generative approaches face the issue of out45;of45;control generation 45;45; a critical problem where answers generated are often incorrect irrelevant or unfaithful to the source text. To address these limitations in generative models for MRC we introduce the Question45;Attended Span Extraction (QASE) module. Integrated during the fine45;tuning phase of pre45;trained generative language models (PLMs) QASE significantly enhances their performance allowing them to surpass the extractive capabilities of advanced Large Language Models (LLMs) such as GPT45;4 in few45;shot settings. Notably these gains in performance do not come with an increase in computational demands. The efficacy of the QASE module has been rigorously tested across various datasets consistently achieving or even surpassing state45;of45;the45;art (SOTA) results thereby bridging the gap between generative and extractive models in extractive MRC tasks.
