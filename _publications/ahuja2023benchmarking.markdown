---
layout: publication
title: MEGAVERSE Benchmarking Large Language Models Across Languages Modalities Models And Tasks
authors: Ahuja Sanchit, Aggarwal Divyanshu, Gumma Varun, Watts Ishaan, Sathe Ashutosh, Ochieng Millicent, Hada Rishav, Jain Prachi, Axmed Maxamed, Bali Kalika, Sitaram Sunayana
conference: "Arxiv"
year: 2023
bibkey: ahuja2023benchmarking
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.07463"}
tags: ['GPT', 'Model Architecture', 'Multimodal Models']
---
There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However much of this research has been confined to English leaving LLM building and evaluation for non45;English languages relatively unexplored. Several new LLMs have been introduced recently necessitating their evaluation on non45;English languages. This study aims to perform a thorough evaluation of the non45;English capabilities of SoTA LLMs (GPT45;3.545;Turbo GPT45;4 PaLM2 Gemini45;Pro Mistral Llama2 and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages including low45;resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models GPT45;445;Vision and Gemini45;Pro45;Vision. Our experiments show that larger models such as GPT45;4 Gemini45;Pro and PaLM2 outperform smaller models on various tasks notably on low45;resource languages with GPT45;4 outperforming PaLM2 and Gemini45;Pro on more datasets. We also perform a study on data contamination and find that several models are likely to be contaminated with multilingual evaluation benchmarks necessitating approaches to detect and handle contamination while assessing the multilingual performance of LLMs.
