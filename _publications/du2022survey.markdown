---
layout: publication
title: A Survey Of Vision45;language Pre45;trained Models
authors: Du Yifan, Liu Zikang, Li Junyi, Zhao Wayne Xin
conference: "Arxiv"
year: 2022
bibkey: du2022survey
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2202.10936"}
tags: ['Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Survey Paper', 'Training Techniques', 'Transformer']
---
As transformer evolves pre45;trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre45;training to the field of Vision45;and45;Language (V45;L) learning and improve downstream task performance becomes a focus of multimodal learning. In this paper we review the recent progress in Vision45;Language Pre45;Trained Models (VL45;PTMs). As the core content we first briefly introduce several ways to encode raw images and texts to single45;modal embeddings before pre45;training. Then we dive into the mainstream architectures of VL45;PTMs in modeling the interaction between text and image representations. We further present widely45;used pre45;training tasks and then we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide researchers with synthesis and pointer to related research.
