---
layout: publication
title: DFM\: Dialogue Foundation Model For Universal Large-scale Dialogue-oriented Task Learning
authors: Chen Zhi, Bao Jijia, Chen Lu, Liu Yuncong, Ma Da, Chen Bei, Wu Mengyue, Zhu Su, Dong Xin, Ge Fujiang, Miao Qingliang, Lou Jian-guang, Yu Kai
conference: "Arxiv"
year: 2022
bibkey: chen2022dialogue
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2205.12662"}
tags: ['Agentic', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Building a universal conversational agent has been a long-standing goal of the dialogue research community. Most previous works only focus on a small set of dialogue tasks. In this work we aim to build a unified dialogue foundation model (DFM) which can be used to solve massive diverse dialogue tasks. To achieve this goal a large-scale well-annotated dialogue dataset with rich task diversity (DialogZoo) is collected. We introduce a framework to unify all dialogue tasks and propose novel auxiliary self-supervised tasks to achieve stable training of DFM on the highly diverse large scale DialogZoo corpus. Experiments show that compared with models of the same size DFM can achieve state-of-the-art or competitive performance on very rich cross-domain downstream dialogue tasks. This demonstrates that DFM largely extends the ability of unified dialogue pre-trained model.
