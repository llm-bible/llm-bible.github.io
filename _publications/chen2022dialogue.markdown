---
layout: publication
title: DFM Dialogue Foundation Model For Universal Large45;scale Dialogue45;oriented Task Learning
authors: Chen Zhi, Bao Jijia, Chen Lu, Liu Yuncong, Ma Da, Chen Bei, Wu Mengyue, Zhu Su, Dong Xin, Ge Fujiang, Miao Qingliang, Lou Jian-guang, Yu Kai
conference: "Arxiv"
year: 2022
bibkey: chen2022dialogue
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2205.12662"}
tags: ['Agentic', 'Reinforcement Learning', 'Tools', 'Training Techniques']
---
Building a universal conversational agent has been a long45;standing goal of the dialogue research community. Most previous works only focus on a small set of dialogue tasks. In this work we aim to build a unified dialogue foundation model (DFM) which can be used to solve massive diverse dialogue tasks. To achieve this goal a large45;scale well45;annotated dialogue dataset with rich task diversity (DialogZoo) is collected. We introduce a framework to unify all dialogue tasks and propose novel auxiliary self45;supervised tasks to achieve stable training of DFM on the highly diverse large scale DialogZoo corpus. Experiments show that compared with models of the same size DFM can achieve state45;of45;the45;art or competitive performance on very rich cross45;domain downstream dialogue tasks. This demonstrates that DFM largely extends the ability of unified dialogue pre45;trained model.
