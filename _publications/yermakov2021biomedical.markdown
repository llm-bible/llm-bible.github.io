---
layout: publication
title: 'Biomedical Data-to-text Generation Via Fine-tuning Transformers'
authors: Ruslan Yermakov, Nicholas Drago, Angelo Ziletti
conference: "Arxiv"
year: 2021
bibkey: yermakov2021biomedical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2109.01518"}
tags: ['Training Techniques', 'Model Architecture', 'Reinforcement Learning', 'Language Modeling', 'Pretraining Methods', 'Transformer', 'Fine-Tuning', 'Applications']
---
Data-to-text (D2T) generation in the biomedical domain is a promising - yet
mostly unexplored - field of research. Here, we apply neural models for D2T
generation to a real-world dataset consisting of package leaflets of European
medicines. We show that fine-tuned transformers are able to generate realistic,
multisentence text from data in the biomedical domain, yet have important
limitations. We also release a new dataset (BioLeaflets) for benchmarking D2T
generation models in the biomedical domain.
