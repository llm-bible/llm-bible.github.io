---
layout: publication
title: 'R-eval: A Unified Toolkit For Evaluating Domain Knowledge Of Retrieval Augmented Large Language Models'
authors: Shangqing Tu, Yuanchun Wang, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, Juanzi Li
conference: "Arxiv"
year: 2024
bibkey: tu2024r
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2406.11681'}
  - {name: "Code", url: 'https://github.com/THU-KEG/R-Eval'}
tags: ['Reinforcement Learning', 'RAG', 'Has Code', 'Tools']
---
Large language models have achieved remarkable success on general NLP tasks,
but they may fall short for domain-specific problems. Recently, various
Retrieval-Augmented Large Language Models (RALLMs) are proposed to address this
shortcoming. However, existing evaluation tools only provide a few baselines
and evaluate them on various domains without mining the depth of domain
knowledge. In this paper, we address the challenges of evaluating RALLMs by
introducing the R-Eval toolkit, a Python toolkit designed to streamline the
evaluation of different RAG workflows in conjunction with LLMs. Our toolkit,
which supports popular built-in RAG workflows and allows for the incorporation
of customized testing data on the specific domain, is designed to be
user-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs
across three task levels and two representative domains, revealing significant
variations in the effectiveness of RALLMs across different tasks and domains.
Our analysis emphasizes the importance of considering both task and domain
requirements when choosing a RAG workflow and LLM combination. We are committed
to continuously maintaining our platform at https://github.com/THU-KEG/R-Eval
to facilitate both the industry and the researchers.
