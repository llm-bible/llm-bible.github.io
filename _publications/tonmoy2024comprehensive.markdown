---
layout: publication
title: A Comprehensive Survey Of Hallucination Mitigation Techniques In Large Language Models
authors: Tonmoy S. M Towhidul Islam, Zaman S M Mehedi, Jain Vinija, Rani Anku, Rawte Vipula, Chadha Aman, Das Amitava
conference: "Arxiv"
year: 2024
bibkey: tonmoy2024comprehensive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.01313"}
tags: ['Applications', 'Ethics And Bias', 'Prompting', 'RAG', 'Reinforcement Learning', 'Survey Paper', 'Training Techniques']
---
As Large Language Models (LLMs) continue to advance in their ability to write human-like text a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact peoples lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency it also means they are capable of extrapolating information from the biases in training data misinterpreting ambiguous prompts or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications such as summarizing medical records financial analysis reports etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al 2021) Knowledge Retrieval (Varshney et al2023) CoNLI (Lei et al 2023) and CoVe (Dhuliawala et al 2023). Furthermore we introduce a detailed taxonomy categorizing these methods based on various parameters such as dataset utilization common tasks feedback mechanisms and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally we analyze the challenges and limitations inherent in these techniques providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.
