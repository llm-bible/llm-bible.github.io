---
layout: publication
title: 'What Is The Visual Cognition Gap Between Humans And Multimodal Llms?'
authors: Cao Xu, Lai Bolin, Ye Wenqian, Ma Yunsheng, Heintz Joerg, Chen Jintai, Cao Jianguo, Rehg James M.
conference: "Arxiv"
year: 2024
bibkey: cao2024what
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.10424"}
tags: ['Multimodal Models', 'Reinforcement Learning']
---
Recently, Multimodal Large Language Models (MLLMs) have shown great promise
in language-guided perceptual tasks such as recognition, segmentation, and
object detection. However, their effectiveness in addressing visual cognition
problems that require high-level reasoning is not well-established. One such
challenge is abstract visual reasoning (AVR) -- the cognitive ability to
discern relationships among patterns in a set of images and extrapolate to
predict subsequent patterns. This skill is crucial during the early
neurodevelopmental stages of children. Inspired by the AVR tasks in Raven's
Progressive Matrices (RPM) and Wechsler Intelligence Scale for Children (WISC),
we propose a new dataset MaRs-VQA and a new benchmark VCog-Bench containing
three datasets to evaluate the zero-shot AVR capability of MLLMs and compare
their performance with existing human intelligent investigation. Our
comparative experiments with different open-source and closed-source MLLMs on
the VCog-Bench revealed a gap between MLLMs and human intelligence,
highlighting the visual cognitive limitations of current MLLMs. We believe that
the public release of VCog-Bench, consisting of MaRs-VQA, and the inference
pipeline will drive progress toward the next generation of MLLMs with
human-like visual cognition abilities.
