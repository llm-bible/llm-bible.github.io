---
layout: publication
title: Sequence45;to45;sequence Spanish Pre45;trained Language Models
authors: Araujo Vladimir, Trusca Maria Mihaela, Tufi√±o Rodrigo, Moens Marie-francine
conference: "Arxiv"
year: 2023
bibkey: araujo2023sequence
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2309.11259"}
  - {name: "Code", url: "https://github.com/vgaraujov/Seq2Seq&#45;Spanish&#45;PLMs"}
tags: ['Applications', 'BERT', 'Fine Tuning', 'GPT', 'Has Code', 'Merging', 'Model Architecture']
---
In recent years significant advancements in pre45;trained language models have driven the creation of numerous non45;English language variants with a particular emphasis on encoder45;only and decoder45;only architectures. While Spanish language models based on BERT and GPT have demonstrated proficiency in natural language understanding and generation there remains a noticeable scarcity of encoder45;decoder models explicitly designed for sequence45;to45;sequence tasks which aim to map input sequences to generate output sequences conditionally. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder45;decoder architectures exclusively pre45;trained on Spanish corpora. Specifically we present Spanish versions of BART T5 and BERT2BERT45;style models and subject them to a comprehensive assessment across various sequence45;to45;sequence tasks including summarization question answering split45;and45;rephrase dialogue and translation. Our findings underscore the competitive performance of all models with the BART45; and T545;based models emerging as top performers across all tasks. We have made all models publicly available to the research community to foster future explorations and advancements in Spanish NLP https://github.com/vgaraujov/Seq2Seq&#45;Spanish&#45;PLMs.
