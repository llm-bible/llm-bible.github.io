---
layout: publication
title: 3D45;GRAND A Million45;scale Dataset For 3d45;llms With Better Grounding And Less Hallucination
authors: Yang Jianing, Chen Xuweiyi, Madaan Nikhil, Iyengar Madhavan, Qian Shengyi, Fouhey David F., Chai Joyce
conference: "Arxiv"
year: 2024
bibkey: yang2024million
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.05132"}
tags: ['Agentic', 'Pretraining Methods', 'Reinforcement Learning']
---
The integration of language and 3D perception is crucial for developing embodied agents and robots that comprehend and interact with the physical world. While large language models (LLMs) have demonstrated impressive language understanding and generation capabilities their adaptation to 3D environments (3D45;LLMs) remains in its early stages. A primary challenge is the absence of large45;scale datasets that provide dense grounding between language and 3D scenes. In this paper we introduce 3D45;GRAND a pioneering large45;scale dataset comprising 40087 household scenes paired with 6.2 million densely45;grounded scene45;language instructions. Our results show that instruction tuning with 3D45;GRAND significantly enhances grounding capabilities and reduces hallucinations in 3D45;LLMs. As part of our contributions we propose a comprehensive benchmark 3D45;POPE to systematically evaluate hallucination in 3D45;LLMs enabling fair comparisons among future models. Our experiments highlight a scaling effect between dataset size and 3D45;LLM performance emphasizing the critical role of large45;scale 3D45;text datasets in advancing embodied AI research. Notably our results demonstrate early signals for effective sim45;to45;real transfer indicating that models trained on large synthetic data can perform well on real45;world 3D scans. Through 3D45;GRAND and 3D45;POPE we aim to equip the embodied AI community with essential resources and insights setting the stage for more reliable and better45;grounded 3D45;LLMs. Project website https://3d&#45;grand.github.io
