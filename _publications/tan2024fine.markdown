---
layout: publication
title: Fine45;tuning Large Language Model (LLM) Artificial Intelligence Chatbots In Ophthalmology And Llm45;based Evaluation Using GPT45;4
authors: Tan Ting Fang, Elangovan Kabilan, Jin Liyuan, Jie Yao, Yong Li, Lim Joshua, Poh Stanley, Ng Wei Yan, Lim Daniel, Ke Yuhe, Liu Nan, Ting Daniel Shu Wei
conference: "Arxiv"
year: 2024
bibkey: tan2024fine
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.10083"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Responsible AI']
---
Purpose To assess the alignment of GPT45;445;based evaluation to human clinician experts for the evaluation of responses to ophthalmology45;related patient queries generated by fine45;tuned LLM chatbots. Methods 400 ophthalmology questions and paired answers were created by ophthalmologists to represent commonly asked patient questions divided into fine45;tuning (368; 9237;) and testing (40; 837;). We find45;tuned 5 different LLMs including LLAMA245;7b LLAMA245;7b45;Chat LLAMA245;13b and LLAMA245;13b45;Chat. For the testing dataset additional 8 glaucoma QnA pairs were included. 200 responses to the testing dataset were generated by 5 fine45;tuned LLMs for evaluation. A customized clinical evaluation rubric was used to guide GPT45;4 evaluation grounded on clinical accuracy relevance patient safety and ease of understanding. GPT45;4 evaluation was then compared against ranking by 5 clinicians for clinical alignment. Results Among all fine45;tuned LLMs GPT45;3.5 scored the highest (87.137;) followed by LLAMA245;13b (80.937;) LLAMA245;13b45;chat (75.537;) LLAMA245;7b45;Chat (7037;) and LLAMA245;7b (68.837;) based on the GPT45;4 evaluation. GPT45;4 evaluation demonstrated significant agreement with human clinician rankings with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80 respectively; while correlation based on Cohen Kappa was more modest at 0.50. Notably qualitative analysis and the glaucoma sub45;analysis revealed clinical inaccuracies in the LLM45;generated responses which were appropriately identified by the GPT45;4 evaluation. Conclusion The notable clinical alignment of GPT45;4 evaluation highlighted its potential to streamline the clinical evaluation of LLM chatbot responses to healthcare45;related queries. By complementing the existing clinician45;dependent manual grading this efficient and automated evaluation could assist the validation of future developments in LLM applications for healthcare.
