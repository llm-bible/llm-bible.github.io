---
layout: publication
title: Themis Towards Flexible And Interpretable NLG Evaluation
authors: Hu Xinyu, Lin Li, Gao Mingqi, Yin Xunjian, Wan Xiaojun
conference: "Arxiv"
year: 2024
bibkey: hu2024towards
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.18365"}
tags: ['Ethics And Bias', 'GPT', 'Model Architecture']
---
The evaluation of natural language generation (NLG) tasks is a significant and longstanding research issue. With the recent emergence of powerful large language models (LLMs) some studies have turned to LLM45;based automatic evaluation methods which demonstrate great potential to become a new evaluation paradigm following traditional string45;based and model45;based metrics. However despite the improved performance of existing methods they still possess some deficiencies such as dependency on references and limited evaluation flexibility. Therefore in this paper we meticulously construct a large45;scale NLG evaluation corpus NLG45;Eval with human and GPT45;4 annotations to alleviate the lack of relevant data in this field. Furthermore we propose Themis an LLM dedicated to NLG evaluation which has been trained with our designed multi45;perspective consistency and rating45;oriented preference alignment methods. Themis can conduct flexible and interpretable evaluations without references and it exhibits superior evaluation performance on various NLG tasks simultaneously generalizing well to unseen tasks and surpassing other evaluation models including GPT45;4.
