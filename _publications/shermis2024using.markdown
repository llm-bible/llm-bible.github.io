---
layout: publication
title: Using ChatGPT to Score Essays and Short-Form Constructed Responses
authors: Shermis Mark D.
conference: "Arxiv"
year: 2024
bibkey: shermis2024using
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.09540"}
tags: ['ARXIV', 'Chatgpt', 'Efficiency And Optimization', 'Ethics And Bias', 'Fine Tuning', 'GPT', 'Pretraining Methods', 'Reinforcement Learning']
---
This study aimed to determine if ChatGPTs large language models could match the scoring accuracy of human and machine scores from the ASAP competition. The investigation focused on various prediction models including linear regression random forest gradient boost and boost. ChatGPTs performance was evaluated against human raters using quadratic weighted kappa (QWK) metrics. Results indicated that while ChatGPTs gradient boost model achieved QWKs close to human raters for some data sets its overall performance was inconsistent and often lower than human scores. The study highlighted the need for further refinement particularly in handling biases and ensuring scoring fairness. Despite these challenges ChatGPT demonstrated potential for scoring efficiency especially with domain-specific fine-tuning. The study concludes that ChatGPT can complement human scoring but requires additional development to be reliable for high-stakes assessments. Future research should improve model accuracy address ethical considerations and explore hybrid models combining ChatGPT with empirical methods.
