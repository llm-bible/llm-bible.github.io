---
layout: publication
title: Using Chatgpt To Score Essays And Short45;form Constructed Responses
authors: Shermis Mark D.
conference: "Arxiv"
year: 2024
bibkey: shermis2024using
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.09540"}
tags: ['Bias Mitigation', 'Efficiency And Optimization', 'Ethics And Bias', 'Fairness', 'GPT', 'Model Architecture', 'Reinforcement Learning']
---
This study aimed to determine if ChatGPTs large language models could match the scoring accuracy of human and machine scores from the ASAP competition. The investigation focused on various prediction models including linear regression random forest gradient boost and boost. ChatGPTs performance was evaluated against human raters using quadratic weighted kappa (QWK) metrics. Results indicated that while ChatGPTs gradient boost model achieved QWKs close to human raters for some data sets its overall performance was inconsistent and often lower than human scores. The study highlighted the need for further refinement particularly in handling biases and ensuring scoring fairness. Despite these challenges ChatGPT demonstrated potential for scoring efficiency especially with domain45;specific fine45;tuning. The study concludes that ChatGPT can complement human scoring but requires additional development to be reliable for high45;stakes assessments. Future research should improve model accuracy address ethical considerations and explore hybrid models combining ChatGPT with empirical methods.
