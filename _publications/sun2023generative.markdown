---
layout: publication
title: Generative Multimodal Models Are In45;context Learners
authors: Sun Quan, Cui Yufeng, Zhang Xiaosong, Zhang Fan, Yu Qiying, Luo Zhengxiong, Wang Yueze, Rao Yongming, Liu Jingjing, Huang Tiejun, Wang Xinlong
conference: "Arxiv"
year: 2023
bibkey: sun2023generative
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.13286"}
tags: ['Applications', 'GPT', 'Merging', 'Multimodal Models', 'Pretraining Methods', 'Prompting']
---
The human ability to easily solve multimodal tasks in context (i.e. with only a few demonstrations or simple instructions) is what current multimodal systems have largely struggled to imitate. In this work we demonstrate that the task45;agnostic in45;context learning capabilities of large multimodal models can be significantly enhanced by effective scaling45;up. We introduce Emu2 a generative multimodal model with 37 billion parameters trained on large45;scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in45;context learning abilities even emerging to solve tasks that require on45;the45;fly reasoning such as visual prompting and object45;grounded generation. The model sets a new record on multiple multimodal understanding tasks in few45;shot settings. When instruction45;tuned to follow specific instructions Emu2 further achieves new state45;of45;the45;art on challenging tasks such as question answering benchmarks for large multimodal models and open45;ended subject45;driven generation. These achievements demonstrate that Emu2 can serve as a base model and general45;purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research.
