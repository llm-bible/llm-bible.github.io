---
layout: publication
title: Understanding LLMs A Comprehensive Overview from Training to Inference
authors: Liu Yiheng, He Hao, Han Tianle, Zhang Xu, Liu Mengyuan, Tian Jiaming, Zhang Yutong, Wang Jiaqi, Gao Xiaohui, Zhong Tianyang, Pan Yi, Xu Shaochen, Wu Zihao, Liu Zhengliang, Zhang Xin, Zhang Shu, Hu Xintao, Zhang Tuo, Qiang Ning, Liu Tianming, Ge Bao
conference: "Arxiv"
year: 2024
bibkey: liu2024understanding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.02038"}
tags: ['Efficiency And Optimization', 'Fine Tuning', 'GPT', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Quantization', 'Training Techniques']
---
The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. Theres an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects including data preprocessing training architecture pre-training tasks parallel training and relevant content related to model fine-tuning. On the inference side the paper covers topics such as model compression parallel computation memory scheduling and structural optimization. It also explores LLMs utilization and provides insights into their future development.
