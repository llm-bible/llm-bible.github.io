---
layout: publication
title: 'Understanding Llms: A Comprehensive Overview From Training To Inference'
authors: Yiheng Liu et al.
conference: Arxiv
year: 2024
citations: 16
bibkey: liu2024understanding
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2401.02038'}]
tags: [Fine-Tuning, Pre-Training, Efficiency and Optimization, Quantization, GPT]
---
The introduction of ChatGPT has led to a significant increase in the
utilization of Large Language Models (LLMs) for addressing downstream tasks.
There's an increasing focus on cost-efficient training and deployment within
this context. Low-cost training and deployment of LLMs represent the future
development trend. This paper reviews the evolution of large language model
training techniques and inference deployment technologies aligned with this
emerging trend. The discussion on training includes various aspects, including
data preprocessing, training architecture, pre-training tasks, parallel
training, and relevant content related to model fine-tuning. On the inference
side, the paper covers topics such as model compression, parallel computation,
memory scheduling, and structural optimization. It also explores LLMs'
utilization and provides insights into their future development.