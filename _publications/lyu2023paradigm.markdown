---
layout: publication
title: 'A Paradigm Shift: The Future Of Machine Translation Lies With Large Language Models'
authors: Chenyang Lyu, Zefeng Du, Jitao Xu, Yitao Duan, Minghao Wu, Teresa Lynn, Alham Fikri Aji, Derek F. Wong, Siyou Liu, Longyue Wang
conference: "Arxiv"
year: 2023
bibkey: lyu2023paradigm
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.01181"}
tags: ['Fine-Tuning', 'GPT', 'Applications', 'Model Architecture', 'Reinforcement Learning', 'Prompting']
---
Machine Translation (MT) has greatly advanced over the years due to the
developments in deep neural networks. However, the emergence of Large Language
Models (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MT
domain. In this context, we believe that the future of MT is intricately tied
to the capabilities of LLMs. These models not only offer vast linguistic
understandings but also bring innovative methodologies, such as prompt-based
techniques, that have the potential to further elevate MT. In this paper, we
provide an overview of the significant enhancements in MT that are influenced
by LLMs and advocate for their pivotal role in upcoming MT research and
implementations. We highlight several new MT directions, emphasizing the
benefits of LLMs in scenarios such as Long-Document Translation, Stylized
Translation, and Interactive Translation. Additionally, we address the
important concern of privacy in LLM-driven MT and suggest essential
privacy-preserving strategies. By showcasing practical instances, we aim to
demonstrate the advantages that LLMs offer, particularly in tasks like
translating extended documents. We conclude by emphasizing the critical role of
LLMs in guiding the future evolution of MT and offer a roadmap for future
exploration in the sector.
