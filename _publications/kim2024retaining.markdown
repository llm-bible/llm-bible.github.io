---
layout: publication
title: 'Retaining And Enhancing Pre-trained Knowledge In Vision-language Models With Prompt Ensembling'
authors: Donggeun Kim, Yujin Jo, Myungjoo Lee, Taesup Kim
conference: "Arxiv"
year: 2024
bibkey: kim2024retaining
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.07077"}
tags: ['Pre-Training', 'Efficiency and Optimization', 'Model Architecture', 'Reinforcement Learning', 'Security', 'Training Techniques', 'Attention Mechanism', 'Multimodal Models', 'Prompting']
---
The advancement of vision-language models, particularly the Contrastive
Language-Image Pre-training (CLIP) model, has revolutionized the field of
machine learning by enabling robust zero-shot learning capabilities. These
capabilities allow models to understand and respond to previously unseen data
without task-specific training. However, adapting CLIP to integrate specialized
knowledge from various domains while retaining its zero-shot capabilities
remains a significant challenge. To address this, we introduce a novel prompt
ensemble learning approach called Group-wise Prompt Ensemble (GPE). This method
aims to enhance CLIP's zero-shot capabilities by incorporating new domain
knowledge while improving its adaptability and robustness against data
distribution shifts. Our approach hinges on three main strategies: prompt
grouping with masked attention to optimize CLIP's adaptability while
safeguarding its zero-shot capabilities; the incorporation of auxiliary prompts
for the seamless integration of new domain insights without disrupting the
original model's representation; and an ensemble learning strategy that
effectively merges original and new knowledge. Through rigorous
experimentation, including more challenging cross-dataset transfer evaluations,
our GPE method redefines the benchmarks for the adaptability and efficiency of
vision-language models, surpassing existing models across various scenarios.
