---
layout: publication
title: Ziya45;visual Bilingual Large Vision45;language Model Via Multi45;task Instruction Tuning
authors: Lu Junyu, Zhang Dixiang, Wu Xiaojun, Gao Xinyu, Gan Ruyi, Zhang Jiaxing, Song Yan, Zhang Pingjian
conference: "Arxiv"
year: 2023
bibkey: lu2023ziya
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.08166"}
  - {name: "Code", url: "https://huggingface.co/IDEA&#45;CCNL/Ziya&#45;BLIP2&#45;14B&#45;Visual&#45;v1&#125;"}
tags: ['Applications', 'Efficiency And Optimization', 'GPT', 'Has Code', 'Language Modeling', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques', 'Transformer']
---
Recent advancements enlarge the capabilities of large language models (LLMs) in zero45;shot image45;to45;text generation and understanding by integrating multi45;modal inputs. However such success is typically limited to English scenarios due to the lack of large45;scale and high45;quality non45;English multi45;modal resources making it extremely difficult to establish competitive counterparts in other languages. In this paper we introduce the Ziya45;Visual series a set of bilingual large45;scale vision45;language models (LVLMs) designed to incorporate visual semantics into LLM for multi45;modal dialogue. Composed of Ziya45;Visual45;Base and Ziya45;Visual45;Chat our models adopt the Querying Transformer from BLIP45;2 further exploring the assistance of optimization schemes such as instruction tuning multi45;stage training and low45;rank adaptation module for visual45;language alignment. In addition we stimulate the understanding ability of GPT45;4 in multi45;modal scenarios translating our gathered English image45;text datasets into Chinese and generating instruction45;response through the in45;context learning method. The experiment results demonstrate that compared to the existing LVLMs Ziya45;Visual achieves competitive performance across a wide range of English45;only tasks including zero45;shot image45;text retrieval image captioning and visual question answering. The evaluation leaderboard accessed by GPT45;4 also indicates that our models possess satisfactory image45;text understanding and generation capabilities in Chinese multi45;modal scenario dialogues. Code demo and models are available at ~url123;https://huggingface.co/IDEA&#45;CCNL/Ziya&#45;BLIP2&#45;14B&#45;Visual&#45;v1&#125;.
