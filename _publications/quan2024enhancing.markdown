---
layout: publication
title: Enhancing Ethical Explanations Of Large Language Models Through Iterative Symbolic Refinement
authors: Quan Xin, Valentino Marco, Dennis Louise A., Freitas Andr√©
conference: "Arxiv"
year: 2024
bibkey: quan2024enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.00745"}
tags: ['Interpretability And Explainability', 'Prompting', 'Reinforcement Learning', 'Tools']
---
An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success however LLMs are still prone to factual errors and inconsistencies in their explanations offering limited control and interpretability for inference in complex domains. In this paper we focus on ethical NLI investigating how hybrid neuro45;symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically we present an abductive45;deductive framework named Logic45;Explainer which integrates LLMs with an external backward45;chaining solver to refine step45;wise natural language explanations and jointly verify their correctness reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic45;Explainer can improve explanations generated via in45;context learning methods and Chain45;of45;Thought (CoT) on challenging ethical NLI tasks while at the same time producing formal proofs describing and supporting models reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations our results suggest the effectiveness of neuro45;symbolic methods for multi45;step NLI more broadly opening new opportunities to enhance the logical consistency reliability and alignment of LLMs.
