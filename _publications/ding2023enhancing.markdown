---
layout: publication
title: Enhancing Chat Language Models By Scaling High45;quality Instructional Conversations
authors: Ding Ning, Chen Yulin, Xu Bokai, Qin Yujia, Zheng Zhi, Hu Shengding, Liu Zhiyuan, Sun Maosong, Zhou Bowen
conference: "Arxiv"
year: 2023
bibkey: ding2023enhancing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.14233"}
  - {name: "Code", url: "https://github.com/thunlp/UltraChat&#125;&#125;"}
tags: ['Ethics And Bias', 'GPT', 'Has Code', 'Model Architecture', 'RAG', 'Reinforcement Learning', 'Tools']
---
Fine45;tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data although straightforward stands a great chance of leading to improved performance. This paper aims to improve the upper bound of open45;source models further. We first provide a systematically designed diverse informative large45;scale dataset of instructional conversations UltraChat which does not involve human queries. Our objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi45;turn conversation iteratively. UltraChat contains 1.5 million high45;quality multi45;turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics including scale average length diversity coherence etc. solidifying its position as a leading open45;source dataset. Building upon UltraChat we fine45;tune a LLaMA model to create a powerful conversational model UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently outperforms other open45;source models including Vicuna the previously recognized state45;of45;the45;art open45;source model. The dataset and the model will be publicly releasedfootnote123;url123;https://github.com/thunlp/UltraChat&#125;&#125;.
