---
layout: publication
title: Demystifying Instruction Mixing For Fine45;tuning Large Language Models
authors: Wang Renxi, Li Haonan, Wu Minghao, Wang Yuxia, Han Xudong, Zhang Chiyu, Baldwin Timothy
conference: "Arxiv"
year: 2023
bibkey: wang2023demystifying
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.10793"}
tags: ['Applications', 'Pretraining Methods', 'Reinforcement Learning']
---
Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However the procedure to optimizing the mixing of instruction datasets for LLM fine45;tuning is still poorly understood. This study categorizes instructions into three primary types NLP downstream tasks coding and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures laying the foundations for future research.
