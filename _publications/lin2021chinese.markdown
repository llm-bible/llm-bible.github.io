---
layout: publication
title: 'M6: A Chinese Multimodal Pretrainer'
authors: Junyang Lin et al.
conference: Arxiv
year: 2021
citations: 47
bibkey: lin2021chinese
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2103.00823'}]
tags: [Multimodal Models, Pre-Training, Transformer, Applications]
---
In this work, we construct the largest dataset for multimodal pretraining in
Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide
range of domains. We propose a cross-modal pretraining method called M6,
referring to Multi-Modality to Multi-Modality Multitask Mega-transformer, for
unified pretraining on the data of single modality and multiple modalities. We
scale the model size up to 10 billion and 100 billion parameters, and build the
largest pretrained model in Chinese. We apply the model to a series of
downstream applications, and demonstrate its outstanding performance in
comparison with strong baselines. Furthermore, we specifically design a
downstream task of text-guided image generation, and show that the finetuned M6
can create high-quality images with high resolution and abundant details.