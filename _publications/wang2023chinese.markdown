---
layout: publication
title: Auroraactivating Chinese Chat Capability For Mixtral45;8x7b Sparse Mixture45;of45;experts Through Instruction45;tuning
authors: Wang Rongsheng, Chen Haoming, Zhou Ruizhe, Duan Yaofei, Cai Kunyan, Ma Han, Cui Jiaxi, Li Jian, Pang Patrick Cheong-iao, Wang Yapeng, Tan Tao
conference: "Arxiv"
year: 2023
bibkey: wang2023chinese
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.14557"}
  - {name: "Code", url: "https://github.com/WangRongsheng/Aurora"}
tags: ['Efficiency And Optimization', 'Has Code', 'Model Architecture']
---
Existing research has demonstrated that refining large language models (LLMs) through the utilization of machine45;generated instruction45;following data empowers these models to exhibit impressive zero45;shot capabilities for novel tasks without requiring human45;authored instructions. In this paper we systematically investigate preprocess and integrate three Chinese instruction45;following datasets with the aim of enhancing the Chinese conversational capabilities of Mixtral45;8x7B sparse Mixture45;of45;Experts model. Through instruction fine45;tuning on this carefully processed dataset we successfully construct the Mixtral45;8x7B sparse Mixture45;of45;Experts model named Aurora. To assess the performance of Aurora we utilize three widely recognized benchmark tests C45;Eval MMLU and CMMLU. Empirical studies validate the effectiveness of instruction fine45;tuning applied to Mixtral45;8x7B sparse Mixture45;of45;Experts model. This work is pioneering in the execution of instruction fine45;tuning on a sparse expert45;mixed model marking a significant breakthrough in enhancing the capabilities of this model architecture. Our code data and model are publicly available at https://github.com/WangRongsheng/Aurora
