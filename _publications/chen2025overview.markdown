---
layout: publication
title: 'Overview Of The NTCIR-18 Automatic Evaluation Of Llms (AEOLLM) Task'
authors: Junjie Chen, Haitao Li, Zhumin Chu, Yiqun Liu, Qingyao Ai
conference: "Arxiv"
year: 2025
bibkey: chen2025overview
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.13038"}
tags: ['RAG']
---
In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of
LLMs (AEOLLM) task. As large language models (LLMs) grow popular in both
academia and industry, how to effectively evaluate the capacity of LLMs becomes
an increasingly critical but still challenging issue. Existing methods can be
divided into two types: manual evaluation, which is expensive, and automatic
evaluation, which faces many limitations including task format (the majority
belong to multiple-choice questions) and evaluation criteria (occupied by
reference-based metrics). To advance the innovation of automatic evaluation, we
propose the AEOLLM task which focuses on generative tasks and encourages
reference-free methods. Besides, we set up diverse subtasks such as dialogue
generation, text expansion, summary generation and non-factoid question
answering to comprehensively test different methods. This year, we received 48
runs from 4 teams in total. This paper will describe the background of the
task, the data set, the evaluation measures and the evaluation results,
respectively.
