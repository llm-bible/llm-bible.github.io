---
layout: publication
title: Mupt A Generative Symbolic Music Pretrained Transformer
authors: Qu Xingwei, Bai Yuelin, Ma Yinghao, Zhou Ziya, Lo Ka Man, Liu Jiaheng, Yuan Ruibin, Min Lejun, Liu Xueling, Zhang Tianyu, Du Xinrun, Guo Shuyue, Liang Yiming, Li Yizhi, Wu Shangda, Zhou Junting, Zheng Tianyu, Ma Ziyang, Han Fengze, Xue Wei, Xia Gus, Benetos Emmanouil, Yue Xiang, Lin Chenghua, Tan Xu, Huang Stephen W., Fu Jie, Zhang Ge
conference: "Arxiv"
year: 2024
bibkey: qu2024mupt
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2404.06393"}
tags: ['Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
In this paper we explore the application of Large Language Models (LLMs) to the pre-training of music. While the prevalent use of MIDI in music modeling is well-established our findings suggest that LLMs are inherently more compatible with ABC Notation which aligns more closely with their design and strengths thereby enhancing the models performance in musical composition. To address the challenges associated with misaligned measures from different tracks during generation we propose the development of a Synchronized Multi-Track ABC Notation (SMT-ABC Notation) which aims to preserve coherence across multiple musical tracks. Our contributions include a series of models capable of handling up to 8192 tokens covering 9037; of the symbolic music data in our training set. Furthermore we explore the implications of the Symbolic Music Scaling Law (SMS Law) on model performance. The results indicate a promising direction for future research in music generation offering extensive resources for community-led research through our open-source contributions.
