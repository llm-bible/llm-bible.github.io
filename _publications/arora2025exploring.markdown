---
layout: publication
title: 'Exploring Robustness Of Llms To Sociodemographically-conditioned Paraphrasing'
authors: Pulkit Arora, Akbar Karimi, Lucie Flek
conference: "Arxiv"
year: 2025
bibkey: arora2025exploring
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.08276"}
tags: ['Security', 'Reinforcement Learning', 'Ethics and Bias', 'Interpretability', 'Interpretability and Explainability', 'Prompting']
---
Large Language Models (LLMs) have shown impressive performance in various NLP
tasks. However, there are concerns about their reliability in different domains
of linguistic variations. Many works have proposed robustness evaluation
measures for local adversarial attacks, but we need globally robust models
unbiased to different language styles. We take a broader approach to explore a
wider range of variations across sociodemographic dimensions to perform
structured reliability tests on the reasoning capacity of language models. We
extend the SocialIQA dataset to create diverse paraphrased sets conditioned on
sociodemographic styles. The assessment aims to provide a deeper understanding
of LLMs in (a) their capability of generating demographic paraphrases with
engineered prompts and (b) their reasoning capabilities in real-world, complex
language scenarios. We also explore measures such as perplexity,
explainability, and ATOMIC performance of paraphrases for fine-grained
reliability analysis of LLMs on these sets. We find that demographic-specific
paraphrasing significantly impacts the performance of language models,
indicating that the subtleties of language variations remain a significant
challenge. The code and dataset will be made available for reproducibility and
future research.
