---
layout: publication
title: Inquire Interact And Integrate A Proactive Agent Collaborative Framework For Zero45;shot Multimodal Medical Reasoning
authors: Gu Zishan, Liu Fenglin, Yin Changchang, Zhang Ping
conference: "Arxiv"
year: 2024
bibkey: gu2024proactive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.11640"}
tags: ['Agentic', 'Applications', 'Multimodal Models', 'Tools']
---
The adoption of large language models (LLMs) in healthcare has attracted significant research interest. However their performance in healthcare remains under45;investigated and potentially limited due to i) they lack rich domain45;specific knowledge and medical reasoning skills; and ii) most state45;of45;the45;art LLMs are unimodal text45;only models that cannot directly process multimodal inputs. To this end we propose a multimodal medical collaborative reasoning framework textbf123;MultiMedRes125; which incorporates a learner agent to proactively gain essential information from domain45;specific expert models to solve medical multimodal reasoning problems. Our method includes three steps i) textbf123;Inquire125; The learner agent first decomposes given complex medical reasoning problems into multiple domain45;specific sub45;problems; ii) textbf123;Interact125; The agent then interacts with domain45;specific expert models by repeating the ask45;answer process to progressively obtain different domain45;specific knowledge; iii) textbf123;Integrate125; The agent finally integrates all the acquired domain45;specific knowledge to accurately address the medical reasoning problem. We validate the effectiveness of our method on the task of difference visual question answering for X45;ray images. The experiments demonstrate that our zero45;shot prediction achieves state45;of45;the45;art performance and even outperforms the fully supervised methods. Besides our approach can be incorporated into various LLMs and multimodal LLMs to significantly boost their performance.
