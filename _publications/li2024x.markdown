---
layout: publication
title: X45;instruction Aligning Language Model In Low45;resource Languages With Self45;curated Cross45;lingual Instructions
authors: Li Chong, Yang Wen, Zhang Jiajun, Lu Jinliang, Wang Shaonan, Zong Chengqing
conference: "Arxiv"
year: 2024
bibkey: li2024x
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.19744"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
Large language models respond well in high45;resource languages like English but struggle in low45;resource languages. It may arise from the lack of high45;quality instruction following data in these languages. Directly translating English samples into these languages can be a solution but unreliable leading to responses with translation errors and lacking language45;specific or cultural knowledge. To address this issue we propose a novel method to construct cross45;lingual instruction following samples with instruction in English and response in low45;resource languages. Specifically the language model first learns to generate appropriate English instructions according to the natural web texts in other languages as responses. The candidate cross45;lingual instruction tuning samples are further refined and diversified. We have employed this method to build a large45;scale cross45;lingual instruction tuning dataset on 10 languages namely X45;Instruction. The instruction data built using our method incorporate more language45;specific knowledge compared with the naive translation method. Experimental results have shown that the response quality of the model tuned on X45;Instruction greatly exceeds the model distilled from a powerful teacher model reaching or even surpassing the ones of ChatGPT. In addition we find that models tuned on cross45;lingual instruction following samples can follow the instruction in the output language without further tuning.
