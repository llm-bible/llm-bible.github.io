---
layout: publication
title: What Do You Learn From Context Probing For Sentence Structure In Contextualized Word Representations
authors: Tenney Ian, Xia Patrick, Chen Berlin, Wang Alex, Poliak Adam, Mccoy R Thomas, Kim Najoung, Van Durme Benjamin, Bowman Samuel R., Das Dipanjan, Pavlick Ellie
conference: "Arxiv"
year: 2019
bibkey: tenney2019what
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1905.06316"}
tags: ['BERT', 'Language Modeling', 'Model Architecture', 'Pretraining Methods']
---
Contextualized representation models such as ELMo (Peters et al. 2018a) and BERT (Devlin et al. 2018) have recently achieved state45;of45;the45;art results on a diverse array of downstream NLP tasks. Building on recent token45;level probing work we introduce a novel edge probing task design and construct a broad suite of sub45;sentence tasks derived from the traditional structured NLP pipeline. We probe word45;level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic semantic local and long45;range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena but only offer comparably small improvements on semantic tasks over a non45;contextual baseline.
