---
layout: publication
title: Fine45;tuning With HED45;IT The Impact Of Human Post45;editing For Dialogical Language Models
authors: Occhipinti Daniela, Marchi Michele, Mondella Irene, Lai Huiyuan, Dell'orletta Felice, Nissim Malvina, Guerini Marco
conference: "Arxiv"
year: 2024
bibkey: occhipinti2024fine
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.07288"}
tags: ['Attention Mechanism', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Training Techniques']
---
Automatic methods for generating and gathering linguistic data have proven effective for fine45;tuning Language Models (LMs) in languages less resourced than English. Still while there has been emphasis on data quantity less attention has been given to its quality. In this work we investigate the impact of human intervention on machine45;generated data when fine45;tuning dialogical models. In particular we study (1) whether post45;edited dialogues exhibit higher perceived quality compared to the originals that were automatically generated; (2) whether fine45;tuning with post45;edited dialogues results in noticeable differences in the generated outputs; and (3) whether post45;edited dialogues influence the outcomes when considering the parameter size of the LMs. To this end we created HED45;IT a large45;scale dataset where machine45;generated dialogues are paired with the version post45;edited by humans. Using both the edited and unedited portions of HED45;IT we fine45;tuned three different sizes of an LM. Results from both human and automatic evaluation show that the different quality of training data is clearly perceived and it has an impact also on the models trained on such data. Additionally our findings indicate that larger models are less sensitive to data quality whereas this has a crucial impact on smaller models. These results enhance our comprehension of the impact of human intervention on training data in the development of high45;quality LMs.
