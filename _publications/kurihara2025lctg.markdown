---
layout: publication
title: 'LCTG Bench: LLM Controlled Text Generation Benchmark'
authors: Kentaro Kurihara, Masato Mita, Peinan Zhang, Shota Sasaki, Ryosuke Ishigami, Naoaki Okazaki
conference: "Arxiv"
year: 2025
bibkey: kurihara2025lctg
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2501.15875"}
tags: ['Tools', 'GPT', 'Applications', 'Language Modeling', 'Model Architecture']
---
The rise of large language models (LLMs) has led to more diverse and
higher-quality machine-generated text. However, their high expressive power
makes it difficult to control outputs based on specific business instructions.
In response, benchmarks focusing on the controllability of LLMs have been
developed, but several issues remain: (1) They primarily cover major languages
like English and Chinese, neglecting low-resource languages like Japanese; (2)
Current benchmarks employ task-specific evaluation metrics, lacking a unified
framework for selecting models based on controllability across different use
cases. To address these challenges, this research introduces LCTG Bench, the
first Japanese benchmark for evaluating the controllability of LLMs. LCTG Bench
provides a unified framework for assessing control performance, enabling users
to select the most suitable model for their use cases based on controllability.
By evaluating nine diverse Japanese-specific and multilingual LLMs like GPT-4,
we highlight the current state and challenges of controllability in Japanese
LLMs and reveal the significant gap between multilingual models and
Japanese-specific models.
