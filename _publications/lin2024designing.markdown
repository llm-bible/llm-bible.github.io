---
layout: publication
title: 'Designing Domain-specific Large Language Models: The Critical Role Of Fine-tuning In Public Opinion Simulation'
authors: Haocheng Lin
conference: "Arxiv"
year: 2024
bibkey: lin2024designing
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.19308"}
tags: ['Pretraining Methods', 'Training Techniques', 'Fine-Tuning', 'Reinforcement Learning']
---
Large language models (LLMs) have transformed natural language processing,
yet face challenges in specialized tasks such as simulating opinions on
environmental policies. This paper introduces a novel fine-tuning approach that
integrates socio-demographic data from the UK Household Longitudinal Study,
uniquely using profiling factors, such as age, gender, income, education, and
region. This method enhances the accuracy and representation of generated
views. By emulating diverse synthetic profiles, the fine-tuned models
significantly outperform pre-trained counterparts, achieving measurable
improvements in capturing demographic nuances. Evaluation metrics, including
Chi-Squared, Cosine Similarity, Jaccard Index, and KL-divergence, reveal a
strong alignment between synthetic and real-world opinions. This work
demonstrates the potential of fine-tuned LLMs tailored to societal contexts to
enable more ethical and precise policy simulations. Its broader implications
include deploying LLMs in domains like healthcare and education, fostering
inclusive and data-driven decision-making in both research and practice.
