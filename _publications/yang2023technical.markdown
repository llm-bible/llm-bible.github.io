---
layout: publication
title: 'Skymath: Technical Report'
authors: Liu Yang, Haihua Yang, Wenjun Cheng, Lei Lin, Chenxia Li, Yifu Chen, Lunan Liu, Jianfei Pan, Tianwen Wei, Biye Li, Liang Zhao, Lijie Wang, Bo Zhu, Guoliang Li, Xuejie Wu, Xilin Luo, Rui Hu
conference: "Arxiv"
year: 2023
bibkey: yang2023technical
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2310.16713'}
tags: ['Fine-Tuning', 'Training Techniques', 'Pretraining Methods']
---
Large language models (LLMs) have shown great potential to solve varieties of
natural language processing (NLP) tasks, including mathematical reasoning. In
this work, we present SkyMath, a large language model for mathematics with 13
billion parameters. By applying self-compare fine-tuning, we have enhanced
mathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K,
SkyMath outperforms all known open-source models of similar size and has
established a new SOTA performance.
