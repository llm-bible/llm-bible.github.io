---
layout: publication
title: "Skymath: Technical Report"
authors: Yang Liu, Yang Haihua, Cheng Wenjun, Lin Lei, Li Chenxia, Chen Yifu, Liu Lunan, Pan Jianfei, Wei Tianwen, Li Biye, Zhao Liang, Wang Lijie, Zhu Bo, Li Guoliang, Wu Xuejie, Luo Xilin, Hu Rui
conference: "Arxiv"
year: 2023
bibkey: yang2023technical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.16713"}
tags: ['Fine Tuning', 'Pretraining Methods', 'Training Techniques']
---
Large language models (LLMs) have shown great potential to solve varieties of natural language processing (NLP) tasks including mathematical reasoning. In this work we present SkyMath a large language model for mathematics with 13 billion parameters. By applying self-compare fine-tuning we have enhanced mathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K SkyMath outperforms all known open-source models of similar size and has established a new SOTA performance.
