---
layout: publication
title: Skymath Technical Report
authors: Yang Liu, Yang Haihua, Cheng Wenjun, Lin Lei, Li Chenxia, Chen Yifu, Liu Lunan, Pan Jianfei, Wei Tianwen, Li Biye, Zhao Liang, Wang Lijie, Zhu Bo, Li Guoliang, Wu Xuejie, Luo Xilin, Hu Rui
conference: "Arxiv"
year: 2023
bibkey: yang2023technical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.16713"}
tags: ['Pretraining Methods']
---
Large language models (LLMs) have shown great potential to solve varieties of natural language processing (NLP) tasks including mathematical reasoning. In this work we present SkyMath a large language model for mathematics with 13 billion parameters. By applying self45;compare fine45;tuning we have enhanced mathematical reasoning abilities of Skywork45;13B45;Base remarkably. On GSM8K SkyMath outperforms all known open45;source models of similar size and has established a new SOTA performance.
