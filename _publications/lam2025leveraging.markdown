---
layout: publication
title: 'Leveraging Human Production-interpretation Asymmetries To Test LLM Cognitive Plausibility'
authors: Suet-ying Lam, Qingcheng Zeng, Jingyi Wu, Rob Voigt
conference: "Arxiv"
year: 2025
bibkey: lam2025leveraging
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2503.17579"}
  - {name: "Code", url: "https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025"}
tags: ['Reinforcement Learning', 'RAG', 'Has Code', 'ACL', 'Prompting']
---
Whether large language models (LLMs) process language similarly to humans has been the subject of much theoretical and practical debate. We examine this question through the lens of the production-interpretation distinction found in human sentence processing and evaluate the extent to which instruction-tuned LLMs replicate this distinction. Using an empirically documented asymmetry between pronoun production and interpretation in humans for implicit causality verbs as a testbed, we find that some LLMs do quantitatively and qualitatively reflect human-like asymmetries between production and interpretation. We demonstrate that whether this behavior holds depends upon both model size-with larger models more likely to reflect human-like patterns and the choice of meta-linguistic prompts used to elicit the behavior. Our codes and results are available at https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025.
