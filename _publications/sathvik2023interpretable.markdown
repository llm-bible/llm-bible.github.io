---
layout: publication
title: Interprompt Interpretable Prompting For Interrelated Interpersonal Risk Factors In Reddit Posts
authors: Sathvik Msvpj, Sarkar Surjodeep, Saxena Chandni, Sohn Sunghwan, Garg Muskan
conference: "Arxiv"
year: 2023
bibkey: sathvik2023interpretable
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.12404"}
tags: ['Attention Mechanism', 'GPT', 'Interpretability And Explainability', 'Model Architecture', 'Prompting', 'Reinforcement Learning', 'Transformer']
---
Mental health professionals and clinicians have observed the upsurge of mental disorders due to Interpersonal Risk Factors (IRFs). To simulate the human45;in45;the45;loop triaging scenario for early detection of mental health disorders we recognized textual indications to ascertain these IRFs Thwarted Belongingness (TBe) and Perceived Burdensomeness (PBu) within personal narratives. In light of this we use N45;shot learning with GPT45;3 model on the IRF dataset and underscored the importance of fine45;tuning GPT45;3 model to incorporate the context45;specific sensitivity and the interconnectedness of textual cues that represent both IRFs. In this paper we introduce an Interpretable Prompting (InterPrompt)125; method to boost the attention mechanism by fine45;tuning the GPT45;3 model. This allows a more sophisticated level of language modification by adjusting the pre45;trained weights. Our model learns to detect usual patterns and underlying connections across both the IRFs which leads to better system45;level explainability and trustworthiness. The results of our research demonstrate that all four variants of GPT45;3 model when fine45;tuned with InterPrompt perform considerably better as compared to the baseline methods both in terms of classification and explanation generation.
