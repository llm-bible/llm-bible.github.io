---
layout: publication
title: 'Aligning Visual Regions And Textual Concepts For Semantic-grounded Image Representations'
authors: Fenglin Liu, Yuanxin Liu, Xuancheng Ren, Xiaodong He, Xu Sun
conference: "Arxiv"
year: 2019
bibkey: liu2019aligning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1905.06139"}
  - {name: "Code", url: "https://github.com/fenglinliu98/MIA)"}
tags: ['Attention Mechanism', 'Has Code', 'Applications', 'Model Architecture']
---
In vision-and-language grounding problems, fine-grained representations of
the image are considered to be of paramount importance. Most of the current
systems incorporate visual features and textual concepts as a sketch of an
image. However, plainly inferred representations are usually undesirable in
that they are composed of separate components, the relations of which are
elusive. In this work, we aim at representing an image with a set of integrated
visual regions and corresponding textual concepts, reflecting certain
semantics. To this end, we build the Mutual Iterative Attention (MIA) module,
which integrates correlated visual features and textual concepts, respectively,
by aligning the two modalities. We evaluate the proposed approach on two
representative vision-and-language grounding tasks, i.e., image captioning and
visual question answering. In both tasks, the semantic-grounded image
representations consistently boost the performance of the baseline models under
all metrics across the board. The results demonstrate that our approach is
effective and generalizes well to a wide range of models for image-related
applications. (The code is available at https://github.com/fenglinliu98/MIA)
