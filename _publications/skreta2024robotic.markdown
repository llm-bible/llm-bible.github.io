---
layout: publication
title: Replan Robotic Replanning With Perception And Language Models
authors: Skreta Marta, Zhou Zihan, Yuan Jia Lin, Darvish Kourosh, Aspuru-guzik Al√°n, Garg Animesh
conference: "Arxiv"
year: 2024
bibkey: skreta2024robotic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2401.04157"}
tags: ['Applications', 'RAG', 'Reinforcement Learning', 'TACL', 'Tools']
---
Advancements in large language models (LLMs) have demonstrated their potential in facilitating high45;level reasoning logical reasoning and robotics planning. Recently LLMs have also been able to generate reward functions for low45;level robot actions effectively bridging the interface between high45;level planning and low45;level robot control. However the challenge remains that even with syntactically correct plans robots can still fail to achieve their intended goals due to imperfect plans or unexpected environmental issues. To overcome this Vision Language Models (VLMs) have shown remarkable success in tasks such as visual question answering. Leveraging the capabilities of VLMs we present a novel framework called Robotic Replanning with Perception and Language Models (RePLan) that enables online replanning capabilities for long45;horizon tasks. This framework utilizes the physical grounding provided by a VLMs understanding of the worlds state to adapt robot actions when the initial plan fails to achieve the desired goal. We developed a Reasoning and Control (RC) benchmark with eight long45;horizon tasks to test our approach. We find that RePLan enables a robot to successfully adapt to unforeseen obstacles while accomplishing open45;ended long45;horizon goals where baseline models cannot and can be readily applied to real robots. Find more information at https://replan&#45;lm.github.io/replan.github.io/
