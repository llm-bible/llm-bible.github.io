---
layout: publication
title: Training Deeper Neural Machine Translation Models With Transparent Attention
authors: Bapna Ankur, Chen Mia Xu, Firat Orhan, Cao Yuan, Wu Yonghui
conference: "Arxiv"
year: 2018
bibkey: bapna2018training
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1808.07561"}
tags: ['Applications', 'Attention Mechanism', 'Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Training Techniques', 'Transformer']
---
While current state45;of45;the45;art NMT models such as RNN seq2seq and Transformers possess a large number of parameters they are still shallow in comparison to convolutional models used for both text and vision applications. In this work we attempt to train significantly (245;3x) deeper Transformer and Bi45;RNN encoders for machine translation. We propose a simple modification to the attention mechanism that eases the optimization of deeper models and results in consistent gains of 0.745;1.1 BLEU on the benchmark WMT14 English45;German and WMT15 Czech45;English tasks for both architectures.
