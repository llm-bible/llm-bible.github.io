---
layout: publication
title: 'Prompting Large Language Models To Tackle The Full Software Development Lifecycle: A Case Study'
authors: Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, Zhiyin Yu, He Du, Ping Yang, Dahua Lin, Chao Peng, Kai Chen
conference: "Arxiv"
year: 2024
bibkey: li2024prompting
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2403.08604'}
tags: ['GPT', 'Applications', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
Recent advancements in large language models (LLMs) have significantly
enhanced their coding capabilities. However, existing benchmarks predominantly
focused on simplified or isolated aspects of coding, such as single-file code
generation or repository issue debugging, falling short of measuring the full
spectrum of challenges raised by real-world programming activities. In this
case study, we explore the performance of LLMs across the entire software
development lifecycle with DevEval, encompassing stages including software
design, environment setup, implementation, acceptance testing, and unit
testing. DevEval features four programming languages, multiple domains,
high-quality data collection, and carefully designed and verified metrics for
each task. Empirical studies show that current LLMs, including GPT-4, fail to
solve the challenges presented within DevEval. Our findings offer actionable
insights for the future development of LLMs toward real-world programming
applications.
