---
layout: publication
title: Multimodal Reasoning With Multimodal Knowledge Graph
authors: Lee Junlin, Wang Yequan, Li Jing, Zhang Min
conference: "Arxiv"
year: 2024
bibkey: lee2024multimodal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.02030"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'RAG', 'Training Techniques']
---
Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs but their singular modality of knowledge limits comprehensive cross45;modal understanding. In this paper we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR45;MKG) method which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities significantly enhancing the multimodal reasoning capabilities of LLMs. In particular a relation graph attention network is utilized for encoding MMKGs and a cross45;modal alignment module is designed for optimizing image45;text alignment. A MMKG45;grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably MR45;MKG achieves superior performance while training on only a small fraction of parameters approximately 2.2537; of the LLMs parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR45;MKG method outperforms previous state45;of45;the45;art models.
