---
layout: publication
title: 'M-ped: Multi-prompt Ensemble Decoding For Large Language Models'
authors: Jiaxin Guo, Daimeng Wei, Yuanchang Luo, Shimin Tao, Hengchao Shang, Zongyao Li, Shaojun Li, Jinlong Yang, Zhanglin Wu, Zhiqiang Rao, Hao Yang
conference: "Arxiv"
year: 2024
bibkey: guo2024m
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2412.18299'}
tags: ['RAG', 'Prompting', 'Applications']
---
With the widespread application of Large Language Models (LLMs) in the field
of Natural Language Processing (NLP), enhancing their performance has become a
research hotspot. This paper presents a novel multi-prompt ensemble decoding
approach designed to bolster the generation quality of LLMs by leveraging the
aggregation of outcomes from multiple prompts. Given a unique input \\(X\\), we
submit \\(n\\) variations of prompts with \\(X\\) to LLMs in batch mode to decode and
derive probability distributions. For each token prediction, we calculate the
ensemble probability by averaging the \\(n\\) probability distributions within the
batch, utilizing this aggregated probability to generate the token. This
technique is dubbed Inner-Batch Ensemble. To facilitate efficient batch
inference, we implement a Left-Padding strategy to maintain uniform input
lengths across the n prompts. Through extensive experimentation on diverse NLP
tasks, including machine translation, code generation, and text simplification,
we demonstrate the efficacy of our method in enhancing LLM performance. The
results show substantial improvements in BLEU scores, pass@\\(k\\) rates, and LENS
metrics over conventional methods.
