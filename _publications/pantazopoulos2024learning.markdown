---
layout: publication
title: 'Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes Llms More Prone To Jailbreak Attacks'
authors: Pantazopoulos Georgios, Parekh Amit, Nikandrou Malvina, Suglia Alessandro
conference: "Arxiv"
year: 2024
bibkey: pantazopoulos2024learning
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.04403"}
tags: ['Attention Mechanism', 'Fine Tuning', 'Model Architecture', 'Multimodal Models', 'Responsible AI', 'Security']
---
Augmenting Large Language Models (LLMs) with image-understanding capabilities
has resulted in a boom of high-performing Vision-Language models (VLMs). While
studying the alignment of LLMs to human values has received widespread
attention, the safety of VLMs has not received the same attention. In this
paper, we explore the impact of jailbreaking on three state-of-the-art VLMs,
each using a distinct modeling approach. By comparing each VLM to their
respective LLM backbone, we find that each VLM is more susceptible to
jailbreaking. We consider this as an undesirable outcome from visual
instruction-tuning, which imposes a forgetting effect on an LLM's safety
guardrails. Therefore, we provide recommendations for future work based on
evaluation strategies that aim to highlight the weaknesses of a VLM, as well as
take safety measures into account during visual instruction tuning.
