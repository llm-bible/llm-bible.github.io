---
layout: publication
title: 'Can I Understand What I Create? Self-knowledge Evaluation Of Large Language Models'
authors: Zhiquan Tan, Lai Wei, Jindong Wang, Xing Xie, Weiran Huang
conference: "Arxiv"
year: 2024
bibkey: tan2024can
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2406.06140'}
tags: ['Attention Mechanism', 'Transformer', 'Training Techniques', 'Model Architecture', 'Tools', 'Fine-Tuning', 'Pretraining Methods']
---
Large language models (LLMs) have achieved remarkable progress in linguistic
tasks, necessitating robust evaluation frameworks to understand their
capabilities and limitations. Inspired by Feynman's principle of understanding
through creation, we introduce a self-knowledge evaluation framework that is
easy to implement, evaluating models on their ability to comprehend and respond
to self-generated questions. Our findings, based on testing multiple models
across diverse tasks, reveal significant gaps in the model's self-knowledge
ability. Further analysis indicates these gaps may be due to misalignment with
human attention mechanisms. Additionally, fine-tuning on self-generated math
task may enhance the model's math performance, highlighting the potential of
the framework for efficient and insightful model evaluation and may also
contribute to the improvement of LLMs.
