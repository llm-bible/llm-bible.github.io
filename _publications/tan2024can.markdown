---
layout: publication
title: Can I Understand What I Create Self45;knowledge Evaluation Of Large Language Models
authors: Tan Zhiquan, Wei Lai, Wang Jindong, Xie Xing, Huang Weiran
conference: "Arxiv"
year: 2024
bibkey: tan2024can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.06140"}
tags: ['Attention Mechanism', 'Model Architecture', 'Tools', 'Transformer']
---
Large language models (LLMs) have achieved remarkable progress in linguistic tasks necessitating robust evaluation frameworks to understand their capabilities and limitations. Inspired by Feynmans principle of understanding through creation we introduce a self45;knowledge evaluation framework that is easy to implement evaluating models on their ability to comprehend and respond to self45;generated questions. Our findings based on testing multiple models across diverse tasks reveal significant gaps in the models self45;knowledge ability. Further analysis indicates these gaps may be due to misalignment with human attention mechanisms. Additionally fine45;tuning on self45;generated math task may enhance the models math performance highlighting the potential of the framework for efficient and insightful model evaluation and may also contribute to the improvement of LLMs.
