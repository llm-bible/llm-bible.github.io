---
layout: publication
title: 'Can I Understand What I Create? Self-knowledge Evaluation Of Large Language Models'
authors: Tan Zhiquan, Wei Lai, Wang Jindong, Xie Xing, Huang Weiran
conference: "Arxiv"
year: 2024
bibkey: tan2024can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.06140"}
tags: ['Attention Mechanism', 'Fine Tuning', 'Model Architecture', 'Pretraining Methods', 'Tools', 'Training Techniques', 'Transformer']
---
Large language models (LLMs) have achieved remarkable progress in linguistic tasks necessitating robust evaluation frameworks to understand their capabilities and limitations. Inspired by Feynmans principle of understanding through creation we introduce a self-knowledge evaluation framework that is easy to implement evaluating models on their ability to comprehend and respond to self-generated questions. Our findings based on testing multiple models across diverse tasks reveal significant gaps in the models self-knowledge ability. Further analysis indicates these gaps may be due to misalignment with human attention mechanisms. Additionally fine-tuning on self-generated math task may enhance the models math performance highlighting the potential of the framework for efficient and insightful model evaluation and may also contribute to the improvement of LLMs.
