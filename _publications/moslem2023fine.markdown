---
layout: publication
title: Fine45;tuning Large Language Models For Adaptive Machine Translation
authors: Moslem Yasmin, Haque Rejwanul, Way Andy
conference: "Arxiv"
year: 2023
bibkey: moslem2023fine
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.12740"}
tags: ['Applications', 'GPT', 'Model Architecture', 'Prompting', 'Reinforcement Learning']
---
This paper presents the outcomes of fine45;tuning Mistral 7B a general45;purpose large language model (LLM) for adaptive machine translation (MT). The fine45;tuning process involves utilising a combination of zero45;shot and one45;shot translation prompts within the medical domain. The primary objective is to enhance real45;time adaptive MT capabilities of Mistral 7B enabling it to adapt translations to the required domain at inference time. The results particularly for Spanish45;to45;English MT showcase the efficacy of the fine45;tuned model demonstrating quality improvements in both zero45;shot and one45;shot translation scenarios surpassing Mistral 7Bs baseline performance. Notably the fine45;tuned Mistral outperforms ChatGPT gpt45;3.545;turbo in zero45;shot translation while achieving comparable one45;shot translation quality. Moreover the zero45;shot translation of the fine45;tuned Mistral matches NLLB 3.3Bs performance and its one45;shot translation quality surpasses that of NLLB 3.3B. These findings emphasise the significance of fine45;tuning efficient LLMs like Mistral 7B to yield high45;quality zero45;shot translations comparable to task45;oriented models like NLLB 3.3B. Additionally the adaptive gains achieved in one45;shot translation are comparable to those of commercial LLMs such as ChatGPT. Our experiments demonstrate that with a relatively small dataset of 20000 segments that incorporate a mix of zero45;shot and one45;shot prompts fine45;tuning significantly enhances Mistrals in45;context learning ability especially for real45;time adaptive MT.
