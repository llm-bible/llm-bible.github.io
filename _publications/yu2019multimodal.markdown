---
layout: publication
title: Multimodal Unified Attention Networks For Vision45;and45;language Interactions
authors: Yu Zhou, Cui Yuhao, Yu Jun, Tao Dacheng, Tian Qi
conference: "Arxiv"
year: 2019
bibkey: yu2019multimodal
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1908.04107"}
tags: ['Applications', 'Attention Mechanism', 'Model Architecture', 'Multimodal Models', 'Transformer']
---
Learning an effective attention mechanism for multimodal data is important in many vision45;and45;language tasks that require a synergic understanding of both the visual and textual contents. Existing state45;of45;the45;art approaches use co45;attention models to associate each visual object (e.g. image region) with each textual object (e.g. query word). Despite the success of these co45;attention models they only model inter45;modal interactions while neglecting intra45;modal interactions. Here we propose a general unified attention model that simultaneously captures the intra45; and inter45;modal interactions of multimodal features and outputs their corresponding attended representations. By stacking such unified attention blocks in depth we obtain the deep Multimodal Unified Attention Network (MUAN) which can seamlessly be applied to the visual question answering (VQA) and visual grounding tasks. We evaluate our MUAN models on two VQA datasets and three visual grounding datasets and the results show that MUAN achieves top45;level performance on both tasks without bells and whistles.
