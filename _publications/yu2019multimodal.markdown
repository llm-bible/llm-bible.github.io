---
layout: publication
title: 'Multimodal Unified Attention Networks For Vision-and-language Interactions'
authors: Zhou Yu, Yuhao Cui, Jun Yu, Dacheng Tao, Qi Tian
conference: "Arxiv"
year: 2019
bibkey: yu2019multimodal
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/1908.04107'}
tags: ['Attention Mechanism', 'Transformer', 'Applications', 'Model Architecture', 'Multimodal Models']
---
Learning an effective attention mechanism for multimodal data is important in
many vision-and-language tasks that require a synergic understanding of both
the visual and textual contents. Existing state-of-the-art approaches use
co-attention models to associate each visual object (e.g., image region) with
each textual object (e.g., query word). Despite the success of these
co-attention models, they only model inter-modal interactions while neglecting
intra-modal interactions. Here we propose a general `unified attention' model
that simultaneously captures the intra- and inter-modal interactions of
multimodal features and outputs their corresponding attended representations.
By stacking such unified attention blocks in depth, we obtain the deep
Multimodal Unified Attention Network (MUAN), which can seamlessly be applied to
the visual question answering (VQA) and visual grounding tasks. We evaluate our
MUAN models on two VQA datasets and three visual grounding datasets, and the
results show that MUAN achieves top-level performance on both tasks without
bells and whistles.
