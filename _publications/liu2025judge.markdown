---
layout: publication
title: 'Judge As A Judge: Improving The Evaluation Of Retrieval-augmented Generation Through The Judge-consistency Of Large Language Models'
authors: Shuliang Liu, Xinze Li, Zhenghao Liu, Yukun Yan, Cheng Yang, Zheni Zeng, Zhiyuan Liu, Maosong Sun, Ge Yu
conference: "Arxiv"
year: 2025
bibkey: liu2025judge
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2502.18817"}
  - {name: "Code", url: "https://github.com/OpenBMB/ConsJudge"}
tags: ['Training Techniques', 'Reinforcement Learning', 'RAG', 'Has Code', 'Prompting']
---
Retrieval-Augmented Generation (RAG) has proven its effectiveness in
alleviating hallucinations for Large Language Models (LLMs). However, existing
automated evaluation metrics cannot fairly evaluate the outputs generated by
RAG models during training and evaluation. LLM-based judgment models provide
the potential to produce high-quality judgments, but they are highly sensitive
to evaluation prompts, leading to inconsistencies when judging the output of
RAG models. This paper introduces the Judge-Consistency (ConsJudge) method,
which aims to enhance LLMs to generate more accurate evaluations for RAG
models. Specifically, ConsJudge prompts LLMs to generate different judgments
based on various combinations of judgment dimensions, utilize the
judge-consistency to evaluate these judgments and select the accepted and
rejected judgments for DPO training. Our experiments show that ConsJudge can
effectively provide more accurate judgments for optimizing RAG models across
various RAG models and datasets. Further analysis reveals that judgments
generated by ConsJudge have a high agreement with the superior LLM. All codes
are available at https://github.com/OpenBMB/ConsJudge.
