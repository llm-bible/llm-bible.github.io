---
layout: publication
title: 'Secret Use Of Large Language Model (LLM)'
authors: Zhiping Zhang, Chenxinran Shen, Bingsheng Yao, Dakuo Wang, Tianshi Li
conference: "Arxiv"
year: 2024
bibkey: zhang2024secret
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2409.19450"}
tags: ['Fine-Tuning', 'Survey Paper', 'Applications', 'Ethics and Bias', 'RAG', 'Reinforcement Learning', 'Merging', 'Interpretability']
---
The advancements of Large Language Models (LLMs) have decentralized the
responsibility for the transparency of AI usage. Specifically, LLM users are
now encouraged or required to disclose the use of LLM-generated content for
varied types of real-world tasks. However, an emerging phenomenon, users'
secret use of LLM, raises challenges in ensuring end users adhere to the
transparency requirement. Our study used mixed-methods with an exploratory
survey (125 real-world secret use cases reported) and a controlled experiment
among 300 users to investigate the contexts and causes behind the secret use of
LLMs. We found that such secretive behavior is often triggered by certain
tasks, transcending demographic and personality differences among users. Task
types were found to affect users' intentions to use secretive behavior,
primarily through influencing perceived external judgment regarding LLM usage.
Our results yield important insights for future work on designing interventions
to encourage more transparent disclosure of the use of LLMs or other AI
technologies.
