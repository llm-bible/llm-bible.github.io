---
layout: publication
title: 'Co-attentional Transformers For Story-based Video Understanding'
authors: Bebensee Bj√∂rn, Zhang Byoung-tak
conference: "Arxiv"
year: 2020
bibkey: bebensee2020co
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2010.14104"}
tags: ['Agentic', 'Applications', 'Attention Mechanism', 'Merging', 'Model Architecture', 'Pretraining Methods', 'Transformer']
---
Inspired by recent trends in vision and language learning we explore applications of attention mechanisms for visio-lingual fusion within an application to story-based video understanding. Like other video-based QA tasks video story understanding requires agents to grasp complex temporal dependencies. However as it focuses on the narrative aspect of video it also requires understanding of the interactions between different characters as well as their actions and their motivations. We propose a novel co-attentional transformer model to better capture long-term dependencies seen in visual stories such as dramas and measure its performance on the video question answering task. We evaluate our approach on the recently introduced DramaQA dataset which features character-centered video story understanding questions. Our model outperforms the baseline model by 8 percentage points overall at least 4.95 and up to 12.8 percentage points on all difficulty levels and manages to beat the winner of the DramaQA challenge.
