---
layout: publication
title: From Text To Insight&#58; Leveraging Large Language Models For Performance Evaluation In Management
authors: Li Ning, Zhou Huaikang, Xu Mingze
conference: "Arxiv"
year: 2024
bibkey: li2024from
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.05328"}
tags: ['Applications', 'Ethics And Bias', 'GPT', 'Model Architecture', 'RAG']
---
This study explores the potential of Large Language Models (LLMs) specifically GPT-4 to enhance objectivity in organizational task performance evaluations. Through comparative analyses across two studies including various task performance outputs we demonstrate that LLMs can serve as a reliable and even superior alternative to human raters in evaluating knowledge-based performance outputs which are a key contribution of knowledge workers. Our results suggest that GPT ratings are comparable to human ratings but exhibit higher consistency and reliability. Additionally combined multiple GPT ratings on the same performance output show strong correlations with aggregated human performance ratings akin to the consensus principle observed in performance evaluation literature. However we also find that LLMs are prone to contextual biases such as the halo effect mirroring human evaluative biases. Our research suggests that while LLMs are capable of extracting meaningful constructs from text-based data their scope is currently limited to specific forms of performance evaluation. By highlighting both the potential and limitations of LLMs our study contributes to the discourse on AI role in management studies and sets a foundation for future research to refine AI theoretical and practical applications in management.
