---
layout: publication
title: 'Investigating Low-cost LLM Annotation For~spoken Dialogue Understanding Datasets'
authors: Lucas Lia Druart, Valentin Lia Vielzeuf, Yannick Lia Est√®ve
conference: "27th International Conference on Text Speech and Dialogue Sep 2024 Brno (Rep. Tch`eque) Czech Republic"
year: 2024
bibkey: druart2024investigating
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2406.13269"}
tags: ['Pretraining Methods', 'Training Techniques', 'Fine-Tuning']
---
In spoken Task-Oriented Dialogue (TOD) systems, the choice of the semantic
representation describing the users' requests is key to a smooth interaction.
Indeed, the system uses this representation to reason over a database and its
domain knowledge to choose its next action. The dialogue course thus depends on
the information provided by this semantic representation. While textual
datasets provide fine-grained semantic representations, spoken dialogue
datasets fall behind. This paper provides insights into automatic enhancement
of spoken dialogue datasets' semantic representations. Our contributions are
three fold: (1) assess the relevance of Large Language Model fine-tuning, (2)
evaluate the knowledge captured by the produced annotations and (3) highlight
semi-automatic annotation implications.
