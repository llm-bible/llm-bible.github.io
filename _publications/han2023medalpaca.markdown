---
layout: publication
title: MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data
authors: Han Tianyu, Adams Lisa C., Papaioannou Jens-michalis, Grundmann Paul, Oberhauser Tom, LÃ¶ser Alexander, Truhn Daniel, Bressem Keno K.
conference: "Arxiv"
year: 2023
bibkey: han2023medalpaca
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.08247"}
tags: ['Applications', 'Fine Tuning', 'GPT', 'Model Architecture', 'Pretraining Methods', 'Training Techniques']
---
As large language models (LLMs) like OpenAIs GPT series continue to make strides we witness the emergence of artificial intelligence applications in an ever-expanding range of fields. In medicine these LLMs hold considerable promise for improving medical workflows diagnostics patient care and education. Yet there is an urgent need for open-source models that can be deployed on-premises to safeguard patient privacy. In our work we present an innovative dataset consisting of over 160000 entries specifically crafted to fine-tune LLMs for effective medical applications. We investigate the impact of fine-tuning these datasets on publicly accessible pre-trained LLMs and subsequently we juxtapose the performance of pre-trained-only models against the fine-tuned models concerning the examinations that future medical doctors must pass to achieve certification.
