---
layout: publication
title: 'Medalpaca -- An Open-source Collection Of Medical Conversational AI Models And Training Data'
authors: Tianyu Han, Lisa C. Adams, Jens-michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexei Figueroa, Alexander LÃ¶ser, Daniel Truhn, Keno K. Bressem
conference: "Arxiv"
year: 2023
bibkey: han2023medalpaca
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2304.08247"}
tags: ['Training Techniques', 'Model Architecture', 'GPT', 'Pretraining Methods', 'Fine-Tuning', 'Applications']
---
As large language models (LLMs) like OpenAI's GPT series continue to make
strides, we witness the emergence of artificial intelligence applications in an
ever-expanding range of fields. In medicine, these LLMs hold considerable
promise for improving medical workflows, diagnostics, patient care, and
education. Yet, there is an urgent need for open-source models that can be
deployed on-premises to safeguard patient privacy. In our work, we present an
innovative dataset consisting of over 160,000 entries, specifically crafted to
fine-tune LLMs for effective medical applications. We investigate the impact of
fine-tuning these datasets on publicly accessible pre-trained LLMs, and
subsequently, we juxtapose the performance of pre-trained-only models against
the fine-tuned models concerning the examinations that future medical doctors
must pass to achieve certification.
