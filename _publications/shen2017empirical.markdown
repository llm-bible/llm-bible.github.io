---
layout: publication
title: An Empirical Analysis Of Multiple45;turn Reasoning Strategies In Reading Comprehension Tasks
authors: Shen Yelong, Liu Xiaodong, Duh Kevin, Gao Jianfeng
conference: "Arxiv"
year: 2017
bibkey: shen2017empirical
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1711.03230"}
tags: ['Agentic', 'Attention Mechanism', 'Model Architecture', 'Reinforcement Learning']
---
Reading comprehension (RC) is a challenging task that requires synthesis of information across sentences and multiple turns of reasoning. Using a state45;of45;the45;art RC model we empirically investigate the performance of single45;turn and multiple45;turn reasoning on the SQuAD and MS MARCO datasets. The RC model is an end45;to45;end neural network with iterative attention and uses reinforcement learning to dynamically control the number of turns. We find that multiple45;turn reasoning outperforms single45;turn reasoning for all question and answer types; further we observe that enabling a flexible number of turns generally improves upon a fixed multiple45;turn strategy. 37;across all question types and is particularly beneficial to questions with lengthy descriptive answers. We achieve results competitive to the state45;of45;the45;art on these two datasets.
