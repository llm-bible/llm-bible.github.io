---
layout: publication
title: 'An Empirical Analysis Of Multiple-turn Reasoning Strategies In Reading Comprehension Tasks'
authors: Yelong Shen, Xiaodong Liu, Kevin Duh, Jianfeng Gao
conference: "Arxiv"
year: 2017
bibkey: shen2017empirical
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/1711.03230'}
tags: ['Reinforcement Learning', 'Agentic', 'Attention Mechanism', 'Model Architecture']
---
Reading comprehension (RC) is a challenging task that requires synthesis of
information across sentences and multiple turns of reasoning. Using a
state-of-the-art RC model, we empirically investigate the performance of
single-turn and multiple-turn reasoning on the SQuAD and MS MARCO datasets. The
RC model is an end-to-end neural network with iterative attention, and uses
reinforcement learning to dynamically control the number of turns. We find that
multiple-turn reasoning outperforms single-turn reasoning for all question and
answer types; further, we observe that enabling a flexible number of turns
generally improves upon a fixed multiple-turn strategy. %across all question
types, and is particularly beneficial to questions with lengthy, descriptive
answers. We achieve results competitive to the state-of-the-art on these two
datasets.
