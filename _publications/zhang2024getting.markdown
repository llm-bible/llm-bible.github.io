---
layout: publication
title: 'Getting More From Less: Large Language Models Are Good Spontaneous Multilingual Learners'
authors: Shimao Zhang, Changjiang Gao, Wenhao Zhu, Jiajun Chen, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Shujian Huang
conference: "Arxiv"
year: 2024
bibkey: zhang2024getting
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.13816"}
tags: ['Fine-Tuning', 'RAG', 'Interpretability and Explainability']
---
Recently, Large Language Models (LLMs) have shown impressive language
capabilities. While most of the existing LLMs have very unbalanced performance
across different languages, multilingual alignment based on translation
parallel data is an effective method to enhance the LLMs' multilingual
capabilities. In this work, we discover and comprehensively investigate the
spontaneous multilingual alignment improvement of LLMs. We find that LLMs
instruction-tuned on the question translation data (i.e. without annotated
answers) are able to encourage the alignment between English and a wide range
of languages, even including those unseen during instruction-tuning.
Additionally, we utilize different settings and mechanistic interpretability
methods to analyze the LLM's performance in the multilingual scenario
comprehensively. Our work suggests that LLMs have enormous potential for
improving multilingual alignment efficiently with great language and task
generalization.
