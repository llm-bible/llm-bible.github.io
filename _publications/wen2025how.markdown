---
layout: publication
title: 'How Well Can Vison-language Models Understand Humans'' Intention? An Open-ended Theory Of Mind Question Evaluation Benchmark'
authors: Ximing Wen, Mallika Mainali, Anik Sen
conference: "Arxiv"
year: 2025
bibkey: wen2025how
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2503.22093'}
  - {name: "Code", url: 'https://github.com/ximingwen/ToM-AAAI25-Multimodal'}
tags: ['Has Code', 'AAAI', 'GPT', 'Tools', 'Applications', 'Model Architecture', 'Multimodal Models']
---
Vision Language Models (VLMs) have demonstrated strong reasoning capabilities
in Visual Question Answering (VQA) tasks; however, their ability to perform
Theory of Mind (ToM) tasks, such as inferring human intentions, beliefs, and
mental states, remains underexplored. We propose an open-ended question
framework to evaluate VLMs' performance across diverse categories of ToM tasks.
We curated and annotated a benchmark dataset of 30 images and evaluated the
performance of four VLMs of varying sizes. Our results show that the GPT-4
model outperformed all the others, with only one smaller model, GPT-4o-mini,
achieving comparable performance. We observed that VLMs often struggle to infer
intentions in complex scenarios such as bullying or cheating. Our findings
reveal that smaller models can sometimes infer correct intentions despite
relying on incorrect visual cues. The dataset is available at
https://github.com/ximingwen/ToM-AAAI25-Multimodal.
