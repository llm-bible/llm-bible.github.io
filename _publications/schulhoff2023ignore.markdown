---
layout: publication
title: 'Ignore This Title And Hackaprompt: Exposing Systemic Vulnerabilities Of Llms Through A Global Scale Prompt Hacking Competition'
authors: Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-fran√ßois Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, Jordan Boyd-graber
conference: "Arxiv"
year: 2023
bibkey: schulhoff2023ignore
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.16119"}
tags: ['Prompting', 'Security', 'Applications']
---
Large Language Models (LLMs) are deployed in interactive contexts with direct
user engagement, such as chatbots and writing assistants. These deployments are
vulnerable to prompt injection and jailbreaking (collectively, prompt hacking),
in which models are manipulated to ignore their original instructions and
follow potentially malicious ones. Although widely acknowledged as a
significant security threat, there is a dearth of large-scale resources and
quantitative studies on prompt hacking. To address this lacuna, we launch a
global prompt hacking competition, which allows for free-form human input
attacks. We elicit 600K+ adversarial prompts against three state-of-the-art
LLMs. We describe the dataset, which empirically verifies that current LLMs can
indeed be manipulated via prompt hacking. We also present a comprehensive
taxonomical ontology of the types of adversarial prompts.
