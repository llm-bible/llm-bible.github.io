---
layout: publication
title: 'Codejudge-eval: Can Large Language Models Be Good Judges In Code Understanding?'
authors: Zhao Yuwei, Luo Ziyang, Tian Yuchen, Lin Hongzhan, Yan Weixiang, Li Annan, Ma Jing
conference: "Arxiv"
year: 2024
bibkey: zhao2024codejudge
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.10718"}
  - {name: "Code", url: "https://github.com/CodeLLM-Research/CodeJudge-Eval"}
tags: ['Applications', 'Has Code', 'RAG', 'Reinforcement Learning']
---
Recent advancements in large language models (LLMs) have showcased impressive code generation capabilities primarily evaluated through language-to-code benchmarks. However these benchmarks may not fully capture a models code understanding abilities. We introduce CodeJudge-Eval (CJ-Eval) a novel benchmark designed to assess LLMs code understanding abilities from the perspective of code judging rather than code generation. CJ-Eval challenges models to determine the correctness of provided code solutions encompassing various error types and compilation issues. By leveraging a diverse set of problems and a fine-grained judging system CJ-Eval addresses the limitations of traditional benchmarks including the potential memorization of solutions. Evaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art models struggle highlighting the benchmarks ability to probe deeper into models code understanding abilities. Our benchmark will be available at (url)https://github.com/CodeLLM-Research/CodeJudge-Eval\}."
