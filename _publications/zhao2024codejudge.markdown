---
layout: publication
title: Codejudge45;eval Can Large Language Models Be Good Judges In Code Understanding
authors: Zhao Yuwei, Luo Ziyang, Tian Yuchen, Lin Hongzhan, Yan Weixiang, Li Annan, Ma Jing
conference: "Arxiv"
year: 2024
bibkey: zhao2024codejudge
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.10718"}
  - {name: "Code", url: "https://github.com/CodeLLM&#45;Research/CodeJudge&#45;Eval&#125;"}
tags: ['Applications', 'Has Code', 'RAG', 'Reinforcement Learning']
---
Recent advancements in large language models (LLMs) have showcased impressive code generation capabilities primarily evaluated through language45;to45;code benchmarks. However these benchmarks may not fully capture a models code understanding abilities. We introduce CodeJudge45;Eval (CJ45;Eval) a novel benchmark designed to assess LLMs code understanding abilities from the perspective of code judging rather than code generation. CJ45;Eval challenges models to determine the correctness of provided code solutions encompassing various error types and compilation issues. By leveraging a diverse set of problems and a fine45;grained judging system CJ45;Eval addresses the limitations of traditional benchmarks including the potential memorization of solutions. Evaluation of 12 well45;known LLMs on CJ45;Eval reveals that even state45;of45;the45;art models struggle highlighting the benchmarks ability to probe deeper into models code understanding abilities. Our benchmark will be available at url123;https://github.com/CodeLLM&#45;Research/CodeJudge&#45;Eval&#125;.
