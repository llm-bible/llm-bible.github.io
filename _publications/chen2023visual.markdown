---
layout: publication
title: 'LL3DA: Visual Interactive Instruction Tuning For Omni-3d Understanding, Reasoning,
  And Planning'
authors: Sijin Chen et al.
conference: Arxiv
year: 2023
citations: 15
bibkey: chen2023visual
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2311.18651'}]
tags: [Applications, Prompting, Multimodal Models]
---
Recent advances in Large Multimodal Models (LMM) have made it possible for
various applications in human-machine interactions. However, developing LMMs
that can comprehend, reason, and plan in complex and diverse 3D environments
remains a challenging topic, especially considering the demand for
understanding permutation-invariant point cloud 3D representations of the 3D
scene. Existing works seek help from multi-view images, and project 2D features
to 3D space as 3D scene representations. This, however, leads to huge
computational overhead and performance degradation. In this paper, we present
LL3DA, a Large Language 3D Assistant that takes point cloud as direct input and
respond to both textual-instructions and visual-prompts. This help LMMs better
comprehend human interactions and further help to remove the ambiguities in
cluttered 3D scenes. Experiments show that LL3DA achieves remarkable results,
and surpasses various 3D vision-language models on both 3D Dense Captioning and
3D Question Answering.