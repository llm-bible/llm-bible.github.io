---
layout: publication
title: 'FG-PRM: Fine-grained Hallucination Detection And Mitigation In Language Model Mathematical Reasoning'
authors: Ruosen Li, Ziming Luo, Xinya Du
conference: "Arxiv"
year: 2024
bibkey: li2024fg
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.06304"}
tags: ['Training Techniques', 'GPT', 'Model Architecture', 'Reinforcement Learning']
---
Hallucinations in large language models (LLMs) pose significant challenges in
tasks requiring complex multi-step reasoning, such as mathematical
problem-solving. Existing approaches primarily detect the presence of
hallucinations but lack a nuanced understanding of their types and
manifestations. In this paper, we first introduce a comprehensive taxonomy that
categorizes the common hallucinations in mathematical reasoning task into six
types: fabrication, factual inconsistency, context inconsistency, instruction
inconsistency, logical inconsistency, and logical error. We then propose FG-PRM
(Fine-Grained Process Reward Model), an augmented model designed to detect and
mitigate hallucinations in a fine-grained, step-level manner. To address the
limitations of manually labeling training data, we propose an automated method
for generating fine-grained hallucination data using LLMs. By injecting
hallucinations into reasoning steps of correct solutions, we create a diverse
and balanced synthetic dataset for training FG-PRM, which consists of six
specialized Process Reward Models (PRMs), each tailored to detect a specific
hallucination type. Our FG-PRM demonstrates superior performance across two key
tasks: 1) Fine-grained hallucination detection: classifying hallucination types
for each reasoning step; and 2) Verification: ranking multiple LLM-generated
outputs to select the most accurate solution, mitigating reasoning
hallucinations. Our experiments show that FG-PRM outperforms ChatGPT-3.5 and
Claude-3 on fine-grained hallucination detection and substantially boosts the
performance of LLMs on GSM8K and MATH benchmarks.
