---
layout: publication
title: 'Language Models Are Few-shot Multilingual Learners'
authors: Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, Pascale Fung
conference: "Arxiv"
year: 2021
bibkey: winata2021language
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2109.07684'}
tags: ['Few-Shot', 'GPT', 'Model Architecture']
---
General-purpose language models have demonstrated impressive capabilities,
performing on par with state-of-the-art approaches on a range of downstream
natural language processing (NLP) tasks and benchmarks when inferring
instructions from very few examples. Here, we evaluate the multilingual skills
of the GPT and T5 models in conducting multi-class classification on
non-English languages without any parameter updates. We show that, given a few
English examples as context, pre-trained language models can predict not only
English test samples but also non-English ones. Finally, we find the in-context
few-shot cross-lingual prediction results of language models are significantly
better than random prediction, and they are competitive compared to the
existing state-of-the-art cross-lingual models.
