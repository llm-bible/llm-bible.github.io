---
layout: publication
title: Language Models Are Few45;shot Multilingual Learners
authors: Winata Genta Indra, Madotto Andrea, Lin Zhaojiang, Liu Rosanne, Yosinski Jason, Fung Pascale
conference: "Arxiv"
year: 2021
bibkey: winata2021language
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2109.07684"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods']
---
General45;purpose language models have demonstrated impressive capabilities performing on par with state45;of45;the45;art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here we evaluate the multilingual skills of the GPT and T5 models in conducting multi45;class classification on non45;English languages without any parameter updates. We show that given a few English examples as context pre45;trained language models can predict not only English test samples but also non45;English ones. Finally we find the in45;context few45;shot cross45;lingual prediction results of language models are significantly better than random prediction and they are competitive compared to the existing state45;of45;the45;art cross45;lingual models.
