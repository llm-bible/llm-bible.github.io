---
layout: publication
title: An Affect45;rich Neural Conversational Model With Biased Attention And Weighted Cross45;entropy Loss
authors: Zhong Peixiang, Wang Di, Miao Chunyan
conference: "Arxiv"
year: 2018
bibkey: zhong2018affect
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1811.07078"}
tags: ['Attention Mechanism', 'Ethics And Bias', 'Model Architecture', 'RAG', 'Transformer']
---
Affect conveys important implicit information in human communication. Having the capability to correctly express affect during human45;machine conversations is one of the major milestones in artificial intelligence. In recent years extensive research on open45;domain neural conversational models has been conducted. However embedding affect into such models is still under explored. In this paper we propose an end45;to45;end affect45;rich open45;domain neural conversational model that produces responses not only appropriate in syntax and semantics but also with rich affect. Our model extends the Seq2Seq model and adopts VAD (Valence Arousal and Dominance) affective notations to embed each word with affects. In addition our model considers the effect of negators and intensifiers via a novel affective attention mechanism which biases attention towards affect45;rich words in input sentences. Lastly we train our model with an affect45;incorporated objective function to encourage the generation of affect45;rich words in the output responses. Evaluations based on both perplexity and human evaluations show that our model outperforms the state45;of45;the45;art baseline model of comparable size in producing natural and affect45;rich responses.
