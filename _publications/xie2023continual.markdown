---
layout: publication
title: QUERT Continual Pre45;training Of Language Model For Query Understanding In Travel Domain Search
authors: Xie Jian, Liang Yidan, Liu Jingping, Xiao Yanghua, Wu Baohua, Ni Shenghua
conference: "Arxiv"
year: 2023
bibkey: xie2023continual
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2306.06707"}
tags: ['Fine Tuning', 'RAG', 'Training Techniques']
---
In light of the success of the pre45;trained language models (PLMs) continual pre45;training of generic PLMs has been the paradigm of domain adaption. In this paper we propose QUERT A Continual Pre45;trained Language Model for QUERy Understanding in Travel Domain Search. QUERT is jointly trained on four tailored pre45;training tasks to the characteristics of query in travel domain search Geography45;aware Mask Prediction Geohash Code Prediction User Click Behavior Learning and Phrase and Token Order Prediction. Performance improvement of downstream tasks and ablation experiment demonstrate the effectiveness of our proposed pre45;training tasks. To be specific the average performance of downstream tasks increases by 2.0237; and 30.9337; in supervised and unsupervised settings respectively. To check on the improvement of QUERT to online business we deploy QUERT and perform A/B testing on Fliggy APP. The feedback results show that QUERT increases the Unique Click45;Through Rate and Page Click45;Through Rate by 0.8937; and 1.0337; when applying QUERT as the encoder. Our code and downstream task data will be released for future research.
