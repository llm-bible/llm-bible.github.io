---
layout: publication
title: Thread Of Thought Unraveling Chaotic Contexts
authors: Zhou Yucheng, Geng Xiubo, Shen Tao, Tao Chongyang, Long Guodong, Lou Jian-guang, Shen Jianbing
conference: "Arxiv"
year: 2023
bibkey: zhou2023thread
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2311.08734"}
tags: ['Pretraining Methods', 'Prompting']
---
Large Language Models (LLMs) have ushered in a transformative era in the field of natural language processing excelling in tasks related to text comprehension and generation. Nevertheless they encounter difficulties when confronted with chaotic contexts (e.g. distractors rather than long irrelevant context) leading to the inadvertent omission of certain details within the chaotic context. In response to these challenges we introduce the Thread of Thought (ThoT) strategy which draws inspiration from human cognitive processes. ThoT systematically segments and analyzes extended contexts while adeptly selecting pertinent information. This strategy serves as a versatile plug45;and45;play module seamlessly integrating with various LLMs and prompting techniques. In the experiments we utilize the PopQA and EntityQ datasets as well as a Multi45;Turn Conversation Response dataset (MTCR) we collected to illustrate that ThoT significantly improves reasoning performance compared to other prompting techniques.
