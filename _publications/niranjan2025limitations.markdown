---
layout: publication
title: 'On The Limitations Of Steering In Language Model Alignment'
authors: Chebrolu Niranjan, Kokil Jaidka, Gerard Christopher Yeo
conference: "Arxiv"
year: 2025
bibkey: niranjan2025limitations
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2505.01162"}
tags: ['Model Architecture', 'Tools', 'Reinforcement Learning', 'Pretraining Methods', 'Transformer', 'Prompting']
---
Steering vectors are a promising approach to aligning language model behavior
at inference time. In this paper, we propose a framework to assess the
limitations of steering vectors as alignment mechanisms. Using a framework of
transformer hook interventions and antonym-based function vectors, we evaluate
the role of prompt structure and context complexity in steering effectiveness.
Our findings indicate that steering vectors are promising for specific
alignment tasks, such as value alignment, but may not provide a robust
foundation for general-purpose alignment in LLMs, particularly in complex
scenarios. We establish a methodological foundation for future investigations
into steering capabilities of reasoning models.
