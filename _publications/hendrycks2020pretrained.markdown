---
layout: publication
title: Pretrained Transformers Improve Out45;of45;distribution Robustness
authors: Hendrycks Dan, Liu Xiaoyuan, Wallace Eric, Dziedzic Adam, Krishnan Rishabh, Song Dawn
conference: "Arxiv"
year: 2020
bibkey: hendrycks2020pretrained
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2004.06100"}
tags: ['BERT', 'Distillation', 'Efficiency And Optimization', 'Model Architecture', 'Pretraining Methods', 'Security', 'Training Techniques', 'Transformer']
---
Although pretrained Transformers such as BERT achieve high accuracy on in45;distribution examples do they generalize to new distributions We systematically measure out45;of45;distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag45;of45;words models ConvNets and LSTMs and we show that pretrained Transformers performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples while many previous models are frequently worse than chance. We examine which factors affect robustness finding that larger models are not necessarily more robust distillation can be harmful and more diverse pretraining data can enhance robustness. Finally we show where future work can improve OOD robustness.
