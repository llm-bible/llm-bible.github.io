---
layout: publication
title: Surprising Efficacy Of Fine45;tuned Transformers For Fact45;checking Over Larger Language Models
authors: Setty Vinay
conference: "Arxiv"
year: 2024
bibkey: setty2024surprising
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.12147"}
tags: ['GPT', 'Model Architecture', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
In this paper we explore the challenges associated with establishing an end45;to45;end fact45;checking pipeline in a real45;world context covering over 90 languages. Our real45;world experimental benchmarks demonstrate that fine45;tuning Transformer models specifically for fact45;checking tasks such as claim detection and veracity prediction provide superior performance over large language models (LLMs) like GPT45;4 GPT45;3.545;Turbo and Mistral45;7b. However we illustrate that LLMs excel in generative tasks such as question decomposition for evidence retrieval. Through extensive evaluation we show the efficacy of fine45;tuned models for fact45;checking in a multilingual setting and complex claims that include numerical quantities.
