---
layout: publication
title: Localized Symbolic Knowledge Distillation For Visual Commonsense Models
authors: Park Jae Sung, Hessel Jack, Chandu Khyathi Raghavi, Liang Paul Pu, Lu Ximing, West Peter, Yu Youngjae, Huang Qiuyuan, Gao Jianfeng, Farhadi Ali, Choi Yejin
conference: "Arxiv"
year: 2023
bibkey: park2023localized
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.04837"}
tags: ['Applications', 'Distillation', 'Efficiency And Optimization', 'Multimodal Models', 'Prompting', 'Reinforcement Learning', 'Training Techniques']
---
Instruction following vision45;language (VL) models offer a flexible interface that supports a broad range of multimodal tasks in a zero45;shot fashion. However interfaces that operate on full images do not directly enable the user to point to and access specific regions within images. This capability is important not only to support reference45;grounded VL benchmarks but also for practical applications that require precise within45;image reasoning. We build Localized Visual Commonsense models which allow users to specify (multiple) regions as input. We train our model by sampling localized commonsense knowledge from a large language model (LLM) specifically we prompt an LLM to collect commonsense knowledge given a global literal image description and a local literal region description automatically generated by a set of VL models. With a separately trained critic model that selects high45;quality examples we find that training on the localized commonsense corpus can successfully distill existing VL models to support a reference45;as45;input interface. Empirical results and human evaluations in a zero45;shot setup demonstrate that our distillation method results in more precise VL models of reasoning compared to a baseline of passing a generated referring expression to an LLM.
