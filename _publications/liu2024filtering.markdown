---
layout: publication
title: 'Filtering Discomforting Recommendations With Large Language Models'
authors: Jiahao Liu, Yiyang Shao, Peng Zhang, Dongsheng Li, Hansu Gu, Chao Chen, Longzhi Du, Tun Lu, Ning Gu
conference: "Arxiv"
year: 2024
bibkey: liu2024filtering
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2410.05411"}
tags: ['Interpretability', 'Tools', 'Ethics and Bias']
---
Personalized algorithms can inadvertently expose users to discomforting
recommendations, potentially triggering negative consequences. The subjectivity
of discomfort and the black-box nature of these algorithms make it challenging
to effectively identify and filter such content. To address this, we first
conducted a formative study to understand users' practices and expectations
regarding discomforting recommendation filtering. Then, we designed a Large
Language Model (LLM)-based tool named DiscomfortFilter, which constructs an
editable preference profile for a user and helps the user express filtering
needs through conversation to mask discomforting preferences within the
profile. Based on the edited profile, DiscomfortFilter facilitates the
discomforting recommendations filtering in a plug-and-play manner, maintaining
flexibility and transparency. The constructed preference profile improves LLM
reasoning and simplifies user alignment, enabling a 3.8B open-source LLM to
rival top commercial models in an offline proxy task. A one-week user study
with 24 participants demonstrated the effectiveness of DiscomfortFilter, while
also highlighting its potential impact on platform recommendation outcomes. We
conclude by discussing the ongoing challenges, highlighting its relevance to
broader research, assessing stakeholder impact, and outlining future research
directions.
