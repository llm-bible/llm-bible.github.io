---
layout: publication
title: Self45;amplify Improving Small Language Models With Self Post Hoc Explanations
authors: Bhan Milan, Vittaut Jean-noel, Chesneau Nicolas, Lesot Marie-jeanne
conference: "Arxiv"
year: 2024
bibkey: bhan2024self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2402.12038"}
tags: ['GPT', 'Interpretability And Explainability', 'Pretraining Methods', 'Prompting', 'RAG']
---
Incorporating natural language rationales in the prompt and In45;Context Learning (ICL) have led to a significant improvement of Large Language Models (LLMs) performance. However generating high45;quality rationales require human45;annotation or the use of auxiliary proxy models. In this work we propose Self45;AMPLIFY to automatically generate rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self45;AMPLIFY is a 345;step method that targets samples generates rationales and builds a final prompt to leverage ICL. Self45;AMPLIFY performance is evaluated on four SLMs and five datasets requiring strong reasoning abilities. Self45;AMPLIFY achieves good results against competitors leading to strong accuracy improvement. Self45;AMPLIFY is the first method to apply post hoc explanation methods to autoregressive language models to generate rationales to improve their own performance in a fully automated manner.
