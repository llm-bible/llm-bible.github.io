---
layout: publication
title: 'Generative Language Models Exhibit Social Identity Biases'
authors: Tiancheng Hu, Yara Kyrychenko, Steve Rathje, Nigel Collier, Sander Van Der Linden, Jon Roozenbeek
conference: "Arxiv"
year: 2023
bibkey: hu2023generative
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.15819"}
tags: ['Training Techniques', 'Reinforcement Learning', 'Ethics and Bias', 'Pretraining Methods', 'Fine-Tuning', 'Prompting']
---
The surge in popularity of large language models has given rise to concerns
about biases that these models could learn from humans. We investigate whether
ingroup solidarity and outgroup hostility, fundamental social identity biases
known from social psychology, are present in 56 large language models. We find
that almost all foundational language models and some instruction fine-tuned
models exhibit clear ingroup-positive and outgroup-negative associations when
prompted to complete sentences (e.g., "We are..."). Our findings suggest that
modern language models exhibit fundamental social identity biases to a similar
degree as humans, both in the lab and in real-world conversations with LLMs,
and that curating training data and instruction fine-tuning can mitigate such
biases. Our results have practical implications for creating less biased
large-language models and further underscore the need for more research into
user interactions with LLMs to prevent potential bias reinforcement in humans.
