---
layout: publication
title: 'Looking Beyond Sentence-level Natural Language Inference For Downstream Tasks'
authors: Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Li, Pavan Kapanipathi, Kartik Talamadupula
conference: "Arxiv"
year: 2020
bibkey: mishra2020looking
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2009.09099'}
tags: ['Attention Mechanism', 'Training Techniques', 'Applications', 'Model Architecture', 'Reinforcement Learning']
---
In recent years, the Natural Language Inference (NLI) task has garnered
significant attention, with new datasets and models achieving near human-level
performance on it. However, the full promise of NLI -- particularly that it
learns knowledge that should be generalizable to other downstream NLP tasks --
has not been realized. In this paper, we study this unfulfilled promise from
the lens of two downstream tasks: question answering (QA), and text
summarization. We conjecture that a key difference between the NLI datasets and
these downstream tasks concerns the length of the premise; and that creating
new long premise NLI datasets out of existing QA datasets is a promising avenue
for training a truly generalizable NLI model. We validate our conjecture by
showing competitive results on the task of QA and obtaining the best reported
results on the task of Checking Factual Correctness of Summaries.
