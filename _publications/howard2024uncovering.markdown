---
layout: publication
title: "Uncovering Bias In Large Vision-language Models At Scale With Counterfactuals"
authors: Howard Phillip, Fraser Kathleen C., Bhiwandiwalla Anahita, Kiritchenko Svetlana
conference: "Arxiv"
year: 2024
bibkey: howard2024uncovering
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2405.20152"}
tags: ['Applications', 'Ethics And Bias', 'Multimodal Models', 'Prompting', 'Reinforcement Learning']
---
With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images. Specifically we present LVLMs with identical open-ended text prompts while conditioning on images from different counterfactual sets where each set contains images which are largely identical in their depiction of a common subject (e.g. a doctor) but vary only in terms of intersectional social attributes (e.g. race and gender). We comprehensively evaluate the text produced by different models under this counterfactual generation setting at scale producing over 57 million responses from popular LVLMs. Our multi-dimensional analysis reveals that social attributes such as race gender and physical characteristics depicted in input images can significantly influence the generation of toxic content competency-associated words harmful stereotypes and numerical ratings of depicted individuals. We additionally explore the relationship between social bias in LVLMs and their corresponding LLMs as well as inference-time strategies to mitigate bias.
