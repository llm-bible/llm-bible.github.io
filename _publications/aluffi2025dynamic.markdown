---
layout: publication
title: 'Dynamic Benchmarking Framework For Llm-based Conversational Data Capture'
authors: Pietro Alessandro Aluffi, Patrick Zietkiewicz, Marya Bazzi, Matt Arderne, Vladimirs Murevics
conference: "Arxiv"
year: 2025
bibkey: aluffi2025dynamic
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.04349'}
tags: ['Reinforcement Learning', 'Agentic', 'Few-Shot', 'Tools']
---
The rapid evolution of large language models (LLMs) has transformed
conversational agents, enabling complex human-machine interactions. However,
evaluation frameworks often focus on single tasks, failing to capture the
dynamic nature of multi-turn dialogues. This paper introduces a dynamic
benchmarking framework to assess LLM-based conversational agents through
interactions with synthetic users. The framework integrates generative agent
simulation to evaluate performance on key dimensions: information extraction,
context awareness, and adaptive engagement. By simulating various aspects of
user behavior, our work provides a scalable, automated, and flexible
benchmarking approach. Experimental evaluation - within a loan application use
case - demonstrates the framework's effectiveness under one-shot and few-shot
extraction conditions. Results show that adaptive strategies improve data
extraction accuracy, especially when handling ambiguous responses. Future work
will extend its applicability to broader domains and incorporate additional
metrics (e.g., conversational coherence, user engagement). This study
contributes a structured, scalable approach to evaluating LLM-based
conversational agents, facilitating real-world deployment.
