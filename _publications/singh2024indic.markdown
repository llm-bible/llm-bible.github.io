---
layout: publication
title: INDIC QA BENCHMARK A Multilingual Benchmark To Evaluate Question Answering Capability Of Llms For Indic Languages
authors: Singh Abhishek Kumar, Murthy Rudra, Kumar Vishwajeet, Sen Jaydeep, Ramakrishnan Ganesh
conference: "Arxiv"
year: 2024
bibkey: singh2024indic
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2407.13522"}
tags: ['Applications', 'Reinforcement Learning']
---
Large Language Models (LLMs) have demonstrated remarkable zero45;shot and few45;shot capabilities in unseen tasks including context45;grounded question answering (QA) in English. However the evaluation of LLMs capabilities in non45;English languages for context45;based QA is limited by the scarcity of benchmarks in non45;English languages. To address this gap we introduce Indic45;QA the largest publicly available context45;grounded question45;answering dataset for 11 major Indian languages from two language families. The dataset comprises both extractive and abstractive question45;answering tasks and includes existing datasets as well as English QA datasets translated into Indian languages. Additionally we generate a synthetic dataset using the Gemini model to create question45;answer pairs given a passage which is then manually verified for quality assurance. We evaluate various multilingual Large Language Models and their instruction45;fine45;tuned variants on the benchmark and observe that their performance is subpar particularly for low45;resource languages. We hope that the release of this dataset will stimulate further research on the question45;answering abilities of LLMs for low45;resource languages.
