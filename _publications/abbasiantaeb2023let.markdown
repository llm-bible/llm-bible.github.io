---
layout: publication
title: Let The Llms Talk Simulating Human45;to45;human Conversational QA Via Zero45;shot Llm45;to45;llm Interactions
authors: Abbasiantaeb Zahra, Yuan Yifei, Kanoulas Evangelos, Aliannejadi Mohammad
conference: "Arxiv"
year: 2023
bibkey: abbasiantaeb2023let
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2312.02913"}
tags: ['GPT', 'Model Architecture', 'Prompting', 'Tools']
---
Conversational question45;answering (CQA) systems aim to create interactive search systems that effectively retrieve information by interacting with users. To replicate human45;to45;human conversations existing work uses human annotators to play the roles of the questioner (student) and the answerer (teacher). Despite its effectiveness challenges exist as human annotation is time45;consuming inconsistent and not scalable. To address this issue and investigate the applicability of large language models (LLMs) in CQA simulation we propose a simulation framework that employs zero45;shot learner LLMs for simulating teacher45;student interactions. Our framework involves two LLMs interacting on a specific topic with the first LLM acting as a student generating questions to explore a given search topic. The second LLM plays the role of a teacher by answering questions and is equipped with additional information including a text on the given topic. We implement both the student and teacher by zero45;shot prompting the GPT45;4 model. To assess the effectiveness of LLMs in simulating CQA interactions and understand the disparities between LLM45; and human45;generated conversations we evaluate the simulated data from various perspectives. We begin by evaluating the teachers performance through both automatic and human assessment. Next we evaluate the performance of the student analyzing and comparing the disparities between questions generated by the LLM and those generated by humans. Furthermore we conduct extensive analyses to thoroughly examine the LLM performance by benchmarking state45;of45;the45;art reading comprehension models on both datasets. Our results reveal that the teacher LLM generates lengthier answers that tend to be more accurate and complete. The student LLM generates more diverse questions covering more aspects of a given topic.
