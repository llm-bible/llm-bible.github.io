---
layout: publication
title: Cross45;lingual Supervision Improves Large Language Models Pre45;training
authors: Schioppa Andrea, Garcia Xavier, Firat Orhan
conference: "Arxiv"
year: 2023
bibkey: schioppa2023cross
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2305.11778"}
tags: ['Applications', 'Language Modeling', 'Tools', 'Training Techniques']
---
The recent rapid progress in pre45;training Large Language Models has relied on using self45;supervised language modeling objectives like next token prediction or span corruption. On the other hand Machine Translation Systems are mostly trained using cross45;lingual supervision that requires aligned data between source and target languages. We demonstrate that pre45;training Large Language Models on a mixture of a self45;supervised Language Modeling objective and the supervised Machine Translation objective therefore including cross45;lingual parallel data during pre45;training yields models with better in45;context learning abilities. As pre45;training is a very resource45;intensive process and a grid search on the best mixing ratio between the two objectives is prohibitively expensive we propose a simple yet effective strategy to learn it during pre45;training.
