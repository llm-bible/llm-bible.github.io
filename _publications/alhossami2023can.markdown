---
layout: publication
title: Can Language Models Employ The Socratic Method Experiments With Code Debugging
authors: Al-hossami Erfan, Bunescu Razvan, Smith Justin, Teehan Ryan
conference: "Arxiv"
year: 2023
bibkey: alhossami2023can
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2310.03210"}
  - {name: "Code", url: "https://github.com/taisazero/socratic&#45;debugging&#45;benchmark"}
tags: ['Agentic', 'GPT', 'Has Code', 'Model Architecture', 'Pretraining Methods', 'Prompting', 'Training Techniques', 'Transformer']
---
When employing the Socratic method of teaching instructors guide students toward solving a problem on their own rather than providing the solution directly. While this strategy can substantially improve learning outcomes it is usually time45;consuming and cognitively demanding. Automated Socratic conversational agents can augment human instruction and provide the necessary scale however their development is hampered by the lack of suitable data for training and evaluation. In this paper we introduce a manually created dataset of multi45;turn Socratic advice that is aimed at helping a novice programmer fix buggy solutions to simple computational problems. The dataset is then used for benchmarking the Socratic debugging abilities of a number of language models ranging from fine45;tuning the instruction45;based text45;to45;text transformer Flan45;T5 to zero45;shot and chain of thought prompting of the much larger GPT45;4. The code and datasets are made freely available for research at the link below. https://github.com/taisazero/socratic&#45;debugging&#45;benchmark
