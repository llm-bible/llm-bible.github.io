---
layout: publication
title: 'Re-evaluating Theory Of Mind Evaluation In Large Language Models'
authors: Jennifer Hu, Felix Sosa, Tomer Ullman
conference: "Arxiv"
year: 2025
bibkey: hu2025re
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2502.21098'}
tags: ['Reinforcement Learning', 'RAG', 'Merging']
---
The question of whether large language models (LLMs) possess Theory of Mind
(ToM) -- often defined as the ability to reason about others' mental states --
has sparked significant scientific and public interest. However, the evidence
as to whether LLMs possess ToM is mixed, and the recent growth in evaluations
has not resulted in a convergence. Here, we take inspiration from cognitive
science to re-evaluate the state of ToM evaluation in LLMs. We argue that a
major reason for the disagreement on whether LLMs have ToM is a lack of clarity
on whether models should be expected to match human behaviors, or the
computations underlying those behaviors. We also highlight ways in which
current evaluations may be deviating from "pure" measurements of ToM abilities,
which also contributes to the confusion. We conclude by discussing several
directions for future research, including the relationship between ToM and
pragmatic communication, which could advance our understanding of artificial
systems as well as human cognition.
