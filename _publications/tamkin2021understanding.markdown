---
layout: publication
title: 'Understanding The Capabilities, Limitations, And Societal Impact Of Large Language Models'
authors: Alex Tamkin, Miles Brundage, Jack Clark, Deep Ganguli
conference: "Arxiv"
year: 2021
bibkey: tamkin2021understanding
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2102.02503"}
tags: ['Model Architecture', 'GPT']
---
On October 14th, 2020, researchers from OpenAI, the Stanford Institute for
Human-Centered Artificial Intelligence, and other universities convened to
discuss open research questions surrounding GPT-3, the largest
publicly-disclosed dense language model at the time. The meeting took place
under Chatham House Rules. Discussants came from a variety of research
backgrounds including computer science, linguistics, philosophy, political
science, communications, cyber policy, and more. Broadly, the discussion
centered around two main questions: 1) What are the technical capabilities and
limitations of large language models? 2) What are the societal effects of
widespread use of large language models? Here, we provide a detailed summary of
the discussion organized by the two themes above.
