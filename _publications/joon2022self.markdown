---
layout: publication
title: Self45;generated In45;context Learning Leveraging Auto45;regressive Language Models As A Demonstration Generator
authors: Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, Sang-goo Lee
conference: "Arxiv"
year: 2022
bibkey: joon2022self
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/http://arxiv.org/abs/2206.08082v1"}
tags: ['Pretraining Methods', 'Prompting', 'RAG', 'Training Techniques']
---
Large45;scale pre45;trained language models (PLMs) are well45;known for being capable of solving a task simply by conditioning a few input45;label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e. in45;context learning) however naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper we propose self45;generated in45;context learning (SG45;ICL) which generates demonstrations for in45;context learning from PLM itself to minimize the reliance on the external demonstration. We conduct experiments on four different text classification tasks and show SG45;ICL significantly outperforms zero45;shot learning and is generally worth approximately 0.6 gold training samples. Moreover our generated demonstrations show more consistent performance with low variance compared to randomly selected demonstrations from the training dataset.
