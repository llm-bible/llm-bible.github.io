---
layout: publication
title: Audio45;visual Scene45;aware Dialog And Reasoning Using Audio45;visual Transformers With Joint Student45;teacher Learning
authors: Shah Ankit P., Geng Shijie, Gao Peng, Cherian Anoop, Hori Takaaki, Marks Tim K., Roux Jonathan Le, Hori Chiori
conference: "Arxiv"
year: 2021
bibkey: shah2021audio
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2110.06894"}
tags: ['Applications', 'Attention Mechanism', 'Merging', 'Model Architecture', 'Multimodal Models', 'Pretraining Methods', 'Reinforcement Learning', 'Transformer']
---
In previous work we have proposed the Audio45;Visual Scene45;Aware Dialog (AVSD) task collected an AVSD dataset developed AVSD technologies and hosted an AVSD challenge track at both the 7th and 8th Dialog System Technology Challenges (DSTC7 DSTC8). In these challenges the best45;performing systems relied heavily on human45;generated descriptions of the video content which were available in the datasets but would be unavailable in real45;world applications. To promote further advancements for real45;world applications we proposed a third AVSD challenge at DSTC10 with two modifications 1) the human45;created description is unavailable at inference time and 2) systems must demonstrate temporal reasoning by finding evidence from the video to support each answer. This paper introduces the new task that includes temporal reasoning and our new extension of the AVSD dataset for DSTC10 for which we collected human45;generated temporal reasoning data. We also introduce a baseline system built using an AV45;transformer which we released along with the new dataset. Finally this paper introduces a new system that extends our baseline system with attentional multimodal fusion joint student45;teacher learning (JSTL) and model combination techniques achieving state45;of45;the45;art performances on the AVSD datasets for DSTC7 DSTC8 and DSTC10. We also propose two temporal reasoning methods for AVSD one attention45;based and one based on a time45;domain region proposal network.
