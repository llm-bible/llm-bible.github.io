---
layout: publication
title: 'Teaching Llms To Refine With Tools'
authors: Dian Yu, Yuheng Zhang, Jiahao Xu, Tian Liang, Linfeng Song, Zhaopeng Tu, Haitao Mi, Dong Yu
conference: "Arxiv"
year: 2024
bibkey: yu2024teaching
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2412.16871"}
tags: ['Fine-Tuning', 'Tools', 'Efficiency and Optimization', 'RAG', 'Reinforcement Learning', 'Training Techniques', 'Pretraining Methods']
---
Large language models (LLMs) can refine their responses based on feedback,
enabling self-improvement through iterative training or test-time refinement.
However, existing methods predominantly focus on refinement within the same
reasoning format, which may lead to non-correcting behaviors. We propose CaP, a
novel approach that uses external tools to refine chain-of-thought (CoT)
responses generated by the same or other LLMs. CaP employs a two-stage training
process: supervised fine-tuning followed by preference optimization with DPO
variants. Our observations highlight the critical role of preference
optimization in enabling effective refinement. Additionally, we compare several
sampling strategies to leverage CoT and tools at inference time. Experimental
results demonstrate CaP's potential for effective cross-reasoning refinement
and efficient inference.
