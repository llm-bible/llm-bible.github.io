---
layout: publication
title: Re45;task Revisiting LLM Tasks From Capability Skill And Knowledge Perspectives
authors: Wang Zhihu, Zhao Shiwan, Wang Yu, Huang Heyuan, Shi Jiaxin, Xie Sitao, Wang Zhixing, Zhang Yubo, Li Hongyan, Yan Junchi
conference: "Arxiv"
year: 2024
bibkey: wang2024re
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/2408.06904"}
tags: ['Pretraining Methods', 'Prompting', 'Tools']
---
As large language models (LLMs) continue to scale their enhanced performance often proves insufficient for solving domain45;specific tasks. Systematically analyzing their failures and effectively enhancing their performance remain significant challenges. This paper introduces the Re45;TASK framework a novel theoretical model that Revisits LLM Tasks from cApability Skill Knowledge perspectives guided by the principles of Blooms Taxonomy and Knowledge Space Theory. The Re45;TASK framework provides a systematic methodology to deepen our understanding evaluation and enhancement of LLMs for domain45;specific tasks. It explores the interplay among an LLMs capabilities the knowledge it processes and the skills it applies elucidating how these elements are interconnected and impact task performance. Our application of the Re45;TASK framework reveals that many failures in domain45;specific tasks can be attributed to insufficient knowledge or inadequate skill adaptation. With this insight we propose structured strategies for enhancing LLMs through targeted knowledge injection and skill adaptation. Specifically we identify key capability items associated with tasks and employ a deliberately designed prompting strategy to enhance task performance thereby reducing the need for extensive fine45;tuning. Alternatively we fine45;tune the LLM using capability45;specific instructions further validating the efficacy of our framework. Experimental results confirm the frameworks effectiveness demonstrating substantial improvements in both the performance and applicability of LLMs.
