---
layout: publication
title: 'MMMU: A Massive Multi-discipline Multimodal Understanding And Reasoning Benchmark
  For Expert AGI'
authors: Xiang Yue et al.
conference: Arxiv
year: 2023
citations: 50
bibkey: yue2023massive
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2311.16502'}]
tags: [Multimodal Models, GPT]
---
We introduce MMMU: a new benchmark designed to evaluate multimodal models on
massive multi-discipline tasks demanding college-level subject knowledge and
deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal
questions from college exams, quizzes, and textbooks, covering six core
disciplines: Art & Design, Business, Science, Health & Medicine, Humanities &
Social Science, and Tech & Engineering. These questions span 30 subjects and
183 subfields, comprising 30 highly heterogeneous image types, such as charts,
diagrams, maps, tables, music sheets, and chemical structures. Unlike existing
benchmarks, MMMU focuses on advanced perception and reasoning with
domain-specific knowledge, challenging models to perform tasks akin to those
faced by experts. The evaluation of 14 open-source LMMs as well as the
proprietary GPT-4V(ision) and Gemini highlights the substantial challenges
posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve
accuracies of 56% and 59% respectively, indicating significant room for
improvement. We believe MMMU will stimulate the community to build
next-generation multimodal foundation models towards expert artificial general
intelligence.