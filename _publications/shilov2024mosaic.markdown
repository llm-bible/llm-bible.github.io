---
layout: publication
title: 'The Mosaic Memory Of Large Language Models'
authors: Igor Shilov, Matthieu Meeus, Yves-alexandre De Montjoye
conference: "Arxiv"
year: 2024
bibkey: shilov2024mosaic
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2405.15523'}
tags: ['Reinforcement Learning', 'Training Techniques']
---
As Large Language Models (LLMs) become widely adopted, understanding how they learn from, and memorize, training data becomes crucial. Memorization in LLMs is widely assumed to only occur as a result of sequences being repeated in the training data. Instead, we show that LLMs memorize by assembling information from similar sequences, a phenomena we call mosaic memory. We show major LLMs to exhibit mosaic memory, with fuzzy duplicates contributing to memorization as much as 0.8 of an exact duplicate and even heavily modified sequences contributing substantially to memorization. Despite models display reasoning capabilities, we somewhat surprisingly show memorization to be predominantly syntactic rather than semantic. We finally show fuzzy duplicates to be ubiquitous in real-world data, untouched by deduplication techniques. Taken together, our results challenge widely held beliefs and show memorization to be a more complex, mosaic process, with real-world implications for privacy, confidentiality, model utility and evaluation.
