<!DOCTYPE html>
<html lang="en-us">

  <head>
  <!-- Begin Web-Stat code v 7.0 -->
  <span id="wts2185292"></span>
  <script>
    var wts = document.createElement('script');
    wts.async = true;
    wts.src = 'https://app.ardalio.com/log7.js';
    document.head.appendChild(wts);
    wts.onload = function() {
      wtslog7(2185292, 2);
    };
  </script>
  <noscript>
    <a href="https://www.web-stat.com">
      <img src="https://app.ardalio.com/7/2/2185292.png" alt="Web-Stat analytics">
    </a>
  </noscript>
  <!-- End Web-Stat code v 7.0 -->

  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta
    name="keywords"
    content="large language models, GPT, transformers, BERT, natural language processing, NLP, deep learning, machine learning, source code, big code, naturalness, software engineering, programming languages, fine-tuning, pretraining, autoregressive models, attention mechanisms, multi-modal models, text generation, reinforcement learning with human feedback, RLHF, GPT-3, GPT-4, ChatGPT, OpenAI, model architectures, transfer learning, LLM applications, LLM benchmarks, LLM interpretability"
  >
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]],
      },
      options: {
        processHtmlClass: "mathjax-content",
        processEscapes: true,
      }
    };
  </script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <title>
    
      Publications by Tag &middot; The Large Language Model Bible
    
  </title>

  <!-- Enable responsiveness on mobile devices -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link
    rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"
  >

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link
    rel="search"
    href="/public/opensearchdescription.xml"
    type="application/opensearchdescription+xml"
    title="LLM-Bible"
  />

  <!-- jQuery Library -->
  <script
    src="https://code.jquery.com/jquery-3.2.1.min.js"
    integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
    crossorigin="anonymous"
  ></script>

  <!-- DataTables CSS -->
  <link
    rel="stylesheet"
    type="text/css"
    href="https://cdn.datatables.net/1.10.20/css/jquery.dataTables.css"
  >

  <!-- DataTables JS -->
  <script
    type="text/javascript"
    charset="utf8"
    src="https://cdn.datatables.net/1.10.20/js/jquery.dataTables.js"
  ></script>
</head>


  <body class="theme-base-0d layout-reverse">

    <a href='/contributing.html' class='ribbon'>Contribute to LLM-Bible</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          The Large Language Model Bible
        </a>
      </h1>
      <p class="lead">Research on Large Language Models. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>      
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">List of Papers</a>
      <a class="sidebar-nav-item active" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/chatbot.html">Chatbot-based Explorer</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses & Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">Contact <a href="https://sjmoran.github.io">Sean Moran</a> about this survey or website.
      <span style="font-size: 9px">
        Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
      </span></p>
    </div>
  </div>
</div>

<script>
  // Initiate search when 'Enter' is pressed in the search input
  $("#searchTarget").keydown(function (e) {	
    if (e.keyCode == 13) {
      search();
    }
  });

  function search() {
    var query = $("#searchTarget").val();
    try {
      ga('send', 'event', 'search', 'search', query);  // Google Analytics event tracking
    } finally {
      // Redirect to papers.html with the query as a hash
      window.location = "/papers.html#" + encodeURIComponent(query);
    }
  }
</script>


    <div class="content container">
      



<h1>Publications by Tag</h1>
<p>
The following tags appear in the publications listed in the review:
</p>
<tag><a href="/tags.html#ACL">ACL</a></tag> <tag><a href="/tags.html#Agentic">Agentic</a></tag> <tag><a href="/tags.html#Applications">Applications</a></tag> <tag><a href="/tags.html#Attention Mechanism">Attention Mechanism</a></tag> <tag><a href="/tags.html#BART">BART</a></tag> <tag><a href="/tags.html#BERT">BERT</a></tag> <tag><a href="/tags.html#Bias Mitigation">Bias Mitigation</a></tag> <tag><a href="/tags.html#Distillation">Distillation</a></tag> <tag><a href="/tags.html#Efficiency and Optimization">Efficiency and Optimization</a></tag> <tag><a href="/tags.html#EMNLP">EMNLP</a></tag> <tag><a href="/tags.html#Ethics and Bias">Ethics and Bias</a></tag> <tag><a href="/tags.html#Fairness">Fairness</a></tag> <tag><a href="/tags.html#Few-Shot">Few-Shot</a></tag> <tag><a href="/tags.html#Fine-Tuning">Fine-Tuning</a></tag> <tag><a href="/tags.html#GPT">GPT</a></tag> <tag><a href="/tags.html#Has Code">Has Code</a></tag> <tag><a href="/tags.html#In-Context Learning">In-Context Learning</a></tag> <tag><a href="/tags.html#Interpretability and Explainability">Interpretability and Explainability</a></tag> <tag><a href="/tags.html#Language Modeling">Language Modeling</a></tag> <tag><a href="/tags.html#Large-Scale Training">Large-Scale Training</a></tag> <tag><a href="/tags.html#Masked Language Model">Masked Language Model</a></tag> <tag><a href="/tags.html#Model Architecture">Model Architecture</a></tag> <tag><a href="/tags.html#Multimodal Models">Multimodal Models</a></tag> <tag><a href="/tags.html#NeurIPS">NeurIPS</a></tag> <tag><a href="/tags.html#Pre-Training">Pre-Training</a></tag> <tag><a href="/tags.html#Prompting">Prompting</a></tag> <tag><a href="/tags.html#Pruning">Pruning</a></tag> <tag><a href="/tags.html#Quantization">Quantization</a></tag> <tag><a href="/tags.html#RAG">RAG</a></tag> <tag><a href="/tags.html#Reinforcement Learning">Reinforcement Learning</a></tag> <tag><a href="/tags.html#Responsible AI">Responsible AI</a></tag> <tag><a href="/tags.html#Scaling Laws">Scaling Laws</a></tag> <tag><a href="/tags.html#Security">Security</a></tag> <tag><a href="/tags.html#Survey Paper">Survey Paper</a></tag> <tag><a href="/tags.html#Tokenization">Tokenization</a></tag> <tag><a href="/tags.html#Tools">Tools</a></tag> <tag><a href="/tags.html#Training Techniques">Training Techniques</a></tag> <tag><a href="/tags.html#Transformer">Transformer</a></tag> <tag><a href="/tags.html#Uncategorized">Uncategorized</a></tag> <tag><a href="/tags.html#Vector Indexing">Vector Indexing</a></tag> 

<h2>Tags</h2>
<p>See below a list of all tags and the related papers</p>




   <h3>üè∑ ACL <a id="ACL"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2021few/">Few-shot Conversational Dense Retrieval</a> Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, Zhiyuan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2023improving/">Improving CLIP Training With Language Rewrites</a> Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, Yonglong Tian </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Agentic <a id="Agentic"></a></h3>
   <ul>
   
     
       <li> <a href="/publications/li2016deep/">Deep Reinforcement Learning For Dialogue Generation</a> Jiwei Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/asghar2016deep/">Deep Active Learning For Dialogue Generation</a> Nabiha Asghar, Pascal Poupart, Xin Jiang, Hang Li </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/das2016visual/">Visual Dialog</a> Abhishek Das et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2016fast/">A Simple, Fast Diverse Decoding Algorithm For Neural Generation</a> Jiwei Li, Will Monroe, Dan Jurafsky </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2016user/">A User Simulator For Task-completion Dialogues</a> Xiujun Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/eric2017copy/">A Copy-augmented Sequence-to-sequence Architecture Gives Good Performance On Task-oriented Dialogue</a> Mihail Eric, Christopher D. Manning </li>
     
   
     
       <li> <a href="/publications/anderson2017vision/">Vision-and-language Navigation: Interpreting Visually-grounded Navigation Instructions In Real Environments</a> Peter Anderson et al. </li>
     
   
     
       <li> <a href="/publications/narasimhan2017grounding/">Grounding Language For Transfer In Deep Reinforcement Learning</a> Karthik Narasimhan, Regina Barzilay, Tommi Jaakkola </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kandasamy2017batch/">Batch Policy Gradient Methods For Improving Neural Conversation Models</a> Kirthevasan Kandasamy, Yoram Bachrach, Ryota Tomioka, Daniel Tarlow, David Carter </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/strub2017end/">End-to-end Optimization Of Goal-driven And Visually Grounded Dialogue Systems</a> Florian Strub et al. </li>
     
   
     
   
     
       <li> <a href="/publications/buck2017ask/">Ask The Right Questions: Active Question Reformulation With Reinforcement Learning</a> Christian Buck et al. </li>
     
   
     
       <li> <a href="/publications/chaplot2017gated/">Gated-attention Architectures For Task-oriented Language Grounding</a> Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov </li>
     
   
     
       <li> <a href="/publications/li2017data/">Data Distillation For Controlling Specificity In Dialogue Generation</a> Jiwei Li, Will Monroe, Dan Jurafsky </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/miller2017dialog/">Parlai: A Dialog Research Software Platform</a> Alexander H. Miller et al. </li>
     
   
     
       <li> <a href="/publications/wen2017latent/">Latent Intention Dialogue Models</a> Tsung-hsien Wen, Yishu Miao, Phil Blunsom, Steve Young </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2017steering/">Steering Output Style And Topic In Neural Response Generation</a> Di Wang, Nebojsa Jojic, Chris Brockett, Eric Nyberg </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2017sample/">Sample-efficient Actor-critic Reinforcement Learning With Supervised Data For Dialogue Management</a> Pei-hao Su, Pawel Budzianowski, Stefan Ultes, Milica Gasic, Steve Young </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2017asking/">Asking The Difficult Questions: Goal-oriented Visual Question Generation Via Intermediate Rewards</a> Junjie Zhang et al. </li>
     
   
     
       <li> <a href="/publications/serban2017deep/">A Deep Reinforcement Learning Chatbot</a> Iulian V. Serban et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2017reinforced/">R\(^3\): Reinforced Reader-ranker For Open-domain Question Answering</a> Shuohang Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2017adversarial/">Adversarial Learning For Neural Dialogue Generation</a> Jiwei Li et al. </li>
     
   
     
       <li> <a href="/publications/young2017augmenting/">Augmenting End-to-end Dialog Systems With Commonsense Knowledge</a> Tom Young et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2017generating/">Mojitalk: Generating Emotional Responses At Scale</a> Xianda Zhou, William Yang Wang </li>
     
   
     
       <li> <a href="/publications/guu2017from/">From Language To Programs: Bridging Reinforcement Learning And Maximum Marginal Likelihood</a> Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fast2017conversational/">Iris: A Conversational Agent For Complex Tasks</a> Ethan Fast, Binbin Chen, Julia Mendelsohn, Jonathan Bassen, Michael Bernstein </li>
     
   
     
       <li> <a href="/publications/lu2017practical/">A Practical Approach To Dialogue Response Generation In Closed Domains</a> Yichao Lu, Phillip Keung, Shaonan Zhang, Jason Sun, Vikas Bhardwaj </li>
     
   
     
   
     
       <li> <a href="/publications/rothe2017question/">Question Asking As Program Generation</a> Anselm Rothe, Brenden M. Lake, Todd M. Gureckis </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2017are/">Are You Talking To Me? Reasoned Visual Dialog Generation Through Adversarial Learning</a> Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, Anton Van Den Hengel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2018generating/">Livebot: Generating Live Video Comments Based On Visual And Textual Contexts</a> Shuming Ma, Lei Cui, Damai Dai, Furu Wei, Xu Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rashkin2018towards/">Towards Empathetic Open-domain Conversation Models: A New Benchmark And Dataset</a> Hannah Rashkin, Eric Michael Smith, Margaret Li, Y-lan Boureau </li>
     
   
     
       <li> <a href="/publications/li2018dialogue/">Dialogue Generation: From Imitation Learning To Inverse Reinforcement Learning</a> Ziming Li, Julia Kiseleva, Maarten De Rijke </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chevalierboisvert2018platform/">Babyai: A Platform To Study The Sample Efficiency Of Grounded Language Learning</a> Maxime Chevalier-boisvert et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ilievski2018goal/">Goal-oriented Chatbot Dialog Management Bootstrapping With Transfer Learning</a> Vladimir Ilievski, Claudiu Musat, Andreea Hossmann, Michael Baeriswyl </li>
     
   
     
   
     
       <li> <a href="/publications/lee2018zero/">Zero-shot Adaptive Transfer For Conversational Language Understanding</a> Sungjin Lee, Rahul Jha </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/agarwal2018knowledge/">A Knowledge-grounded Multimodal Search-based Conversational Agent</a> Shubham Agarwal, Ondrej Dusek, Ioannis Konstas, Verena Rieser </li>
     
   
     
       <li> <a href="/publications/mazar%C3%A92018training/">Training Millions Of Personalized Dialogue Agents</a> Pierre-emmanuel Mazar√©, Samuel Humeau, Martin Raison, Antoine Bordes </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/niu2018polite/">Polite Dialogue Generation Without Parallel Data</a> Tong Niu, Mohit Bansal </li>
     
   
     
       <li> <a href="/publications/parthasarathi2018extending/">Extending Neural Generative Conversational Model Using External Knowledge Sources</a> Prasanna Parthasarathi, Joelle Pineau </li>
     
   
     
       <li> <a href="/publications/li2018hybrid/">Hybrid Retrieval-generation Reinforced Agent For Medical Image Report Generation</a> Christy Y. Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing </li>
     
   
     
       <li> <a href="/publications/bunel2018leveraging/">Leveraging Grammar And Reinforcement Learning For Neural Program Synthesis</a> Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, Pushmeet Kohli </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2018guided/">Guided Feature Transformation (GFT): A Neural Language Grounding Module For Embodied Agents</a> Haonan Yu, Xiaochen Lian, Haichao Zhang, Wei Xu </li>
     
   
     
   
     
       <li> <a href="/publications/wang2018reinforced/">Reinforced Cross-modal Matching And Self-supervised Imitation Learning For Vision-language Navigation</a> Xin Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/juraska2018deep/">A Deep Ensemble Model With Slot Alignment For Sequence-to-sequence Natural Language Generation</a> Juraj Juraska, Panagiotis Karagiannis, Kevin K. Bowden, Marilyn A. Walker </li>
     
   
     
   
     
       <li> <a href="/publications/venkatesh2018evaluating/">On Evaluating And Comparing Open Domain Dialog Systems</a> Anu Venkatesh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/coreyes2018guiding/">Guiding Policies With Language Via Meta-learning</a> John D. Co-reyes et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dinan2018wizard/">Wizard Of Wikipedia: Knowledge-powered Conversational Agents</a> Emily Dinan et al. </li>
     
   
     
       <li> <a href="/publications/shi2018sentiment/">Sentiment Adaptive End-to-end Dialog Systems</a> Weiyan Shi, Zhou Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/das2018neural/">Neural Modular Control For Embodied Question Answering</a> Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2018context/">Context-aware Visual Policy Network For Sequence-level Image Captioning</a> Daqing Liu, Zheng-jun Zha, Hanwang Zhang, Yongdong Zhang, Feng Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2018vision/">Vision-based Navigation With Language-based Assistance Via Imitation Learning With Indirect Intervention</a> Khanh Nguyen, Debadeepta Dey, Chris Brockett, Bill Dolan </li>
     
   
     
       <li> <a href="/publications/kreyssig2018neural/">Neural User Simulation For Corpus-based Policy Optimisation For Spoken Dialogue Systems</a> Florian Kreyssig, Inigo Casanueva, Pawel Budzianowski, Milica Gasic </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2018towards/">Towards Explainable And Controllable Open Domain Dialogue Generation With Dialogue Acts</a> Can Xu, Wei Wu, Yu Wu </li>
     
   
     
   
     
       <li> <a href="/publications/ram2018conversational/">Conversational AI: The Science Behind The Alexa Prize</a> Ashwin Ram et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/niu2018recursive/">Recursive Visual Attention In Visual Dialog</a> Yulei Niu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shah2018building/">Building A Conversational Agent Overnight With Dialogue Self-play</a> Pararth Shah et al. </li>
     
   
     
       <li> <a href="/publications/c%C3%B4t%C3%A92018learning/">Textworld: A Learning Environment For Text-based Games</a> Marc-alexandre C√¥t√© et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2019personalizing/">Personalizing Dialogue Agents Via Meta-learning</a> Zhaojiang Lin, Andrea Madotto, Chien-sheng Wu, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019reinforcement/">Reinforcement Learning Based Graph-to-sequence Model For Natural Question Generation</a> Yu Chen, Lingfei Wu, Mohammed J. Zaki </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cornia2019training/">Smart: Training Shallow Memory-aware Transformers For Robotic Explainability</a> Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/byrne2019taskmaster/">Taskmaster-1: Toward A Realistic And Diverse Dialog Dataset</a> Bill Byrne et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/budzianowski2019gpt/">Hello, It's GPT-2 -- How Can I Help You? Towards The Use Of Pretrained Language Models For Task-oriented Dialogue Systems</a> Pawe≈Ç Budzianowski, Ivan Vuliƒá </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2019reinforced/">Reinforced Dynamic Reasoning For Conversational Question Generation</a> Boyuan Pan, Hao Li, Ziyu Yao, Deng Cai, Huan Sun </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cuay%C3%A1huitl2019ensemble/">Ensemble-based Deep Reinforcement Learning For Chatbots</a> Heriberto Cuay√°huitl et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hancock2019learning/">Learning From Dialogue After Deployment: Feed Yourself, Chatbot!</a> Braden Hancock, Antoine Bordes, Pierre-emmanuel Mazar√©, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/le2019multimodal/">Multimodal Transformer Networks For End-to-end Video-grounded Dialogue Systems</a> Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/alamri2019audio/">Audio-visual Scene-aware Dialog</a> Huda Alamri et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ghandeharioun2019approximating/">Approximating Interactive Human Evaluation With Self-play For Open-domain Dialog Systems</a> Asma Ghandeharioun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2019from/">From Language To Goals: Inverse Reinforcement Learning For Vision-based Instruction Following</a> Justin Fu, Anoop Korattikara, Sergey Levine, Sergio Guadarrama </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019reinforcement/">Reinforcement Learning Based Emotional Editing Constraint Conversation Generation</a> Jia Li, Xiao Sun, Xing Wei, Changliang Li, Jianhua Tao </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/goyal2019using/">Using Natural Language For Reward Shaping In Reinforcement Learning</a> Prasoon Goyal, Scott Niekum, Raymond J. Mooney </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thomason2019improving/">Improving Grounded Natural Language Understanding Through Human-robot Dialog</a> Jesse Thomason et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019robust/">Robust Navigation With Language Pretraining And Stochastic Sampling</a> Xiujun Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2019transferable/">Transferable Representation Learning In Vision-and-language Navigation</a> Haoshuo Huang et al. </li>
     
   
     
       <li> <a href="/publications/nguyen2019visual/">Help, Anna! Visual Navigation With Natural Multimodal Assistance Via Retrospective Curiosity-encouraging Imitation Learning</a> Khanh Nguyen, Hal Iii Daum√© </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/paxton2019interpretable/">Prospection: Interpretable Plans From Language By Predicting The Future</a> Chris Paxton, Yonatan Bisk, Jesse Thomason, Arunkumar Byravan, Dieter Fox </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sankar2019do/">Do Neural Dialog Systems Use The Conversation History Effectively? An Empirical Study</a> Chinnadhurai Sankar, Sandeep Subramanian, Christopher Pal, Sarath Chandar, Yoshua Bengio </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ehsan2019automated/">Automated Rationale Generation: A Technique For Explainable AI And Its Effects On Human Perceptions</a> Upol Ehsan, Pradyumna Tambwekar, Larry Chan, Brent Harrison, Mark Riedl </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2019empathetic/">Caire: An Empathetic Neural Chatbot</a> Zhaojiang Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2019curiosity/">Curiosity-driven Reinforcement Learning For Diverse Visual Paragraph Generation</a> Yadan Luo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019consistent/">Consistent Dialogue Generation With Self-supervised Feature Learning</a> Yizhe Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wolf2019transfer/">Transfertransfo: A Transfer Learning Approach For Neural Network Based Conversational Agents</a> Thomas Wolf, Victor Sanh, Julien Chaumond, Clement Delangue </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2019vision/">Vision-language Navigation With Self-supervised Auxiliary Reasoning Tasks</a> Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yogatama2019learning/">Learning And Evaluating General Linguistic Intelligence</a> Dani Yogatama et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/landi2019multimodal/">Multimodal Attention Networks For Low-level Vision-and-language Navigation</a> Federico Landi, Lorenzo Baraldi, Marcella Cornia, Massimiliano Corsini, Rita Cucchiara </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019dual/">DMRM: A Dual-channel Multi-hop Reasoning Model For Visual Dialog</a> Feilong Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ziegler2019fine/">Fine-tuning Language Models From Human Preferences</a> Daniel M. Ziegler et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shin2019generating/">Generating Empathetic Responses By Looking Ahead The User's Sentiment</a> Jamin Shin, Peng Xu, Andrea Madotto, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chi2019just/">Just Ask:an Interactive Learning Framework For Vision And Language Navigation</a> Ta-chung Chi, Mihail Eric, Seokhwan Kim, Minmin Shen, Dilek Hakkani-tur </li>
     
   
     
       <li> <a href="/publications/jain2019stay/">Stay On The Path: Instruction Fidelity In Vision-and-language Navigation</a> Vihan Jain et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lee2019countering/">Countering Language Drift Via Visual Grounding</a> Jason Lee, Kyunghyun Cho, Douwe Kiela </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/see2019what/">What Makes A Good Conversation? How Controllable Attributes Affect Human Judgments</a> Abigail See, Stephen Roller, Douwe Kiela, Jason Weston </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/rastogi2019towards/">Towards Scalable Multi-domain Conversational Agents: The Schema-guided Dialogue Dataset</a> Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, Pranav Khaitan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2019generating/">Generating Persona Consistent Dialogues By Exploiting Natural Language Inference</a> Haoyu Song, Wei-nan Zhang, Jingwen Hu, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2019language/">Language As An Abstraction For Hierarchical Deep Reinforcement Learning</a> Yiding Jiang, Shixiang Gu, Kevin Murphy, Chelsea Finn </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parisotto2019stabilizing/">Stabilizing Transformers For Reinforcement Learning</a> Emilio Parisotto et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2020countering/">Countering Language Drift With Seeded Iterated Learning</a> Yuchen Lu, Soumye Singhal, Florian Strub, Olivier Pietquin, Aaron Courville </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shuster2020multi/">Multi-modal Open-domain Dialogue</a> Kurt Shuster, Eric Michael Smith, Da Ju, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hill2020human/">Human Instruction-following With Deep Reinforcement Learning Via Transfer-learning From Text</a> Felix Hill, Sona Mokra, Nathaniel Wong, Tim Harley </li>
     
   
     
       <li> <a href="/publications/hill2020grounded/">Grounded Language Learning Fast And Slow</a> Felix Hill et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nichols2020collaborative/">Collaborative Storytelling With Large-scale Neural Language Models</a> Eric Nichols, Leo Gao, Randy Gomez </li>
     
   
     
   
     
       <li> <a href="/publications/smith2020can/">Can You Put It All Together: Evaluating Conversational Agents' Ability To Blend Skills</a> Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, Y-lan Boureau </li>
     
   
     
       <li> <a href="/publications/smith2020controlling/">Controlling Style In Generated Dialogue</a> Eric Michael Smith, Diana Gonzalez-rico, Emily Dinan, Y-lan Boureau </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2020vision/">Vision-dialog Navigation By Exploring Cross-modal Memory</a> Yi Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2020sub/">Sub-instruction Aware Vision-and-language Navigation</a> Yicong Hong, Cristian Rodriguez-opazo, Qi Wu, Stephen Gould </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2020going/">Babywalk: Going Farther In Vision-and-language Navigation By Taking Baby Steps</a> Wang Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2020low/">Low-resource Knowledge-grounded Dialogue Generation</a> Xueliang Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/paranjape2020neural/">Neural Generation Meets Real People: Towards Emotionally Engaging Mixed-initiative Conversations</a> Ashwin Paranjape et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2020addressing/">Addressing Some Limitations Of Transformers With Feedback Memory</a> Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moon2020situated/">Situated And Interactive Multimodal Conversations</a> Seungwhan Moon et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/papangelis2020plato/">Plato Dialogue System: A Flexible Conversational AI Research Platform</a> Alexandros Papangelis et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020vd/">VD-BERT: A Unified Vision And Dialog Transformer With BERT</a> Yue Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/akakzia2020grounding/">Grounding Language To Autonomously-acquired Skills Via Goal Generation</a> Ahmed Akakzia, C√©dric Colas, Pierre-yves Oudeyer, Mohamed Chetouani, Olivier Sigaud </li>
     
   
     
   
     
       <li> <a href="/publications/kumar2020ma/">MA-DST: Multi-attention Based Scalable Dialog State Tracking</a> Adarsh Kumar, Peter Ku, Anuj Kumar Goyal, Angeliki Metallinou, Dilek Hakkani-tur </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shridhar2020aligning/">Alfworld: Aligning Text And Embodied Environments For Interactive Learning</a> Mohit Shridhar et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geng2020dynamic/">Dynamic Graph Representation Learning For Video Dialog Via Multi-modal Shuffled Transformers</a> Shijie Geng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020data/">Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation</a> Ruibo Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2020will/">Will I Sound Like Me? Improving Persona Consistency In Dialogues Through Pragmatic Self-consciousness</a> Hyunwoo Kim, Byeongchang Kim, Gunhee Kim </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2020recurrent/">A Recurrent Vision-and-language BERT For Navigation</a> Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-opazo, Stephen Gould </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2020towards/">Towards Learning A Generic Agent For Vision-and-language Navigation Via Pre-training</a> Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/roller2020open/">Open-domain Conversational Agents: Current Progress, Open Problems, And Future Directions</a> Stephen Roller et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020modelling/">Modelling Hierarchical Structure Between Dialogue Policy And Natural Language Generator With Option Framework For Task-oriented Dialogue System</a> Jianhong Wang, Yuan Zhang, Tae-kyun Kim, Yunjie Gu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021generate/">Generate Natural Language Explanations For Recommendation</a> Hanxiong Chen, Xu Chen, Shaoyun Shi, Yongfeng Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2021hierarchical/">Hierarchical Task Learning From Language Instructions With Unified Transformers And Self-monitoring</a> Yichi Zhang, Joyce Chai </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zaib2021short/">A Short Survey Of Pre-trained Language Models For Conversational AI-A Newage In NLP</a> Munazza Zaib, Quan Z. Sheng, Wei Emma Zhang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021bilingual/">Bitod: A Bilingual Multi-domain Dataset For Task-oriented Dialogue Modeling</a> Zhaojiang Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qu2021asking/">Asking Questions Like Educational Experts: Automatically Generating Question-answer Pairs On Real-world Examination Data</a> Fanyi Qu, Xin Jia, Yunfang Wu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/razumovskaia2021crossing/">Crossing The Conversational Chasm: A Primer On Natural Language Processing For Multilingual Task-oriented Dialogue Systems</a> Evgeniia Razumovskaia et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021scene/">Scene-intuitive Agent For Remote Embodied Visual Grounding</a> Xiangru Lin, Guanbin Li, Yizhou Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mees2021benchmark/">CALVIN: A Benchmark For Language-conditioned Policy Learning For Long-horizon Robot Manipulation Tasks</a> Oier Mees, Lukas Hermann, Erick Rosete-beas, Wolfram Burgard </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/blukis2021persistent/">A Persistent Spatial Semantic Representation For High-level Natural Language Instruction Execution</a> Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guhur2021pretraining/">Airbert: In-domain Pretraining For Vision-and-language Navigation</a> Pierre-louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, Cordelia Schmid </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/komeili2021internet/">Internet-augmented Dialogue Generation</a> Mojtaba Komeili, Kurt Shuster, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2021diagnosing/">Diagnosing Vision-and-language Navigation: What Really Matters</a> Wanrong Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2021bert/">Bob: BERT Over BERT For Training Persona-based Dialogue Models From Limited Personalized Data</a> Haoyu Song, Yan Wang, Kaiyan Zhang, Wei-nan Zhang, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/young2021fusing/">Fusing Task-oriented And Open-domain Dialogues In Conversational Agents</a> Tom Young, Frank Xing, Vlad Pandelea, Jinjie Ni, Erik Cambria </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021multimodal/">Multimodal Transformer With Variable-length Memory For Vision-and-language Navigation</a> Chuang Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2021learning/">Learning To Ask Appropriate Questions In Conversational Recommendation</a> Xuhui Ren et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sharma2021towards/">Towards Facilitating Empathic Conversations In Online Mental Health Support: A Reinforcement Learning Approach</a> Ashish Sharma, Inna W. Lin, Adam S. Miner, David C. Atkins, Tim Althoff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/motger2021software/">Software-based Dialogue Systems: Survey, Taxonomy And Challenges</a> Quim Motger, Xavier Franch, Jordi Marco </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021history/">History Aware Multimodal Transformer For Vision-and-language Navigation</a> Shizhe Chen, Pierre-louis Guhur, Cordelia Schmid, Ivan Laptev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pashevich2021episodic/">Episodic Transformer For Vision-and-language Navigation</a> Alexander Pashevich, Cordelia Schmid, Chen Sun </li>
     
   
     
   
     
       <li> <a href="/publications/suglia2021embodied/">Embodied BERT: A Transformer Model For Embodied, Language-guided Visual Task Completion</a> Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2021multimodal/">Multimodal Dialogue Response Generation</a> Qingfeng Sun et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moudgil2021scene/">SOAT: A Scene- And Object-aware Transformer For Vision-and-language Navigation</a> Abhinav Moudgil, Arjun Majumdar, Harsh Agrawal, Stefan Lee, Dhruv Batra </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gaur2021information/">ISEEQ: Information Seeking Question Generation Using Dynamic Meta-information Retrieval And Knowledge Graphs</a> Manas Gaur, Kalpa Gunaratna, Vijay Srinivasan, Hongxia Jin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022think/">Think Global, Act Local: Dual-scale Graph Transformer For Vision-and-language Navigation</a> Shizhe Chen, Pierre-louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/valmeekam2022extensible/">Planbench: An Extensible Benchmark For Evaluating Large Language Models On Planning And Reasoning About Change</a> Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2022multimodal/">Multimodal Knowledge Alignment With Reinforcement Learning</a> Youngjae Yu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lehman2022evolution/">Evolution Through Large Models</a> Joel Lehman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/menick2022teaching/">Teaching Language Models To Support Answers With Verified Quotes</a> Jacob Menick et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/andreas2022language/">Language Models As Agent Models</a> Jacob Andreas </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2022model/">A Model-agnostic Data Manipulation Method For Persona-based Dialogue Generation</a> Yu Cao, Wei Bi, Meng Fang, Shuming Shi, Dacheng Tao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2022language/">Language Models As Zero-shot Planners: Extracting Actionable Knowledge For Embodied Agents</a> Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch </li>
     
   
     
   
     
       <li> <a href="/publications/huang2022inner/">Inner Monologue: Embodied Reasoning Through Planning With Language Models</a> Wenlong Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/perez2022red/">Red Teaming Language Models With Language Models</a> Ethan Perez et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gong2022future/">Future Transformer For Long-term Action Anticipation</a> Dayoung Gong, Joonseok Lee, Manjin Kim, Seong Jong Ha, Minsu Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2022dialogue/">Dialfred: Dialogue-enabled Agents For Embodied Instruction Following</a> Xiaofeng Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/colas2022language/">Language And Culture Internalisation For Human-like Autotelic AI</a> C√©dric Colas, Tristan Karch, Cl√©ment Moulin-frier, Pierre-yves Oudeyer </li>
     
   
     
       <li> <a href="/publications/lu2022controllable/">Quark: Controllable Text Generation With Reinforced Unlearning</a> Ximing Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2022contrastive/">Contrastive Learning Reduces Hallucination In Conversations</a> Weiwei Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/song2022llm/">Llm-planner: Few-shot Grounded Planning For Embodied Agents With Large Language Models</a> Chan Hee Song et al. </li>
     
   
     
       <li> <a href="/publications/gu2022proposal/">Don't Generate, Discriminate: A Proposal For Grounding Language Models To Real-world Environments</a> Yu Gu, Xiang Deng, Yu Su </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2022vision/">ADAPT: Vision-language Navigation With Modality-aligned Action Prompts</a> Bingqian Lin et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022enabling/">Enabling Conversational Interaction With Mobile UI Using Large Language Models</a> Bryan Wang, Gang Li, Yang Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bucker2022reshaping/">Reshaping Robot Trajectories Using Natural Language Commands: A Study Of Multi-modal Data Alignment Using Transformers</a> Arthur Bucker et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2022general/">VIMA: General Robot Manipulation With Multimodal Prompts</a> Yunfan Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qiao2022history/">HOP: History-and-order Aware Pre-training For Vision-and-language Navigation</a> Yanyuan Qiao et al. </li>
     
   
     
       <li> <a href="/publications/burns2022dataset/">A Dataset For Interactive Vision-language Navigation With Unknown Command Feasibility</a> Andrea Burns et al. </li>
     
   
     
       <li> <a href="/publications/tack2022ai/">The AI Teacher Test: Measuring The Pedagogical Ability Of Blender And GPT-3 In Educational Dialogues</a> Ana√Øs Tack, Chris Piech </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/glaese2022improving/">Improving Alignment Of Dialogue Agents Via Targeted Human Judgements</a> Amelia Glaese et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kamath2022new/">A New Path: Scaling Vision-and-language Navigation With Synthetic Instructions And Imitation Learning</a> Aishwarya Kamath et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahn2022do/">Do As I Can, Not As I Say: Grounding Language In Robotic Affordances</a> Michael Ahn et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ouyang2022training/">Training Language Models To Follow Instructions With Human Feedback</a> Long Ouyang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/susnjak2022end/">Chatgpt: The End Of Online Exam Integrity?</a> Teo Susnjak </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shuster2022blenderbot/">Blenderbot 3: A Deployed Conversational Agent That Continually Learns To Responsibly Engage</a> Kurt Shuster et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/abdelghani2022gpt/">Gpt-3-driven Pedagogical Agents For Training Children's Curious Question-asking Skills</a> Rania Abdelghani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chu2022meta/">Meta Policy Learning For Cold-start Conversational Recommendation</a> Zhendong Chu, Hongning Wang, Yun Xiao, Bo Long, Lingfei Wu </li>
     
   
     
   
     
       <li> <a href="/publications/yao2022towards/">Webshop: Towards Scalable Real-world Web Interaction With Grounded Language Agents</a> Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan </li>
     
   
     
   
     
       <li> <a href="/publications/yao2022synergizing/">React: Synergizing Reasoning And Acting In Language Models</a> Shunyu Yao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ramamurthy2022is/">Is Reinforcement Learning (not) For Natural Language Processing: Benchmarks, Baselines, And Building Blocks For Natural Language Policy Optimization</a> Rajkumar Ramamurthy et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023secrets/">Secrets Of RLHF In Large Language Models Part I: PPO</a> Rui Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mandi2023dialectic/">Roco: Dialectic Multi-robot Collaboration With Large Language Models</a> Zhao Mandi, Shreeya Jain, Shuran Song </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pryzant2023automatic/">Automatic Prompt Optimization With "gradient Descent" And Beam Search</a> Reid Pryzant et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schumann2023verbalization/">VELMA: Verbalization Embodiment Of LLM Agents For Vision And Language Navigation In Street View</a> Raphael Schumann et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023next/">Next-gpt: Any-to-any Multimodal LLM</a> Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-seng Chua </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2023reasoning/">Reasoning With Language Model Is Planning With World Model</a> Shibo Hao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rafailov2023direct/">Direct Preference Optimization: Your Language Model Is Secretly A Reward Model</a> Rafael Rafailov et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023enabling/">Autogen: Enabling Next-gen LLM Applications Via Multi-agent Conversation</a> Qingyun Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khattab2023compiling/">Dspy: Compiling Declarative Language Model Calls Into Self-improving Pipelines</a> Omar Khattab et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shinn2023language/">Reflexion: Language Agents With Verbal Reinforcement Learning</a> Noah Shinn et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/choi2023do/">Do Llms Understand Social Knowledge? Evaluating The Sociability Of Large Language Models With Socket Benchmark</a> Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, David Jurgens </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kwon2023reward/">Reward Design With Language Models</a> Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023unleashing/">Unleashing The Emergent Cognitive Synergy In Large Language Models: A Task-solving Agent Through Multi-persona Self-collaboration</a> Zhenhailong Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phute2023llm/">LLM Self Defense: By Self Examination, Llms Know They Are Being Tricked</a> Mansi Phute et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023driving/">Driving With Llms: Fusing Object-level Vector Modality For Explainable Autonomous Driving</a> Long Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maaz2023video/">Video-chatgpt: Towards Detailed Video Understanding Via Large Vision And Language Models</a> Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yin2023language/">LAMM: Language-assisted Multi-modal Instruction-tuning Dataset, Framework, And Benchmark</a> Zhenfei Yin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tian2023just/">Just Ask For Calibration: Strategies For Eliciting Calibrated Confidence Scores From Language Models Fine-tuned With Human Feedback</a> Katherine Tian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023collaborative/">Agentcf: Collaborative Learning With Autonomous Language Agents For Recommender Systems</a> Junjie Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bai2023qwen/">Qwen Technical Report</a> Jinze Bai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xi2023rise/">The Rise And Potential Of Large Language Model Based Agents: A Survey</a> Zhiheng Xi et al. </li>
     
   
     
       <li> <a href="/publications/sun2023think/">Think-on-graph: Deep And Responsible Reasoning Of Large Language Model On Knowledge Graph</a> Jiashuo Sun et al. </li>
     
   
     
       <li> <a href="/publications/xiang2023language/">Language Models Meet World Models: Embodied Experiences Enhance Language Models</a> Jiannan Xiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023llm/">Llm-grounder: Open-vocabulary 3D Visual Grounding With Large Language Model As An Agent</a> Jianing Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/l%C3%A1la2023retrieval/">Paperqa: Retrieval-augmented Generative Agent For Scientific Research</a> Jakub L√°la et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023theory/">Theory Of Mind For Multi-agent Collaboration Via Large Language Models</a> Huao Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023building/">Building Cooperative Embodied Agents Modularly With Large Language Models</a> Hongxin Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023investigating/">Personallm: Investigating The Ability Of Large Language Models To Express Personality Traits</a> Hang Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023open/">Voyager: An Open-ended Embodied Agent With Large Language Models</a> Guanzhi Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023fair/">"it's A Fair Game", Or Is It? Examining How Users Navigate Disclosure Risks And Benefits When Using Llm-based Conversational Agents</a> Zhiping Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023explicit/">Navgpt: Explicit Reasoning In Vision-and-language Navigation With Large Language Models</a> Gengze Zhou, Yicong Hong, Qi Wu </li>
     
   
     
       <li> <a href="/publications/kim2023language/">Language Models Can Solve Computer Tasks</a> Geunwoo Kim, Pierre Baldi, Stephen Mcaleer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2023preference/">Preference Ranking Optimization For Human Alignment</a> Feifan Song et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023principle/">Principle-driven Self-alignment Of Language Models From Scratch With Minimal Human Supervision</a> Zhiqing Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023chain/">Chain Of Hindsight Aligns Language Models With Feedback</a> Hao Liu, Carmelo Sferrazza, Pieter Abbeel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/burns2023weak/">Weak-to-strong Generalization: Eliciting Strong Capabilities With Weak Supervision</a> Collin Burns et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sima2023driving/">Drivelm: Driving With Graph Visual Question Answering</a> Chonghao Sima et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chan2023towards/">Chateval: Towards Better Llm-based Evaluators Through Multi-agent Debate</a> Chi-min Chan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023less/">LIMA: Less Is More For Alignment</a> Chunting Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/packer2023towards/">Memgpt: Towards Llms As Operating Systems</a> Charles Packer et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qian2023communicative/">Chatdev: Communicative Agents For Software Development</a> Chen Qian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023generative/">Swiftsage: A Generative Agent With Fast And Slow Thinking For Complex Interactive Tasks</a> Bill Yuchen Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023instructing/">Expertprompting: Instructing Large Language Models To Be Distinguished Experts</a> Benfeng Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023llm/">Expel: LLM Agents Are Experiential Learners</a> Andrew Zhao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bran2023augmenting/">Chemcrow: Augmenting Large-language Models With Chemistry Tools</a> Andres M Bran et al. </li>
     
   
     
       <li> <a href="/publications/k%C3%B6pf2023openassistant/">Openassistant Conversations -- Democratizing Large Language Model Alignment</a> Andreas K√∂pf et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023generative/">On Generative Agents In Recommendation</a> An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, Tat-seng Chua </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/madaan2023self/">Self-refine: Iterative Refinement With Self-feedback</a> Aman Madaan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023ghost/">Ghost In The Minecraft: Generally Capable Agents For Open-world Environments Via Large Language Models With Text-based Knowledge And Memory</a> Xizhou Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2023large/">Medagents: Large Language Models As Collaborators For Zero-shot Medical Reasoning</a> Xiangru Tang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2023visual/">Cogagent: A Visual Language Model For GUI Agents</a> Wenyi Hong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023is/">Is Chatgpt Good At Search? Investigating Large Language Models As Re-ranking Agents</a> Weiwei Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2023encouraging/">Encouraging Divergent Thinking In Large Language Models Through Multi-agent Debate</a> Tian Liang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hagendorff2023deception/">Deception Abilities Emerged In Large Language Models</a> Thilo Hagendorff </li>
     
   
     
       <li> <a href="/publications/carta2023grounding/">Grounding Large Language Models In Interactive Environments With Online Reinforcement Learning</a> Thomas Carta et al. </li>
     
   
     
       <li> <a href="/publications/sumers2023cognitive/">Cognitive Architectures For Language Agents</a> Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L. Griffiths </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2023improving/">Improving Factuality And Reasoning In Language Models Through Multiagent Debate</a> Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mu2023vision/">Embodiedgpt: Vision-language Pre-training Via Embodied Chain Of Thought</a> Yao Mu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023improving/">Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback</a> Yao Fu, Hao Peng, Tushar Khot, Mirella Lapata </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qiao2023march/">March In Chat: Interactive Prompting For Remote Embodied Referring Expression</a> Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, Qi Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023large/">Recmind: Large Language Model Powered Agent For Recommendation</a> Yancheng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023chat/">Chat With The Environment: Interactive Multimodal Perception Using Large Language Models</a> Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023solving/">Hugginggpt: Solving AI Tasks With Chatgpt And Its Friends In Hugging Face</a> Yongliang Shen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shao2023character/">Character-llm: A Trainable Agent For Role-playing</a> Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2023guiding/">Guiding Pretraining In Reinforcement Learning With Large Language Models</a> Yuqing Du et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023enhancing/">Zhongjing: Enhancing The Chinese Medical Capabilities Of Large Language Model Through Expert Feedback And Real-world Multi-turn Dialogue</a> Songhua Yang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2023meta/">Metagpt: Meta Programming For A Multi-agent Collaborative Framework</a> Sirui Hong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023plan/">Describe, Explain, Plan And Select: Interactive Planning With Large Language Models Enables Open-world Multi-task Agents</a> Zihao Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023guiding/">Guiding Large Language Models Via Directional Stimulus Prompting</a> Zekun Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2024when/">When Large Language Model Agents Meet 6G Networks: Perception, Grounding, And Alignment</a> Minrui Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ramos2024review/">A Review Of Large Language Models And Autonomous Agents In Chemistry</a> Mayk Caldas Ramos, Christopher J. Collison, Andrew D. White </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ha2024understanding/">Clochat: Understanding How People Customize, Interact, And Experience Personas In Large Language Models</a> Juhye Ha, Hyeon Jeon, Daeun Han, Jinwook Seo, Changhoon Oh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2024understanding/">Understanding Large-language Model (llm)-powered Human-robot Interaction</a> Callie Y. Kim, Christine P. Lee, Bilge Mutlu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2024autonomous/">Autocoderover: Autonomous Program Improvement</a> Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, Abhik Roychoudhury </li>
     
   
     
       <li> <a href="/publications/cao2024survey/">Survey On Large Language Model-enhanced Reinforcement Learning: Concept, Taxonomy, And Methods</a> Yuji Cao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deepseekai2025deepseek/">Deepseek-r1: Incentivizing Reasoning Capability In Llms Via Reinforcement Learning</a> Deepseek-ai et al. </li>
     
   
     
   
   </ul>

   <h3>üè∑ Applications <a id="Applications"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/serban2016generative/">Generative Deep Neural Networks For Dialogue: A Short Review</a> Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, Joelle Pineau </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2016attentional/">An Attentional Neural Conversation Model With Improved Specificity</a> Kaisheng Yao, Baolin Peng, Geoffrey Zweig, Kam-fai Wong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bordes2016learning/">Learning End-to-end Goal-oriented Dialog</a> Antoine Bordes, Y-lan Boureau, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2017survey/">A Survey On Dialogue Systems: Recent Advances And New Frontiers</a> Hongshen Chen, Xiaorui Liu, Dawei Yin, Jiliang Tang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2017long/">Long Text Generation Via Adversarial Training With Leaked Information</a> Jiaxian Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2017steering/">Steering Output Style And Topic In Neural Response Generation</a> Di Wang, Nebojsa Jojic, Chris Brockett, Eric Nyberg </li>
     
   
     
       <li> <a href="/publications/he2017chinese/">Dureader: A Chinese Machine Reading Comprehension Dataset From Real-world Applications</a> Wei He et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2017neural/">Neural Response Generation With Dynamic Vocabularies</a> Yu Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2017neural/">Neural Text Generation: A Practical Guide</a> Ziang Xie </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alon2018generating/">Code2seq: Generating Sequences From Structured Representations Of Code</a> Uri Alon, Shaked Brody, Omer Levy, Eran Yahav </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moon2018multimodal/">Multimodal Named Entity Recognition For Short Social Media Posts</a> Seungwhan Moon, Leonardo Neves, Vitor Carvalho </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2018language/">Language Modeling Teaches You More Syntax Than Translation Does: Lessons Learned Through Auxiliary Task Analysis</a> Kelly W. Zhang, Samuel R. Bowman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/niu2018polite/">Polite Dialogue Generation Without Parallel Data</a> Tong Niu, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hayati2018retrieval/">Retrieval-based Neural Code Generation</a> Shirley Anugrah Hayati et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2018response/">Response Generation By Context-aware Prototype Editing</a> Yu Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bapna2018training/">Training Deeper Neural Machine Translation Models With Transparent Attention</a> Ankur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao, Yonghui Wu </li>
     
   
     
   
     
       <li> <a href="/publications/fan2018can/">"bilingual Expert" Can Find Translation Errors</a> Kai Fan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nishida2018retrieve/">Retrieve-and-read: Multi-task Learning Of Information Retrieval And Reading Comprehension</a> Kyosuke Nishida, Itsumi Saito, Atsushi Otsuka, Hisako Asano, Junji Tomita </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/weston2018retrieve/">Retrieve And Refine: Improved Sequence Generation Models For Dialogue</a> Jason Weston, Emily Dinan, Alexander H. Miller </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/raghu2018disentangling/">Disentangling Language And Knowledge In Task-oriented Dialogs</a> Dinesh Raghu, Nikhil Gupta, Mausam </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2018study/">A Study Of Reinforcement Learning For Neural Machine Translation</a> Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, Tie-yan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/strobelt2018visual/">Seq2seq-vis: A Visual Debugging Tool For Sequence-to-sequence Models</a> Hendrik Strobelt et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dathathri2019plug/">Plug And Play Language Models: A Simple Approach To Controlled Text Generation</a> Sumanth Dathathri et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shoeybi2019megatron/">Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism</a> Mohammad Shoeybi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ferreira2019neural/">Neural Data-to-text Generation: A Comparison Between Pipeline And End-to-end Architectures</a> Thiago Castro Ferreira, Chris Van Der Lee, Emiel Van Miltenburg, Emiel Krahmer </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019learning/">Learning From Explanations With Neural Execution Tree</a> Ziqi Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yavuz2019grounded/">Deepcopy: Grounded Response Generation With Hierarchical Pointer Networks</a> Semih Yavuz, Abhinav Rastogi, Guan-lin Chao, Dilek Hakkani-tur </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hoang2019efficient/">Efficient Adaptation Of Pretrained Transformers For Abstractive Summarization</a> Andrew Hoang, Antoine Bosselut, Asli Celikyilmaz, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vig2019multiscale/">A Multiscale Visualization Of Attention In The Transformer Model</a> Jesse Vig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/henderson2019training/">Training Neural Response Selection For Task-oriented Dialogue Systems</a> Matthew Henderson et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019say/">Say What I Want: Towards The Dark Side Of Neural Dialogue Models</a> Haochen Liu, Tyler Derr, Zitao Liu, Jiliang Tang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahuja2019natural/">Language2pose: Natural Language Grounded Pose Forecasting</a> Chaitanya Ahuja, Louis-philippe Morency </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019few/">Few-shot NLG With Pre-trained Language Model</a> Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kuchaiev2019toolkit/">Nemo: A Toolkit For Building AI Applications Using Neural Modules</a> Oleksii Kuchaiev et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hrinchuk2019correction/">Correction Of Automatic Speech Recognition With Transformer Sequence-to-sequence Model</a> Oleksii Hrinchuk, Mariya Popova, Boris Ginsburg </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cho2019mixture/">Mixture Content Selection For Diverse Sequence Generation</a> Jaemin Cho, Minjoon Seo, Hannaneh Hajishirzi </li>
     
   
     
   
     
       <li> <a href="/publications/shridhar2019benchmark/">ALFRED: A Benchmark For Interpreting Grounded Instructions For Everyday Tasks</a> Mohit Shridhar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/edunov2019pre/">Pre-trained Language Model Representations For Language Generation</a> Sergey Edunov, Alexei Baevski, Michael Auli </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vig2019visualizing/">Visualizing Attention In Transformer-based Language Representation Models</a> Jesse Vig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tuan2019capturing/">Capturing Greater Context For Question Generation</a> Luu Anh Tuan, Darsh J Shah, Regina Barzilay </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/henderson2019efficient/">Convert: Efficient And Accurate Conversational Representations From Transformers</a> Matthew Henderson et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khot2019dataset/">QASC: A Dataset For Question Answering Via Sentence Composition</a> Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, Ashish Sabharwal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clinchant2019use/">On The Use Of BERT For Neural Machine Translation</a> St√©phane Clinchant, Kweon Woo Jung, Vassilina Nikoulina </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cai2019graph/">Graph Transformer For Graph-to-sequence Learning</a> Deng Cai, Wai Lam </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liao2019gpt/">Gpt-based Generation For Classical Chinese Poetry</a> Yi Liao, Yasheng Wang, Qun Liu, Xin Jiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2020pre/">Phobert: Pre-trained Language Models For Vietnamese</a> Dat Quoc Nguyen, Anh Tuan Nguyen </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020high/">Lightseq: A High Performance Inference Library For Transformers</a> Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clement2020multi/">Pymt5: Multi-mode Translation Of Natural Language And Python Code With Transformers</a> Colin B. Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, Neel Sundaresan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pfeiffer2020mad/">MAD-X: An Adapter-based Framework For Multi-task Cross-lingual Transfer</a> Jonas Pfeiffer, Ivan Vuliƒá, Iryna Gurevych, Sebastian Ruder </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020meaningful/">Meaningful Answer Generation Of E-commerce Question-answering</a> Shen Gao, Xiuying Chen, Zhaochun Ren, Dongyan Zhao, Rui Yan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2020tod/">TOD-BERT: Pre-trained Natural Language Understanding For Task-oriented Dialogue</a> Chien-sheng Wu, Steven Hoi, Richard Socher, Caiming Xiong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mi2020continual/">Continual Learning For Natural Language Generation In Task-oriented Dialog Systems</a> Fei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang, Boi Faltings </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2020massively/">XTREME: A Massively Multilingual Multi-task Benchmark For Evaluating Cross-lingual Generalization</a> Junjie Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lepikhin2020scaling/">Gshard: Scaling Giant Models With Conditional Computation And Automatic Sharding</a> Dmitry Lepikhin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020very/">Very Deep Transformers For Neural Machine Translation</a> Xiaodong Liu, Kevin Duh, Liyuan Liu, Jianfeng Gao </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/koutsikakis2020greek/">GREEK-BERT: The Greeks Visiting Sesame Street</a> John Koutsikakis, Ilias Chalkidis, Prodromos Malakasiotis, Ion Androutsopoulos </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/antoun2020pre/">Aragpt2: Pre-trained Transformer For Arabic Language Generation</a> Wissam Antoun, Fady Baly, Hazem Hajj </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020deep/">Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers</a> Wenhui Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2020survey/">A Survey Of Knowledge-enhanced Text Generation</a> Wenhao Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020knowledge/">KGPT: Knowledge-grounded Pre-training For Data-to-text Generation</a> Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2020pre/">Codebert: A Pre-trained Model For Programming And Natural Languages</a> Zhangyin Feng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2020detecting/">Detecting Hallucinated Content In Conditional Neural Sequence Generation</a> Chunting Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/webster2020measuring/">Measuring And Reducing Gendered Correlations In Pre-trained Models</a> Kellie Webster et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zaheer2020big/">Big Bird: Transformers For Longer Sequences</a> Manzil Zaheer et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cohan2020document/">SPECTER: Document-level Representation Learning Using Citation-informed Transformers</a> Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S. Weld </li>
     
   
     
       <li> <a href="/publications/peng2020few/">Few-shot Natural Language Generation For Task-oriented Dialog</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/soldaini2020cascade/">The Cascade Transformer: An Application For Efficient Answer Sentence Selection</a> Luca Soldaini, Alessandro Moschitti </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ganesh2020compressing/">Compressing Large-scale Transformer-based Models: A Case Study On BERT</a> Prakhar Ganesh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zadeh2020quantizing/">GOBO: Quantizing Attention-based NLP Models For Low Latency And Energy Efficient Inference</a> Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, Andreas Moshovos </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hofst%C3%A4tter2020improving/">Improving Efficient Neural Ranking Models With Cross-architecture Knowledge Distillation</a> Sebastian Hofst√§tter, Sophia Althammer, Michael Schr√∂der, Mete Sertkan, Allan Hanbury </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2020lite/">Lite Transformer With Long-short Range Attention</a> Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, Song Han </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020self/">Linformer: Self-attention With Linear Complexity</a> Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/han2020effective/">ECONET: Effective Continual Pretraining Of Language Models For Event Temporal Reasoning</a> Rujun Han, Xiang Ren, Nanyun Peng </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/geng2020dynamic/">Dynamic Graph Representation Learning For Video Dialog Via Multi-modal Shuffled Transformers</a> Shijie Geng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahmad2020transformer/">A Transformer-based Approach For Source Code Summarization</a> Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-wei Chang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/filippova2020controlled/">Controlled Hallucinations: Learning To Generate Faithfully From Noisy Data</a> Katja Filippova </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2020edit/">EDITOR: An Edit-based Transformer With Repositioning For Neural Machine Translation With Soft Lexical Constraints</a> Weijia Xu, Marine Carpuat </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xin2020dynamic/">Deebert: Dynamic Early Exiting For Accelerating BERT Inference</a> Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021human/">Human Parity On Commonsenseqa: Augmenting Self-attention With External Attention</a> Yichong Xu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yuan2021evaluating/">Bartscore: Evaluating Generated Text As Text Generation</a> Weizhe Yuan, Graham Neubig, Pengfei Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021one/">One Chatbot Per Person: Creating Personalized Chatbots Based On Implicit User Profiles</a> Zhengyi Ma, Zhicheng Dou, Yutao Zhu, Hanxun Zhong, Ji-rong Wen </li>
     
   
     
   
     
       <li> <a href="/publications/li2021contrast/">Contrast And Generation Make BART A Good Dialogue Emotion Recognizer</a> Shimin Li, Hang Yan, Xipeng Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2021good/">A Good Prompt Is Worth Millions Of Parameters: Low-resource Prompt-based Learning For Vision-language Models</a> Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, Xiang Ren </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nooralahzadeh2021progressive/">Progressive Transformer-based Generation Of Radiology Reports</a> Farhad Nooralahzadeh, Nicolas Perez Gonzalez, Thomas Frauenfelder, Koji Fujimoto, Michael Krauthammer </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2021language/">Language Model As An Annotator: Exploring Dialogpt For Dialogue Summarization</a> Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2021differentiable/">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners</a> Ningyu Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sheng2021societal/">Societal Biases In Language Generation: Progress And Challenges</a> Emily Sheng, Kai-wei Chang, Premkumar Natarajan, Nanyun Peng </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021controllable/">Controllable Neural Dialogue Summarization With Personal Named Entity Planning</a> Zhengyuan Liu, Nancy F. Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2021tr/">TR-BERT: Dynamic Token Reduction For Accelerating BERT Inference</a> Deming Ye, Yankai Lin, Yufei Huang, Maosong Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021fast/">Fastmoe: A Fast Mixture-of-expert Training System</a> Jiaao He et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tinn2021fine/">Fine-tuning Large Neural Language Models For Biomedical Natural Language Processing</a> Robert Tinn et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schick2021true/">True Few-shot Learning With Prompts -- A Real-world Perspective</a> Timo Schick, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021chinese/">M6: A Chinese Multimodal Pretrainer</a> Junyang Lin et al. </li>
     
   
     
       <li> <a href="/publications/wu2021ai/">AI Chains: Transparent And Controllable Human-ai Interaction By Chaining Large Language Model Prompts</a> Tongshuang Wu, Michael Terry, Carrie J. Cai </li>
     
   
     
       <li> <a href="/publications/li2021dialogue/">Dialogue History Matters! Personalized Response Selectionin Multi-turn Retrieval-based Chatbots</a> Juntao Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021data/">Visualgpt: Data-efficient Adaptation Of Pretrained Language Models For Image Captioning</a> Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021token/">A Token-level Reference-free Hallucination Detection Benchmark For Free-form Text Generation</a> Tianyu Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2021plan/">Plan-then-generate: Controlled Data-to-text Generation Via Planning</a> Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, Nigel Collier </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2021pre/">Lightningdot: Pre-training Visual-semantic Embeddings For Real-time Image-text Retrieval</a> Siqi Sun et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021one/">One Teacher Is Enough? Pre-trained Language Model Distillation From Multiple Teachers</a> Chuhan Wu, Fangzhao Wu, Yongfeng Huang </li>
     
   
     
       <li> <a href="/publications/wu2021distilling/">Newsbert: Distilling Pre-trained Language Model For Intelligent News Application</a> Chuhan Wu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021empowering/">Empowering News Recommendation With Pre-trained Language Models</a> Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang </li>
     
   
     
       <li> <a href="/publications/zhang2021counterfactual/">Counterfactual Memorization In Neural Language Models</a> Chiyuan Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2021pre/">Lawformer: A Pre-trained Language Model For Chinese Legal Long Documents</a> Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, Maosong Sun </li>
     
   
     
   
     
       <li> <a href="/publications/tafjord2021general/">General-purpose Question-answering With Macaw</a> Oyvind Tafjord, Peter Clark </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021automatic/">Screen2words: Automatic Mobile UI Summarization With Multimodal Learning</a> Bryan Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021nlp/">Generate, Annotate, And Learn: NLP With Synthetic Text</a> Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, Mohammad Norouzi </li>
     
   
     
   
     
       <li> <a href="/publications/kim2021what/">What Changes Can Large-scale Language Models Bring? Intensive Study On Hyperclova: Billions-scale Korean Generative Pretrained Transformers</a> Boseop Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lazaridou2021mind/">Mind The Gap: Assessing Temporal Generalization In Neural Language Models</a> Angeliki Lazaridou et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/coenen2021human/">Wordcraft: A Human-ai Collaborative Editor For Story Writing</a> Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, Ann Yuan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/uchendu2021benchmark/">TURINGBENCH: A Benchmark Environment For Turing Test In The Age Of Neural Text Generation</a> Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, Dongwon Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kaliamoorthi2021distilling/">Distilling Large Language Models Into Tiny And Effective Students Using Pqrnn</a> Prabhu Kaliamoorthi, Aditya Siddhant, Edward Li, Melvin Johnson </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phan2021multi/">Cotext: Multi-task Learning With Code-text Transformer</a> Long Phan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021linear/">Linear-time Self Attention With Codeword Histogram For Efficient Recommendation</a> Yongji Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/reynolds2021prompt/">Prompt Programming For Large Language Models: Beyond The Few-shot Paradigm</a> Laria Reynolds, Kyle Mcdonell </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shuster2021retrieval/">Retrieval Augmentation Reduces Hallucination In Conversation</a> Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021evaluating/">Evaluating Large Language Models Trained On Code</a> Mark Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meng2022mass/">Mass-editing Memory In A Transformer</a> Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David Bau </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singhal2022large/">Large Language Models Encode Clinical Knowledge</a> Karan Singhal et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2022contrastive/">Contrastive Search Is What You Need For Neural Text Generation</a> Yixuan Su, Nigel Collier </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/oppenlaender2022taxonomy/">A Taxonomy Of Prompt Modifiers For Text-to-image Generation</a> Jonas Oppenlaender </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2022investigating/">Investigating Explainability Of Generative AI For Code Through Scenario-based Design</a> Jiao Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2022shifting/">Shifting More Attention To Visual Backbone: Query-modulated Refinement Networks For End-to-end Visual Grounding</a> Jiabo Ye et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022end/">End-to-end Transformer Based Model For Image Captioning</a> Yiyu Wang, Jungang Xu, Yingfei Sun </li>
     
   
     
   
     
       <li> <a href="/publications/li2022clinical/">Clinical-longformer And Clinical-bigbird: Transformers For Long Clinical Sequences</a> Yikuan Li, Ramsey M. Wehbe, Faraz S. Ahmad, Hanyin Wang, Yuan Luo </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2022description/">Description-driven Task-oriented Dialog Modeling</a> Jeffrey Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wu2022tune/">Tune-a-video: One-shot Tuning Of Image Diffusion Models For Text-to-video Generation</a> Jay Zhangjie Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singer2022make/">Make-a-video: Text-to-video Generation Without Text-video Data</a> Uriel Singer et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2022integrating/">3DALL-E: Integrating Text-to-image AI In 3D Design Workflows</a> Vivian Liu, Jo Vermeulen, George Fitzmaurice, Justin Matejka </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/strobelt2022interactive/">Interactive And Visual Prompt Engineering For Ad-hoc Task Adaptation With Large Language Models</a> Hendrik Strobelt et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022survey/">A Survey Of Controllable Text Generation Using Transformer-based Pre-trained Language Models</a> Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dang2022how/">How To Prompt? Opportunities And Challenges Of Zero- And Few-shot Learning For Human-ai Interaction In Creative Applications Of Generative Models</a> Hai Dang, Lukas Mecke, Florian Lehmann, Sven Goller, Daniel Buschek </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2022retrieval/">Retrieval Augmented Visual Question Answering With Outside Knowledge</a> Weizhe Lin, Bill Byrne </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aher2022using/">Using Large Language Models To Simulate Multiple Humans And Replicate Human Subject Studies</a> Gati Aher, Rosa I. Arriaga, Adam Tauman Kalai </li>
     
   
     
   
     
       <li> <a href="/publications/perez2022ignore/">Ignore Previous Prompt: Attack Techniques For Language Models</a> F√°bio Perez, Ian Ribeiro </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2022what/">What Do Llms Know About Financial Markets? A Case Study On Reddit Market Sentiment Analysis</a> Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, Michael Bendersky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022library/">LAVIS: A Library For Language-vision Intelligence</a> Dongxu Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kocetkov2022tb/">The Stack: 3 TB Of Permissively Licensed Source Code</a> Denis Kocetkov et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zan2022large/">Large Language Models Meet Nl2code: A Survey</a> Daoguang Zan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/si2022prompting/">Prompting GPT-3 To Be Reliable</a> Chenglei Si et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2022dialog/">Dialog Inpainting: Turning Documents Into Dialogs</a> Zhuyun Dai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022is/">Is GPT-3 A Good Data Annotator?</a> Bosheng Ding et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/workshop2022open/">BLOOM: A 176b-parameter Open-access Multilingual Language Model</a> Bigscience Workshop et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bulatov2022recurrent/">Recurrent Memory Transformer</a> Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2022socratic/">Socratic Models: Composing Zero-shot Multimodal Reasoning With Language</a> Andy Zeng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/drozdov2022compositional/">Compositional Semantic Parsing With Large Language Models</a> Andrew Drozdov et al. </li>
     
   
     
       <li> <a href="/publications/zhai2022high/">Bytetransformer: A High-performance Transformer Boosted For Variable-length Inputs</a> Yujia Zhai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2022retrieval/">Retrieval Augmentation Of Large Language Models For Lay Language Generation</a> Yue Guo, Wei Qiu, Gondy Leroy, Sheng Wang, Trevor Cohen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2022language/">Language Models Are General-purpose Interfaces</a> Yaru Hao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hertz2022prompt/">Prompt-to-prompt Image Editing With Cross Attention Control</a> Amir Hertz et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2022holistic/">Holistic Evaluation Of Language Models</a> Percy Liang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qiao2022reasoning/">Reasoning With Language Model Prompting: A Survey</a> Shuofei Qiao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dziri2022faithful/">Faithdial: A Faithful Benchmark For Information-seeking Dialogue</a> Nouha Dziri et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2022evaluating/">Evaluating Human-language Model Interaction</a> Mina Lee et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yuksekgonul2022when/">When And Why Vision-language Models Behave Like Bags-of-words, And What To Do About It?</a> Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James Zou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jakesch2022human/">Human Heuristics For Ai-generated Language Are Flawed</a> Maurice Jakesch, Jeffrey Hancock, Mor Naaman </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gupta2022visual/">Visual Programming: Compositional Visual Reasoning Without Training</a> Tanmay Gupta, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/barraco2022mean/">Camel: Mean Teacher Learning For Image Captioning</a> Manuele Barraco et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thoppilan2022language/">Lamda: Language Models For Dialog Applications</a> Romal Thoppilan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/welleck2022generating/">Generating Sequences By Learning To Self-correct</a> Sean Welleck et al. </li>
     
   
     
       <li> <a href="/publications/shahriar2023have/">Let's Have A Chat! A Conversation With Chatgpt: Technology, Applications, And Limitations</a> Sakib Shahriar, Kadhim Hayawi </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023retrieving/">Retrieving Multimodal Information For Augmented Generation: A Survey</a> Ruochen Zhao et al. </li>
     
   
     
       <li> <a href="/publications/zhao2023verify/">Verify-and-edit: A Knowledge-enhanced Chain-of-thought Framework</a> Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, Lidong Bing </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023understanding/">Audiogpt: Understanding And Generating Speech, Music, Sound, And Talking Head</a> Rongjie Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/antonello2023scaling/">Scaling Laws For Language Encoding Models In Fmri</a> Richard Antonello, Aditya Vaidya, Alexander G. Huth </li>
     
   
     
   
     
       <li> <a href="/publications/omar2023universal/">A Universal Question-answering Platform For Knowledge Graphs</a> Reham Omar, Ishika Dhall, Panos Kalnis, Essam Mansour </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sohail2023decoding/">Decoding Chatgpt: A Taxonomy Of Existing Research, Current Challenges, And Possible Future Directions</a> Shahab Saquib Sohail et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023comparative/">A Comparative Study Of Open-source Large Language Models, GPT-4 And Claude 2: Multiple-choice Test Taking In Nephrology</a> Sean Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mitrovi%C4%872023chatgpt/">Chatgpt Or Human? Detect And Explain. Explaining Decisions Of Machine Learning Model For Detecting Short Chatgpt-generated Text</a> Sandra Mitroviƒá, Davide Andreoletti, Omran Ayoub </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023scaling/">Scaling Vision-language Models With Sparse Mixture Of Experts</a> Sheng Shen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023evaluation/">Evaluation Of Chatgpt Family Of Models For Biomedical Reasoning And Classification</a> Shan Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/geng2023towards/">VIP5: Towards Multimodal Foundation Models For Recommendation</a> Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, Yongfeng Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023joint/">SPHINX: The Joint Mixing Of Weights, Tasks, And Visual Embeddings For Multi-modal Large Language Models</a> Ziyi Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liao2023ai/">AI Transparency In The Age Of Llms: A Human-centered Research Roadmap</a> Q. Vera Liao, Jennifer Wortman Vaughan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023enabling/">Autogen: Enabling Next-gen LLM Applications Via Multi-agent Conversation</a> Qingyun Wu et al. </li>
     
   
     
       <li> <a href="/publications/sridhar2023harnessing/">Harnessing Llms In Curricular Design: Using GPT-4 To Support Authoring Of Learning Objectives</a> Pragnya Sridhar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023git/">Git-mol: A Multi-modal Large Language Model For Molecular Science With Graph, Image, And Text</a> Pengfei Liu, Yiming Ren, Jun Tao, Zhixiang Ren </li>
     
   
     
       <li> <a href="/publications/hacker2023regulating/">Regulating Chatgpt And Other Large Generative AI Models</a> Philipp Hacker, Andreas Engel, Marco Mauer </li>
     
   
     
   
     
       <li> <a href="/publications/niszczota2023gpt/">GPT Has Become Financially Literate: Insights From Financial Literacy Tests Of GPT And A Preliminary Test Of How People Use It As A Source Of Advice</a> Pawe≈Ç Niszczota, Sami Abbas </li>
     
   
     
       <li> <a href="/publications/rubenstein2023large/">Audiopalm: A Large Language Model That Can Speak And Listen</a> Paul K. Rubenstein et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cascantebonilla2023going/">Going Beyond Nouns With Vision & Language Models Using Synthetic Data</a> Paola Cascante-bonilla et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guerreiro2023hallucinations/">Hallucinations In Large Multilingual Translation Models</a> Nuno M. Guerreiro et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rao2023cat/">CAT-LM: Training Language Models On Aligned Code And Tests</a> Nikitha Rao, Kush Jain, Uri Alon, Claire Le Goues, Vincent J. Hellendoorn </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pangakis2023automated/">Automated Annotation With Generative AI Requires Validation</a> Nicholas Pangakis, Samuel Wolken, Neil Fasching </li>
     
   
     
       <li> <a href="/publications/nguyen2023multimodal/">Openvivqa: Task, Dataset, And Multimodal Fusion Models For Visual Question Answering In Vietnamese</a> Nghia Hieu Nguyen, Duong T. D. Vo, Kiet Van Nguyen, Ngan Luu-thuy Nguyen </li>
     
   
     
       <li> <a href="/publications/motlagh2023impact/">The Impact Of Artificial Intelligence On The Evolution Of Digital Education: A Comparative Study Of Openai Text Generation Tools Including Chatgpt, Bing Chat, Bard, And Ernie</a> Negin Yazdani Motlagh, Matin Khajavi, Abbas Sharifi, Mohsen Ahmadi </li>
     
   
     
       <li> <a href="/publications/sengupta2023jais/">Jais And Jais-chat: Arabic-centric Foundation And Instruction-tuned Open Generative Large Language Models</a> Neha Sengupta et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kiesler2023exploring/">Exploring The Potential Of Large Language Models To Generate Formative Programming Feedback</a> Natalie Kiesler, Dominic Lohr, Hieke Keuning </li>
     
   
     
   
     
       <li> <a href="/publications/varshney2023stitch/">A Stitch In Time Saves Nine: Detecting And Mitigating Hallucinations Of Llms By Validating Low-confidence Generation</a> Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dechoudhury2023benefits/">Benefits And Harms Of Large Language Models In Digital Mental Health</a> Munmun De Choudhury, Sachin R. Pendse, Neha Kumar </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mizrahi2023state/">State Of What Art? A Call For Multi-prompt LLM Evaluation</a> Moran Mizrahi et al. </li>
     
   
     
       <li> <a href="/publications/sharma2023towards/">Towards Understanding Sycophancy In Language Models</a> Mrinank Sharma et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fraiwan2023review/">A Review Of Chatgpt Applications In Education, Marketing, Software Engineering, And Healthcare: Benefits, Drawbacks, And Research Directions</a> Mohammad Fraiwan, Natheer Khasawneh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023time/">Time-llm: Time Series Forecasting By Reprogramming Large Language Models</a> Ming Jin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parker2023large/">A Large Language Model Approach To Educational Survey Feedback Analysis</a> Michael J. Parker, Caitlin Anderson, Claire Stone, Yearim Oh </li>
     
   
     
       <li> <a href="/publications/moor2023med/">Med-flamingo: A Multimodal Medical Few-shot Learner</a> Michael Moor et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023interpretable/">Drivegpt4: Interpretable End-to-end Autonomous Driving Via Large Language Model</a> Zhenhua Xu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hasan2023zero/">Zero- And Few-shot Prompting With Llms: A Comparative Study With Fine-tuned Models For Bangla Sentiment Analysis</a> Md. Arid Hasan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/laskar2023systematic/">A Systematic Study And Comprehensive Evaluation Of Chatgpt On Benchmark Datasets</a> Md Tahmid Rahman Laskar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nievas2023distilling/">Distilling Large Language Models For Matching Patients To Clinical Trials</a> Mauro Nievas, Aditya Basu, Yanshan Wang, Hrituraj Singh </li>
     
   
     
       <li> <a href="/publications/jakesch2023co/">Co-writing With Opinionated Language Models Affects Users' Views</a> Maurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalmanson, Mor Naaman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/binz2023turning/">Turning Large Language Models Into Cognitive Models</a> Marcel Binz, Eric Schulz </li>
     
   
     
   
     
       <li> <a href="/publications/wong2023natural/">Natural Language Generation And Understanding Of Big Code For Ai-assisted Programming: A Review</a> Man Fai Wong, Shangxin Guo, Ching Nam Hang, Siu Wai Ho, Chee Wei Tan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023active/">Active Retrieval Augmented Generation</a> Zhengbao Jiang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2023bibliometric/">A Bibliometric Review Of Large Language Models Research From 2017 To 2023</a> Lizhou Fan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023give/">Give Us The Facts: Enhancing Large Language Models With Knowledge Graphs For Fact-aware Language Modeling</a> Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, Xindong Wu </li>
     
   
     
       <li> <a href="/publications/xu2023parameter/">Parameter-efficient Fine-tuning Methods For Pretrained Language Models: A Critical Review And Assessment</a> Lingling Xu, Haoran Xie, Si-zhao Joe Qin, Xiaohui Tao, Fu Lee Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023comprehensive/">Superclue: A Comprehensive Chinese Large Language Model Benchmark</a> Liang Xu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2023automatically/">Automatically Correcting Large Language Models: Surveying The Landscape Of Diverse Self-correction Strategies</a> Liangming Pan et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/brocki2023deep/">Deep Learning Mental Health Dialogue System</a> Lennart Brocki, George C. Dyer, Anna G≈Çadka, Neo Christopher Chung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caramancion2023news/">News Verifiers Showdown: A Comparative Performance Evaluation Of Chatgpt 3.5, Chatgpt 4.0, Bing AI, And Bard In News Fact-checking</a> Kevin Matthe Caramancion </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jablonka2023examples/">14 Examples Of How Llms Can Transform Materials Science And Chemistry: A Reflection On A Large Language Model Hackathon</a> Kevin Maik Jablonka et al. </li>
     
   
     
   
     
       <li> <a href="/publications/pandya2023automating/">Automating Customer Service Using Langchain: Building Custom Open-source GPT Chatbot For Organizations</a> Keivalya Pandya, Mehfuza Holia </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singhal2023towards/">Towards Expert-level Medical Question Answering With Large Language Models</a> Karan Singhal et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/greshake2023not/">Not What You've Signed Up For: Compromising Real-world Llm-integrated Applications With Indirect Prompt Injection</a> Kai Greshake et al. </li>
     
   
     
       <li> <a href="/publications/kerr2023language/">LERF: Language Embedded Radiance Fields</a> Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, Matthew Tancik </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023ai/">Ai-augmented Surveys: Leveraging Large Language Models And Surveys For Opinion Prediction</a> Junsol Kim, Byungkyu Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kasai2023evaluating/">Evaluating GPT-4 And Chatgpt On Japanese Medical Licensing Examinations</a> Jungo Kasai, Yuhei Kasai, Keisuke Sakaguchi, Yutaro Yamada, Dragomir Radev </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023minigpt/">Minigpt-v2: Large Language Model As A Unified Interface For Vision-language Multi-task Learning</a> Jun Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hartmann2023political/">The Political Ideology Of Conversational AI: Converging Evidence On Chatgpt's Pro-environmental, Left-libertarian Orientation</a> Jochen Hartmann, Jasper Schwenzow, Maximilian Witte </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bai2023qwen/">Qwen Technical Report</a> Jinze Bai et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xue2023bias/">Bias And Fairness In Chatbots: An Overview</a> Jintang Xue et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023potential/">The Potential And Pitfalls Of Using A Large Language Model Such As Chatgpt Or GPT-4 As A Clinical Assistant</a> Jingqing Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023robustness/">On The Robustness Of Chatgpt: An Adversarial And Out-of-distribution Perspective</a> Jindong Wang et al. </li>
     
   
     
       <li> <a href="/publications/huang2023large/">Large Language Models Cannot Self-correct Reasoning Yet</a> Jie Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xi2023rise/">The Rise And Potential Of Large Language Model Based Agents: A Survey</a> Zhiheng Xi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023ethical/">Ethical Chatgpt: Concerns, Challenges, And Commandments</a> Jianlong Zhou, Heimo M√ºller, Andreas Holzinger, Fang Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023empowering/">Empowering Molecule Discovery For Molecule-caption Translation With Large Language Models: A Chatgpt Perspective</a> Jiatong Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/omiye2023large/">Large Language Models In Medicine: The Potentials And Pitfalls</a> Jesutofunmi A. Omiye, Haiwen Gui, Shawheen J. Rezaei, James Zou, Roxana Daneshjou </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/harte2023leveraging/">Leveraging Large Language Models For Sequential Recommendation</a> Jesse Harte et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yang2023impact/">The Impact Of Chatgpt And Llms On Medical Imaging Stakeholders: Perspectives And Use Cases</a> Jiancheng Yang, Hongwei Bran Li, Donglai Wei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kaddour2023challenges/">Challenges And Applications Of Large Language Models</a> Jean Kaddour et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/m%C3%B6kander2023auditing/">Auditing Large Language Models: A Three-layered Approach</a> Jakob M√∂kander, Jonas Schuett, Hannah Rose Kirk, Luciano Floridi </li>
     
   
     
   
     
       <li> <a href="/publications/joshi2023not/">"it's Not Like Jarvis, But It's Pretty Close!" -- Examining Chatgpt's Usage Among Undergraduate Students In Computer Science</a> Ishika Joshi, Ritvik Budhiraja, Harshal D Akolekar, Jagat Sesh Challa, Dhruv Kumar </li>
     
   
     
       <li> <a href="/publications/joshi2023chatgpt/">Chatgpt In The Classroom: An Analysis Of Its Strengths And Weaknesses For Solving Undergraduate Computer Science Questions</a> Ishika Joshi et al. </li>
     
   
     
       <li> <a href="/publications/jahan2023comprehensive/">A Comprehensive Evaluation Of Large Language Models On Benchmark Biomedical Text Processing Tasks</a> Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang </li>
     
   
     
       <li> <a href="/publications/doughty2023comparative/">A Comparative Study Of Ai-generated (GPT-4) And Human-crafted Mcqs In Programming Education</a> Jacob Doughty et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gou2023large/">CRITIC: Large Language Models Can Self-correct With Tool-interactive Critiquing</a> Zhibin Gou et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chang2023text/">Muse: Text-to-image Generation Via Masked Generative Transformers</a> Huiwen Chang et al. </li>
     
   
     
       <li> <a href="/publications/jiang2023compressing/">Llmlingua: Compressing Prompts For Accelerated Inference Of Large Language Models</a> Huiqiang Jiang, Qianhui Wu, Chin-yew Lin, Yuqing Yang, Lili Qiu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/touvron2023llama/">Llama 2: Open Foundation And Fine-tuned Chat Models</a> Hugo Touvron et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023open/">Fingpt: Open-source Financial Large Language Models</a> Hongyang Yang, Xiao-yang Liu, Christina Dan Wang </li>
     
   
     
       <li> <a href="/publications/augenstein2023factuality/">Factuality Challenges In The Era Of Large Language Models</a> Isabelle Augenstein et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/tran2023instruction/">Bioinstruct: Instruction Tuning Of Large Language Models For Biomedical Natural Language Processing</a> Hieu Tran, Zhichao Yang, Zonghai Yao, Hong Yu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nori2023capabilities/">Capabilities Of GPT-4 On Medical Challenge Problems</a> Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/tian2023is/">Is Chatgpt The Ultimate Programming Assistant -- How Far Is It?</a> Haoye Tian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wen2023llm/">Autodroid: Llm-powered Task Automation In Android</a> Hao Wen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2023geotechnical/">Geotechnical Parrot Tales (GPT): Harnessing Large Language Models In Geotechnical Engineering</a> Krishna Kumar </li>
     
   
     
       <li> <a href="/publications/chen2023democratizing/">Phoenix: Democratizing Chatgpt Across Languages</a> Zhihong Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/huang2023chatgpt/">Chatgpt For Shaping The Future Of Dentistry: The Potential Of Multi-modal Large Language Model</a> Hanyao Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lyu2023llm/">Llm-rec: Personalized Recommendation Via Prompting Large Language Models</a> Hanjia Lyu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jiang2023investigating/">Personallm: Investigating The Ability Of Large Language Models To Express Personality Traits</a> Hang Jiang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/inan2023llama/">Llama Guard: Llm-based Input-output Safeguard For Human-ai Conversations</a> Hakan Inan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liao2023gpt/">GPT-4 Enhanced Multimodal Grounding For Autonomous Driving: Leveraging Cross-modal Attention With Large Language Models</a> Haicheng Liao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2023efficient/">Efficient Streaming Language Models With Attention Sinks</a> Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chalvatzaki2023learning/">Learning To Reason Over Scene Graphs: A Case Study Of Finetuning GPT-2 Into A Robot Language Model For Grounded Task Planning</a> Georgia Chalvatzaki et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geminiteam2023family/">Gemini: A Family Of Highly Capable Multimodal Models</a> Gemini Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yan2023multimodal/">Multimodal Chatgpt For Medical Applications: An Experimental Study Of GPT-4V</a> Zhiling Yan et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023heavy/">H\(_2\)O: Heavy-hitter Oracle For Efficient Generative Inference Of Large Language Models</a> Zhenyu Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gilardi2023chatgpt/">Chatgpt Outperforms Crowd-workers For Text-annotation Tasks</a> Fabrizio Gilardi, Meysam Alizadeh, Ma√´l Kubli </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2023from/">Moviechat: From Dense Token To Sparse Memory For Long Video Understanding</a> Enxin Song et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ferrara2023should/">Should Chatgpt Be Biased? Challenges And Risks Of Bias In Large Language Models</a> Emilio Ferrara </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/garridomerch%C3%A1n2023simulating/">Simulating H.P. Lovecraft Horror Literature With The Chatgpt Large Language Model</a> Eduardo C. Garrido-merch√°n, Jos√© Luis Arroyo-barrig√ºete, Roberto Gozalo-brizuela </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023text/">Text-to-sql Empowered By Large Language Models: A Benchmark Evaluation</a> Dawei Gao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023show/">Show-1: Marrying Pixel And Latent Diffusion Models For Text-to-video Generation</a> David Junhao Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/vanveen2023adapted/">Adapted Large Language Models Can Outperform Medical Experts In Clinical Text Summarization</a> Dave Van Veen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zakka2023retrieval/">Almanac: Retrieval-augmented Language Models For Clinical Medicine</a> Cyril Zakka et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023model/">Model Tuning Or Prompt Tuning? A Study Of Large Language Models For Clinical Concept And Relation Extraction</a> Cheng Peng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hsieh2023distilling/">Distilling Step-by-step! Outperforming Larger Language Models With Less Training Data And Smaller Model Sizes</a> Cheng-yu Hsieh et al. </li>
     
   
     
   
     
       <li> <a href="/publications/deng2023foundation/">K2: A Foundation Language Model For Geoscience Knowledge Understanding And Utilization</a> Cheng Deng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023pmc/">Pmc-llama: Towards Building Open-source Language Models For Medicine</a> Chaoyi Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023one/">One Small Step For Generative AI, One Giant Leap For AGI: A Complete Survey On Chatgpt In AIGC Era</a> Chaoning Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/oh2023black/">Blackvip: Black-box Visual Prompting For Robust Transfer Learning</a> Changdae Oh et al. </li>
     
   
     
   
     
       <li> <a href="/publications/tony2023dataset/">Llmseceval: A Dataset Of Natural Language Prompts For Security Evaluations</a> Catherine Tony, Markus Mutas, Nicol√°s E. D√≠az Ferreyra, Riccardo Scandariato </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023large/">Large Language Models On Graphs: A Comprehensive Survey</a> Bowen Jin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2023how/">How Close Is Chatgpt To Human Experts? Comparison Corpus, Evaluation, And Detection</a> Biyang Guo et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lamichhane2023evaluation/">Evaluation Of Chatgpt For Nlp-based Mental Health Applications</a> Bishal Lamichhane </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fecher2023friend/">Friend Or Foe? Exploring The Implications Of Large Language Models On The Science System</a> Benedikt Fecher, Marcel Hebing, Melissa Laufer, J√∂rg Pohle, Fabian Sofsky </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/rozi%C3%A8re2023code/">Code Llama: Open Foundation Models For Code</a> Baptiste Rozi√®re et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023check/">Check Your Facts And Try Again: Improving Large Language Models With External Knowledge And Automated Feedback</a> Baolin Peng et al. </li>
     
   
     
       <li> <a href="/publications/bulatov2023scaling/">Scaling Transformer To 1M Tokens And Beyond With RMT</a> Aydar Bulatov, Yuri Kuratov, Yermek Kapushev, Mikhail S. Burtsev </li>
     
   
     
       <li> <a href="/publications/toma2023clinical/">Clinical Camel: An Open Expert-level Medical Language Model With Dialogue-based Knowledge Encoding</a> Augustin Toma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023action/">ADAPT: Action-aware Driving Caption Transformer</a> Bu Jin et al. </li>
     
   
     
       <li> <a href="/publications/sun2023short/">A Short Survey Of Viewing Large Language Models In Legal Aspect</a> Zhongxiang Sun </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pal2023med/">Med-halt: Medical Domain Hallucination Test For Large Language Models</a> Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bran2023augmenting/">Chemcrow: Augmenting Large-language Models With Chemistry Tools</a> Andres M Bran et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kucharavy2023fundamentals/">Fundamentals Of Generative Large Language Models And Perspectives In Cyber-defense</a> Andrei Kucharavy et al. </li>
     
   
     
       <li> <a href="/publications/olga2023generative/">Generative AI: Implications And Applications For Education</a> Anastasia Olnancy Olga et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/azaria2023chatgpt/">Chatgpt Is A Remarkable Tool -- For Experts</a> Amos Azaria, Rina Azoulay, Shulamit Reches </li>
     
   
     
       <li> <a href="/publications/bhattacharjee2023fighting/">Fighting Fire With Fire: Can Chatgpt Detect Ai-generated Text?</a> Amrita Bhattacharjee, Huan Liu </li>
     
   
     
   
     
       <li> <a href="/publications/bahrini2023threats/">Chatgpt: Applications, Opportunities, And Threats</a> Aram Bahrini et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maatouk2023large/">Large Language Models For Telecom: Forthcoming Impact On The Industry</a> Ali Maatouk, Nicola Piovesan, Fadhel Ayed, Antonio De Domenico, Merouane Debbah </li>
     
   
     
   
     
       <li> <a href="/publications/alkaswan2023open/">The (ab)use Of Open Source Code To Train Large Language Models</a> Ali Al-kaswan, Maliheh Izadi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shtedritski2023what/">What Does CLIP Know About A Red Circle? Visual Prompt Engineering For Vlms</a> Aleksandar Shtedritski, Christian Rupprecht, Andrea Vedaldi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2023linear/">Mamba: Linear-time Sequence Modeling With Selective State Spaces</a> Albert Gu, Tri Dao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khademi2023can/">Can Chatgpt And Bard Generate Aligned Assessment Items? A Reliability Analysis Against Human Performance</a> Abdolvahab Khademi </li>
     
   
     
       <li> <a href="/publications/kocaballi2023conversational/">Conversational Ai-powered Design: Chatgpt As Designer, User, And Product</a> A. Baki Kocaballi </li>
     
   
     
   
     
       <li> <a href="/publications/peng2023kosmos/">Kosmos-2: Grounding Multimodal Large Language Models To The World</a> Zhiliang Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023vision/">Vision Language Models In Autonomous Driving: A Survey And Outlook</a> Xingcheng Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023unveiling/">Unveiling Security, Privacy, And Ethical Concerns Of Chatgpt</a> Xiaodong Wu, Ran Duan, Jianbing Ni </li>
     
   
     
       <li> <a href="/publications/pu2023summarization/">Summarization Is (almost) Dead</a> Xiao Pu, Mingqi Gao, Xiaojun Wan </li>
     
   
     
       <li> <a href="/publications/qi2023fine/">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</a> Xiangyu Qi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ding2023hpc/">HPC-GPT: Integrating Large Language Model For High-performance Computing</a> Xianzhong Ding et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhan2023deceptive/">Deceptive AI Ecosystems: The Case Of Chatgpt</a> Xiao Zhan, Yifan Xu, Stefan Sarkadi </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023targeted/">Universalner: Targeted Distillation From Large Language Models For Open Named Entity Recognition</a> Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung Poon </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gan2023large/">Large Language Models In Education: Vision And Opportunities</a> Wensheng Gan, Zhenlian Qi, Jiayang Wu, Jerry Chun-wei Lin </li>
     
   
     
       <li> <a href="/publications/wang2023generative/">Generative Recommendation: Towards Next-generation Recommender Paradigm</a> Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, Tat-seng Chua </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2023multilingual/">Multilingual Machine Translation With Large Language Models: Empirical Results And Analysis</a> Wenhao Zhu et al. </li>
     
   
     
       <li> <a href="/publications/hu2023simple/">BLIVA: A Simple Multimodal LLM For Better Handling Of Text-rich Visual Questions</a> Wenbo Hu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023is/">Is Chatgpt Equipped With Emotional Dialogue Capabilities?</a> Weixiang Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023is/">Is Chatgpt Good At Search? Investigating Large Language Models As Re-ranking Agents</a> Weiwei Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sadasivan2023can/">Can Ai-generated Text Be Reliably Detected?</a> Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi </li>
     
   
     
   
     
       <li> <a href="/publications/hackl2023is/">Is GPT-4 A Reliable Rater? Evaluating Consistency In GPT-4 Text Ratings</a> Veronika Hackl, Alexandra Elena M√ºller, Michael Granitzer, Maximilian Sailer </li>
     
   
     
       <li> <a href="/publications/lai2023chatgpt/">Chatgpt Beyond English: Towards A Comprehensive Evaluation Of Large Language Models In Multilingual Learning</a> Viet Dac Lai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chakrabarty2023art/">Art Or Artifice? Large Language Models And The False Promise Of Creativity</a> Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, Chien-sheng Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rebedea2023nemo/">Nemo Guardrails: A Toolkit For Controllable And Safe LLM Applications With Programmable Rails</a> Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, Jonathan Cohen </li>
     
   
     
       <li> <a href="/publications/hariri2023unlocking/">Unlocking The Potential Of Chatgpt: A Comprehensive Exploration Of Its Applications, Advantages, Limitations, And Future Directions In Natural Language Processing</a> Walid Hariri </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023rlhf/">RLHF-V: Towards Trustworthy Mllms Via Behavior Alignment From Fine-grained Correctional Human Feedback</a> Tianyu Yu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/han2023medalpaca/">Medalpaca -- An Open-source Collection Of Medical Conversational AI Models And Training Data</a> Tianyu Han et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023recommender/">Recommender Systems In The Era Of Large Language Models (llms)</a> Zihuai Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhuo2023red/">Red Teaming Chatgpt Via Jailbreaking: Bias, Robustness, Reliability And Toxicity</a> Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing </li>
     
   
     
       <li> <a href="/publications/wang2023caption/">Caption Anything: Interactive Image Description With Diverse Multimodal Controls</a> Teng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023interactive/">Evallm: Interactive Evaluation Of Large Language Model Prompts On User-defined Criteria</a> Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-ho Kim, Juho Kim </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gill2023transformative/">Transformative Effects Of Chatgpt On Modern Education: Emerging Era Of AI Chatbots</a> Sukhpal Singh Gill et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023element/">Element-aware Summarization With Large Language Models: Expert-aligned Evaluation And Chain-of-thought Method</a> Yiming Wang, Zhuosheng Zhang, Rui Wang </li>
     
   
     
       <li> <a href="/publications/zhu2023can/">Can Chatgpt Reproduce Human-generated Labels? A Study Of Social Computing Tasks</a> Yiming Zhu, Peixian Zhang, Ehsan-ul Haq, Pan Hui, Gareth Tyson </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2023mol/">Mol-instructions: A Large-scale Biomolecular Instruction Dataset For Large Language Models</a> Yin Fang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2023summary/">Summary Of Chatgpt-related Research And Perspective Towards The Future Of Large Language Models</a> Yiheng Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yao2023survey/">A Survey On Large Language Model (LLM) Security And Privacy: The Good, The Bad, And The Ugly</a> Yifan Yao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chia2023towards/">INSTRUCTEVAL: Towards Holistic Evaluation Of Instruction-tuned Large Language Models</a> Yew Ken Chia, Pengfei Hong, Lidong Bing, Soujanya Poria </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023trustworthy/">Trustworthy Llms: A Survey And Guideline For Evaluating Large Language Models' Alignment</a> Yang Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hu2023improving/">Improving Large Language Models For Clinical Named Entity Recognition Via Prompt Engineering</a> Yan Hu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023mental/">Mental-llm: Leveraging Large Language Models For Mental Health Prediction Via Online Text Data</a> Xuhai Xu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023emotional/">Emotional Intelligence Of Large Language Models</a> Xuena Wang, Xueting Li, Zi Yin, Yue Wu, Liu Jia </li>
     
   
     
       <li> <a href="/publications/du2023manually/">Classeval: A Manually-crafted Benchmark For Evaluating Llms On Class-level Code Generation</a> Xueying Du et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2023can/">Can Chatgpt Pass The Vietnamese National High School Graduation Examination?</a> Xuan-quy Dao, Ngoc-bich Le, Xuan-dung Phan, Bac-bien Ngo </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dan2023large/">Educhat: A Large-scale Language Model-based Chatbot System For Intelligent Education</a> Yuhao Dan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023interpretable/">Chatgraph: Interpretable Text Classification By Converting Chatgpt Knowledge To Graphs</a> Yucheng Shi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023transforming/">NL2TL: Transforming Natural Languages To Temporal Logics Using Large Language Models</a> Yongchao Chen, Rujul Gandhi, Yang Zhang, Chuchu Fan </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ji2023exploring/">Exploring The Impact Of Instruction Data Scaling On Large Language Models: An Empirical Study On Real-world Use Cases</a> Yunjie Ji et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chakraborty2023possibilities/">On The Possibilities Of Ai-generated Text Detection</a> Souradip Chakraborty et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023enhancing/">Zhongjing: Enhancing The Chinese Medical Capabilities Of Large Language Model Through Expert Feedback And Real-world Multi-turn Dialogue</a> Songhua Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023visual/">LL3DA: Visual Interactive Instruction Tuning For Omni-3d Understanding, Reasoning, And Planning</a> Sijin Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023automl/">Automl-gpt: Automatic Machine Learning With GPT</a> Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, Mingyuan Zhou </li>
     
   
     
       <li> <a href="/publications/tian2023opportunities/">Opportunities And Challenges For Chatgpt And Large Language Models In Biomedicine And Health</a> Shubo Tian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023pushing/">Pushing Large Language Models To The 6G Edge: Vision, Challenges, And Opportunities</a> Zheng Lin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nazi2023large/">Large Language Models In Healthcare And Medical Domain: A Review</a> Zabir Al Nazi, Wei Peng </li>
     
   
     
   
     
       <li> <a href="/publications/pardos2023learning/">Learning Gain Differences Between Chatgpt And Human Tutor Generated Algebra Hints</a> Zachary A. Pardos, Shreya Bhandari </li>
     
   
     
   
     
       <li> <a href="/publications/wen2023hard/">Hard Prompts Made Easy: Gradient-based Discrete Optimization For Prompt Tuning And Discovery</a> Yuxin Wen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023generating/">Generating With Confidence: Uncertainty Quantification For Black-box Large Language Models</a> Zhen Lin, Shubhendu Trivedi, Jimeng Sun </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/tonmoy2024comprehensive/">A Comprehensive Survey Of Hallucination Mitigation Techniques In Large Language Models</a> S. M Towhidul Islam Tonmoy et al. </li>
     
   
     
       <li> <a href="/publications/gallotta2024large/">Large Language Models And Games: A Survey And Roadmap</a> Roberto Gallotta et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kaur2024from/">From Text To Transformation: A Comprehensive Review Of Large Language Models' Versatility</a> Pravneet Kaur et al. </li>
     
   
     
       <li> <a href="/publications/sahoo2024systematic/">A Systematic Survey Of Prompt Engineering In Large Language Models: Techniques And Applications</a> Pranab Sahoo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shaer2024ai/">Ai-augmented Brainwriting: Investigating The Use Of Llms In Group Ideation</a> Orit Shaer, Angelora Cooper, Osnat Mokryn, Andrew L. Kun, Hagit Ben Shoshan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/maleki2024ai/">AI Hallucinations: A Misnomer Worth Clarifying</a> Negar Maleki, Balaji Padmanabhan, Kaushik Dutta </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/haque2024exploring/">Exploring Chatgpt And Its Impact On Society</a> Md. Asraful Haque, Shuai Li </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dahl2024large/">Large Legal Fictions: Profiling Legal Hallucinations In Large Language Models</a> Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E. Ho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/saab2024capabilities/">Capabilities Of Gemini Models In Medicine</a> Khaled Saab et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2024bge/">BGE M3-embedding: Multi-lingual, Multi-functionality, Multi-granularity Text Embeddings Through Self-knowledge Distillation</a> Jianlv Chen et al. </li>
     
   
     
       <li> <a href="/publications/maharjan2024prompt/">Openmedlm: Prompt Engineering Can Out-perform Fine-tuning In Medical Question-answering With Open-source Large Language Models</a> Jenish Maharjan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhao2024revolutionizing/">Revolutionizing Finance With Llms: An Overview Of Applications And Insights</a> Huaqin Zhao et al. </li>
     
   
     
       <li> <a href="/publications/hartsock2024vision/">Vision-language Models For Medical Report Generation And Visual Question Answering: A Review</a> Iryna Hartsock, Ghulam Rasool </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/geminiteam2024gemini/">Gemini 1.5: Unlocking Multimodal Understanding Across Millions Of Tokens Of Context</a> Gemini Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/motlagh2024large/">Large Language Models In Cybersecurity: State-of-the-art</a> Farzad Nourmohammadzadeh Motlagh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2024chemical/">Chemllm: A Chemical Large Language Model</a> Di Zhang et al. </li>
     
   
     
       <li> <a href="/publications/hagos2024recent/">Recent Advances In Generative AI And Large Language Models: Current Status, Challenges, And Perspectives</a> Desta Haileselassie Hagos, Rick Battle, Danda B. Rawat </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/caffagni2024revolution/">The Revolution Of Multimodal Large Language Models: A Survey</a> Davide Caffagni et al. </li>
     
   
     
   
     
       <li> <a href="/publications/singh2024rethinking/">Rethinking Interpretability In The Era Of Large Language Models</a> Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, Jianfeng Gao </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bewersdorff2024taking/">Taking The Next Step With Generative Artificial Intelligence: The Transformative Role Of Multimodal Large Language Models In Science Education</a> Arne Bewersdorff et al. </li>
     
   
     
   
     
       <li> <a href="/publications/balaguer2024rag/">RAG Vs Fine-tuning: Pipelines, Tradeoffs, And A Case Study On Agriculture</a> Angels Balaguer et al. </li>
     
   
     
       <li> <a href="/publications/salemi2024optimization/">Optimization Methods For Personalizing Large Language Models Through Retrieval Augmentation</a> Alireza Salemi, Surya Kallumadi, Hamed Zamani </li>
     
   
     
       <li> <a href="/publications/gholami2024ai/">AI And Memory Wall</a> Amir Gholami et al. </li>
     
   
     
       <li> <a href="/publications/kim2024financial/">Financial Statement Analysis With Large Language Models</a> Alex Kim, Maximilian Muhn, Valeri Nikolaev </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2024survey/">Survey On Large Language Model-enhanced Reinforcement Learning: Concept, Taxonomy, And Methods</a> Yuji Cao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lyu2024crud/">CRUD-RAG: A Comprehensive Chinese Benchmark For Retrieval-augmented Generation Of Large Language Models</a> Yuanjie Lyu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hua2024large/">Large Language Models In Mental Health Care: A Scoping Review</a> Yining Hua et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/labrak2024collection/">Biomistral: A Collection Of Open-source Pretrained Large Language Models For Medical Domains</a> Yanis Labrak et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2024generalized/">Mgte: Generalized Long-context Text Representation And Reranking Models For Multilingual Text Retrieval</a> Xin Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chiang2024chatbot/">Chatbot Arena: An Open Platform For Evaluating Llms By Human Preference</a> Wei-lin Chiang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parthasarathy2024ultimate/">The Ultimate Guide To Fine-tuning Llms From Basics To Breakthroughs: An Exhaustive Review Of Technologies, Research, Best Practices, Applied Research Challenges And Opportunities</a> Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/glm2024family/">Chatglm: A Family Of Large Language Models From GLM-130B To GLM-4 All Tools</a> Team Glm et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nepal2024contextual/">Contextual AI Journaling: Integrating LLM And Time Series Behavioral Sensing Technology To Promote Self-reflection And Well-being Using The Mindscape App</a> Subigya Nepal et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2024hallucination/">Hallucination Detection: Robustly Discerning Reliable Answers In Large Language Models</a> Yuyan Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/tan2024large/">Large Language Models For Data Annotation And Synthesis: A Survey</a> Zhen Tan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2024fostering/">Farsight: Fostering Responsible AI Awareness During AI Application Prototyping</a> Zijie J. Wang, Chinmay Kulkarni, Lauren Wilcox, Michael Terry, Michael Madaio </li>
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Attention Mechanism <a id="Attention Mechanism"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2016end/">End-to-end Answer Chunk Extraction And Ranking For Reading Comprehension</a> Yang Yu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chen2016latent/">Latent Attention For If-then Program Synthesis</a> Xinyun Chen, Chang Liu, Richard Shin, Dawn Song, Mingcheng Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meng2016interactive/">Interactive Attention For Neural Machine Translation</a> Fandong Meng, Zhengdong Lu, Hang Li, Qun Liu </li>
     
   
     
   
     
       <li> <a href="/publications/yao2016attentional/">An Attentional Neural Conversation Model With Improved Specificity</a> Kaisheng Yao, Baolin Peng, Geoffrey Zweig, Kam-fai Wong </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/seo2016bidirectional/">Bidirectional Attention Flow For Machine Comprehension</a> Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/firat2016multi/">Multi-way, Multilingual Neural Machine Translation With A Shared Attention Mechanism</a> Orhan Firat, Kyunghyun Cho, Yoshua Bengio </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2016neural/">Neural Machine Translation Advised By Statistical Machine Translation</a> Xing Wang et al. </li>
     
   
     
       <li> <a href="/publications/wang2016chinese/">Chinese Song Iambics Generation With Neural Attention-based Model</a> Qixin Wang, Tianyi Luo, Dong Wang, Chao Xing </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/johnson2016multilingual/">Google's Multilingual Neural Machine Translation System: Enabling Zero-shot Translation</a> Melvin Johnson et al. </li>
     
   
     
       <li> <a href="/publications/li2016context/">A Context-aware Attention Network For Interactive Question Answering</a> Huayu Li, Martin Renqiang Min, Yong Ge, Asim Kadav </li>
     
   
     
       <li> <a href="/publications/liu2016neural/">Neural Machine Translation With Supervised Attention</a> Lemao Liu, Masao Utiyama, Andrew Finch, Eiichiro Sumita </li>
     
   
     
   
     
       <li> <a href="/publications/sordoni2016iterative/">Iterative Alternating Neural Attention For Machine Reading</a> Alessandro Sordoni, Philip Bachman, Adam Trischler, Yoshua Bengio </li>
     
   
     
       <li> <a href="/publications/eriguchi2016tree/">Tree-to-sequence Attentional Neural Machine Translation</a> Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/agrawal2017just/">Don't Just Assume; Look And Answer: Overcoming Priors For Visual Question Answering</a> Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2017phase/">Phase Conductor On Multi-layered Attentions For Machine Comprehension</a> Rui Liu, Wei Wei, Weiguang Mao, Maria Chikina </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2017best/">Best Of Both Worlds: Transferring Knowledge From Discriminative Learning To A Generative Visual Dialog Model</a> Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/calixto2017incorporating/">Incorporating Global Visual Features Into Attention-based Neural Machine Translation</a> Iacer Calixto, Qun Liu, Nick Campbell </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2017learning/">Learning To Ask: Neural Question Generation For Reading Comprehension</a> Xinya Du, Junru Shao, Claire Cardie </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2018topic/">Topic-based Evaluation For Conversational Bots</a> Fenfei Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dziri2018augmenting/">Augmenting Neural Response Generation With Context-aware Topical Attention</a> Nouha Dziri, Ehsan Kamalloo, Kory W. Mathewson, Osmar Zaiane </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vlasov2018few/">Few-shot Generalization Across Dialogue Tasks</a> Vladimir Vlasov, Akela Drissner-schmid, Alan Nichol </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/asai2018multilingual/">Multilingual Extractive Reading Comprehension By Runtime Machine Translation</a> Akari Asai, Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2018contextualized/">Sdnet: Contextualized Attention-based Deep Network For Conversational Question Answering</a> Chenguang Zhu, Michael Zeng, Xuedong Huang </li>
     
   
     
       <li> <a href="/publications/madotto2018effectively/">Mem2seq: Effectively Incorporating Knowledge Bases Into End-to-end Task-oriented Dialog Systems</a> Andrea Madotto, Chien-sheng Wu, Pascale Fung </li>
     
   
     
       <li> <a href="/publications/zhou2018visual/">A Visual Attention Grounding Neural Model For Multimodal Machine Translation</a> Mingyang Zhou, Runxiang Cheng, Yong Jae Lee, Zhou Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2018affect/">An Affect-rich Neural Conversational Model With Biased Attention And Weighted Cross-entropy Loss</a> Peixiang Zhong, Di Wang, Chunyan Miao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dou2018exploiting/">Exploiting Deep Representations For Neural Machine Translation</a> Zi-yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, Tong Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bauer2018commonsense/">Commonsense For Generative Multi-hop Question Answering Tasks</a> Lisa Bauer, Yicheng Wang, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2018dual/">Dual Ask-answer Network For Machine Reading Comprehension</a> Han Xiao, Feng Wang, Jianfeng Yan, Jingyao Zheng </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2018multi/">Multi-granularity Hierarchical Attention Fusion Networks For Reading Comprehension And Question Answering</a> Wei Wang, Ming Yan, Chen Wu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2018deriving/">Deriving Machine Attention From Human Rationales</a> Yujia Bao, Shiyu Chang, Mo Yu, Regina Barzilay </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/choi2018fine/">Fine-grained Attention Mechanism For Neural Machine Translation</a> Heeyoul Choi, Kyunghyun Cho, Yoshua Bengio </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wen2018sequence/">Sequence-to-sequence Learning For Task-oriented Dialogue With Dialogue State Representation</a> Haoyang Wen, Yijia Liu, Wanxiang Che, Libo Qin, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pei2019modular/">A Modular Task-oriented Dialogue System Using A Neural Mixture-of-experts</a> Jiahuan Pei, Pengjie Ren, Maarten De Rijke </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2019transferring/">Macnet: Transferring Knowledge From Machine Comprehension To Sequence-to-sequence Models</a> Boyuan Pan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019improving/">Improving Knowledge-aware Dialogue Generation Via Knowledge Base Question Answering</a> Jian Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2019masked/">MASS: Masked Sequence To Sequence Pre-training For Language Generation</a> Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-yan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nema2019ask/">Let's Ask Again: Refine Network For Automatic Question Generation</a> Preksha Nema, Akash Kumar Mohankumar, Mitesh M. Khapra, Balaji Vasan Srinivasan, Balaraman Ravindran </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2019towards/">Towards Transfer Learning For End-to-end Speech Synthesis From Deep Pre-trained Language Models</a> Wei Fang, Yu-an Chung, James Glass </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maruf2019selective/">Selective Attention For Context-aware Neural Machine Translation</a> Sameen Maruf, Andr√© F. T. Martins, Gholamreza Haffari </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/naseem2019rewarding/">Rewarding Smatch: Transition-based AMR Parsing With Reinforcement Learning</a> Tahira Naseem et al. </li>
     
   
     
       <li> <a href="/publications/fan2019heterogeneous/">Heterogeneous Memory Enhanced Multimodal Attention Model For Video Question Answering</a> Chenyou Fan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zenkel2019adding/">Adding Interpretable Attention To Neural Translation Models Improves Word Alignment</a> Thomas Zenkel, Joern Wuebker, John Denero </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qu2019attentive/">Attentive History Selection For Conversational Question Answering</a> Chen Qu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2019pre/">A Pre-training Based Personalized Dialogue Generation Model With Persona-sparse Data</a> Yinhe Zheng, Rongsheng Zhang, Xiaoxi Mao, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vig2019visualizing/">Visualizing Attention In Transformer-based Language Representation Models</a> Jesse Vig </li>
     
   
     
       <li> <a href="/publications/wang2019tree/">Tree Transformer: Integrating Tree Structures Into Self-attention</a> Yau-shian Wang, Hung-yi Lee, Yun-nung Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tuan2019capturing/">Capturing Greater Context For Question Generation</a> Luu Anh Tuan, Darsh J Shah, Regina Barzilay </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ran2019option/">Option Comparison Network For Multiple-choice Reading Comprehension</a> Qiu Ran, Peng Li, Weiwei Hu, Jie Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2019generating/">Generating Persona Consistent Dialogues By Exploiting Natural Language Inference</a> Haoyu Song, Wei-nan Zhang, Jingwen Hu, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2019multi/">MMM: Multi-stage Multi-task Learning For Multi-choice Reading Comprehension</a> Di Jin, Shuyang Gao, Jiun-yu Kao, Tagyoung Chung, Dilek Hakkani-tur </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qiu2019blockwise/">Blockwise Self-attention For Long Document Understanding</a> Jiezhong Qiu et al. </li>
     
   
     
       <li> <a href="/publications/hao2019multi/">Multi-granularity Self-attention For Neural Machine Translation</a> Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, Zhaopeng Tu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/puduppully2019data/">Data-to-text Generation With Entity Modeling</a> Ratish Puduppully, Li Dong, Mirella Lapata </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2019parallel/">MUSE: Parallel Multi-scale Attention For Sequence To Sequence Learning</a> Guangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, Liangchen Luo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2019making/">Making History Matter: History-advantage Sequence Training For Visual Dialog</a> Tianhao Yang, Zheng-jun Zha, Hanwang Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019attention/">Attention-informed Mixed-language Training For Zero-shot Cross-lingual Task-oriented Dialogue Systems</a> Zihan Liu, Genta Indra Winata, Zhaojiang Lin, Peng Xu, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019detecting/">Recosa: Detecting The Relevant Contexts With Self-attention For Multi-turn Dialogue Generation</a> Hainan Zhang, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2020incorporating/">Incorporating BERT Into Neural Machine Translation</a> Jinhua Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sood2020improving/">Improving Natural Language Processing Tasks With Human Gaze-guided Neural Attention</a> Ekta Sood, Simon Tannert, Philipp Mueller, Andreas Bulling </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bhojanapalli2020low/">Low-rank Bottleneck In Multi-head Attention Models</a> Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2020reading/">DUMA: Reading Comprehension With Transposition Thinking</a> Pengfei Zhu, Hai Zhao, Xiaoguang Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mohankumar2020towards/">Towards Transparent And Explainable Attention Models</a> Akash Kumar Mohankumar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rahman2020improved/">An Improved Attention For Visual Question Answering</a> Tanzila Rahman, Shih-han Chou, Leonid Sigal, Giuseppe Carenini </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020comparison/">A Comparison Of Pre-trained Vision-and-language Models For Multimodal Representation Learning Across Medical Images And Reports</a> Yikuan Li, Hanyin Wang, Yuan Luo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021human/">Human Parity On Commonsenseqa: Augmenting Self-attention With External Attention</a> Yichong Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kacupaj2021conversational/">Conversational Question Answering Over Knowledge Graphs With Transformer And Graph Attention Networks</a> Endri Kacupaj et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2021pre/">Dialoglm: Pre-trained Model For Long Dialogue Understanding And Summarization</a> Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dejong2021mention/">Mention Memory: Incorporating Textual Knowledge Into Transformers Through Entity Mention Attention</a> Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, William Cohen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2021using/">Using Prior Knowledge To Guide Bert's Attention In Semantic Textual Matching Tasks</a> Tingyu Xia, Yue Wang, Yuan Tian, Yi Chang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/manakul2021long/">Long-span Summarization Via Local Attention And Content Selection</a> Potsawee Manakul, Mark J. F. Gales </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021non/">Non-invasive Self-attention For Side Information Fusion In Sequential Recommendation</a> Chang Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kao2021optimized/">FLAT: An Optimized Dataflow For Mitigating Attention Bottlenecks</a> Sheng-chun Kao, Suvinay Subramanian, Gaurav Agrawal, Amir Yazdanbakhsh, Tushar Krishna </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2021intent/">Intent Detection And Slot Filling For Vietnamese</a> Mai Hoang Dao, Thinh Hung Truong, Dat Quoc Nguyen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2022fast/">Flashattention: Fast And Memory-efficient Exact Attention With Io-awareness</a> Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R√© </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2022shifting/">Shifting More Attention To Visual Backbone: Query-modulated Refinement Networks For End-to-end Visual Grounding</a> Jiabo Ye et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/smith2022coda/">Coda-prompt: Continual Decomposed Attention-based Prompting For Rehearsal-free Continual Learning</a> James Seale Smith et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2022multi/">MIST: Multi-modal Iterative Spatial-temporal Transformer For Long-form Video Question Answering</a> Difei Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hertz2022prompt/">Prompt-to-prompt Image Editing With Cross Attention Control</a> Amir Hertz et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jia2022mner/">MNER-QG: An End-to-end MRC Framework For Multimodal Named Entity Recognition With Query Grounding</a> Meihuizi Jia et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023inference/">Inference-time Intervention: Eliciting Truthful Answers From A Language Model</a> Kenneth Li, Oam Patel, Fernanda Vi√©gas, Hanspeter Pfister, Martin Wattenberg </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023llm/">Llm-blender: Ensembling Large Language Models With Pairwise Ranking And Generative Fusion</a> Dongfu Jiang, Xiang Ren, Bill Yuchen Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aberdam2023looking/">CLIPTER: Looking At The Bigger Picture In Scene Text Recognition</a> Aviad Aberdam et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023trusting/">Trusting Your Evidence: Hallucinate Less With Context-aware Decoding</a> Weijia Shi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ge2023expressive/">Expressive Text-to-image Generation With Rich Text</a> Songwei Ge, Taesung Park, Jun-yan Zhu, Jia-bin Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pramanick2023egocentric/">Egovlpv2: Egocentric Video-language Pre-training With Fusion In The Backbone</a> Shraman Pramanick et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ BART <a id="BART"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021contrast/">Contrast And Generation Make BART A Good Dialogue Emotion Recognizer</a> Shimin Li, Hang Yan, Xipeng Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021can/">Can Generative Pre-trained Language Models Serve As Knowledge Bases For Closed-book QA?</a> Cunxiang Wang, Pai Liu, Yue Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ BERT <a id="BERT"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/devlin2018pre/">BERT: Pre-training Of Deep Bidirectional Transformers For Language Understanding</a> Jacob Devlin, Ming-wei Chang, Kenton Lee, Kristina Toutanova </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2018can/">Can You Tell Me How To Get Past Sesame Street? Sentence-level Pretraining Beyond Language Modeling</a> Alex Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2018contextualized/">Sdnet: Contextualized Attention-based Deep Network For Conversational Question Answering</a> Chenguang Zhu, Michael Zeng, Xuedong Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2018can/">"bilingual Expert" Can Find Translation Errors</a> Kai Fan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kitaev2018multilingual/">Multilingual Constituency Parsing With Self-attention And Pre-training</a> Nikita Kitaev, Steven Cao, Dan Klein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phang2018sentence/">Sentence Encoders On Stilts: Supplementary Training On Intermediate Labeled-data Tasks</a> Jason Phang, Thibault F√©vry, Samuel R. Bowman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vanaken2019how/">How Does BERT Answer Questions? A Layer-wise Analysis Of Transformer Representations</a> Betty Van Aken, Benjamin Winter, Alexander L√∂ser, Felix A. Gers </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2019multi/">Multi-modality Latent Interaction Network For Visual Question Answering</a> Peng Gao, Haoxuan You, Zhanpeng Zhang, Xiaogang Wang, Hongsheng Li </li>
     
   
     
       <li> <a href="/publications/rothe2019leveraging/">Leveraging Pre-trained Checkpoints For Sequence Generation Tasks</a> Sascha Rothe, Shashi Narayan, Aliaksei Severyn </li>
     
   
     
   
     
       <li> <a href="/publications/hoover2019visual/">Exbert: A Visual Analysis Tool To Explore Learned Representations In Transformers Models</a> Benjamin Hoover, Hendrik Strobelt, Sebastian Gehrmann </li>
     
   
     
       <li> <a href="/publications/lu2019pretraining/">Vilbert: Pretraining Task-agnostic Visiolinguistic Representations For Vision-and-language Tasks</a> Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2019evaluating/">Evaluating Commonsense In Pre-trained Language Models</a> Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang </li>
     
   
     
   
     
       <li> <a href="/publications/shoeybi2019megatron/">Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism</a> Mohammad Shoeybi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/michel2019are/">Are Sixteen Heads Really Better Than One?</a> Paul Michel, Omer Levy, Graham Neubig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019pretrained/">Pretrained Language Models For Document-level Neural Machine Translation</a> Liangyou Li, Xin Jiang, Qun Liu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2019vl/">VL-BERT: Pre-training Of Generic Visual-linguistic Representations</a> Weijie Su et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019text/">Text Summarization With Pretrained Encoders</a> Yang Liu, Mirella Lapata </li>
     
   
     
   
     
       <li> <a href="/publications/liu2019multi/">MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models</a> Linqing Liu, Huan Wang, Jimmy Lin, Richard Socher, Caiming Xiong </li>
     
   
     
   
     
       <li> <a href="/publications/wang2019incorporating/">Structbert: Incorporating Language Structures Into Pre-training For Deep Language Understanding</a> Wei Wang et al. </li>
     
   
     
       <li> <a href="/publications/xu2019review/">Review Conversational Reading Comprehension</a> Hu Xu, Bing Liu, Lei Shu, Philip S. Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cohan2019pretrained/">Pretrained Language Models For Sequential Sentence Classification</a> Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, Daniel S. Weld </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019simple/">Visualbert: A Simple And Performant Baseline For Vision And Language</a> Liunian Harold Li, Mark Yatskar, Da Yin, Cho-jui Hsieh, Kai-wei Chang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/toneva2019interpreting/">Interpreting And Improving Natural-language Processing (in Machines) With Natural Language-processing (in The Brain)</a> Mariya Toneva, Leila Wehbe </li>
     
   
     
   
     
       <li> <a href="/publications/sanh2019distilled/">Distilbert, A Distilled Version Of BERT: Smaller, Faster, Cheaper And Lighter</a> Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019multi/">Multi-passage BERT: A Globally Normalized BERT Model For Open-domain Question Answering</a> Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, Bing Xiang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salazar2019masked/">Masked Language Model Scoring</a> Julian Salazar, Davis Liang, Toan Q. Nguyen, Katrin Kirchhoff </li>
     
   
     
       <li> <a href="/publications/song2019masked/">MASS: Masked Sequence To Sequence Pre-training For Language Generation</a> Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-yan Liu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2019reducing/">Reducing Transformer Depth On Demand With Structured Dropout</a> Angela Fan, Edouard Grave, Armand Joulin </li>
     
   
     
       <li> <a href="/publications/wen2019adapting/">Adapting And Evaluating A Deep Learning Language Model For Clinical Why-question Answering</a> Andrew Wen, Mohamed Y. Elwazir, Sungrim Moon, Jungwei Fan </li>
     
   
     
       <li> <a href="/publications/klein2019learning/">Learning To Answer By Learning To Ask: Getting The Best Of GPT-2 And BERT Worlds</a> Tassilo Klein, Moin Nabi </li>
     
   
     
       <li> <a href="/publications/zhang2019evaluating/">Bertscore: Evaluating Text Generation With BERT</a> Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, Yoav Artzi </li>
     
   
     
   
     
       <li> <a href="/publications/tenney2019what/">What Do You Learn From Context? Probing For Sentence Structure In Contextualized Word Representations</a> Ian Tenney et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aliannejadi2019harnessing/">Harnessing Evolution Of Multi-turn Conversations For Effective Answer Retrieval</a> Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andr√©s R√≠ssola, Fabio Crestani </li>
     
   
     
       <li> <a href="/publications/turc2019well/">Well-read Students Learn Better: On The Importance Of Pre-training Compact Models</a> Iulia Turc, Ming-wei Chang, Kenton Lee, Kristina Toutanova </li>
     
   
     
   
     
       <li> <a href="/publications/yang2019towards/">Towards Making The Most Of BERT In Neural Machine Translation</a> Jiacheng Yang et al. </li>
     
   
     
       <li> <a href="/publications/vig2019multiscale/">A Multiscale Visualization Of Attention In The Transformer Model</a> Jesse Vig </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019bert/">BERT For Joint Intent Classification And Slot Filling</a> Qian Chen, Zhu Zhuo, Wen Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2019open/">UER: An Open-source Toolkit For Pre-training Models</a> Zhe Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fang2019towards/">Towards Transfer Learning For End-to-end Speech Synthesis From Deep Pre-trained Language Models</a> Wei Fang, Yu-an Chung, James Glass </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/talmor2019olmpics/">Olmpics -- On What Language Model Pre-training Captures</a> Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2019what/">What Would Elsa Do? Freezing Layers During Transformer Fine-tuning</a> Jaejun Lee, Raphael Tang, Jimmy Lin </li>
     
   
     
   
     
       <li> <a href="/publications/yang2019data/">Data Augmentation For BERT Fine-tuning In Open-domain Question Answering</a> Wei Yang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/si2019what/">What Does BERT Learn From Multiple-choice Reading Comprehension Datasets?</a> Chenglei Si, Shuohang Wang, Min-yen Kan, Jing Jiang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2019mask/">Align, Mask And Select: A Simple Method For Incorporating Commonsense Knowledge Into Language Representation Models</a> Zhi-xiu Ye, Qian Chen, Wen Wang, Zhen-hua Ling </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2019enhanced/">Freelb: Enhanced Adversarial Training For Natural Language Understanding</a> Chen Zhu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/niven2019probing/">Probing Neural Network Comprehension Of Natural Language Arguments</a> Timothy Niven, Hung-yu Kao </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alberti2019bert/">A BERT Baseline For The Natural Questions</a> Chris Alberti, Kenton Lee, Michael Collins </li>
     
   
     
       <li> <a href="/publications/alberti2019synthetic/">Synthetic QA Corpora Generation With Roundtrip Consistency</a> Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, Michael Collins </li>
     
   
     
   
     
       <li> <a href="/publications/chen2019distilling/">Distilling Knowledge Learned In BERT For Text Generation</a> Yen-chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2019how/">How To Fine-tune BERT For Text Classification?</a> Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang </li>
     
   
     
   
     
       <li> <a href="/publications/r%C3%B6nnqvist2019is/">Is Multilingual BERT Fluent In Language Generation?</a> Samuel R√∂nnqvist, Jenna Kanerva, Tapio Salakoski, Filip Ginter </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clark2019exploring/">Boolq: Exploring The Surprising Difficulty Of Natural Yes/no Questions</a> Christopher Clark et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2019sequential/">Bert4rec: Sequential Recommendation With Bidirectional Encoder Representations From Transformer</a> Fei Sun et al. </li>
     
   
     
       <li> <a href="/publications/huang2019universal/">Unicoder: A Universal Language Encoder By Pre-training With Multiple Cross-lingual Tasks</a> Haoyang Huang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019structured/">Structured Pruning Of Large Language Models</a> Ziheng Wang, Jeremy Wohlwend, Tao Lei </li>
     
   
     
   
     
       <li> <a href="/publications/tan2019learning/">LXMERT: Learning Cross-modality Encoder Representations From Transformers</a> Hao Tan, Mohit Bansal </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/qu2019attentive/">Attentive History Selection For Conversational Question Answering</a> Chen Qu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/junczysdowmunt2019microsoft/">Microsoft Translator At WMT 2019: Towards Large-scale Document-level Neural Machine Translation</a> Marcin Junczys-dowmunt </li>
     
   
     
   
     
       <li> <a href="/publications/glass2019span/">Span Selection Pre-training For Question Answering</a> Michael Glass et al. </li>
     
   
     
   
     
       <li> <a href="/publications/carrino2019automatic/">Automatic Spanish Translation Of The Squad Dataset For Multilingual Question Answering</a> Casimiro Pio Carrino, Marta R. Costa-juss√†, Jos√© A. R. Fonollosa </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/whang2019effective/">An Effective Domain Adaptive Post-training Method For BERT In Response Selection</a> Taesun Whang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2019frustratingly/">Frustratingly Easy Natural Question Answering</a> Lin Pan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schwartz2019inducing/">Inducing Brain-relevant Bias In Natural Language Processing Models</a> Dan Schwartz, Mariya Toneva, Leila Wehbe </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019semantics/">Semantics-aware BERT For Language Understanding</a> Zhuosheng Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/htut2019do/">Do Attention Heads In BERT Track Syntactic Dependencies?</a> Phu Mon Htut, Jason Phang, Shikha Bordia, Samuel R. Bowman </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/richardson2019probing/">Probing Natural Language Inference Models Through Semantic Fragments</a> Kyle Richardson, Hai Hu, Lawrence S. Moss, Ashish Sabharwal </li>
     
   
     
   
     
       <li> <a href="/publications/jiao2019distilling/">Tinybert: Distilling BERT For Natural Language Understanding</a> Xiaoqi Jiao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dong2019unified/">Unified Language Model Pre-training For Natural Language Understanding And Generation</a> Li Dong et al. </li>
     
   
     
       <li> <a href="/publications/weng2019acquiring/">Acquiring Knowledge From Pre-trained Model To Neural Machine Translation</a> Rongxiang Weng, Heng Yu, Shujian Huang, Shanbo Cheng, Weihua Luo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019sg/">Sg-net: Syntax-guided Machine Reading Comprehension</a> Zhuosheng Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kovaleva2019revealing/">Revealing The Dark Secrets Of BERT</a> Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky </li>
     
   
     
       <li> <a href="/publications/barkan2019scalable/">Scalable Attentive Sentence-pair Modeling Via Distilled Sentence Embedding</a> Oren Barkan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lewis2019denoising/">BART: Denoising Sequence-to-sequence Pre-training For Natural Language Generation, Translation, And Comprehension</a> Mike Lewis et al. </li>
     
   
     
       <li> <a href="/publications/petroni2019language/">Language Models As Knowledge Bases?</a> Fabio Petroni et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/talmor2019empirical/">Multiqa: An Empirical Investigation Of Generalization And Transfer In Reading Comprehension</a> Alon Talmor, Jonathan Berant </li>
     
   
     
   
     
       <li> <a href="/publications/murahari2019large/">Large-scale Pretraining For Visual Dialog: A Simple State-of-the-art Baseline</a> Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das </li>
     
   
     
   
     
       <li> <a href="/publications/hao2019visualizing/">Visualizing And Understanding The Effectiveness Of BERT</a> Yaru Hao, Li Dong, Furu Wei, Ke Xu </li>
     
   
     
       <li> <a href="/publications/conneau2019unsupervised/">Unsupervised Cross-lingual Representation Learning At Scale</a> Alexis Conneau et al. </li>
     
   
     
   
     
       <li> <a href="/publications/baevski2019cloze/">Cloze-driven Pretraining Of Self-attention Networks</a> Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael Auli </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/martin2019tasty/">Camembert: A Tasty French Language Model</a> Louis Martin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mccarley2019structured/">Structured Pruning Of A Bert-based Question Answering Model</a> J. S. Mccarley, Rishav Chakravarti, Avirup Sil </li>
     
   
     
   
     
       <li> <a href="/publications/wang2019bert/">BERT Has A Mouth, And It Must Speak: BERT As A Markov Random Field Language Model</a> Alex Wang, Kyunghyun Cho </li>
     
   
     
       <li> <a href="/publications/vig2019visualizing/">Visualizing Attention In Transformer-based Language Representation Models</a> Jesse Vig </li>
     
   
     
       <li> <a href="/publications/wang2019tree/">Tree Transformer: Integrating Tree Structures Into Self-attention</a> Yau-shian Wang, Hung-yi Lee, Yun-nung Chen </li>
     
   
     
       <li> <a href="/publications/bae2019summary/">Summary Level Training Of Sentence Rewriting For Abstractive Summarization</a> Sanghwan Bae, Taeuk Kim, Jihoon Kim, Sang-goo Lee </li>
     
   
     
   
     
       <li> <a href="/publications/lan2019lite/">ALBERT: A Lite BERT For Self-supervised Learning Of Language Representations</a> Zhenzhong Lan et al. </li>
     
   
     
       <li> <a href="/publications/ohsugi2019simple/">A Simple But Effective Method To Incorporate Multi-turn Context With BERT For Conversational Machine Comprehension</a> Yasuhito Ohsugi, Itsumi Saito, Kyosuke Nishida, Hisako Asano, Junji Tomita </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mccoy2019berts/">Berts Of A Feather Do Not Generalize Together: Large Variability In Generalization Across Models With Similar Test Set Performance</a> R. Thomas Mccoy, Junghyun Min, Tal Linzen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kanade2019learning/">Learning And Evaluating Contextual Embedding Of Source Code</a> Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fisch2019mrqa/">MRQA 2019 Shared Task: Evaluating Generalization In Reading Comprehension</a> Adam Fisch et al. </li>
     
   
     
   
     
       <li> <a href="/publications/nogueira2019passage/">Passage Re-ranking With BERT</a> Rodrigo Nogueira, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019story/">Story Ending Prediction By Transferable BERT</a> Zhongyang Li, Xiao Ding, Ting Liu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2019surprising/">Beto, Bentz, Becas: The Surprising Cross-lingual Effectiveness Of BERT</a> Shijie Wu, Mark Dredze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/keh2019myers/">Myers-briggs Personality Classification And Personality-specific Language Generation Using Pre-trained Language Models</a> Sedrick Scott Keh, I-tsun Cheng </li>
     
   
     
       <li> <a href="/publications/liu2019robustly/">Roberta: A Robustly Optimized BERT Pretraining Approach</a> Yinhan Liu et al. </li>
     
   
     
       <li> <a href="/publications/zafrir2019quantized/">Q8BERT: Quantized 8bit BERT</a> Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/qiao2019understanding/">Understanding The Behaviors Of BERT In Ranking</a> Yifan Qiao, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qiu2019blockwise/">Blockwise Self-attention For Long Document Understanding</a> Jiezhong Qiu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sundararaman2019syntax/">Syntax-infused Transformer And BERT Models For Machine Translation And Natural Language Understanding</a> Dhanasekar Sundararaman et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gauthier2019linking/">Linking Artificial And Human Neural Representations Of Language</a> Jon Gauthier, Roger Levy </li>
     
   
     
   
     
       <li> <a href="/publications/ma2019universal/">Universal Text Representation From BERT: An Empirical Study</a> Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, Bing Xiang </li>
     
   
     
   
     
       <li> <a href="/publications/clinchant2019use/">On The Use Of BERT For Neural Machine Translation</a> St√©phane Clinchant, Kweon Woo Jung, Vassilina Nikoulina </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ju2019technical/">Technical Report On Conversational Question Answering</a> Ying Ju et al. </li>
     
   
     
       <li> <a href="/publications/mass2019study/">A Study Of BERT For Non-factoid Question-answering Under Passage Length Constraints</a> Yosi Mass et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2019linguistic/">Linguistic Knowledge And Transferability Of Contextual Representations</a> Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, Noah A. Smith </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiong2019pretrained/">Pretrained Encyclopedia: Weakly Supervised Knowledge-pretrained Language Model</a> Wenhan Xiong, Jingfei Du, William Yang Wang, Veselin Stoyanov </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/voita2019bottom/">The Bottom-up Evolution Of Representations In The Transformer: A Study With Machine Translation And Language Modeling Objectives</a> Elena Voita, Rico Sennrich, Ivan Titov </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zellers2019can/">Hellaswag: Can A Machine Really Finish Your Sentence?</a> Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2019model/">Model Compression With Two-stage Multi-teacher Knowledge Distillation For Web Question Answering System</a> Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, Daxin Jiang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mansimov2019generalized/">A Generalized Framework Of Sequence Generation With Application To Undirected Sequence Models</a> Elman Mansimov, Alex Wang, Sean Welleck, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019addressing/">Addressing Semantic Drift In Question Generation For Semi-supervised Question Answering</a> Shiyue Zhang, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nangia2019human/">Human Vs. Muppet: A Conservative Estimate Of Human Performance On The GLUE Benchmark</a> Nikita Nangia, Samuel R. Bowman </li>
     
   
     
       <li> <a href="/publications/chen2019multi/">Multi-hop Question Answering Via Reasoning Chains</a> Jifan Chen, Shih-ting Lin, Greg Durrett </li>
     
   
     
       <li> <a href="/publications/houlsby2019parameter/">Parameter-efficient Transfer Learning For NLP</a> Neil Houlsby et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wallace2019allennlp/">Allennlp Interpret: A Framework For Explaining Predictions Of NLP Models</a> Eric Wallace et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wallace2019do/">Do NLP Models Know Numbers? Probing Numeracy In Embeddings</a> Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/malmi2019high/">Encode, Tag, Realize: High-precision Text Editing</a> Eric Malmi, Sebastian Krause, Sascha Rothe, Daniil Mirylenka, Aliaksei Severyn </li>
     
   
     
   
     
       <li> <a href="/publications/guo2019reweighted/">Reweighted Proximal Pruning For Large-scale Language Representation</a> Fu-ming Guo, Sijia Liu, Finlay S. Mungall, Xue Lin, Yanzhi Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2020pre/">Phobert: Pre-trained Language Models For Vietnamese</a> Dat Quoc Nguyen, Anh Tuan Nguyen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/khattab2020relevance/">Relevance-guided Supervision For Openqa With Colbert</a> Omar Khattab, Christopher Potts, Matei Zaharia </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guu2020retrieval/">REALM: Retrieval-augmented Language Model Pre-training</a> Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-wei Chang </li>
     
   
     
       <li> <a href="/publications/song2020kvl/">KVL-BERT: Knowledge Enhanced Visual-and-linguistic BERT For Visual Commonsense Reasoning</a> Dandan Song, Siyi Ma, Zhanchen Sun, Sicheng Yang, Lejian Liao </li>
     
   
     
   
     
       <li> <a href="/publications/hendrycks2020pretrained/">Pretrained Transformers Improve Out-of-distribution Robustness</a> Dan Hendrycks et al. </li>
     
   
     
       <li> <a href="/publications/lukovnikov2020pretrained/">Pretrained Transformers For Simple Question Answering Over Knowledge Graphs</a> D. Lukovnikov, A. Fischer, J. Lehmann </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2020cg/">CG-BERT: Conditional Text Generation With BERT For Generalized Few-shot Intent Detection</a> Congying Xia, Chenwei Zhang, Hoang Nguyen, Jiawei Zhang, Philip Yu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020organizing/">Optimus: Organizing Sentences Via Pre-trained Modeling Of A Latent Space</a> Chunyuan Li et al. </li>
     
   
     
       <li> <a href="/publications/pfeiffer2020mad/">MAD-X: An Adapter-based Framework For Multi-task Cross-lingual Transfer</a> Jonas Pfeiffer, Ivan Vuliƒá, Iryna Gurevych, Sebastian Ruder </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mehri2020natural/">Dialoglue: A Natural Language Understanding Benchmark For Task-oriented Dialogue</a> Shikib Mehri, Mihail Eric, Dilek Hakkani-tur </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2020tod/">TOD-BERT: Pre-trained Natural Language Understanding For Task-oriented Dialogue</a> Chien-sheng Wu, Steven Hoi, Richard Socher, Caiming Xiong </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2020bert/">Bert-hlstms: BERT And Hierarchical Lstms For Visual Storytelling</a> Jing Su, Qingyun Dai, Frank Guerin, Mian Zhou </li>
     
   
     
       <li> <a href="/publications/radiyadixit2020how/">How Fine Can Fine-tuning Be? Learning Efficient Language Models</a> Evani Radiya-dixit, Xin Wang </li>
     
   
     
       <li> <a href="/publications/hill2020human/">Human Instruction-following With Deep Reinforcement Learning Via Transfer-learning From Text</a> Felix Hill, Sona Mokra, Nathaniel Wong, Tim Harley </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020when/">When Do You Need Billions Of Words Of Pretraining Data?</a> Yian Zhang, Alex Warstadt, Haau-sing Li, Samuel R. Bowman </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/petroni2020how/">How Context Affects Language Models' Factual Predictions</a> Fabio Petroni et al. </li>
     
   
     
       <li> <a href="/publications/zhu2020incorporating/">Incorporating BERT Into Neural Machine Translation</a> Jinhua Zhu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jones2020robust/">Robust Encodings: A Framework For Combating Adversarial Typos</a> Erik Jones, Robin Jia, Aditi Raghunathan, Percy Liang </li>
     
   
     
   
     
       <li> <a href="/publications/kale2020text/">Text-to-text Pre-training For Data-to-text Tasks</a> Mihir Kale, Abhinav Rastogi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2020compact/">Mobilebert: A Compact Task-agnostic BERT For Resource-limited Devices</a> Zhiqing Sun et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bugliarello2020multimodal/">Multimodal Pretraining Unmasked: A Meta-analysis And A Unified Framework Of Vision-and-language Berts</a> Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020contextualized/">Contextualized Perturbation For Textual Adversarial Attack</a> Dianqi Li et al. </li>
     
   
     
       <li> <a href="/publications/liu2020exploring/">Exploring Fine-tuning Techniques For Pre-trained Cross-lingual Models Via Continual Learning</a> Zihan Liu, Genta Indra Winata, Andrea Madotto, Pascale Fung </li>
     
   
     
       <li> <a href="/publications/qi2020cross/">Imagebert: Cross-modal Pre-training With Large-scale Weak-supervised Image-text Data</a> Di Qi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2020learning/">On Learning Universal Representations Across Languages</a> Xiangpeng Wei et al. </li>
     
   
     
       <li> <a href="/publications/clark2020pre/">ELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators</a> Kevin Clark, Minh-thang Luong, Quoc V. Le, Christopher D. Manning </li>
     
   
     
       <li> <a href="/publications/gu2020discourse/">Dialogbert: Discourse-aware Response Generation Via Learning To Recover And Rank Utterances</a> Xiaodong Gu, Kang Min Yoo, Jung-woo Ha </li>
     
   
     
       <li> <a href="/publications/liu2020adversarial/">Adversarial Training For Large Neural Language Models</a> Xiaodong Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/koutsikakis2020greek/">GREEK-BERT: The Greeks Visiting Sesame Street</a> John Koutsikakis, Ilias Chalkidis, Prodromos Malakasiotis, Ion Androutsopoulos </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2020coreferential/">Coreferential Reasoning Learning For Language Representation</a> Deming Ye et al. </li>
     
   
     
       <li> <a href="/publications/pfeiffer2020framework/">Adapterhub: A Framework For Adapting Transformers</a> Jonas Pfeiffer et al. </li>
     
   
     
       <li> <a href="/publications/sun2020contextualized/">Colake: Contextualized Language And Knowledge Embedding</a> Tianxiang Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sun2020contrastive/">Contrastive Distillation On Intermediate Representations For Language Model Compression</a> Siqi Sun et al. </li>
     
   
     
       <li> <a href="/publications/yang2020text/">TAP: Text-aware Pre-training For Text-vqa And Text-caption</a> Zhengyuan Yang et al. </li>
     
   
     
       <li> <a href="/publications/chen2020recall/">Recall And Learn: Fine-tuning Deep Pretrained Language Models With Less Forgetting</a> Sanyuan Chen et al. </li>
     
   
     
       <li> <a href="/publications/kolluru2020iterative/">Imojie: Iterative Memory-based Joint Open Information Extraction</a> Keshav Kolluru, Samarth Aggarwal, Vipul Rathore, Mausam, Soumen Chakrabarti </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gupta2020bert/">BERT Based Multilingual Machine Comprehension In English And Hindi</a> Somil Gupta, Nilesh Khade </li>
     
   
     
       <li> <a href="/publications/ma2020character/">Charbert: Character-aware Pre-trained Language Model</a> Wentao Ma et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ke2020rethinking/">Rethinking Positional Encoding In Language Pre-training</a> Guolin Ke, Di He, Tie-yan Liu </li>
     
   
     
       <li> <a href="/publications/penha2020what/">What Does BERT Know About Books, Movies And Music? Probing BERT For Conversational Recommendation</a> Gustavo Penha, Claudia Hauff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020multi/">Minilmv2: Multi-head Self-attention Relation Distillation For Compressing Pretrained Transformers</a> Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, Furu Wei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2020pre/">Pre-training Via Paraphrasing</a> Mike Lewis et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020deep/">Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers</a> Wenhui Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2020pseudo/">Unilmv2: Pseudo-masked Language Models For Unified Language Model Pre-training</a> Hangbo Bao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2020lrc/">LRC-BERT: Latent-representation Contrastive Knowledge Distillation For Natural Language Understanding</a> Hao Fu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2020residual/">Residual Energy-based Models For Text Generation</a> Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, Marc'aurelio Ranzato </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/longpre2020how/">How Effective Is Task-agnostic Data Augmentation For Pretrained Transformers?</a> Shayne Longpre, Yu Wang, Christopher Dubois </li>
     
   
     
   
     
       <li> <a href="/publications/jeretic2020are/">Are Natural Language Inference Models Imppressive? Learning Implicature And Presupposition</a> Paloma Jeretic, Alex Warstadt, Suvrat Bhooshan, Adina Williams </li>
     
   
     
   
     
       <li> <a href="/publications/wang2020pretrain/">To Pretrain Or Not To Pretrain: Examining The Benefits Of Pretraining On Resource Rich Tasks</a> Sinong Wang, Madian Khabsa, Hao Ma </li>
     
   
     
       <li> <a href="/publications/tambe2020sentence/">Edgebert: Sentence-level Energy Optimizations For Latency-aware Multi-task NLP Inference</a> Thierry Tambe et al. </li>
     
   
     
       <li> <a href="/publications/si2020better/">Better Robustness By More Coverage: Adversarial Training With Mixup Augmentation For Robust Fine-tuning</a> Chenglei Si et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020trojaning/">Trojaning Language Models For Fun And Profit</a> Xinyang Zhang, Zheng Zhang, Shouling Ji, Ting Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bartolo2020beat/">Beat The AI: Investigating Adversarial Human Annotation For Reading Comprehension</a> Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, Pontus Stenetorp </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2020pre/">Codebert: A Pre-trained Model For Programming And Natural Languages</a> Zhangyin Feng et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jain2020contrastive/">Contrastive Code Representation Learning</a> Paras Jain et al. </li>
     
   
     
       <li> <a href="/publications/shleifer2020pre/">Pre-trained Summarization Distillation</a> Sam Shleifer, Alexander M. Rush </li>
     
   
     
   
     
       <li> <a href="/publications/liang2020new/">XGLUE: A New Benchmark Dataset For Cross-lingual Pre-training, Understanding And Generation</a> Yaobo Liang et al. </li>
     
   
     
       <li> <a href="/publications/xu2020bert/">Bert-of-theseus: Compressing BERT By Progressive Module Replacing</a> Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bunk2020lightweight/">DIET: Lightweight Language Understanding For Dialogue Systems</a> Tanja Bunk, Daksh Varshneya, Vladimir Vlasov, Alan Nichol </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2020cosda/">Cosda-ml: Multi-lingual Code-switching Data Augmentation For Zero-shot Cross-lingual NLP</a> Libo Qin, Minheng Ni, Yue Zhang, Wanxiang Che </li>
     
   
     
       <li> <a href="/publications/pruksachatkun2020intermediate/">Intermediate-task Transfer Learning With Pretrained Models For Natural Language Understanding: When And Why Does It Work?</a> Yada Pruksachatkun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sun2020mixup/">Mixup-transformer: Dynamic Data Augmentation For NLP Tasks</a> Lichao Sun et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020efficient/">Efficient Transformer-based Large Scale Language Representations Using Hardware-friendly Block Structured Pruning</a> Bingbing Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sellam2020learning/">BLEURT: Learning Robust Metrics For Text Generation</a> Thibault Sellam, Dipanjan Das, Ankur P. Parikh </li>
     
   
     
       <li> <a href="/publications/pudipeddi2020training/">Training Large Neural Networks With Constant Memory Using A New Execution Algorithm</a> Bharadwaj Pudipeddi, Maral Mesmakhosroshahi, Jinwen Xi, Sujeeth Bharadwaj </li>
     
   
     
       <li> <a href="/publications/zaheer2020big/">Big Bird: Transformers For Longer Sequences</a> Manzil Zaheer et al. </li>
     
   
     
       <li> <a href="/publications/vanaken2020hidden/">Visbert: Hidden-state Visualizations For Transformers</a> Betty Van Aken, Benjamin Winter, Alexander L√∂ser, Felix A. Gers </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2020retrospective/">Retrospective Reader For Machine Reading Comprehension</a> Zhuosheng Zhang, Junjie Yang, Hai Zhao </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ruan2020fine/">Fine-tuning BERT For Schema-guided Zero-shot Dialogue State Tracking</a> Yu-ping Ruan, Zhen-hua Ling, Jia-chen Gu, Quan Liu </li>
     
   
     
       <li> <a href="/publications/desai2020calibration/">Calibration Of Pre-trained Transformers</a> Shrey Desai, Greg Durrett </li>
     
   
     
       <li> <a href="/publications/zhang2020unified/">Unified Mandarin TTS Front-end Based On Distilled BERT Model</a> Yang Zhang, Liqun Deng, Yasheng Wang </li>
     
   
     
       <li> <a href="/publications/muller2020when/">When Being Unseen From Mbert Is Just The Beginning: Handling New Languages With Multilingual Language Models</a> Benjamin Muller, Antonis Anastasopoulos, Beno√Æt Sagot, Djam√© Seddah </li>
     
   
     
       <li> <a href="/publications/yin2020pretraining/">Tabert: Pretraining For Joint Understanding Of Textual And Tabular Data</a> Pengcheng Yin, Graham Neubig, Wen-tau Yih, Sebastian Riedel </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ezencan2020comparison/">A Comparison Of LSTM And BERT For Small Corpus</a> Aysu Ezen-can </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/majumdar2020improving/">Improving Vision-and-language Navigation With Image-text Pairs From The Web</a> Arjun Majumdar et al. </li>
     
   
     
       <li> <a href="/publications/cohan2020document/">SPECTER: Document-level Representation Learning Using Citation-informed Transformers</a> Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S. Weld </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tu2020empirical/">An Empirical Study On Robustness To Spurious Correlations Using Pre-trained Language Models</a> Lifu Tu, Garima Lalwani, Spandana Gella, He He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lauscher2020from/">From Zero To Hero: On The Limitations Of Zero-shot Cross-lingual Transfer With Multilingual Transformers</a> Anne Lauscher, Vinit Ravishankar, Ivan Vuliƒá, Goran Glava≈° </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gupta2020global/">GMAT: Global Memory Augmentation For Transformers</a> Ankit Gupta, Jonathan Berant </li>
     
   
     
       <li> <a href="/publications/qu2020contrast/">Coda: Contrast-enhanced And Diversity-promoting Data Augmentation For Natural Language Understanding</a> Yanru Qu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/macavaney2020analyzing/">ABNIRML: Analyzing The Behavior Of Neural IR Models</a> Sean Macavaney, Sergey Feldman, Nazli Goharian, Doug Downey, Arman Cohan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ganesh2020compressing/">Compressing Large-scale Transformer-based Models: A Case Study On BERT</a> Prakhar Ganesh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/merchant2020what/">What Happens To BERT Embeddings During Fine-tuning?</a> Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, Ian Tenney </li>
     
   
     
       <li> <a href="/publications/liu2020survey/">A Survey On Contextual Embeddings</a> Qi Liu, Matt J. Kusner, Phil Blunsom </li>
     
   
     
       <li> <a href="/publications/guo2020cross/">Multireqa: A Cross-domain Evaluation For Retrieval Question Answering Models</a> Mandy Guo, Yinfei Yang, Daniel Cer, Qinlan Shen, Noah Constant </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mosbach2020stability/">On The Stability Of Fine-tuning BERT: Misconceptions, Explanations, And Strong Baselines</a> Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow </li>
     
   
     
       <li> <a href="/publications/zadeh2020quantizing/">GOBO: Quantizing Attention-based NLP Models For Low Latency And Energy Efficient Inference</a> Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, Andreas Moshovos </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/prasanna2020when/">When BERT Plays The Lottery, All Tickets Are Winning</a> Sai Prasanna, Anna Rogers, Anna Rumshisky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hofst%C3%A4tter2020improving/">Improving Efficient Neural Ranking Models With Cross-architecture Knowledge Distillation</a> Sebastian Hofst√§tter, Sophia Althammer, Michael Schr√∂der, Mete Sertkan, Allan Hanbury </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2020decomposing/">Deformer: Decomposing Pre-trained Transformers For Faster Question Answering</a> Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian </li>
     
   
     
   
     
       <li> <a href="/publications/wang2020vd/">VD-BERT: A Unified Vision And Dialog Transformer With BERT</a> Yue Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/whang2020do/">Do Response Selection Models Really Know What's Next? Utterance Manipulation Strategies For Multi-turn Response Selection</a> Taesun Whang et al. </li>
     
   
     
       <li> <a href="/publications/asai2020logic/">Logic-guided Data Augmentation And Regularization For Consistent Question Answering</a> Akari Asai, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sen2020what/">What Do Models Learn From Question Answering Datasets?</a> Priyanka Sen, Amir Saffari </li>
     
   
     
       <li> <a href="/publications/chen2020efficient/">Earlybert: Efficient BERT Training Via Early-bird Lottery Tickets</a> Xiaohan Chen et al. </li>
     
   
     
       <li> <a href="/publications/wu2020are/">Are All Languages Created Equal In Multilingual BERT?</a> Shijie Wu, Mark Dredze </li>
     
   
     
       <li> <a href="/publications/xia2020which/">Which *BERT? A Survey Organizing Contextualized Encoders</a> Patrick Xia, Shijie Wu, Benjamin Van Durme </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nogueira2020document/">Document Ranking With A Pretrained Sequence-to-sequence Model</a> Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin </li>
     
   
     
   
     
       <li> <a href="/publications/cho2020x/">X-LXMERT: Paint, Caption And Answer Questions With Multi-modal Transformers</a> Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geva2020injecting/">Injecting Numerical Reasoning Skills Into Language Models</a> Mor Geva, Ankit Gupta, Jonathan Berant </li>
     
   
     
   
     
       <li> <a href="/publications/vuli%C4%872020probing/">Probing Pretrained Language Models For Lexical Semantics</a> Ivan Vuliƒá, Edoardo Maria Ponti, Robert Litschko, Goran Glava≈°, Anna Korhonen </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2020incorporating/">Incorporating BERT Into Parallel Sequence Decoding With Adapters</a> Junliang Guo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/beltagy2020long/">Longformer: The Long-document Transformer</a> Iz Beltagy, Matthew E. Peters, Arman Cohan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/casanueva2020efficient/">Efficient Intent Detection With Dual Sentence Encoders</a> I√±igo Casanueva, Tadas Temƒçinas, Daniela Gerz, Matthew Henderson, Ivan Vuliƒá </li>
     
   
     
       <li> <a href="/publications/zhou2020bert/">BERT Loses Patience: Fast And Robust Inference With Early Exit</a> Wangchunshu Zhou et al. </li>
     
   
     
       <li> <a href="/publications/zhang2020accelerating/">Accelerating Training Of Transformer-based Language Models With Progressive Layer Dropping</a> Minjia Zhang, Yuxiong He </li>
     
   
     
   
     
       <li> <a href="/publications/min2020syntactic/">Syntactic Data Augmentation Increases Robustness To Inference Heuristics</a> Junghyun Min, R. Thomas Mccoy, Dipanjan Das, Emily Pitler, Tal Linzen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2020recurrent/">A Recurrent Vision-and-language BERT For Navigation</a> Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-opazo, Stephen Gould </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2020distillation/">Ternarybert: Distillation-aware Ultra-low Bit BERT</a> Wei Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chung2020bert/">A Bert-based Distractor Generation Scheme With Multi-tasking And Negative Answer Training Strategies</a> Ho-lam Chung, Ying-hong Chan, Yao-chung Fan </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sajjad2020effect/">On The Effect Of Dropping Layers Of Pre-trained Transformer Models</a> Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Preslav Nakov </li>
     
   
     
       <li> <a href="/publications/zhao2020inducing/">Inducing Language-agnostic Multilingual Representations</a> Wei Zhao, Steffen Eger, Johannes Bjerva, Isabelle Augenstein </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2020beyond/">Beyond I.I.D.: Three Levels Of Generalization For Question Answering On Knowledge Bases</a> Yu Gu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bostrom2020byte/">Byte Pair Encoding Is Suboptimal For Language Model Pretraining</a> Kaj Bostrom, Greg Durrett </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2020trans/">TRANS-BLSTM: Transformer With Bidirectional LSTM For Language Understanding</a> Zhiheng Huang, Peng Xu, Davis Liang, Ajay Mishra, Bing Xiang </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2020gruen/">GRUEN For Evaluating Linguistic Quality Of Generated Text</a> Wanzheng Zhu, Suma Bhat </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2020masking/">Masking As An Efficient Alternative To Finetuning For Pretrained Language Models</a> Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bird2020chatbot/">Chatbot Interaction With Artificial Intelligence: Human Data Augmentation With T5 And Language Transformer Ensemble For Text Classification</a> Jordan J. Bird, Anik√≥ Ek√°rt, Diego R. Faria </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2020data/">Data Augmentation Using Pre-trained Transformer Models</a> Varun Kumar, Ashutosh Choudhary, Eunah Cho </li>
     
   
     
       <li> <a href="/publications/gu2020speaker/">Speaker-aware BERT For Multi-turn Response Selection In Retrieval-based Chatbots</a> Jia-chen Gu et al. </li>
     
   
     
       <li> <a href="/publications/xin2020dynamic/">Deebert: Dynamic Early Exiting For Accelerating BERT Inference</a> Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin </li>
     
   
     
   
     
       <li> <a href="/publications/dodge2020fine/">Fine-tuning Pretrained Language Models: Weight Initializations, Data Orders, And Early Stopping</a> Jesse Dodge et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/goyal2021larger/">Larger-scale Transformers For Multilingual Masked Language Modeling</a> Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/penha2021evaluating/">Evaluating The Robustness Of Retrieval Pipelines With Query Variation Generators</a> Gustavo Penha, Arthur C√¢mara, Claudia Hauff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/webersinke2021pretrained/">Climatebert: A Pretrained Language Model For Climate-related Text</a> Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler, Markus Leippold </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/flemotomos2021automated/">Automated Quality Assessment Of Cognitive Behavioral Therapy Sessions Through Highly Contextualized Language Representations</a> Nikolaos Flemotomos et al. </li>
     
   
     
       <li> <a href="/publications/zhang2021improving/">Improving Stack Overflow Question Title Generation With Copying Enhanced Codebert Model And Bi-modal Information</a> Fengji Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2021prompt/">Prompt-learning For Fine-grained Entity Typing</a> Ning Ding et al. </li>
     
   
     
       <li> <a href="/publications/mitchell2021fast/">Fast Model Editing At Scale</a> Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D. Manning </li>
     
   
     
   
     
       <li> <a href="/publications/chen2021knowledge/">Knowprompt: Knowledge-aware Prompt-tuning With Synergistic Optimization For Relation Extraction</a> Xiang Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zaken2021simple/">Bitfit: Simple Parameter-efficient Fine-tuning For Transformer-based Masked Language-models</a> Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg </li>
     
   
     
       <li> <a href="/publications/hu2021low/">Lora: Low-rank Adaptation Of Large Language Models</a> Edward J. Hu et al. </li>
     
   
     
       <li> <a href="/publications/kassner2021multilingual/">Multilingual LAMA: Investigating Knowledge In Multilingual Pretrained Language Models</a> Nora Kassner, Philipp Dufter, Hinrich Sch√ºtze </li>
     
   
     
       <li> <a href="/publications/liu2021augmenting/">Augmenting Sequential Recommendation With Pseudo-prior Items Via Reversely Pre-training Transformer</a> Zhiwei Liu, Ziwei Fan, Yu Wang, Philip S. Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021explaining/">On Explaining Your Explanations Of BERT: An Empirical Study With Sequence Classification</a> Zhengxuan Wu, Desmond C. Ong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2021tr/">TR-BERT: Dynamic Token Reduction For Accelerating BERT Inference</a> Deming Ye, Yankai Lin, Yufei Huang, Maosong Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2021knowledge/">Knowledge Neurons In Pretrained Transformers</a> Damai Dai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021using/">Using Adversarial Attacks To Reveal The Statistical Bias In Machine Reading Comprehension Models</a> Jieyu Lin, Jiajie Zou, Nai Ding </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moradi2021evaluating/">Evaluating The Robustness Of Neural Language Models To Input Perturbations</a> Milad Moradi, Matthias Samwald </li>
     
   
     
       <li> <a href="/publications/wang2021unified/">UFO: A Unified Transformer For Vision-language Representation Learning</a> Jianfeng Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/straka2021czech/">Robeczech: Czech Roberta, A Monolingual Contextualized Language Representation Model</a> Milan Straka, Jakub N√°plava, Jana Strakov√°, David Samuel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guhur2021pretraining/">Airbert: In-domain Pretraining For Vision-and-language Navigation</a> Pierre-louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, Cordelia Schmid </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chada2021simple/">Fewshotqa: A Simple Framework For Few-shot Learning Of Question Answering Tasks Using Pre-trained Text-to-text Models</a> Rakesh Chada, Pradeep Natarajan </li>
     
   
     
       <li> <a href="/publications/turc2021revisiting/">Revisiting The Primacy Of English In Zero-shot Cross-lingual Transfer</a> Iulia Turc, Kenton Lee, Jacob Eisenstein, Ming-wei Chang, Kristina Toutanova </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tinn2021fine/">Fine-tuning Large Neural Language Models For Biomedical Natural Language Processing</a> Robert Tinn et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2021bert/">Bob: BERT Over BERT For Training Persona-based Dialogue Models From Limited Personalized Data</a> Haoyu Song, Yan Wang, Kaiyan Zhang, Wei-nan Zhang, Ting Liu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021study/">BERT, Mbert, Or Bibert? A Study On Contextualized Embeddings For Neural Machine Translation</a> Haoran Xu, Benjamin Van Durme, Kenton Murray </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2021improving/">Tacl: Improving BERT Pre-training With Token-aware Contrastive Learning</a> Yixuan Su et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liang2021mwp/">MWP-BERT: Numeracy-augmented Pre-training For Math Word Problem Solving</a> Zhenwen Liang et al. </li>
     
   
     
       <li> <a href="/publications/xia2021using/">Using Prior Knowledge To Guide Bert's Attention In Semantic Textual Matching Tasks</a> Tingyu Xia, Yue Wang, Yuan Tian, Yi Chang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/p%C3%A9rez2021pre/">Robertuito: A Pre-trained Language Model For Social Media Text In Spanish</a> Juan Manuel P√©rez, Dami√°n A. Furman, Laura Alonso Alemany, Franco Luque </li>
     
   
     
       <li> <a href="/publications/herzig2021open/">Open Domain Question Answering Over Tables Via Dense Retrieval</a> Jonathan Herzig, Thomas M√ºller, Syrine Krichene, Julian Martin Eisenschlos </li>
     
   
     
       <li> <a href="/publications/clark2021pre/">CANINE: Pre-training An Efficient Tokenization-free Encoder For Language Representation</a> Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/moon2021multi/">Multi-modal Understanding And Generation For Medical Images And Text Via Vision-language Pre-training</a> Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-hak Kim, Edward Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kalyan2021ammus/">AMMUS : A Survey Of Transformer-based Pretrained Models In Natural Language Processing</a> Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, Sivanesan Sangeetha </li>
     
   
     
   
     
       <li> <a href="/publications/pearce2021comparative/">A Comparative Study Of Transformer-based Language Models On Extractive Question Answering</a> Kate Pearce, Tiffany Zhan, Aneesh Komanduri, Justin Zhan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frank2021vision/">Vision-and-language Or Vision-for-language? On Cross-modal Influence In Multimodal Transformers</a> Stella Frank, Emanuele Bugliarello, Desmond Elliott </li>
     
   
     
       <li> <a href="/publications/santhanam2021effective/">Colbertv2: Effective And Efficient Retrieval Via Lightweight Late Interaction</a> Keshav Santhanam, Omar Khattab, Jon Saad-falcon, Christopher Potts, Matei Zaharia </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lowphansirikul2021pretraining/">Wangchanberta: Pretraining Transformer-based Thai Language Models</a> Lalita Lowphansirikul, Charin Polpanumas, Nawat Jantrakulchai, Sarana Nutanong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021one/">One Teacher Is Enough? Pre-trained Language Model Distillation From Multiple Teachers</a> Chuhan Wu, Fangzhao Wu, Yongfeng Huang </li>
     
   
     
       <li> <a href="/publications/wu2021distilling/">Newsbert: Distilling Pre-trained Language Model For Intelligent News Application</a> Chuhan Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dong2021how/">How Should Pre-trained Language Models Be Fine-tuned Towards Adversarial Robustness?</a> Xinhsuai Dong, Luu Anh Tuan, Min Lin, Shuicheng Yan, Hanwang Zhang </li>
     
   
     
       <li> <a href="/publications/li2021scheduled/">Scheduled Sampling In Vision-language Pretraining With Decoupled Encoder-decoder Network</a> Yehao Li, Yingwei Pan, Ting Yao, Jingwen Chen, Tao Mei </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bondarenko2021understanding/">Understanding And Overcoming The Challenges Of Efficient Transformer Quantization</a> Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021non/">Non-invasive Self-attention For Side Information Fusion In Sequential Recommendation</a> Chang Liu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2021nsp/">NSP-BERT: A Prompt-based Few-shot Learner Through An Original Pre-training Task--next Sentence Prediction</a> Yi Sun, Yu Zheng, Chao Hao, Hangping Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/min2021recent/">Recent Advances In Natural Language Processing Via Large Pre-trained Language Models: A Survey</a> Bonan Min et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caciularu2021cross/">CDLM: Cross-document Language Modeling</a> Avi Caciularu et al. </li>
     
   
     
       <li> <a href="/publications/he2021improving/">Debertav3: Improving Deberta Using Electra-style Pre-training With Gradient-disentangled Embedding Sharing</a> Pengcheng He, Jianfeng Gao, Weizhu Chen </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aghajanyan2021massive/">Muppet: Massive Multi-task Representations With Pre-finetuning</a> Armen Aghajanyan et al. </li>
     
   
     
       <li> <a href="/publications/guti%C3%A9rrezfandi%C3%B1o2021spanish/">Maria: Spanish Language Models</a> Asier Guti√©rrez-fandi√±o et al. </li>
     
   
     
       <li> <a href="/publications/mahabadi2021variational/">Variational Information Bottleneck For Effective Low-resource Fine-tuning</a> Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson </li>
     
   
     
       <li> <a href="/publications/lu2021machine/">Codexglue: A Machine Learning Benchmark Dataset For Code Understanding And Generation</a> Shuai Lu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lauscher2021sustainable/">Sustainable Modular Debiasing Of Language Models</a> Anne Lauscher, Tobias L√ºken, Goran Glava≈° </li>
     
   
     
   
     
       <li> <a href="/publications/karmakar2021what/">What Do Pre-trained Code Models Know About Code?</a> Anjan Karmakar, Romain Robbes </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/srinivasan2021predicting/">Predicting The Performance Of Multilingual NLP Models</a> Anirudh Srinivasan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maneriker2021improving/">Urltran: Improving Phishing URL Detection Using Transformers</a> Pranav Maneriker et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021identifier/">Codet5: Identifier-aware Unified Pre-trained Encoder-decoder Models For Code Understanding And Generation</a> Yue Wang, Weishi Wang, Shafiq Joty, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/suglia2021embodied/">Embodied BERT: A Transformer Model For Embodied, Language-guided Visual Task Completion</a> Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme </li>
     
   
     
   
     
       <li> <a href="/publications/khare2021multimodal/">MMBERT: Multimodal BERT Pretraining For Improved Medical VQA</a> Yash Khare et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/haviv2021learning/">Bertese: Learning To Speak To BERT</a> Adi Haviv, Jonathan Berant, Amir Globerson </li>
     
   
     
   
     
       <li> <a href="/publications/xiao2021training/">Training Large-scale News Recommenders With Pretrained Language Models In The Loop</a> Shitao Xiao, Zheng Liu, Yingxia Shao, Tao Di, Xing Xie </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zafrir2021prune/">Prune Once For All: Sparse Pre-trained Language Models</a> Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, Moshe Wasserblat </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phan2021text/">Scifive: A Text-to-text Transformer Model For Biomedical Literature</a> Long N. Phan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2021improved/">Improved Text Classification Via Contrastive Adversarial Training</a> Lin Pan, Chung-wei Hang, Avirup Sil, Saloni Potdar </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shleifer2021improved/">Normformer: Improved Transformer Pretraining With Extra Normalization</a> Sam Shleifer, Jason Weston, Myle Ott </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2021general/">GLM: General Language Model Pretraining With Autoregressive Blank Infilling</a> Zhengxiao Du et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2021personalized/">Personalized Transformer For Explainable Recommendation</a> Lei Li, Yongfeng Zhang, Li Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2021intent/">Intent Detection And Slot Filling For Vietnamese</a> Mai Hoang Dao, Thinh Hung Truong, Dat Quoc Nguyen </li>
     
   
     
       <li> <a href="/publications/gao2021rethink/">Rethink Training Of BERT Rerankers In Multi-stage Retrieval Pipeline</a> Luyu Gao, Zhuyun Dai, Jamie Callan </li>
     
   
     
   
     
       <li> <a href="/publications/topal2021exploring/">Exploring Transformers In Natural Language Generation: GPT, BERT, And Xlnet</a> M. Onat Topal, Anil Bas, Imke Van Heerden </li>
     
   
     
       <li> <a href="/publications/kim2021i/">I-BERT: Integer-only BERT Quantization</a> Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021self/">Self-guided Contrastive Learning For BERT Sentence Representations</a> Taeuk Kim, Kang Min Yoo, Sang-goo Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/misra2022enabling/">Minicons: Enabling Flexible Behavioral And Representational Analyses Of Transformer Language Models</a> Kanishka Misra </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022what/">What Language Model Architecture And Pretraining Objective Work Best For Zero-shot Generalization?</a> Thomas Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022use/">On The Use Of BERT For Automated Essay Scoring: Joint Learning Of Multi-scale Essay Representation</a> Yongjie Wang, Chuan Wang, Ruobing Li, Hui Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2022fast/">Flashattention: Fast And Memory-efficient Exact Attention With Io-awareness</a> Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R√© </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022efficient/">Zeroquant: Efficient And Affordable Post-training Quantization For Large-scale Transformers</a> Zhewei Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alabi2022adapting/">Adapting Pre-trained Language Models To African Languages Via Multilingual Adaptive Fine-tuning</a> Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, Dietrich Klakow </li>
     
   
     
       <li> <a href="/publications/li2022clinical/">Clinical-longformer And Clinical-bigbird: Transformers For Long Clinical Sequences</a> Yikuan Li, Ramsey M. Wehbe, Faraz S. Ahmad, Hanyin Wang, Yuan Luo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/delarosa2022efficient/">BERTIN: Efficient Pre-training Of A Spanish Language Model Using Perplexity Sampling</a> Javier De La Rosa et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022linguistically/">LERT: A Linguistically-motivated Pre-trained Language Model</a> Yiming Cui, Wanxiang Che, Shijin Wang, Ting Liu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022prompt/">Prompt Tuning For Discriminative Pre-trained Language Models</a> Yuan Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yuan2022pretraining/">Biobart: Pretraining And Evaluation Of A Biomedical Generative Language Model</a> Hongyi Yuan et al. </li>
     
   
     
       <li> <a href="/publications/zhang2022paradox/">On The Paradox Of Learning To Reason From Data</a> Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-wei Chang, Guy Van Den Broeck </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hua2022transformer/">Transformer Quality In Linear Time</a> Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc V. Le </li>
     
   
     
       <li> <a href="/publications/du2022contrastive/">Contrastive Learning With Bidirectional Transformers For Sequential Recommendation</a> Hanwen Du et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2022vl/">Vl-beit: Generative Vision-language Pretraining</a> Hangbo Bao, Wenhui Wang, Li Dong, Furu Wei </li>
     
   
     
       <li> <a href="/publications/meng2022generating/">Generating Training Data With Language Models: Towards Zero-shot Language Understanding</a> Yu Meng, Jiaxin Huang, Yu Zhang, Jiawei Han </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kurtic2022optimal/">The Optimal BERT Surgeon: Scalable And Accurate Second-order Pruning For Large Language Models</a> Eldar Kurtic et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022no/">No More Fine-tuning? An Experimental Evaluation Of Prompt Tuning In Code Intelligence</a> Chaozheng Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2022proposal/">Don't Generate, Discriminate: A Proposal For Grounding Language Models To Real-world Environments</a> Yu Gu, Xiang Deng, Yu Su </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2022pre/">Layoutlmv3: Pre-training For Document AI With Unified Text And Image Masking</a> Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei </li>
     
   
     
   
     
       <li> <a href="/publications/bucker2022reshaping/">Reshaping Robot Trajectories Using Natural Language Commands: A Study Of Multi-modal Data Alignment Using Transformers</a> Arthur Bucker et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2022zero/">Zero-shot Video Question Answering Via Frozen Bidirectional Language Models</a> Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhai2022high/">Bytetransformer: A High-performance Transformer Boosted For Variable-length Inputs</a> Yujia Zhai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wan2022what/">What Do They Capture? -- A Structural Analysis Of Pre-trained Language Models For Source Code</a> Yao Wan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wettig2022should/">Should You Mask 15% In Masked Language Modeling?</a> Alexander Wettig, Tianyu Gao, Zexuan Zhong, Danqi Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/petrov2022systematic/">A Systematic Review And Replicability Study Of Bert4rec For Sequential Recommendation</a> Aleksandr Petrov, Craig Macdonald </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/diao2022black/">Black-box Prompt Learning For Pre-trained Language Models</a> Shizhe Diao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022ernie/">Ernie-search: Bridging Cross-encoder With Dual-encoder Via Self On-the-fly Distillation For Dense Passage Retrieval</a> Yuxiang Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022accelerating/">Accelerating Attention Through Gradient-based Learned Runtime Pruning</a> Zheng Li, Soroush Ghodrati, Amir Yazdanbakhsh, Hadi Esmaeilzadeh, Mingu Kang </li>
     
   
     
   
     
       <li> <a href="/publications/eddine2022pretrained/">Arabart: A Pretrained Arabic Sequence-to-sequence Model For Abstractive Summarization</a> Moussa Kamal Eddine, Nadi Tomeh, Nizar Habash, Joseph Le Roux, Michalis Vazirgiannis </li>
     
   
     
       <li> <a href="/publications/valipour2022parameter/">Dylora: Parameter Efficient Tuning Of Pre-trained Models Using Dynamic Search-free Low-rank Adaptation</a> Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, Ali Ghodsi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2022optimizing/">Rlprompt: Optimizing Discrete Text Prompts With Reinforcement Learning</a> Mingkai Deng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yasunaga2022deep/">Deep Bidirectional Language-knowledge Graph Pretraining</a> Michihiro Yasunaga et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2022pre/">Retromae: Pre-training Retrieval-oriented Language Models Via Masked Auto-encoder</a> Shitao Xiao, Zheng Liu, Yingxia Shao, Zhao Cao </li>
     
   
     
   
     
       <li> <a href="/publications/hu2022empowering/">Empowering Language Models With Knowledge Graph Reasoning For Question Answering</a> Ziniu Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tirumala2022memorization/">Memorization Without Overfitting: Analyzing The Training Dynamics Of Large Language Models</a> Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, Armen Aghajanyan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ravi2022vlc/">VLC-BERT: Visual Question Answering With Contextualized Commonsense Knowledge</a> Sahithya Ravi, Aditya Chinchure, Leonid Sigal, Renjie Liao, Vered Shwartz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2023prompting/">Prompting For Multimodal Hateful Meme Classification</a> Rui Cao, Roy Ka-wei Lee, Wen-haw Chong, Jing Jiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ramos2023retrieval/">Retrieval-augmented Image Captioning</a> Rita Ramos, Desmond Elliott, Bruno Martins </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023label/">Label Supervised Llama Finetuning</a> Zongxi Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rafiepour2023cnn/">CTRAN: Cnn-transformer-based Network For Natural Language Understanding</a> Mehrdad Rafiepour, Javad Salimi Sartakhti </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/harte2023leveraging/">Leveraging Large Language Models For Sequential Recommendation</a> Jesse Harte et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023towards/">Towards Efficient Fine-tuning Of Pre-trained Code Models: An Experimental Study And Beyond</a> Ensheng Shi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/whitehouse2023llm/">Llm-powered Data Augmentation For Enhanced Cross-lingual Performance</a> Chenxi Whitehouse, Monojit Choudhury, Alham Fikri Aji </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2023bad/">Bad Actor, Good Advisor: Exploring The Role Of Large Language Models In Fake News Detection</a> Beizhe Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/klenitskiy2023turning/">Turning Dross Into Gold Loss: Is Bert4rec Really Better Than Sasrec?</a> Anton Klenitskiy, Alexey Vasilev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023comparative/">A Comparative Study Of Pretrained Language Models For Long Clinical Text</a> Yikuan Li, Ramsey M. Wehbe, Faraz S. Ahmad, Hanyin Wang, Yuan Luo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023low/">Low-rank Adaptation Of Large Language Model Rescoring For Parameter-efficient Speech Recognition</a> Yu Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chakraborty2023possibilities/">On The Possibilities Of Ai-generated Text Detection</a> Souradip Chakraborty et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023is/">Is Chatgpt A Good Sentiment Analyzer? A Preliminary Study</a> Zengzhi Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kaur2024from/">From Text To Transformation: A Comprehensive Review Of Large Language Models' Versatility</a> Pravneet Kaur et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/warstadt2025findings/">Findings Of The Babylm Challenge: Sample-efficient Pretraining On Developmentally Plausible Corpora</a> Alex Warstadt et al. </li>
     
   
   </ul>

   <h3>üè∑ Bias Mitigation <a id="Bias Mitigation"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wan2023is/">"kelly Is A Warm Person, Joseph Is A Role Model": Gender Biases In Llm-generated Reference Letters</a> Yixin Wan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Distillation <a id="Distillation"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2016sequence/">Sequence-level Knowledge Distillation</a> Yoon Kim, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2017data/">Data Distillation For Controlling Specificity In Dialogue Generation</a> Jiwei Li, Will Monroe, Dan Jurafsky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2017non/">Non-autoregressive Neural Machine Translation</a> Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, Richard Socher </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2018attention/">Attention-guided Answer Distillation For Machine Reading Comprehension</a> Minghao Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ghandeharioun2019approximating/">Approximating Interactive Human Evaluation With Self-play For Open-domain Dialog Systems</a> Asma Ghandeharioun et al. </li>
     
   
     
       <li> <a href="/publications/liu2019multi/">MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models</a> Linqing Liu, Huan Wang, Jimmy Lin, Richard Socher, Caiming Xiong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sanh2019distilled/">Distilbert, A Distilled Version Of BERT: Smaller, Faster, Cheaper And Lighter</a> Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2019reducing/">Reducing Transformer Depth On Demand With Structured Dropout</a> Angela Fan, Edouard Grave, Armand Joulin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/turc2019well/">Well-read Students Learn Better: On The Importance Of Pre-training Compact Models</a> Iulia Turc, Ming-wei Chang, Kenton Lee, Kristina Toutanova </li>
     
   
     
   
     
       <li> <a href="/publications/yang2019towards/">Towards Making The Most Of BERT In Neural Machine Translation</a> Jiacheng Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hsu2019knowledge/">Knowledge-enriched Visual Storytelling</a> Chao-chun Hsu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiao2019distilling/">Tinybert: Distilling BERT For Natural Language Understanding</a> Xiaoqi Jiao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/weng2019acquiring/">Acquiring Knowledge From Pre-trained Model To Neural Machine Translation</a> Rongxiang Weng, Heng Yu, Shujian Huang, Shanbo Cheng, Weihua Luo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/barkan2019scalable/">Scalable Attentive Sentence-pair Modeling Via Distilled Sentence Embedding</a> Oren Barkan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gordon2019explaining/">Explaining Sequence-level Knowledge Distillation As Data-augmentation For Neural Machine Translation</a> Mitchell A. Gordon, Kevin Duh </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mccarley2019structured/">Structured Pruning Of A Bert-based Question Answering Model</a> J. S. Mccarley, Rishav Chakravarti, Avirup Sil </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ju2019technical/">Technical Report On Conversational Question Answering</a> Ying Ju et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2019model/">Model Compression With Two-stage Multi-teacher Knowledge Distillation For Web Question Answering System</a> Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, Daxin Jiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hendrycks2020pretrained/">Pretrained Transformers Improve Out-of-distribution Robustness</a> Dan Hendrycks et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2020contrastive/">Contrastive Distillation On Intermediate Representations For Language Model Compression</a> Siqi Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020multi/">Minilmv2: Multi-head Self-attention Relation Distillation For Compressing Pretrained Transformers</a> Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, Furu Wei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020deep/">Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers</a> Wenhui Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2020lrc/">LRC-BERT: Latent-representation Contrastive Knowledge Distillation For Natural Language Understanding</a> Hao Fu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/you2020knowledge/">Knowledge Distillation For Improved Accuracy In Spoken Question Answering</a> Chenyu You, Nuo Chen, Yuexian Zou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shleifer2020pre/">Pre-trained Summarization Distillation</a> Sam Shleifer, Alexander M. Rush </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2020bert/">Bert-of-theseus: Compressing BERT By Progressive Module Replacing</a> Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020efficient/">Efficient Transformer-based Large Scale Language Representations Using Hardware-friendly Block Structured Pruning</a> Bingbing Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020unified/">Unified Mandarin TTS Front-end Based On Distilled BERT Model</a> Yang Zhang, Liqun Deng, Yasheng Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hofst%C3%A4tter2020improving/">Improving Efficient Neural Ranking Models With Cross-architecture Knowledge Distillation</a> Sebastian Hofst√§tter, Sophia Althammer, Michael Schr√∂der, Mete Sertkan, Allan Hanbury </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2020decomposing/">Deformer: Decomposing Pre-trained Transformers For Faster Question Answering</a> Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2020towards/">Mixkd: Towards Efficient Distillation Of Large-scale Language Models</a> Kevin J Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020distillation/">Ternarybert: Distillation-aware Ultra-low Bit BERT</a> Wei Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sajjad2020effect/">On The Effect Of Dropping Layers Of Pre-trained Transformer Models</a> Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Preslav Nakov </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peer2021greedy/">Greedy-layer Pruning: Speeding Up Transformer Models For Natural Language Processing</a> David Peer, Sebastian Stabinger, Stefan Engl, Antonio Rodriguez-sanchez </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singh2021nlp/">The NLP Cookbook: Modern Recipes For Transformer Based Deep Learning Architectures</a> Sushant Singh, Ausif Mahmood </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021one/">One Teacher Is Enough? Pre-trained Language Model Distillation From Multiple Teachers</a> Chuhan Wu, Fangzhao Wu, Yongfeng Huang </li>
     
   
     
       <li> <a href="/publications/wu2021distilling/">Newsbert: Distilling Pre-trained Language Model For Intelligent News Application</a> Chuhan Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021stacked/">Stacked Acoustic-and-textual Encoding: Integrating The Pre-trained Models Into Speech Translation Encoders</a> Chen Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2021compressing/">Compressing Visual-linguistic Model Via Knowledge Distillation</a> Zhiyuan Fang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021nlp/">Generate, Annotate, And Learn: NLP With Synthetic Text</a> Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, Mohammad Norouzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021ernie/">ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training For Language Understanding And Generation</a> Shuohuan Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kaliamoorthi2021distilling/">Distilling Large Language Models Into Tiny And Effective Students Using Pqrnn</a> Prabhu Kaliamoorthi, Aditya Siddhant, Edward Li, Melvin Johnson </li>
     
   
     
       <li> <a href="/publications/west2021symbolic/">Symbolic Knowledge Distillation: From General Language Models To Commonsense Models</a> Peter West et al. </li>
     
   
     
       <li> <a href="/publications/zafrir2021prune/">Prune Once For All: Sparse Pre-trained Language Models</a> Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, Moshe Wasserblat </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2021joint/">Rocketqav2: A Joint Training Method For Dense Passage Retrieval And Passage Re-ranking</a> Ruiyang Ren et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022efficient/">Zeroquant: Efficient And Affordable Post-training Quantization For Large-scale Transformers</a> Zhewei Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2022efficient/">Zerogen: Efficient Zero-shot Learning Via Dataset Generation</a> Jiacheng Ye et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2022enabling/">Enabling Multimodal Generation On CLIP Via Vision-language Knowledge Distillation</a> Wenliang Dai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2022sgva/">Sgva-clip: Semantic-guided Visual Adapting Of Vision-language Models For Few-shot Image Classification</a> Fang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang, Changsheng Xu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2022what/">What Do Llms Know About Financial Markets? A Case Study On Reddit Market Sentiment Analysis</a> Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, Michael Bendersky </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kurtic2022optimal/">The Optimal BERT Surgeon: Scalable And Accurate Second-order Pruning For Large Language Models</a> Eldar Kurtic et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dong2022masked/">Maskclip: Masked Self-distillation Advances Contrastive Language-image Pretraining</a> Xiaoyi Dong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022ernie/">Ernie-search: Bridging Cross-encoder With Dual-encoder Via Self On-the-fly Distillation For Dense Passage Retrieval</a> Yuxiang Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/glass2022generate/">Re2g: Retrieve, Rerank, Generate</a> Michael Glass et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2022structured/">Structured Pruning Learns Compact And Accurate Models</a> Mengzhou Xia, Zexuan Zhong, Danqi Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/barraco2022mean/">Camel: Mean Teacher Learning For Image Captioning</a> Manuele Barraco et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/magister2022teaching/">Teaching Small Language Models To Reason</a> Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shridhar2022distilling/">Distilling Reasoning Capabilities Into Smaller Language Models</a> Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022clip/">CLIP-TD: CLIP Targeted Distillation For Vision-language Tasks</a> Zhecan Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tunstall2023direct/">Zephyr: Direct Distillation Of LM Alignment</a> Lewis Tunstall et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023clip/">Tinyclip: CLIP Distillation Via Affinity Mimicking And Weight Inheritance</a> Kan Stephen Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mu2023learning/">Learning To Compress Prompts With Gist Tokens</a> Jesse Mu, Xiang Lisa Li, Noah Goodman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2023distilled/">Distilled GPT For Source Code Summarization</a> Chia-yi Su, Collin Mcmillan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hsieh2023distilling/">Distilling Step-by-step! Outperforming Larger Language Models With Less Training Data And Smaller Model Sizes</a> Cheng-yu Hsieh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2023bad/">Bad Actor, Good Advisor: Exploring The Role Of Large Language Models In Fake News Detection</a> Beizhe Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2023knowledge/">Minillm: Knowledge Distillation Of Large Language Models</a> Yuxian Gu, Li Dong, Furu Wei, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023targeted/">Universalner: Targeted Distillation From Large Language Models For Open Named Entity Recognition</a> Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung Poon </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023is/">Is Chatgpt Good At Search? Investigating Large Language Models As Re-ranking Agents</a> Weiwei Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023survey/">A Survey On Model Compression For Large Language Models</a> Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gholami2023can/">Can A Student Large Language Model Perform As Well As It's Teacher?</a> Sia Gholami, Marwan Omar </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023preventing/">Preventing Zero-shot Transfer Degradation In Continual Learning Of Vision-language Models</a> Zangwei Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2024bge/">BGE M3-embedding: Multi-lingual, Multi-functionality, Multi-granularity Text Embeddings Through Self-knowledge Distillation</a> Jianlv Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gemmateam2024gemma/">Gemma 2: Improving Open Language Models At A Practical Size</a> Gemma Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salemi2024optimization/">Optimization Methods For Personalizing Large Language Models Through Retrieval Augmentation</a> Alireza Salemi, Surya Kallumadi, Hamed Zamani </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2024unsupervised/">Promptkd: Unsupervised Prompt Distillation For Vision-language Models</a> Zheng Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Efficiency and Optimization <a id="Efficiency and Optimization"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/see2016compression/">Compression Of Neural Machine Translation Models Via Pruning</a> Abigail See, Minh-thang Luong, Christopher D. Manning </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2016sequence/">Sequence-level Knowledge Distillation</a> Yoon Kim, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wiseman2016sequence/">Sequence-to-sequence Learning As Beam-search Optimization</a> Sam Wiseman, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2017reinforced/">Reinforced Mnemonic Reader For Machine Reading Comprehension</a> Minghao Hu et al. </li>
     
   
     
       <li> <a href="/publications/strub2017end/">End-to-end Optimization Of Goal-driven And Visually Grounded Dialogue Systems</a> Florian Strub et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2017data/">Data Distillation For Controlling Specificity In Dialogue Generation</a> Jiwei Li, Will Monroe, Dan Jurafsky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kenter2017attentive/">Attentive Memory Networks: Efficient Machine Reading For Conversational Search</a> Tom Kenter, Maarten De Rijke </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2017non/">Non-autoregressive Neural Machine Translation</a> Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, Richard Socher </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chevalierboisvert2018platform/">Babyai: A Platform To Study The Sample Efficiency Of Grounded Language Learning</a> Maxime Chevalier-boisvert et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ott2018scaling/">Scaling Neural Machine Translation</a> Myle Ott, Sergey Edunov, David Grangier, Michael Auli </li>
     
   
     
   
     
       <li> <a href="/publications/nie2018operations/">Operations Guided Neural Networks For High Fidelity Data-to-text Generation</a> Feng Nie, Jinpeng Wang, Jin-ge Yao, Rong Pan, Chin-yew Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bapna2018training/">Training Deeper Neural Machine Translation Models With Transparent Attention</a> Ankur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao, Yonghui Wu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2018efficient/">Efficient Contextualized Representation: Language Model Pruning For Sequence Labeling</a> Liyuan Liu, Xiang Ren, Jingbo Shang, Jian Peng, Jiawei Han </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/das2018neural/">Neural Modular Control For Embodied Question Answering</a> Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2018attention/">Attention-guided Answer Distillation For Machine Reading Comprehension</a> Minghao Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2018reinforcement/">Reinforcement Learning On Web Interfaces Using Workflow-guided Exploration</a> Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019synchronous/">Synchronous Bidirectional Inference For Neural Sequence Generation</a> Jiajun Zhang, Long Zhou, Yang Zhao, Chengqing Zong </li>
     
   
     
       <li> <a href="/publications/pei2019modular/">A Modular Task-oriented Dialogue System Using A Neural Mixture-of-experts</a> Jiahuan Pei, Pengjie Ren, Maarten De Rijke </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shoeybi2019megatron/">Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism</a> Mohammad Shoeybi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/michel2019are/">Are Sixteen Heads Really Better Than One?</a> Paul Michel, Omer Levy, Graham Neubig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ghandeharioun2019approximating/">Approximating Interactive Human Evaluation With Self-play For Open-domain Dialog Systems</a> Asma Ghandeharioun et al. </li>
     
   
     
       <li> <a href="/publications/liu2019multi/">MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models</a> Linqing Liu, Huan Wang, Jimmy Lin, Richard Socher, Caiming Xiong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sanh2019distilled/">Distilbert, A Distilled Version Of BERT: Smaller, Faster, Cheaper And Lighter</a> Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2019reducing/">Reducing Transformer Depth On Demand With Structured Dropout</a> Angela Fan, Edouard Grave, Armand Joulin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/turc2019well/">Well-read Students Learn Better: On The Importance Of Pre-training Compact Models</a> Iulia Turc, Ming-wei Chang, Kenton Lee, Kristina Toutanova </li>
     
   
     
   
     
       <li> <a href="/publications/yang2019towards/">Towards Making The Most Of BERT In Neural Machine Translation</a> Jiacheng Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2019levenshtein/">Levenshtein Transformer</a> Jiatao Gu, Changhan Wang, Jake Zhao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khandelwal2019sample/">Sample Efficient Text Summarization Using A Single Pre-trained Transformer</a> Urvashi Khandelwal, Kevin Clark, Dan Jurafsky, Lukasz Kaiser </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019structured/">Structured Pruning Of Large Language Models</a> Ziheng Wang, Jeremy Wohlwend, Tao Lei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rajbhandari2019memory/">Zero: Memory Optimizations Toward Training Trillion Parameter Models</a> Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He </li>
     
   
     
       <li> <a href="/publications/jiao2019distilling/">Tinybert: Distilling BERT For Natural Language Understanding</a> Xiaoqi Jiao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/weng2019acquiring/">Acquiring Knowledge From Pre-trained Model To Neural Machine Translation</a> Rongxiang Weng, Heng Yu, Shujian Huang, Shanbo Cheng, Weihua Luo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/barkan2019scalable/">Scalable Attentive Sentence-pair Modeling Via Distilled Sentence Embedding</a> Oren Barkan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cho2019mixture/">Mixture Content Selection For Diverse Sequence Generation</a> Jaemin Cho, Minjoon Seo, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gordon2019explaining/">Explaining Sequence-level Knowledge Distillation As Data-augmentation For Neural Machine Translation</a> Mitchell A. Gordon, Kevin Duh </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mccarley2019structured/">Structured Pruning Of A Bert-based Question Answering Model</a> J. S. Mccarley, Rishav Chakravarti, Avirup Sil </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/henderson2019efficient/">Convert: Efficient And Accurate Conversational Representations From Transformers</a> Matthew Henderson et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/seo2019real/">Real-time Open-domain Question Answering With Dense-sparse Phrase Index</a> Minjoon Seo et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/merity2019single/">Single Headed Attention RNN: Stop Thinking With Your Head</a> Stephen Merity </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zafrir2019quantized/">Q8BERT: Quantized 8bit BERT</a> Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ju2019technical/">Technical Report On Conversational Question Answering</a> Ying Ju et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/voita2019analyzing/">Analyzing Multi-head Self-attention: Specialized Heads Do The Heavy Lifting, The Rest Can Be Pruned</a> Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/platanios2019competence/">Competence-based Curriculum Learning For Neural Machine Translation</a> Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas Poczos, Tom M. Mitchell </li>
     
   
     
       <li> <a href="/publications/yang2019model/">Model Compression With Two-stage Multi-teacher Knowledge Distillation For Web Question Answering System</a> Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, Daxin Jiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2019reinforcement/">Reinforcement Learning Based Curriculum Optimization For Neural Machine Translation</a> Gaurav Kumar, George Foster, Colin Cherry, Maxim Krikun </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/prato2019fully/">Fully Quantized Transformer For Machine Translation</a> Gabriele Prato, Ella Charlaix, Mehdi Rezagholizadeh </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2019reweighted/">Reweighted Proximal Pruning For Large-scale Language Representation</a> Fu-ming Guo, Sijia Liu, Finlay S. Mungall, Xue Lin, Yanzhi Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020high/">Lightseq: A High Performance Inference Library For Transformers</a> Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020multilingual/">Multilingual Speech Translation With Efficient Finetuning Of Pretrained Models</a> Xian Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2020variational/">Variational Transformers For Diverse Response Generation</a> Zhaojiang Lin, Genta Indra Winata, Peng Xu, Zihan Liu, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2020compact/">Mobilebert: A Compact Task-agnostic BERT For Resource-limited Devices</a> Zhiqing Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2020contrastive/">Contrastive Distillation On Intermediate Representations For Language Model Compression</a> Siqi Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2020funnel/">Funnel-transformer: Filtering Out Sequential Redundancy For Efficient Language Processing</a> Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le </li>
     
   
     
       <li> <a href="/publications/pang2020text/">Text Generation By Learning From Demonstrations</a> Richard Yuanzhe Pang, He He </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2020length/">Length-adaptive Transformer: Train Once With Length Drop, Use Anytime With Search</a> Gyuwan Kim, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020multi/">Minilmv2: Multi-head Self-attention Relation Distillation For Compressing Pretrained Transformers</a> Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, Furu Wei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020deep/">Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers</a> Wenhui Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2020lrc/">LRC-BERT: Latent-representation Contrastive Knowledge Distillation For Natural Language Understanding</a> Hao Fu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2020minimalist/">Mintl: Minimalist Transfer Learning For Task-oriented Dialogue Systems</a> Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Pascale Fung </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/you2020knowledge/">Knowledge Distillation For Improved Accuracy In Spoken Question Answering</a> Chenyu You, Nuo Chen, Yuexian Zou </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tambe2020sentence/">Edgebert: Sentence-level Energy Optimizations For Latency-aware Multi-task NLP Inference</a> Thierry Tambe et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2020cross/">Cross-modality Relevance For Reasoning On Language And Vision</a> Chen Zheng, Quan Guo, Parisa Kordjamshidi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shleifer2020pre/">Pre-trained Summarization Distillation</a> Sam Shleifer, Alexander M. Rush </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2020bert/">Bert-of-theseus: Compressing BERT By Progressive Module Replacing</a> Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2020progressive/">Progressive Generation Of Long Text With Pretrained Language Models</a> Bowen Tan, Zichao Yang, Maruan Ai-shedivat, Eric P. Xing, Zhiting Hu </li>
     
   
     
   
     
       <li> <a href="/publications/li2020train/">Train Large, Then Compress: Rethinking Model Size For Efficient Training And Inference Of Transformers</a> Zhuohan Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2020towards/">UBAR: Towards Fully End-to-end Task-oriented Dialog Systems With GPT-2</a> Yunyi Yang, Yunhao Li, Xiaojun Quan </li>
     
   
     
   
     
       <li> <a href="/publications/li2020efficient/">Efficient Transformer-based Large Scale Language Representations Using Hardware-friendly Block Structured Pruning</a> Bingbing Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pudipeddi2020training/">Training Large Neural Networks With Constant Memory Using A New Execution Algorithm</a> Bharadwaj Pudipeddi, Maral Mesmakhosroshahi, Jinwen Xi, Sujeeth Bharadwaj </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020unified/">Unified Mandarin TTS Front-end Based On Distilled BERT Model</a> Yang Zhang, Liqun Deng, Yasheng Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2020efficient/">SPARTA: Efficient Open-domain Question Answering Via Sparse Transformer Matching Retrieval</a> Tiancheng Zhao, Xiaopeng Lu, Kyusong Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qu2020contrast/">Coda: Contrast-enhanced And Diversity-promoting Data Augmentation For Natural Language Understanding</a> Yanru Qu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/soldaini2020cascade/">The Cascade Transformer: An Application For Efficient Answer Sentence Selection</a> Luca Soldaini, Alessandro Moschitti </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/r%C3%BCckl%C3%A92020efficiency/">Adapterdrop: On The Efficiency Of Adapters In Transformers</a> Andreas R√ºckl√© et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ganesh2020compressing/">Compressing Large-scale Transformer-based Models: A Case Study On BERT</a> Prakhar Ganesh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zadeh2020quantizing/">GOBO: Quantizing Attention-based NLP Models For Low Latency And Energy Efficient Inference</a> Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, Andreas Moshovos </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/prasanna2020when/">When BERT Plays The Lottery, All Tickets Are Winning</a> Sai Prasanna, Anna Rogers, Anna Rumshisky </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/svyatkovskiy2020intellicode/">Intellicode Compose: Code Generation Using Transformer</a> Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, Neel Sundaresan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hofst%C3%A4tter2020improving/">Improving Efficient Neural Ranking Models With Cross-architecture Knowledge Distillation</a> Sebastian Hofst√§tter, Sophia Althammer, Michael Schr√∂der, Mete Sertkan, Allan Hanbury </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2020decomposing/">Deformer: Decomposing Pre-trained Transformers For Faster Question Answering</a> Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian </li>
     
   
     
       <li> <a href="/publications/wu2020lite/">Lite Transformer With Long-short Range Attention</a> Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, Song Han </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020efficient/">Earlybert: Efficient BERT Training Via Early-bird Lottery Tickets</a> Xiaohan Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kaplan2020scaling/">Scaling Laws For Neural Language Models</a> Jared Kaplan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2020towards/">Mixkd: Towards Efficient Distillation Of Large-scale Language Models</a> Kevin J Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2020bert/">BERT Loses Patience: Fast And Robust Inference With Early Exit</a> Wangchunshu Zhou et al. </li>
     
   
     
       <li> <a href="/publications/zhang2020accelerating/">Accelerating Training Of Transformer-based Language Models With Progressive Layer Dropping</a> Minjia Zhang, Yuxiong He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chung2020rethinking/">Rethinking Embedding Coupling In Pre-trained Language Models</a> Hyung Won Chung, Thibault F√©vry, Henry Tsai, Melvin Johnson, Sebastian Ruder </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2020trading/">Trading Off Diversity And Quality In Natural Language Generation</a> Hugh Zhang, Daniel Duckworth, Daphne Ippolito, Arvind Neelakantan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020distillation/">Ternarybert: Distillation-aware Ultra-low Bit BERT</a> Wei Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sajjad2020effect/">On The Effect Of Dropping Layers Of Pre-trained Transformer Models</a> Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Preslav Nakov </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kale2020template/">Template Guided Text Generation For Task-oriented Dialogue</a> Mihir Kale, Abhinav Rastogi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schick2020not/">It's Not Just Size That Matters: Small Language Models Are Also Few-shot Learners</a> Timo Schick, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xin2020dynamic/">Deebert: Dynamic Early Exiting For Accelerating BERT Inference</a> Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lourie2021unicorn/">UNICORN On RAINBOW: A Universal Commonsense Reasoning Model On A New Multitask Benchmark</a> Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021knowledge/">Knowprompt: Knowledge-aware Prompt-tuning With Synergistic Optimization For Relation Extraction</a> Xiang Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2021tr/">TR-BERT: Dynamic Token Reduction For Accelerating BERT Inference</a> Deming Ye, Yankai Lin, Yufei Huang, Maosong Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peer2021greedy/">Greedy-layer Pruning: Speeding Up Transformer Models For Natural Language Processing</a> David Peer, Sebastian Stabinger, Stefan Engl, Antonio Rodriguez-sanchez </li>
     
   
     
       <li> <a href="/publications/so2021searching/">Primer: Searching For Efficient Transformers For Language Modeling</a> David R. So et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021stability/">The Stability-efficiency Dilemma: Investigating Sequence Length Warmup For Training GPT Models</a> Conglong Li, Minjia Zhang, Yuxiong He </li>
     
   
     
       <li> <a href="/publications/eichenberg2021magma/">MAGMA -- Multimodal Augmentation Of Generative Models Through Adapter-based Finetuning</a> Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, Anette Frank </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aribandi2021towards/">Ext5: Towards Extreme Multi-task Scaling For Transfer Learning</a> Vamsi Aribandi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meng2021coco/">COCO-LM: Correcting And Contrasting Text Sequences For Language Model Pretraining</a> Yu Meng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sung2021vl/">Vl-adapter: Parameter-efficient Transfer Learning For Vision-and-language Tasks</a> Yi-lin Sung, Jaemin Cho, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021scalable/">Scalable And Efficient Moe Training For Multitask Multilingual Models</a> Young Jin Kim et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singh2021nlp/">The NLP Cookbook: Modern Recipes For Transformer Based Deep Learning Architectures</a> Sushant Singh, Ausif Mahmood </li>
     
   
     
       <li> <a href="/publications/chen2021data/">Visualgpt: Data-efficient Adaptation Of Pretrained Language Models For Image Captioning</a> Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny </li>
     
   
     
       <li> <a href="/publications/eisenschlos2021multi/">MATE: Multi-view Attention For Table Transformer Efficiency</a> Julian Martin Eisenschlos, Maharshi Gor, Thomas M√ºller, William W. Cohen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2021pretrained/">Pretrained Transformers As Universal Computation Engines</a> Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/buschek2021impact/">The Impact Of Multiple Parallel Phrase Suggestions On Email Input And Composition Behaviour Of Native And Non-native English Writers</a> Daniel Buschek, Martin Z√ºrn, Malin Eiband </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021additive/">Fastformer: Additive Attention Can Be All You Need</a> Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, Xing Xie </li>
     
   
     
       <li> <a href="/publications/wu2021one/">One Teacher Is Enough? Pre-trained Language Model Distillation From Multiple Teachers</a> Chuhan Wu, Fangzhao Wu, Yongfeng Huang </li>
     
   
     
       <li> <a href="/publications/wu2021distilling/">Newsbert: Distilling Pre-trained Language Model For Intelligent News Application</a> Chuhan Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bondarenko2021understanding/">Understanding And Overcoming The Challenges Of Efficient Transformer Quantization</a> Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021stacked/">Stacked Acoustic-and-textual Encoding: Integrating The Pre-trained Models Into Speech Translation Encoders</a> Chen Xu et al. </li>
     
   
     
       <li> <a href="/publications/zhang2021cpm/">CPM-2: Large-scale Cost-effective Pre-trained Language Models</a> Zhengyan Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2021compressing/">Compressing Visual-linguistic Model Via Knowledge Distillation</a> Zhiyuan Fang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021linear/">Luna: Linear Unified Nested Attention</a> Xuezhe Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021improving/">Debertav3: Improving Deberta Using Electra-style Pre-training With Gradient-disentangled Embedding Sharing</a> Pengcheng He, Jianfeng Gao, Weizhu Chen </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aghajanyan2021massive/">Muppet: Massive Multi-task Representations With Pre-finetuning</a> Armen Aghajanyan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mahabadi2021efficient/">Compacter: Efficient Low-rank Hypercomplex Adapter Layers</a> Rabeeh Karimi Mahabadi, James Henderson, Sebastian Ruder </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/karmakar2021what/">What Do Pre-trained Code Models Know About Code?</a> Anjan Karmakar, Romain Robbes </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021ernie/">ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training For Language Understanding And Generation</a> Shuohuan Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kao2021optimized/">FLAT: An Optimized Dataflow For Mitigating Attention Bottlenecks</a> Sheng-chun Kao, Suvinay Subramanian, Gaurav Agrawal, Amir Yazdanbakhsh, Tushar Krishna </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2021training/">Training Large-scale News Recommenders With Pretrained Language Models In The Loop</a> Shitao Xiao, Zheng Liu, Yingxia Shao, Tao Di, Xing Xie </li>
     
   
     
   
     
       <li> <a href="/publications/kaliamoorthi2021distilling/">Distilling Large Language Models Into Tiny And Effective Students Using Pqrnn</a> Prabhu Kaliamoorthi, Aditya Siddhant, Edward Li, Melvin Johnson </li>
     
   
     
       <li> <a href="/publications/west2021symbolic/">Symbolic Knowledge Distillation: From General Language Models To Commonsense Models</a> Peter West et al. </li>
     
   
     
       <li> <a href="/publications/zafrir2021prune/">Prune Once For All: Sparse Pre-trained Language Models</a> Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, Moshe Wasserblat </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lei2021when/">When Attention Meets Fast Recurrence: Training Language Models With Reduced Compute</a> Tao Lei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021linear/">Linear-time Self Attention With Codeword Histogram For Efficient Recommendation</a> Yongji Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2021fine/">FILIP: Fine-grained Interactive Language-image Pre-training</a> Lewei Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021learned/">Learned Token Pruning For Transformers</a> Sehoon Kim et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021personalized/">Personalized Transformer For Explainable Recommendation</a> Lei Li, Yongfeng Zhang, Li Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2021joint/">Rocketqav2: A Joint Training Method For Dense Passage Retrieval And Passage Re-ranking</a> Ruiyang Ren et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021i/">I-BERT: Integer-only BERT Quantization</a> Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xing2022dual/">Dual Modality Prompt Tuning For Vision-language Pre-trained Model</a> Yinghui Xing et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dettmers2022matrix/">Llm.int8(): 8-bit Matrix Multiplication For Transformers At Scale</a> Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2022conditional/">Conditional Prompt Learning For Vision-language Models</a> Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2022adaptive/">Adamct: Adaptive Mixture Of Cnn-transformer For Sequential Recommendation</a> Juyong Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geiping2022training/">Cramming: Training A Language Model On A Single GPU In One Day</a> Jonas Geiping, Tom Goldstein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022efficient/">Zeroquant: Efficient And Affordable Post-training Quantization For Large-scale Transformers</a> Zhewei Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2022efficient/">Zerogen: Efficient Zero-shot Learning Via Dataset Generation</a> Jiacheng Ye et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2022black/">Black-box Tuning For Language-model-as-a-service</a> Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2022emergent/">Emergent Abilities Of Large Language Models</a> Jason Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022generative/">M6-rec: Generative Pretrained Language Models Are Open-ended Recommender Systems</a> Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/park2022lut/">LUT-GEMM: Quantized Matrix Multiplication Based On Luts For Efficient Inference In Large-scale Generative Language Models</a> Gunho Park et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2022accurate/">Smoothquant: Accurate And Efficient Post-training Quantization For Large Language Models</a> Guangxuan Xiao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2022enabling/">Enabling Multimodal Generation On CLIP Via Vision-language Knowledge Distillation</a> Wenliang Dai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2022what/">What Do Llms Know About Financial Markets? A Case Study On Reddit Market Sentiment Analysis</a> Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, Michael Bendersky </li>
     
   
     
       <li> <a href="/publications/frantar2022accurate/">GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers</a> Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh </li>
     
   
     
   
     
       <li> <a href="/publications/kurtic2022optimal/">The Optimal BERT Surgeon: Scalable And Accurate Second-order Pruning For Large Language Models</a> Eldar Kurtic et al. </li>
     
   
     
       <li> <a href="/publications/li2022contrastive/">Contrastive Decoding: Open-ended Text Generation As Optimization</a> Xiang Lisa Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2022multi/">MIST: Multi-modal Iterative Spatial-temporal Transformer For Long-form Video Question Answering</a> Difei Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022prototypical/">Protoclip: Prototypical Contrastive Language Image Pretraining</a> Delong Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hernandez2022scaling/">Scaling Laws And Interpretability Of Learning From Repeated Data</a> Danny Hernandez et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fu2022hungry/">Hungry Hungry Hippos: Towards Language Modeling With State Space Models</a> Daniel Y. Fu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022benchmark/">ELEVATER: A Benchmark And Toolkit For Evaluating Language-augmented Visual Models</a> Chunyuan Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2022new/">Mixgen: A New Multi-modal Data Augmentation</a> Xiaoshuai Hao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022effective/">Mplug: Effective And Efficient Vision-language Learning By Cross-modal Skip-connections</a> Chenliang Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/tu2022visual/">Visual Query Tuning: Towards Effective Usage Of Intermediate Representations For Parameter And Memory Efficient Transfer Learning</a> Cheng-hao Tu, Zheda Mai, Wei-lun Chao </li>
     
   
     
       <li> <a href="/publications/hsu2022language/">Language Model Compression With Weighted Low-rank Factorization</a> Yen-chang Hsu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2022visual/">Visual-language Navigation Pretraining Via Prompt-based Environmental Self-exploration</a> Xiwen Liang, Fengda Zhu, Lingling Li, Hang Xu, Xiaodan Liang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2022contrastive/">Contrastive Learning Reduces Hallucination In Conversations</a> Weiwei Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022exploring/">Exploring The Limits Of Domain-adaptive Training For Detoxifying Large-scale Language Models</a> Boxin Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2022prompt/">Hyperprompt: Prompt-based Task-conditioning Of Transformers</a> Yun He et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2022glm/">GLM-130B: An Open Bilingual Pre-trained Model</a> Aohan Zeng et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2022prompting/">Qaner: Prompting Question Answering Models For Few-shot Named Entity Recognition</a> Andy T. Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2022general/">VIMA: General Robot Manipulation With Multimodal Prompts</a> Yunfan Jiang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhai2022high/">Bytetransformer: A High-performance Transformer Boosted For Variable-length Inputs</a> Yujia Zhai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wettig2022should/">Should You Mask 15% In Masked Language Modeling?</a> Alexander Wettig, Tianyu Gao, Zexuan Zhong, Danqi Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bulat2022text/">LASP: Text-to-text Optimization For Language-aware Soft Prompting Of Vision & Language Models</a> Adrian Bulat, Georgios Tzimiropoulos </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/diao2022black/">Black-box Prompt Learning For Pre-trained Language Models</a> Shizhe Diao et al. </li>
     
   
     
       <li> <a href="/publications/lahiri2022interactive/">Interactive Code Generation Via Test-driven User-intent Formalization</a> Shuvendu K. Lahiri et al. </li>
     
   
     
       <li> <a href="/publications/buch2022revisiting/">Revisiting The "video" In Video-language Understanding</a> Shyamal Buch et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022ernie/">Ernie-search: Bridging Cross-encoder With Dual-encoder Via Self On-the-fly Distillation For Dense Passage Retrieval</a> Yuxiang Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022delta/">Delta Tuning: A Comprehensive Study Of Parameter Efficient Methods For Pre-trained Language Models</a> Ning Ding et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022accelerating/">Accelerating Attention Through Gradient-based Learned Runtime Pruning</a> Zheng Li, Soroush Ghodrati, Amir Yazdanbakhsh, Hadi Esmaeilzadeh, Mingu Kang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geva2022transformer/">Transformer Feed-forward Layers Build Predictions By Promoting Concepts In The Vocabulary Space</a> Mor Geva, Avi Caciularu, Kevin Ro Wang, Yoav Goldberg </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bavarian2022efficient/">Efficient Training Of Language Models To Fill In The Middle</a> Mohammad Bavarian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/glass2022generate/">Re2g: Retrieve, Rerank, Generate</a> Michael Glass et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022efficient/">An Efficient Memory-augmented Transformer For Knowledge-intensive NLP Tasks</a> Yuxiang Wu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2022structured/">Structured Pruning Learns Compact And Accurate Models</a> Mengzhou Xia, Zexuan Zhong, Danqi Chen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cherti2022reproducible/">Reproducible Scaling Laws For Contrastive Language-image Learning</a> Mehdi Cherti et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/barraco2022mean/">Camel: Mean Teacher Learning For Image Captioning</a> Manuele Barraco et al. </li>
     
   
     
       <li> <a href="/publications/schuster2022confident/">Confident Adaptive Language Modeling</a> Tal Schuster et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/magister2022teaching/">Teaching Small Language Models To Reason</a> Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shridhar2022distilling/">Distilling Reasoning Capabilities Into Smaller Language Models</a> Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/abdelghani2022gpt/">Gpt-3-driven Pedagogical Agents For Training Children's Curious Question-asking Skills</a> Rania Abdelghani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022clip/">CLIP-TD: CLIP Targeted Distillation For Vision-language Tasks</a> Zhecan Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ramamurthy2022is/">Is Reinforcement Learning (not) For Natural Language Processing: Benchmarks, Baselines, And Building Blocks For Natural Language Policy Optimization</a> Rajkumar Ramamurthy et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rajbhandari2022deepspeed/">Deepspeed-moe: Advancing Mixture-of-experts Inference And Training To Power Next-generation AI Scale</a> Samyam Rajbhandari et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/anil2023palm/">Palm 2 Technical Report</a> Rohan Anil et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pryzant2023automatic/">Automatic Prompt Optimization With "gradient Descent" And Beam Search</a> Reid Pryzant et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2023sur/">Sur-adapter: Enhancing Text-to-image Pre-trained Diffusion Models With Large Language Models</a> Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Lin </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/geng2023towards/">VIP5: Towards Multimodal Foundation Models For Recommendation</a> Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, Yongfeng Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023pre/">Codegeex: A Pre-trained Model For Code Generation With Multilingual Benchmarking On Humaneval-x</a> Qinkai Zheng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023diversity/">Diversity-aware Meta Visual Prompting</a> Qidong Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023pre/">Pre-train, Prompt And Recommendation: A Comprehensive Survey Of Language Modelling Paradigm Adaptations In Recommender Systems</a> Peng Liu, Lemei Zhang, Jon Atle Gulla </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/openai2023gpt/">GPT-4 Technical Report</a> Openai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023simple/">A Simple And Effective Pruning Approach For Large Language Models</a> Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yan2023human/">Human-ai Collaboration In Thematic Analysis Using Chatgpt: A User Study And Design Recommendations</a> Lixiang Yan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023parameter/">Parameter-efficient Fine-tuning Methods For Pretrained Language Models: A Critical Review And Assessment</a> Lingling Xu, Haoran Xie, Si-zhao Joe Qin, Xiaohui Tao, Fu Lee Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tunstall2023direct/">Zephyr: Direct Distillation Of LM Alignment</a> Lewis Tunstall et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alizadeh2023llm/">LLM In A Flash: Efficient Large Language Model Inference With Limited Memory</a> Keivan Alizadeh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/soman2023biomedical/">Biomedical Knowledge Graph-optimized Prompt Generation For Large Language Models</a> Karthik Soman et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wu2023clip/">Tinyclip: CLIP Distillation Via Affinity Mimicking And Weight Inheritance</a> Kan Stephen Wu et al. </li>
     
   
     
       <li> <a href="/publications/chang2023how/">Chipgpt: How Far Are We From Natural Language Hardware Design</a> Kaiyan Chang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023adaptive/">ALIP: Adaptive Language-image Pre-training With Synthetic Caption</a> Kaicheng Yang et al. </li>
     
   
     
       <li> <a href="/publications/chang2023speechprompt/">Speechprompt V2: Prompt Tuning For Speech Classification Tasks</a> Kai-wei Chang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cha2023locality/">Honeybee: Locality-enhanced Projector For Multimodal LLM</a> Junbum Cha, Wooyoung Kang, Jonghwan Mun, Byungseok Roh </li>
     
   
     
       <li> <a href="/publications/chen2023minigpt/">Minigpt-v2: Large Language Model As A Unified Interface For Vision-language Multi-task Learning</a> Jun Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2023scaling/">Longnet: Scaling Transformers To 1,000,000,000 Tokens</a> Jiayu Ding et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiang2023language/">Language Models Meet World Models: Embodied Experiences Enhance Language Models</a> Jiannan Xiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/espejel2023gpt/">GPT-3.5, GPT-4, Or BARD? Evaluating Llms Reasoning Ability In Zero-shot Setting And Performance Boosting Through Prompts</a> Jessica L√≥pez Espejel, El Hassane Ettifouri, Mahaman Sanoussi Yahaya Alassan, El Mehdi Chouham, Walid Dahhane </li>
     
   
     
       <li> <a href="/publications/mu2023learning/">Learning To Compress Prompts With Gist Tokens</a> Jesse Mu, Xiang Lisa Li, Noah Goodman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023memory/">Memory-efficient Fine-tuning Of Compressed Large Language Models Via Sub-4-bit Integer Quantization</a> Jeonghoon Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wen2023llm/">Autodroid: Llm-powered Task Automation In Android</a> Hao Wen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2023visual/">Visual-language Prompt Tuning With Knowledge-guided Context Optimization</a> Hantao Yao, Rui Zhang, Changsheng Xu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liao2023gpt/">GPT-4 Enhanced Multimodal Grounding For Autonomous Driving: Leveraging Cross-modal Attention With Large Language Models</a> Haicheng Liao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023cheap/">Cheap And Quick: Efficient Vision-language Instruction Tuning For Large Language Models</a> Gen Luo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023heavy/">H\(_2\)O: Heavy-hitter Oracle For Efficient Generative Inference Of Large Language Models</a> Zhenyu Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gilardi2023chatgpt/">Chatgpt Outperforms Crowd-workers For Text-annotation Tasks</a> Fabrizio Gilardi, Meysam Alizadeh, Ma√´l Kubli </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lehman2023do/">Do We Still Need Clinical Language Models?</a> Eric Lehman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frantar2023massive/">Sparsegpt: Massive Language Models Can Be Accurately Pruned In One-shot</a> Elias Frantar, Dan Alistarh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2023read/">Read-only Prompt Optimization For Vision-language Few-shot Learning</a> Dongjun Lee et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2023distilled/">Distilled GPT For Source Code Summarization</a> Chia-yi Su, Collin Mcmillan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hsieh2023distilling/">Distilling Step-by-step! Outperforming Larger Language Models With Less Training Data And Smaller Model Sizes</a> Cheng-yu Hsieh et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/han2023effective/">E^2VPT: An Effective And Efficient Approach For Visual Prompt Tuning</a> Cheng Han et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023dipping/">Dipping Plms Sauce: Bridging Structure And Text For Effective Knowledge Graph Completion Via Conditional Soft Prompting</a> Chen Chen, Yufei Wang, Aixin Sun, Bing Li, Kwok-yan Lam </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023comprehensive/">MME: A Comprehensive Evaluation Benchmark For Multimodal Large Language Models</a> Chaoyou Fu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023adapting/">Adapting Large Language Models By Integrating Collaborative Semantics For Recommendation</a> Bowen Zheng et al. </li>
     
   
     
   
     
       <li> <a href="/publications/peng2023reinventing/">RWKV: Reinventing Rnns For The Transformer Era</a> Bo Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2023bad/">Bad Actor, Good Advisor: Exploring The Role Of Large Language Models In Fake News Detection</a> Beizhe Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2023knowledge/">Minillm: Knowledge Distillation Of Large Language Models</a> Yuxian Gu, Li Dong, Furu Wei, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2023robots/">Robots That Ask For Help: Uncertainty Alignment For Large Language Model Planners</a> Allen Z. Ren et al. </li>
     
   
     
       <li> <a href="/publications/maatouk2023large/">Large Language Models For Telecom: Forthcoming Impact On The Industry</a> Ali Maatouk, Nicola Piovesan, Fadhel Ayed, Antonio De Domenico, Merouane Debbah </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sha2023large/">Languagempc: Large Language Models As Decision Makers For Autonomous Driving</a> Hao Sha et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lopezlira2023can/">Can Chatgpt Forecast Stock Price Movements? Return Predictability And Large Language Models</a> Alejandro Lopez-lira, Yuehua Tang </li>
     
   
     
   
     
       <li> <a href="/publications/gu2023linear/">Mamba: Linear-time Sequence Modeling With Selective State Spaces</a> Albert Gu, Tri Dao </li>
     
   
     
       <li> <a href="/publications/jiang2023mistral/">Mistral 7B</a> Albert Q. Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023teaching/">Teaching Large Language Models To Self-debug</a> Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, Denny Zhou </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023llm/">Llm-pruner: On The Structural Pruning Of Large Language Models</a> Xinyin Ma, Gongfan Fang, Xinchao Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023delving/">Delving Into Multimodal Prompting For Fine-grained Visual Classification</a> Xin Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023targeted/">Universalner: Targeted Distillation From Large Language Models For Open Named Entity Recognition</a> Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung Poon </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023is/">Is Chatgpt Good At Search? Investigating Large Language Models As Re-ranking Agents</a> Weiwei Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wei2023large/">Llmrec: Large Language Models With Graph Augmentation For Recommendation</a> Wei Wei et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kang2023do/">Do Llms Understand User Preferences? Evaluating Llms On User Rating Prediction</a> Wang-cheng Kang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bezirhan2023automated/">Automated Reading Passage Generation With Openai's Large Language Model</a> Ummugul Bezirhan, Matthias Von Davier </li>
     
   
     
   
     
       <li> <a href="/publications/lialin2023scaling/">Scaling Down To Scale Up: A Guide To Parameter-efficient Fine-tuning</a> Vladislav Lialin, Vijeta Deshpande, Xiaowei Yao, Anna Rumshisky </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2023flashattention/">Flashattention-2: Faster Attention With Better Parallelism And Work Partitioning</a> Tri Dao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dettmers2023efficient/">Qlora: Efficient Finetuning Of Quantized Llms</a> Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer </li>
     
   
     
       <li> <a href="/publications/yu2023rlhf/">RLHF-V: Towards Trustworthy Mllms Via Behavior Alignment From Fine-grained Correctional Human Feedback</a> Tianyu Yu et al. </li>
     
   
     
       <li> <a href="/publications/dettmers2023sparse/">Spqr: A Sparse-quantized Representation For Near-lossless LLM Weight Compression</a> Tim Dettmers et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/carta2023grounding/">Grounding Large Language Models In Interactive Environments With Online Reinforcement Learning</a> Thomas Carta et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2023fast/">Powerinfer: Fast Large Language Model Serving With A Consumer-grade GPU</a> Yixin Song, Zeyu Mi, Haotong Xie, Haibo Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sheng2023high/">Flexgen: High-throughput Generative Inference Of Large Language Models With A Single GPU</a> Ying Sheng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2023efficient/">Efficient And Effective Text Encoding For Chinese Llama And Alpaca</a> Yiming Cui, Ziqing Yang, Xin Yao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023llm/">Llm-eval: Unified Multi-dimensional Automatic Evaluation For Open-domain Conversations With Large Language Models</a> Yen-ting Lin, Yun-nung Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023survey/">A Survey On Model Compression For Large Language Models</a> Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023large/">Large Language Model As Attributed Training Data Generator: A Tale Of Diversity And Bias</a> Yue Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2023when/">When Prompt-based Incremental Learning Does Not Meet Strong Pretraining</a> Yu-ming Tang, Yi-xing Peng, Wei-shi Zheng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2023representation/">Representation Learning With Large Language Models For Recommendation</a> Xubin Ren et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/biderman2023emergent/">Emergent And Predictable Memorization In Large Language Models</a> Stella Biderman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/samsi2023from/">From Words To Watts: Benchmarking The Energy Costs Of Large Language Model Inference</a> Siddharth Samsi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gholami2023do/">Do Generative Large Language Models Need Billions Of Parameters?</a> Sia Gholami, Marwan Omar </li>
     
   
     
   
     
       <li> <a href="/publications/gholami2023can/">Can A Student Large Language Model Perform As Well As It's Teacher?</a> Sia Gholami, Marwan Omar </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023automl/">Automl-gpt: Automatic Machine Learning With GPT</a> Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, Mingyuan Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023extending/">Extending Context Window Of Large Language Models Via Positional Interpolation</a> Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023radiology/">R2gengpt: Radiology Report Generation With Frozen Llms</a> Zhanyu Wang, Lingqiao Liu, Lei Wang, Luping Zhou </li>
     
   
     
       <li> <a href="/publications/lin2023pushing/">Pushing Large Language Models To The 6G Edge: Vision, Challenges, And Opportunities</a> Zheng Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2023large/">Large Language Models Are Effective Text Rankers With Pairwise Ranking Prompting</a> Zhen Qin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2024large/">Large Language Models Meet Collaborative Filtering: An Efficient All-round Llm-based Recommender System</a> Sein Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2024linear/">Linrec: Linear Attention Mechanism For Long-term Sequential Recommender Systems</a> Langming Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2024pixart/">Pixart-\sigma: Weak-to-strong Training Of Diffusion Transformer For 4K Text-to-image Generation</a> Junsong Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hong2024monolithic/">ORPO: Monolithic Preference Optimization Without Reference Model</a> Jiwoo Hong, Noah Lee, James Thorne </li>
     
   
     
   
     
       <li> <a href="/publications/chen2024bge/">BGE M3-embedding: Multi-lingual, Multi-functionality, Multi-granularity Text Embeddings Through Self-knowledge Distillation</a> Jianlv Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geminiteam2024gemini/">Gemini 1.5: Unlocking Multimodal Understanding Across Millions Of Tokens Of Context</a> Gemini Team et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gemmateam2024gemma/">Gemma 2: Improving Open Language Models At A Practical Size</a> Gemma Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salemi2024optimization/">Optimization Methods For Personalizing Large Language Models Through Retrieval Augmentation</a> Alireza Salemi, Surya Kallumadi, Hamed Zamani </li>
     
   
     
       <li> <a href="/publications/gholami2024ai/">AI And Memory Wall</a> Amir Gholami et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2024understanding/">Understanding Llms: A Comprehensive Overview From Training To Inference</a> Yiheng Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2024unified/">Llamafactory: Unified Efficient Fine-tuning Of 100+ Language Models</a> Yaowei Zheng et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2024data/">Data-efficient Fine-tuning For Llm-based Recommendation</a> Xinyu Lin et al. </li>
     
   
     
       <li> <a href="/publications/zhang2024generalized/">Mgte: Generalized Long-context Text Representation And Reranking Models For Multilingual Text Retrieval</a> Xin Zhang et al. </li>
     
   
     
       <li> <a href="/publications/wang2024searching/">Searching For Best Practices In Retrieval-augmented Generation</a> Xiaohua Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2024pushing/">Billm: Pushing The Limit Of Post-training Quantization For Llms</a> Wei Huang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/parthasarathy2024ultimate/">The Ultimate Guide To Fine-tuning Llms From Basics To Breakthroughs: An Exhaustive Review Of Technologies, Research, Best Practices, Applied Research Challenges And Opportunities</a> Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2024era/">The Era Of 1-bit Llms: All Large Language Models Are In 1.58 Bits</a> Shuming Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ EMNLP <a id="EMNLP"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2024monitoring/">Monitoring Ai-modified Content At Scale: A Case Study On The Impact Of Chatgpt On AI Conference Peer Reviews</a> Weixin Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Ethics and Bias <a id="Ethics and Bias"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xing2016topic/">Topic Aware Neural Response Generation</a> Chen Xing et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/agrawal2017just/">Don't Just Assume; Look And Answer: Overcoming Priors For Visual Question Answering</a> Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2017steering/">Steering Output Style And Topic In Neural Response Generation</a> Di Wang, Nebojsa Jojic, Chris Brockett, Eric Nyberg </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caccia2018language/">Language Gans Falling Short</a> Massimo Caccia et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2018affect/">An Affect-rich Neural Conversational Model With Biased Attention And Weighted Cross-entropy Loss</a> Peixiang Zhong, Di Wang, Chunyan Miao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tevet2018evaluating/">Evaluating Text Gans As Language Models</a> Guy Tevet, Gavriel Habib, Vered Shwartz, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tandon2018reasoning/">Reasoning About Actions And State Changes By Injecting Commonsense Knowledge</a> Niket Tandon et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2018toward/">Toward Diverse Text Generation With Inverse Reinforcement Learning</a> Zhan Shi, Xinchi Chen, Xipeng Qiu, Xuanjing Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cer2018universal/">Universal Sentence Encoder</a> Daniel Cer et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hoover2019visual/">Exbert: A Visual Analysis Tool To Explore Learned Representations In Transformers Models</a> Benjamin Hoover, Hendrik Strobelt, Sebastian Gehrmann </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lake2019human/">Human Few-shot Learning Of Compositional Instructions</a> Brenden M. Lake, Tal Linzen, Marco Baroni </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mahabadi2019end/">End-to-end Bias Mitigation By Modelling Biases In Corpora</a> Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vig2019multiscale/">A Multiscale Visualization Of Attention In The Transformer Model</a> Jesse Vig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jain2019attention/">Attention Is Not Explanation</a> Sarthak Jain, Byron C. Wallace </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schmidt2019generalization/">Generalization In Generation: A Closer Look At Exposure Bias</a> Florian Schmidt </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/stahlberg2019nmt/">On NMT Search Errors And Model Errors: Cat Got Your Tongue?</a> Felix Stahlberg, Bill Byrne </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sakaguchi2019adversarial/">Winogrande: An Adversarial Winograd Schema Challenge At Scale</a> Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lakew2019controlling/">Controlling The Output Length Of Neural Machine Translation</a> Surafel Melaku Lakew, Mattia Di Gangi, Marcello Federico </li>
     
   
     
       <li> <a href="/publications/schwartz2019inducing/">Inducing Brain-relevant Bias In Natural Language Processing Models</a> Dan Schwartz, Mariya Toneva, Leila Wehbe </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pruthi2019learning/">Learning To Deceive With Attention-based Explanations</a> Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, Zachary C. Lipton </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019semantically/">Semantically Conditioned Dialog Response Generation Via Hierarchical Disentangled Self-attention</a> Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vig2019visualizing/">Visualizing Attention In Transformer-based Language Representation Models</a> Jesse Vig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mccoy2019berts/">Berts Of A Feather Do Not Generalize Together: Large Variability In Generalization Across Models With Similar Test Set Performance</a> R. Thomas Mccoy, Junghyun Min, Tal Linzen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wallace2019universal/">Universal Adversarial Triggers For Attacking And Analyzing NLP</a> Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2019visual/">Visual Entailment: A Novel Task For Fine-grained Image Understanding</a> Ning Xie, Farley Lai, Derek Doran, Asim Kadav </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/saunders2020reducing/">Reducing Gender Bias In Neural Machine Translation As A Domain Adaptation Problem</a> Danielle Saunders, Bill Byrne </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2020recipes/">Recipes For Safety In Open-domain Chatbots</a> Jing Xu et al. </li>
     
   
     
       <li> <a href="/publications/k%C3%B6bis2020artificial/">Artificial Intelligence Versus Maya Angelou: Experimental Evidence That People Cannot Differentiate Ai-generated From Human-written Poetry</a> Nils K√∂bis, Luca Mossink </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/smith2020can/">Can You Put It All Together: Evaluating Conversational Agents' Ability To Blend Skills</a> Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, Y-lan Boureau </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parcalabescu2020seeing/">Seeing Past Words: Testing The Cross-modal Capabilities Of Pretrained V&L Models On Counting Tasks</a> Letitia Parcalabescu, Albert Gatt, Anette Frank, Iacer Calixto </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2020modifying/">Modifying Memories In Transformer Models</a> Chen Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/verga2020facts/">Facts As Experts: Adaptable And Interpretable Neural Memory Over Symbolic Knowledge</a> Pat Verga, Haitian Sun, Livio Baldini Soares, William W. Cohen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/webster2020measuring/">Measuring And Reducing Gendered Correlations In Pre-trained Models</a> Kellie Webster et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sellam2020learning/">BLEURT: Learning Robust Metrics For Text Generation</a> Thibault Sellam, Dipanjan Das, Ankur P. Parikh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/krause2020generative/">Gedi: Generative Discriminator Guided Sequence Generation</a> Ben Krause et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2020just/">Just Ask: Learning To Answer Questions From Millions Of Narrated Videos</a> Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020unqovering/">Unqovering Stereotyping Biases Via Underspecified Questions</a> Tao Li, Tushar Khot, Daniel Khashabi, Ashish Sabharwal, Vivek Srikumar </li>
     
   
     
   
     
       <li> <a href="/publications/macavaney2020analyzing/">ABNIRML: Analyzing The Behavior Of Neural IR Models</a> Sean Macavaney, Sergey Feldman, Nazli Goharian, Doug Downey, Arman Cohan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jain2020learning/">Learning To Faithfully Rationalize By Construction</a> Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, Byron C. Wallace </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tenney2020language/">The Language Interpretability Tool: Extensible, Interactive Visualizations And Analysis For NLP Models</a> Ian Tenney et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mcguffie2020radicalization/">The Radicalization Risks Of GPT-3 And Advanced Neural Language Models</a> Kris Mcguffie, Alex Newhouse </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2020reading/">Reclor: A Reading Comprehension Dataset Requiring Logical Reasoning</a> Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020mitigating/">Mitigating Gender Bias For Neural Dialogue Generation With Adversarial Learning</a> Haochen Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/roller2020open/">Open-domain Conversational Agents: Current Progress, Open Problems, And Future Directions</a> Stephen Roller et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020modelling/">Modelling Hierarchical Structure Between Dialogue Policy And Natural Language Generator With Option Framework For Task-oriented Dialogue System</a> Jianhong Wang, Yuan Zhang, Tae-kyun Kim, Yunjie Gu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021generate/">Generate Natural Language Explanations For Recommendation</a> Hanxiong Chen, Xu Chen, Shaoyun Shi, Yongfeng Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kirk2021bias/">Bias Out-of-the-box: An Empirical Analysis Of Intersectional Occupational Biases In Popular Generative Language Models</a> Hannah Kirk et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sheng2021revealing/">Revealing Persona Biases In Dialogue Systems</a> Emily Sheng, Josh Arnold, Zhou Yu, Kai-wei Chang, Nanyun Peng </li>
     
   
     
   
     
       <li> <a href="/publications/sheng2021societal/">Societal Biases In Language Generation: Progress And Challenges</a> Emily Sheng, Kai-wei Chang, Premkumar Natarajan, Nanyun Peng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kassner2021multilingual/">Multilingual LAMA: Investigating Knowledge In Multilingual Pretrained Language Models</a> Nora Kassner, Philipp Dufter, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2021causal/">Causal Attention For Vision-language Tasks</a> Xu Yang, Hanwang Zhang, Guojun Qi, Jianfei Cai </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021using/">Using Adversarial Attacks To Reveal The Statistical Bias In Machine Reading Comprehension Models</a> Jieyu Lin, Jiajie Zou, Nai Ding </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rae2021scaling/">Scaling Language Models: Methods, Analysis & Insights From Training Gopher</a> Jack W. Rae et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/solaiman2021process/">Process For Adapting Language Models To Society (PALMS) With Values-targeted Datasets</a> Irene Openai Solaiman, Christy Openai Dennison </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021ai/">AI Chains: Transparent And Controllable Human-ai Interaction By Chaining Large Language Model Prompts</a> Tongshuang Wu, Michael Terry, Carrie J. Cai </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/welbl2021challenges/">Challenges In Detoxifying Language Models</a> Johannes Welbl et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dancette2021beyond/">Beyond Question-based Biases: Assessing Multimodal Shortcut Learning In Visual Question Answering</a> Corentin Dancette, Remi Cadene, Damien Teney, Matthieu Cord </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schramowski2021large/">Large Pre-trained Language Models Contain Human-like Biases Of What Is Right And Wrong To Do</a> Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A. Rothkopf, Kristian Kersting </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mahabadi2021variational/">Variational Information Bottleneck For Effective Low-resource Fine-tuning</a> Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lauscher2021sustainable/">Sustainable Modular Debiasing Of Language Models</a> Anne Lauscher, Tobias L√ºken, Goran Glava≈° </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tamkin2021understanding/">Understanding The Capabilities, Limitations, And Societal Impact Of Large Language Models</a> Alex Tamkin, Miles Brundage, Jack Clark, Deep Ganguli </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021fine/">Fine-grained Style Control In Transformer-based Text-to-speech Synthesis</a> Li-wei Chen, Alexander Rudnicky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bowman2021what/">What Will It Take To Fix Benchmarking In Natural Language Understanding?</a> Samuel R. Bowman, George E. Dahl </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/barikeri2021real/">Redditbias: A Real-world Resource For Bias Evaluation And Debiasing Of Conversational Language Models</a> Soumya Barikeri, Anne Lauscher, Ivan Vuliƒá, Goran Glava≈° </li>
     
   
     
   
     
       <li> <a href="/publications/liu2021mitigating/">Mitigating Political Bias In Language Models Through Reinforced Calibration</a> Ruibo Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singhal2022large/">Large Language Models Encode Clinical Knowledge</a> Karan Singhal et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2022do/">Do Language Models Plagiarize?</a> Jooyoung Lee, Thai Le, Jinghui Chen, Dongwon Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dinh2022language/">LIFT: Language-interfaced Fine-tuning For Non-language Machine Learning Tasks</a> Tuan Dinh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cho2022dall/">Dall-eval: Probing The Reasoning Skills And Social Biases Of Text-to-image Generation Models</a> Jaemin Cho, Abhay Zala, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sammani2022nlx/">NLX-GPT: A Model For Natural Language Explanations In Vision And Vision-language Tasks</a> Fawaz Sammani, Tanmoy Mukherjee, Nikos Deligiannis </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jones2022capturing/">Capturing Failures Of Large Language Models Via Human Cognitive Biases</a> Erik Jones, Jacob Steinhardt </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/si2022prompting/">Prompting GPT-3 To Be Reliable</a> Chenglei Si et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022exploring/">Exploring The Limits Of Domain-adaptive Training For Detoxifying Large-scale Language Models</a> Boxin Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/workshop2022open/">BLOOM: A 176b-parameter Open-access Multilingual Language Model</a> Bigscience Workshop et al. </li>
     
   
     
       <li> <a href="/publications/zhang2022revisiting/">Revisiting End-to-end Speech-to-text Translation From Scratch</a> Biao Zhang, Barry Haddow, Rico Sennrich </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022prompt/">Prompt Distribution Learning</a> Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, Xinmei Tian </li>
     
   
     
       <li> <a href="/publications/creswell2022selection/">Selection-inference: Exploiting Large Language Models For Interpretable Logical Reasoning</a> Antonia Creswell, Murray Shanahan, Irina Higgins </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/glaese2022improving/">Improving Alignment Of Dialogue Agents Via Targeted Human Judgements</a> Amelia Glaese et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/srivastava2022beyond/">Beyond The Imitation Game: Quantifying And Extrapolating The Capabilities Of Language Models</a> Aarohi Shammie Srivastava et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chowdhery2022scaling/">Palm: Scaling Language Modeling With Pathways</a> Aakanksha Chowdhery et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2022holistic/">Holistic Evaluation Of Language Models</a> Percy Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shaikh2022second/">On Second Thought, Let's Not Think Step By Step! Bias And Toxicity In Zero-shot Reasoning</a> Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, Diyi Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/carlini2022quantifying/">Quantifying Memorization Across Neural Language Models</a> Nicholas Carlini et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2022shortcut/">Shortcut Learning Of Large Language Models In Natural Language Understanding</a> Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, Xia Hu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jakesch2022human/">Human Heuristics For Ai-generated Language Are Flawed</a> Maurice Jakesch, Jeffrey Hancock, Mor Naaman </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022generating/">Emphi: Generating Empathetic Responses With Human-like Intents</a> Mao Yan Chen, Siheng Li, Yujiu Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2022task/">Task Residual For Tuning Vision-language Models</a> Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, Xinchao Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/magee2022structured/">Structured Like A Language Model: Analysing AI As An Automated Subject</a> Liam Magee, Vanicka Arora, Luke Munn </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/susnjak2022end/">Chatgpt: The End Of Online Exam Integrity?</a> Teo Susnjak </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thoppilan2022language/">Lamda: Language Models For Dialog Applications</a> Romal Thoppilan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mirowski2022co/">Co-writing Screenplays And Theatre Scripts With Language Models: An Evaluation By Industry Professionals</a> Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, Richard Evans </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/anil2023palm/">Palm 2 Technical Report</a> Rohan Anil et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023may/">Starcoder: May The Source Be With You!</a> Raymond Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sohail2023decoding/">Decoding Chatgpt: A Taxonomy Of Existing Research, Current Challenges, And Possible Future Directions</a> Shahab Saquib Sohail et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sanner2023large/">Large Language Models Are Competitive Near Cold-start Recommenders For Language- And Item-based Preferences</a> Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, Lucas Dixon </li>
     
   
     
   
     
       <li> <a href="/publications/communication2023multilingual/">Seamless: Multilingual Expressive And Streaming Speech Translation</a> Seamless Communication et al. </li>
     
   
     
       <li> <a href="/publications/kr%C3%BCgel2023moral/">The Moral Authority Of Chatgpt</a> Sebastian Kr√ºgel, Andreas Ostermaier, Matthias Uhl </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dhuliawala2023chain/">Chain-of-verification Reduces Hallucination In Large Language Models</a> Shehzaad Dhuliawala et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lapid2023open/">Open Sesame! Universal Black Box Jailbreaking Of Large Language Models</a> Raz Lapid, Ron Langberg, Moshe Sipper </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/staab2023beyond/">Beyond Memorization: Violating Privacy Via Inference With Large Language Models</a> Robin Staab, Mark Vero, Mislav Balunoviƒá, Martin Vechev </li>
     
   
     
       <li> <a href="/publications/liao2023ai/">AI Transparency In The Age Of Llms: A Human-centered Research Roadmap</a> Q. Vera Liao, Jennifer Wortman Vaughan </li>
     
   
     
       <li> <a href="/publications/peng2023prompting/">Prompting The Hidden Talent Of Web-scale Speech Models For Zero-shot Task Generalization</a> Puyuan Peng, Brian Yan, Shinji Watanabe, David Harwath </li>
     
   
     
       <li> <a href="/publications/liao2023designerly/">Designerly Understanding: Information Needs For Model Transparency To Support Design Ideation For Ai-powered User Experience</a> Q. Vera Liao, Hariharan Subramonyam, Jennifer Wang, Jennifer Wortman Vaughan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023llama/">Llama-adapter V2: Parameter-efficient Visual Instruction Model</a> Peng Gao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hacker2023regulating/">Regulating Chatgpt And Other Large Generative AI Models</a> Philipp Hacker, Andreas Engel, Marco Mauer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mckenna2023sources/">Sources Of Hallucination By Large Language Models On Inference Tasks</a> Nick Mckenna et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dechoudhury2023benefits/">Benefits And Harms Of Large Language Models In Digital Mental Health</a> Munmun De Choudhury, Sachin R. Pendse, Neha Kumar </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023text/">Text Matching Improves Sequential Recommendation By Reducing Popularity Biases</a> Zhenghao Liu et al. </li>
     
   
     
       <li> <a href="/publications/dehghani2023scaling/">Scaling Vision Transformers To 22 Billion Parameters</a> Mostafa Dehghani et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sharma2023towards/">Towards Understanding Sycophancy In Language Models</a> Mrinank Sharma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahmad2023creating/">Creating Trustworthy Llms: Dealing With Hallucinations In Healthcare AI</a> Muhammad Aurangzeb Ahmad, Ilker Yaramis, Taposh Dutta Roy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023scaffolding/">Selenite: Scaffolding Online Sensemaking With Comprehensive Overviews Elicited From Large Language Models</a> Michael Xieyang Liu et al. </li>
     
   
     
       <li> <a href="/publications/laskar2023systematic/">A Systematic Study And Comprehensive Evaluation Of Chatgpt On Benchmark Datasets</a> Md Tahmid Rahman Laskar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jakesch2023co/">Co-writing With Opinionated Language Models Affects Users' Views</a> Maurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalmanson, Mor Naaman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yan2023practical/">Practical And Ethical Challenges Of Large Language Models In Education: A Systematic Scoping Review</a> Lixiang Yan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023judging/">Judging Llm-as-a-judge With Mt-bench And Chatbot Arena</a> Lianmin Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salewski2023impersonation/">In-context Impersonation Reveals Large Language Models' Strengths And Biases</a> Leonard Salewski, Stephan Alaniz, Isabel Rio-torto, Eric Schulz, Zeynep Akata </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023comprehensive/">Mvbench: A Comprehensive Multi-modal Video Understanding Benchmark</a> Kunchang Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yin2023language/">LAMM: Language-assisted Multi-modal Instruction-tuning Dataset, Framework, And Benchmark</a> Zhenfei Yin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/niklaus2023multi/">LEXTREME: A Multi-lingual And Multi-task Benchmark For The Legal Domain</a> Joel Niklaus et al. </li>
     
   
     
       <li> <a href="/publications/schneider2023towards/">Towards Llm-based Autograding For Short Textual Answers</a> Johannes Schneider, Bernd Schenk, Christina Niklaus </li>
     
   
     
       <li> <a href="/publications/hartmann2023political/">The Political Ideology Of Conversational AI: Converging Evidence On Chatgpt's Pro-environmental, Left-libertarian Orientation</a> Jochen Hartmann, Jasper Schwenzow, Maximilian Witte </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023is/">Is Chatgpt Fair For Recommendation? Evaluating Fairness In Large Language Model Recommendation</a> Jizhi Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xue2023bias/">Bias And Fairness In Chatbots: An Overview</a> Jintang Xue et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023robustness/">On The Robustness Of Chatgpt: An Adversarial And Out-of-distribution Perspective</a> Jindong Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023ethical/">Ethical Chatgpt: Concerns, Challenges, And Commandments</a> Jianlong Zhou, Heimo M√ºller, Andreas Holzinger, Fang Chen </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023how/">How Can Recommender Systems Benefit From Large Language Models: A Survey</a> Jianghao Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/prather2023robots/">The Robots Are Here: Navigating The Generative AI Revolution In Computing Education</a> James Prather et al. </li>
     
   
     
   
     
       <li> <a href="/publications/koco%C5%842023jack/">Chatgpt: Jack Of All Trades, Master Of None</a> Jan Koco≈Ñ et al. </li>
     
   
     
   
     
       <li> <a href="/publications/m%C3%B6kander2023auditing/">Auditing Large Language Models: A Three-layered Approach</a> Jakob M√∂kander, Jonas Schuett, Hannah Rose Kirk, Luciano Floridi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lauren%C3%A7on2023bigscience/">The Bigscience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset</a> Hugo Lauren√ßon et al. </li>
     
   
     
       <li> <a href="/publications/touvron2023llama/">Llama 2: Open Foundation And Fine-tuned Chat Models</a> Hugo Touvron et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/augenstein2023factuality/">Factuality Challenges In The Era Of Large Language Models</a> Isabelle Augenstein et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peters2023large/">Large Language Models Can Infer Psychological Dispositions Of Social Media Users</a> Heinrich Peters, Sandra Matz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023privacy/">Privacy In Large Language Models: Attacks, Defenses And Future Directions</a> Haoran Li et al. </li>
     
   
     
       <li> <a href="/publications/rao2023can/">Can Chatgpt Assess Human Personalities? A General Evaluation Framework</a> Haocong Rao, Cyril Leung, Chunyan Miao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023chatgpt/">Chatgpt For Shaping The Future Of Dentistry: The Potential Of Multi-modal Large Language Model</a> Hanyao Huang et al. </li>
     
   
     
       <li> <a href="/publications/fei2023unifying/">Lasuie: Unifying Information Extraction With Latent Adaptive Structure-aware Generative Language Model</a> Hao Fei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023explainability/">Explainability For Large Language Models: A Survey</a> Haiyan Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/almeida2023exploring/">Exploring The Psychology Of Llms' Moral And Legal Reasoning</a> Guilherme F. C. F. Almeida, Jos√© Luiz Nunes, Neele Engelmann, Alex Wiegmann, Marcelo De Ara√∫jo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kotek2023gender/">Gender Bias And Stereotypes In Large Language Models</a> Hadas Kotek, Rikker Dockum, David Q. Sun </li>
     
   
     
   
     
       <li> <a href="/publications/serapiogarc%C3%ADa2023personality/">Personality Traits In Large Language Models</a> Greg Serapio-garc√≠a et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/suri2023do/">Do Large Language Models Show Decision Heuristics Similar To Humans? A Case Study Using GPT-3.5</a> Gaurav Suri, Lily R. Slater, Ali Ziaee, Morgan Nguyen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mollick2023assigning/">Assigning AI: Seven Approaches For Students, With Prompts</a> Ethan Mollick, Lilach Mollick </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ferrara2023should/">Should Chatgpt Be Biased? Challenges And Risks Of Bias In Large Language Models</a> Emilio Ferrara </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lenat2023getting/">Getting From Generative AI To Trustworthy AI: What Llms Might Learn From Cyc</a> Doug Lenat, Gary Marcus </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jang2023gpt/">GPT-4 Can Pass The Korean National Licensing Examination For Korean Medicine Doctors</a> Dongyeop Jang, Tae-rim Yun, Choong-yeol Lee, Young-kyu Kwon, Chang-eop Kim </li>
     
   
     
   
     
       <li> <a href="/publications/sun2023principle/">Principle-driven Self-alignment Of Language Models From Scratch With Minimal Human Supervision</a> Zhiqing Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ganguli2023capacity/">The Capacity For Moral Self-correction In Large Language Models</a> Deep Ganguli et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vanveen2023adapted/">Adapted Large Language Models Can Outperform Medical Experts In Clinical Text Summarization</a> Dave Van Veen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/leiter2023meta/">Chatgpt: A Meta-analysis After 2.5 Months</a> Christoph Leiter et al. </li>
     
   
     
       <li> <a href="/publications/chuang2023debiasing/">Debiasing Vision-language Models Via Biased Prompts</a> Ching-yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, Stefanie Jegelka </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chiang2023can/">Can Large Language Models Be An Alternative To Human Evaluations?</a> Cheng-han Chiang, Hung-yi Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rastogi2023supporting/">Supporting Human-ai Collaboration In Auditing Llms With Llms</a> Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, Harsha Nori, Saleema Amershi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lund2023chatgpt/">Chatgpt And A New Academic Reality: Artificial Intelligence-written Research Papers And The Ethics Of The Large Language Models In Scholarly Publishing</a> Brady Lund et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2023how/">How Close Is Chatgpt To Human Experts? Comparison Corpus, Evaluation, And Detection</a> Biyang Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fecher2023friend/">Friend Or Foe? Exploring The Implications Of Large Language Models On The Science System</a> Benedikt Fecher, Marcel Hebing, Melissa Laufer, J√∂rg Pohle, Fabian Sofsky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/toma2023clinical/">Clinical Camel: An Open Expert-level Medical Language Model With Dialogue-based Knowledge Encoding</a> Augustin Toma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023short/">A Short Survey Of Viewing Large Language Models In Legal Aspect</a> Zhongxiang Sun </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pal2023med/">Med-halt: Medical Domain Hallucination Test For Large Language Models</a> Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caines2023application/">On The Application Of Large Language Models For Language Teaching And Assessment Technology</a> Andrew Caines et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liesenfeld2023opening/">Opening Up Chatgpt: Tracking Openness, Transparency, And Accountability In Instruction-tuned Text Generators</a> Andreas Liesenfeld, Alianda Lopez, Mark Dingemanse </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deshpande2023toxicity/">Toxicity In Chatgpt: Analyzing Persona-assigned Language Models</a> Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/borji2023categorical/">A Categorical Archive Of Chatgpt Failures</a> Ali Borji </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sison2023more/">Chatgpt: More Than A Weapon Of Mass Deception, Ethical Challenges And Responses From The Human-centered Artificial Intelligence (HCAI) Perspective</a> Alejo Jose G. Sison, Marco Tulio Daza, Roberto Gozalo-brizuela, Eduardo C. Garrido-merch√°n </li>
     
   
     
       <li> <a href="/publications/lopezlira2023can/">Can Chatgpt Forecast Stock Price Movements? Return Predictability And Large Language Models</a> Alejandro Lopez-lira, Yuehua Tang </li>
     
   
     
       <li> <a href="/publications/petrov2023language/">Language Model Tokenizers Introduce Unfairness Between Languages</a> Aleksandar Petrov, Emanuele La Malfa, Philip H. S. Torr, Adel Bibi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023unveiling/">Unveiling Security, Privacy, And Ethical Concerns Of Chatgpt</a> Xiaodong Wu, Ran Duan, Jianbing Ni </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2023bias/">Bias Of Ai-generated Content: An Examination Of News Produced By Large Language Models</a> Xiao Fang et al. </li>
     
   
     
       <li> <a href="/publications/zhan2023deceptive/">Deceptive AI Ecosystems: The Case Of Chatgpt</a> Xiao Zhan, Yifan Xu, Stefan Sarkadi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2023gpt/">GPT Detectors Are Biased Against Non-native English Writers</a> Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou </li>
     
   
     
       <li> <a href="/publications/li2023dark/">The Dark Side Of Chatgpt: Legal And Ethical Challenges From Stochastic Parrots And Hallucination</a> Zihao Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chang2023language/">Language Model Behavior: A Comprehensive Survey</a> Tyler A. Chang, Benjamin K. Bergen </li>
     
   
     
   
     
       <li> <a href="/publications/chakrabarty2023art/">Art Or Artifice? Large Language Models And The False Promise Of Creativity</a> Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, Chien-sheng Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hariri2023unlocking/">Unlocking The Potential Of Chatgpt: A Comprehensive Exploration Of Its Applications, Advantages, Limitations, And Future Directions In Natural Language Processing</a> Walid Hariri </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhuo2023red/">Red Teaming Chatgpt Via Jailbreaking: Bias, Robustness, Reliability And Toxicity</a> Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/naous2023having/">Having Beer After Prayer? Measuring Cultural Bias In Large Language Models</a> Tarek Naous, Michael J. Ryan, Alan Ritter, Wei Xu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gill2023transformative/">Transformative Effects Of Chatgpt On Modern Education: Emerging Era Of AI Chatbots</a> Sukhpal Singh Gill et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wan2023is/">"kelly Is A Warm Person, Joseph Is A Role Model": Gender Biases In Llm-generated Reference Letters</a> Yixin Wan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wen2023knowledge/">Mindmap: Knowledge Graph Prompting Sparks Graph Of Thoughts In Large Language Models</a> Yilin Wen, Zifeng Wang, Jimeng Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023g/">G-eval: NLG Evaluation Using GPT-4 With Better Human Alignment</a> Yang Liu et al. </li>
     
   
     
       <li> <a href="/publications/liu2023trustworthy/">Trustworthy Llms: A Survey And Guideline For Evaluating Large Language Models' Alignment</a> Yang Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023mental/">Mental-llm: Leveraging Large Language Models For Mental Health Prediction Via Online Text Data</a> Xuhai Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023large/">Large Language Model As Attributed Training Data Generator: A Tale Of Diversity And Bias</a> Yue Yu et al. </li>
     
   
     
       <li> <a href="/publications/wang2023aligning/">Aligning Large Language Models With Human: A Survey</a> Yufei Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023textbooks/">Textbooks Are All You Need II: Phi-1.5 Technical Report</a> Yuanzhi Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2023assessing/">Assessing Cross-cultural Alignment Between Chatgpt And Human Societies: An Empirical Study</a> Yong Cao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023empirical/">An Empirical Study Of Catastrophic Forgetting In Large Language Models During Continual Fine-tuning</a> Yun Luo et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hou2023large/">Large Language Models Are Zero-shot Rankers For Recommender Systems</a> Yupeng Hou et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2023representation/">Representation Learning With Large Language Models For Recommendation</a> Xubin Ren et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ghosh2023chatgpt/">Chatgpt Perpetuates Gender Bias In Machine Translation And Ignores Non-gendered Pronouns: Findings Across Bengali And Five Other Low-resource Languages</a> Sourojit Ghosh, Aylin Caliskan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/biderman2023suite/">Pythia: A Suite For Analyzing Large Language Models Across Training And Scaling</a> Stella Biderman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ott2023central/">Thoughtsource: A Central Hub For Large Language Model Reasoning Data</a> Simon Ott et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/leng2023mitigating/">Mitigating Object Hallucinations In Large Vision-language Models Through Visual Contrastive Decoding</a> Sicong Leng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tonmoy2024comprehensive/">A Comprehensive Survey Of Hallucination Mitigation Techniques In Large Language Models</a> S. M Towhidul Islam Tonmoy et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gruver2024fine/">Fine-tuned Language Models Generate Stable Inorganic Materials As Text</a> Nate Gruver et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/haque2024exploring/">Exploring Chatgpt And Its Impact On Society</a> Md. Asraful Haque, Shuai Li </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dahl2024large/">Large Legal Fictions: Profiling Legal Hallucinations In Large Language Models</a> Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E. Ho </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kazemitabaar2024evaluating/">Codeaid: Evaluating A Classroom Deployment Of An Llm-based Programming Assistant That Balances Student And Educator Needs</a> Majeed Kazemitabaar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheong2024am/">(A)I Am Not A Lawyer, But...: Engaging Legal Experts Towards Responsible LLM Policies For Legal Advice</a> Inyoung Cheong, King Xia, K. J. Kevin Feng, Quan Ze Chen, Amy X. Zhang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2024closing/">Closing The Gap Between Open-source And Commercial Large Language Models For Medical Evidence Summarization</a> Gongbo Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gemmateam2024open/">Gemma: Open Models Based On Gemini Research And Technology</a> Gemma Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/groeneveld2024accelerating/">Olmo: Accelerating The Science Of Language Models</a> Dirk Groeneveld et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/choudhury2024large/">Large Language Models And User Trust: Consequence Of Self-referential Learning Loop And The Deskilling Of Healthcare Professionals</a> Avishek Choudhury, Zaria Chaudhry </li>
     
   
     
   
     
       <li> <a href="/publications/khurana2024why/">Why And When Llm-based Assistants Can Go Wrong: Investigating The Effectiveness Of Prompt-based Interactions For Software Help-seeking</a> Anjali Khurana, Hari Subramonyam, Parmit K Chilana </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2024trustworthiness/">Trustllm: Trustworthiness In Large Language Models</a> Yue Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deldjoo2024understanding/">Understanding Biases In Chatgpt-based Recommender Systems: Provider Fairness, Temporal Stability, And Recency</a> Yashar Deldjoo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hagendorff2024mapping/">Mapping The Ethics Of Generative AI: A Comprehensive Scoping Review</a> Thilo Hagendorff </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lehr2024chatgpt/">Chatgpt As Research Scientist: Probing Gpt's Capabilities As A Research Librarian, Research Ethicist, Data Generator And Data Predictor</a> Steven A. Lehr, Aylin Caliskan, Suneragiri Liyanage, Mahzarin R. Banaji </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2024hallucination/">Hallucination Detection: Robustly Discerning Reliable Answers In Large Language Models</a> Yuyan Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2024fostering/">Farsight: Fostering Responsible AI Awareness During AI Application Prototyping</a> Zijie J. Wang, Chinmay Kulkarni, Lauren Wilcox, Michael Terry, Michael Madaio </li>
     
   
     
       <li> <a href="/publications/ni2025measurement/">Measurement Of Llm's Philosophies Of Human Nature</a> Minheng Ni et al. </li>
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Fairness <a id="Fairness"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dehghani2023scaling/">Scaling Vision Transformers To 22 Billion Parameters</a> Mostafa Dehghani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sharma2023facilitating/">Facilitating Self-guided Mental Health Interventions Through Human-language Model Interaction: A Case Study Of Cognitive Restructuring</a> Ashish Sharma, Kevin Rushton, Inna Wanyin Lin, Theresa Nguyen, Tim Althoff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wan2023is/">"kelly Is A Warm Person, Joseph Is A Role Model": Gender Biases In Llm-generated Reference Letters</a> Yixin Wan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023better/">Better To Ask In English: Cross-lingual Evaluation Of Large Language Models For Healthcare Queries</a> Yiqiao Jin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Few-Shot <a id="Few-Shot"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vlasov2018few/">Few-shot Generalization Across Dialogue Tasks</a> Vladimir Vlasov, Akela Drissner-schmid, Alan Nichol </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2019personalizing/">Personalizing Dialogue Agents Via Meta-learning</a> Zhaojiang Lin, Andrea Madotto, Chien-sheng Wu, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kudugunta2019investigating/">Investigating Multilingual NMT Representations At Scale</a> Sneha Reddy Kudugunta, Ankur Bapna, Isaac Caswell, Naveen Arivazhagan, Orhan Firat </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lake2019human/">Human Few-shot Learning Of Compositional Instructions</a> Brenden M. Lake, Tal Linzen, Marco Baroni </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019few/">Few-shot NLG With Pre-trained Language Model</a> Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qian2019domain/">Domain Adaptive Dialog Generation Via Meta Learning</a> Kun Qian, Zhou Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bansal2019learning/">Learning To Few-shot Learn Across Diverse Natural Language Classification Tasks</a> Trapit Bansal, Rishikesh Jha, Andrew Mccallum </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019attention/">Attention-informed Mixed-language Training For Zero-shot Cross-lingual Task-oriented Dialogue Systems</a> Zihan Liu, Genta Indra Winata, Zhaojiang Lin, Peng Xu, Pascale Fung </li>
     
   
     
       <li> <a href="/publications/nangia2019human/">Human Vs. Muppet: A Conservative Estimate Of Human Performance On The GLUE Benchmark</a> Nikita Nangia, Samuel R. Bowman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2020cg/">CG-BERT: Conditional Text Generation With BERT For Generalized Few-shot Intent Detection</a> Congying Xia, Chenwei Zhang, Hoang Nguyen, Jiawei Zhang, Philip Yu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pfeiffer2020mad/">MAD-X: An Adapter-based Framework For Multi-task Cross-lingual Transfer</a> Jonas Pfeiffer, Ivan Vuliƒá, Iryna Gurevych, Sebastian Ruder </li>
     
   
     
       <li> <a href="/publications/zhang2020large/">CPM: A Large-scale Generative Chinese Pre-trained Language Model</a> Zhengyan Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2020tod/">TOD-BERT: Pre-trained Natural Language Understanding For Task-oriented Dialogue</a> Chien-sheng Wu, Steven Hoi, Richard Socher, Caiming Xiong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/narang2020training/">WT5?! Training Text-to-text Models To Explain Their Predictions</a> Sharan Narang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aky%C3%BCrek2020learning/">Learning To Recombine And Resample Data For Compositional Generalization</a> Ekin Aky√ºrek, Afra Feyza Aky√ºrek, Jacob Andreas </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2020rapidly/">Rapidly Bootstrapping A Question Answering Dataset For COVID-19</a> Raphael Tang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020high/">Logic2text: High-fidelity Natural Language Generation From Logical Forms</a> Zhiyu Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020knowledge/">KGPT: Knowledge-grounded Pre-training For Data-to-text Generation</a> Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2020building/">SOLOIST: Building Task Bots At Scale With Transfer Learning And Machine Teaching</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/efrat2020turking/">The Turking Test: Can Language Models Understand Instructions?</a> Avia Efrat, Omer Levy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2020few/">Few-shot Natural Language Generation For Task-oriented Dialog</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lauscher2020from/">From Zero To Hero: On The Limitations Of Zero-shot Cross-lingual Transfer With Multilingual Transformers</a> Anne Lauscher, Vinit Ravishankar, Ivan Vuliƒá, Goran Glava≈° </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/madotto2020language/">Language Models As Few-shot Learner For Task-oriented Dialogue Systems</a> Andrea Madotto, Zihan Liu, Zhaojiang Lin, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2020few/">Few-shot Generative Conversational Query Rewriting</a> Shi Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/asai2020logic/">Logic-guided Data Augmentation And Regularization For Consistent Question Answering</a> Akari Asai, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rastogi2020schema/">Schema-guided Dialogue State Tracking Task At DSTC8</a> Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, Pranav Khaitan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nogueira2020document/">Document Ranking With A Pretrained Sequence-to-sequence Model</a> Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/b2020language/">Language Models Are Few-shot Learners</a> Tom B. Brown et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/casanueva2020efficient/">Efficient Intent Detection With Dual Sentence Encoders</a> I√±igo Casanueva, Tadas Temƒçinas, Daniela Gerz, Matthew Henderson, Ivan Vuliƒá </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020making/">Making Pre-trained Language Models Better Few-shot Learners</a> Tianyu Gao, Adam Fisch, Danqi Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schick2020few/">Few-shot Text Generation With Pattern-exploiting Training</a> Timo Schick, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020from/">From Machine Reading Comprehension To Dialogue State Tracking: Bridging The Gap</a> Shuyang Gao, Sanchit Agarwal, Tagyoung Chung, Di Jin, Dilek Hakkani-tur </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kale2020template/">Template Guided Text Generation For Task-oriented Dialogue</a> Mihir Kale, Abhinav Rastogi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schick2020not/">It's Not Just Size That Matters: Small Language Models Are Also Few-shot Learners</a> Timo Schick, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alex2021real/">RAFT: A Real-world Few-shot Text Classification Benchmark</a> Neel Alex et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2021towards/">Towards Few-shot Fact-checking Via Perplexity</a> Nayeon Lee, Yejin Bang, Andrea Madotto, Madian Khabsa, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/betz2021thinking/">Thinking Aloud: Dynamic Context Generation Improves Zero-shot Reasoning Performance Of GPT-2</a> Gregor Betz, Kyle Richardson, Christian Voigt </li>
     
   
     
   
     
       <li> <a href="/publications/winata2021language/">Language Models Are Few-shot Multilingual Learners</a> Genta Indra Winata et al. </li>
     
   
     
       <li> <a href="/publications/qin2021learning/">Learning How To Ask: Querying Lms With Mixtures Of Soft Prompts</a> Guanghui Qin, Jason Eisner </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2021good/">A Good Prompt Is Worth Millions Of Parameters: Low-resource Prompt-based Learning For Vision-language Models</a> Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, Xiang Ren </li>
     
   
     
       <li> <a href="/publications/zhang2021tip/">Tip-adapter: Training-free Clip-adapter For Better Vision-language Modeling</a> Renrui Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021few/">Few-shot Learning With Multilingual Language Models</a> Xi Victoria Lin et al. </li>
     
   
     
       <li> <a href="/publications/razumovskaia2021crossing/">Crossing The Conversational Chasm: A Primer On Natural Language Processing For Multilingual Task-oriented Dialogue Systems</a> Evgeniia Razumovskaia et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/perez2021true/">True Few-shot Learning With Language Models</a> Ethan Perez, Douwe Kiela, Kyunghyun Cho </li>
     
   
     
   
     
       <li> <a href="/publications/ding2021prompt/">Prompt-learning For Fine-grained Entity Typing</a> Ning Ding et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021knowledge/">Knowprompt: Knowledge-aware Prompt-tuning With Synergistic Optimization For Relation Extraction</a> Xiang Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2021differentiable/">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners</a> Ningyu Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021lightweight/">Lightner: A Lightweight Tuning Paradigm For Low-resource NER Via Pluggable Prompting</a> Xiang Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021gpt/">GPT Understands, Too</a> Xiao Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tam2021improving/">Improving And Simplifying Pattern Exploiting Training</a> Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, Colin Raffel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021entailment/">Entailment As Few-shot Learner</a> Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, Hao Ma </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/artetxe2021efficient/">Efficient Large Scale Language Modeling With Mixtures Of Experts</a> Mikel Artetxe et al. </li>
     
   
     
       <li> <a href="/publications/moradi2021gpt/">GPT-3 Models Are Poor Few-shot Learners In The Biomedical Domain</a> Milad Moradi, Kathrin Blagec, Florian Haberl, Matthias Samwald </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021revisiting/">Revisiting Self-training For Few-shot Learning Of Language Model</a> Yiming Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021what/">What Makes Good In-context Examples For GPT-\(3\)?</a> Jiachang Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2021finetuned/">Finetuned Language Models Are Zero-shot Learners</a> Jason Wei et al. </li>
     
   
     
       <li> <a href="/publications/guhur2021pretraining/">Airbert: In-domain Pretraining For Vision-and-language Navigation</a> Pierre-louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, Cordelia Schmid </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/austin2021program/">Program Synthesis With Large Language Models</a> Jacob Austin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chada2021simple/">Fewshotqa: A Simple Framework For Few-shot Learning Of Question Answering Tasks Using Pre-trained Text-to-text Models</a> Rakesh Chada, Pradeep Natarajan </li>
     
   
     
   
     
       <li> <a href="/publications/sun2021ernie/">ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training For Language Understanding And Generation</a> Yu Sun et al. </li>
     
   
     
       <li> <a href="/publications/shin2021constrained/">Constrained Language Models Yield Few-shot Semantic Parsers</a> Richard Shin et al. </li>
     
   
     
       <li> <a href="/publications/gu2021pre/">PPT: Pre-trained Prompt Tuning For Few-shot Learning</a> Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021generative/">GALAXY: A Generative Pre-trained Model For Task-oriented Dialog With Semi-supervised Learning And Explicit Policy Injection</a> Wanwei He et al. </li>
     
   
     
   
     
       <li> <a href="/publications/logan2021cutting/">Cutting Down On Prompts And Parameters: Simple Few-shot Learning With Language Models</a> Robert L. Iv Logan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2021pangu/">Pangu-\(Œ±\): Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation</a> Wei Zeng et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yao2021colorful/">CPT: Colorful Prompt Tuning For Pre-trained Vision-language Models</a> Yuan Yao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/min2021learning/">Metaicl: Learning To Learn In Context</a> Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mishra2021reframing/">Reframing Instructional Prompts To Gptk's Language</a> Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/schick2021true/">True Few-shot Learning With Prompts -- A Real-world Perspective</a> Timo Schick, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021few/">Few-shot Knowledge Graph-to-text Generation With Pretrained Language Models</a> Junyi Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bragg2021unifying/">FLEX: Unifying Evaluation For Few-shot NLP</a> Jonathan Bragg, Arman Cohan, Kyle Lo, Iz Beltagy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2021calibrate/">Calibrate Before Use: Improving Few-shot Performance Of Language Models</a> Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hambardzumyan2021word/">WARP: Word-level Adversarial Reprogramming</a> Karen Hambardzumyan, Hrant Khachatrian, Jonathan May </li>
     
   
     
       <li> <a href="/publications/yoo2021leveraging/">Gpt3mix: Leveraging Large-scale Language Models For Text Augmentation</a> Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-woo Lee, Woomyeong Park </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2021empirical/">An Empirical Study Of GPT-3 For Few-shot Knowledge-based VQA</a> Zhengyuan Yang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mishra2021cross/">Cross-task Generalization Via Natural Language Crowdsourcing Instructions</a> Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/poth2021what/">What To Pre-train On? Efficient Intermediate Task Selection</a> Clifton Poth, Jonas Pfeiffer, Andreas R√ºckl√©, Iryna Gurevych </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2021exploring/">Exploring Prompt-based Few-shot Learning For Grounded Dialog Generation</a> Chujie Zheng, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schuhmann2021laion/">LAION-400M: Open Dataset Of Clip-filtered 400 Million Image-text Pairs</a> Christoph Schuhmann et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2021unified/">LFPT5: A Unified Framework For Lifelong Few-shot Language Learning Based On Prompt Tuning Of T5</a> Chengwei Qin, Shafiq Joty </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ram2021few/">Few-shot Question Answering By Pretraining Span Selection</a> Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ju2021prompting/">Prompting Visual-language Models For Efficient Video Understanding</a> Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tafjord2021general/">General-purpose Question-answering With Macaw</a> Oyvind Tafjord, Peter Clark </li>
     
   
     
   
     
       <li> <a href="/publications/han2021prompt/">PTR: Prompt Tuning With Rules For Text Classification</a> Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, Maosong Sun </li>
     
   
     
       <li> <a href="/publications/sun2021nsp/">NSP-BERT: A Prompt-based Few-shot Learner Through An Original Pre-training Task--next Sentence Prediction</a> Yi Sun, Yu Zheng, Chao Hao, Hangping Qiu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lester2021power/">The Power Of Scale For Parameter-efficient Prompt Tuning</a> Brian Lester, Rami Al-rfou, Noah Constant </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021nlp/">Generate, Annotate, And Learn: NLP With Synthetic Text</a> Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, Mohammad Norouzi </li>
     
   
     
   
     
       <li> <a href="/publications/kim2021what/">What Changes Can Large-scale Language Models Bring? Intensive Study On Hyperclova: Billions-scale Korean Generative Pretrained Transformers</a> Boseop Kim et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chintagunta2021medically/">Medically Aware GPT-3 As A Data Generator For Medical Dialogue Summarization</a> Bharath Chintagunta, Namit Katariya, Xavier Amatriain, Anitha Kannan </li>
     
   
     
       <li> <a href="/publications/ye2021few/">Crossfit: A Few-shot Learning Challenge For Cross-task Generalization In NLP</a> Qinyuan Ye, Bill Yuchen Lin, Xiang Ren </li>
     
   
     
       <li> <a href="/publications/gao2021clip/">Clip-adapter: Better Vision-language Models With Feature Adapters</a> Peng Gao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2021pre/">Pre-train, Prompt, And Predict: A Systematic Survey Of Prompting Methods In Natural Language Processing</a> Pengfei Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/coenen2021human/">Wordcraft: A Human-ai Collaborative Editor For Story Writing</a> Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, Ann Yuan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/madotto2021few/">Few-shot Bot: Prompt-based Learning For Dialogue Systems</a> Andrea Madotto, Zhaojiang Lin, Genta Indra Winata, Pascale Fung </li>
     
   
     
       <li> <a href="/publications/lu2021fantastically/">Fantastically Ordered Prompts And Where To Find Them: Overcoming Few-shot Prompt Order Sensitivity</a> Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp </li>
     
   
     
   
     
       <li> <a href="/publications/lu2021less/">Less Is More: Pre-train A Strong Text Encoder For Dense Retrieval Using A Weak Decoder</a> Shuqi Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/asai2021one/">One Question Answering Model For Many Languages With Cross-lingual Dense Passage Retrieval</a> Akari Asai, Xinyan Yu, Jungo Kasai, Hannaneh Hajishirzi </li>
     
   
     
       <li> <a href="/publications/webson2021do/">Do Prompt-based Models Really Understand The Meaning Of Their Prompts?</a> Albert Webson, Ellie Pavlick </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qiu2021vt/">VT-CLIP: Enhancing Vision-language Models With Visual-guided Texts</a> Longtian Qiu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wiegreffe2021reframing/">Reframing Human-ai Collaboration For Generating Free-text Explanations</a> Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi </li>
     
   
     
       <li> <a href="/publications/li2021grounded/">Grounded Language-image Pre-training</a> Liunian Harold Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2021explanation/">An Explanation Of In-context Learning As Implicit Bayesian Inference</a> Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/reynolds2021prompt/">Prompt Programming For Large Language Models: Beyond The Few-shot Paradigm</a> Laria Reynolds, Kyle Mcdonell </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tsimpoukelli2021multimodal/">Multimodal Few-shot Learning With Frozen Language Models</a> Maria Tsimpoukelli et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nogueira2021investigating/">Investigating The Limitations Of Transformers With Simple Arithmetic Tasks</a> Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2021few/">Few-shot Conversational Dense Retrieval</a> Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, Zhiyuan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nye2021show/">Show Your Work: Scratchpads For Intermediate Computation With Language Models</a> Maxwell Nye et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2022learning/">In-context Learning For Few-shot Dialogue State Tracking</a> Yushi Hu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/robinson2022leveraging/">Leveraging Large Language Models For Multiple Choice Question Answering</a> Joshua Robinson, Christopher Michael Rytting, David Wingate </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jang2022can/">Can Large Language Models Truly Understand Prompts? A Case Study With Negated Prompts</a> Joel Jang, Seonghyeon Ye, Minjoon Seo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2022unifying/">Unifiedskg: Unifying And Multi-tasking Structured Knowledge Grounding With Text-to-text Language Models</a> Tianbao Xie et al. </li>
     
   
     
       <li> <a href="/publications/khot2022decomposed/">Decomposed Prompting: A Modular Approach For Solving Complex Tasks</a> Tushar Khot et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022personalized/">Personalized Prompt For Sequential Recommendation</a> Yiqing Wu et al. </li>
     
   
     
       <li> <a href="/publications/ahmed2022few/">Few-shot Training Llms For Project-specific Code-summarization</a> Toufique Ahmed, Premkumar Devanbu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li%C3%A9vin2022can/">Can Large Language Models Reason About Medical Questions?</a> Valentin Li√©vin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, Ole Winther </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2022description/">Description-driven Task-oriented Dialog Modeling</a> Jeffrey Zhao et al. </li>
     
   
     
       <li> <a href="/publications/alayrac2022visual/">Flamingo: A Visual Language Model For Few-shot Learning</a> Jean-baptiste Alayrac et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pereira2022multi/">Visconde: Multi-document QA With GPT-3 And Neural Reranking</a> Jayr Pereira, Robson Fidalgo, Roberto Lotufo, Rodrigo Nogueira </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chung2022scaling/">Scaling Instruction-finetuned Language Models</a> Hyung Won Chung et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gonen2022demystifying/">Demystifying Prompts In Language Models Via Perplexity Estimation</a> Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, Luke Zettlemoyer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2022clip/">CLIP Models Are Few-shot Learners: Empirical Studies On VQA And Visual Entailment</a> Haoyu Song, Li Dong, Wei-nan Zhang, Ting Liu, Furu Wei </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2022few/">Few-shot Parameter-efficient Fine-tuning Is Better And Cheaper Than In-context Learning</a> Haokun Liu et al. </li>
     
   
     
       <li> <a href="/publications/su2022selective/">Selective Annotation Makes Language Models Better Few-shot Learners</a> Hongjin Su et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meng2022generating/">Generating Training Data With Language Models: Towards Zero-shot Language Understanding</a> Yu Meng, Jiaxin Huang, Yu Zhang, Jiawei Han </li>
     
   
     
       <li> <a href="/publications/chen2022large/">Large Language Models Are Few(1)-shot Table Reasoners</a> Wenhu Chen </li>
     
   
     
       <li> <a href="/publications/dang2022how/">How To Prompt? Opportunities And Challenges Of Zero- And Few-shot Learning For Human-ai Interaction In Creative Applications Of Generative Models</a> Hai Dang, Lukas Mecke, Florian Lehmann, Sven Goller, Daniel Buschek </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022making/">Making Large Language Models Better Reasoners With Step-aware Verifier</a> Yifei Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022program/">Program Of Thoughts Prompting: Disentangling Computation From Reasoning For Numerical Reasoning Tasks</a> Wenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/izacard2022few/">Atlas: Few-shot Learning With Retrieval Augmented Language Models</a> Gautier Izacard et al. </li>
     
   
     
       <li> <a href="/publications/sahu2022data/">Data Augmentation For Intent Classification With Off-the-shelf Large Language Models</a> Gaurav Sahu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022prototypical/">Prototypical Verbalizer For Prompt-based Few-shot Tuning</a> Ganqu Cui, Shengding Hu, Ning Ding, Longtao Huang, Zhiyuan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/poesia2022reliable/">Synchromesh: Reliable Code Generation From Pre-trained Language Models</a> Gabriel Poesia et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2022one/">Deplot: One-shot Visual Language Reasoning By Plot-to-table Translation</a> Fangyu Liu et al. </li>
     
   
     
       <li> <a href="/publications/peng2022sgva/">Sgva-clip: Semantic-guided Visual Adapting Of Vision-language Models For Few-shot Image Classification</a> Fang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang, Changsheng Xu </li>
     
   
     
       <li> <a href="/publications/yu2022legal/">Legal Prompting: Teaching A Language Model To Think Like A Lawyer</a> Fangyi Yu, Lee Quartey, Frank Schilder </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zelikman2022bootstrapping/">Star: Bootstrapping Reasoning With Reasoning</a> Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D. Goodman </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022decoupling/">Decoupling Knowledge From Memorization: Retrieval-augmented Prompt Learning</a> Xiang Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bugliarello2022benchmark/">IGLUE: A Benchmark For Transfer Learning Across Modalities, Tasks, And Languages</a> Emanuele Bugliarello et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hosseiniasl2022generative/">A Generative Language Model For Few-shot Aspect-based Sentiment Analysis</a> Ehsan Hosseini-asl, Wenhao Liu, Caiming Xiong </li>
     
   
     
       <li> <a href="/publications/he2022space/">SPACE-3: Unified Dialog Model Pre-training For Task-oriented Dialog Understanding And Generation</a> Wanwei He et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wu2022self/">Self-adaptive In-context Learning: An Information Compression Perspective For In-context Example Selection And Ordering</a> Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2022unreliability/">The Unreliability Of Explanations In Few-shot Prompting For Textual Reasoning</a> Xi Ye, Greg Durrett </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dua2022successive/">Successive Prompting For Decomposing Complex Questions</a> Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vilar2022prompting/">Prompting Palm For Translation: Assessing Strategies And Performance</a> David Vilar et al. </li>
     
   
     
       <li> <a href="/publications/wan2022factuality/">Factpegasus: Factuality-aware Pre-training And Fine-tuning For Abstractive Summarization</a> David Wan, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dohan2022language/">Language Model Cascades</a> David Dohan et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2022hungry/">Hungry Hungry Hippos: Towards Language Modeling With State Space Models</a> Daniel Y. Fu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022benchmark/">ELEVATER: A Benchmark And Toolkit For Evaluating Language-augmented Visual Models</a> Chunyuan Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2022few/">Promptagator: Few-shot Dense Retrieval From 8 Examples</a> Zhuyun Dai et al. </li>
     
   
     
   
     
       <li> <a href="/publications/si2022prompting/">Prompting GPT-3 To Be Reliable</a> Chenglei Si et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022code/">Code4struct: Code Generation For Few-shot Event Structure Prediction</a> Xingyao Wang, Sha Li, Heng Ji </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/du2022retrieval/">Retrieval-augmented Generative Question Answering For Event Argument Extraction</a> Xinya Du, Heng Ji </li>
     
   
     
       <li> <a href="/publications/song2022llm/">Llm-planner: Few-shot Grounded Planning For Embodied Agents With Large Language Models</a> Chan Hee Song et al. </li>
     
   
     
       <li> <a href="/publications/gu2022proposal/">Don't Generate, Discriminate: A Proposal For Grounding Language Models To Real-world Environments</a> Yu Gu, Xiang Deng, Yu Su </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022is/">Is GPT-3 A Good Data Annotator?</a> Bosheng Ding et al. </li>
     
   
     
       <li> <a href="/publications/ni2022expanding/">Expanding Language-image Pretrained Models For General Video Recognition</a> Bolin Ni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guti%C3%A9rrez2022thinking/">Thinking About GPT-3 In-context Learning For Biomedical IE? Think Again</a> Bernal Jim√©nez Guti√©rrez et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022rationale/">Rationale-augmented Ensembles In Language Models</a> Xuezhi Wang et al. </li>
     
   
     
       <li> <a href="/publications/zhu2022prompt/">Prompt-aligned Gradient For Prompt Tuning</a> Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, Hanwang Zhang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/athiwaratkun2022multi/">Multi-lingual Evaluation Of Code Generation Models</a> Ben Athiwaratkun et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022adaptive/">Adaprompt: Adaptive Model Training For Prompt-based NLP</a> Yulong Chen et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2022large/">GODEL: Large-scale Pre-training For Goal-directed Dialog</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/creswell2022selection/">Selection-inference: Exploiting Large Language Models For Interpretable Logical Reasoning</a> Antonia Creswell, Murray Shanahan, Irina Higgins </li>
     
   
     
       <li> <a href="/publications/yang2022zero/">Zero-shot Video Question Answering Via Frozen Bidirectional Language Models</a> Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/tiong2022plug/">Plug-and-play VQA: Zero-shot VQA By Conjoining Large Pretrained Models With Zero Training</a> Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lazaridou2022internet/">Internet-augmented Language Models Through Few-shot Prompting For Open-domain Question Answering</a> Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev </li>
     
   
     
       <li> <a href="/publications/liu2022prompting/">Qaner: Prompting Question Answering Models For Few-shot Named Entity Recognition</a> Andy T. Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/k2022can/">Can Language Models Learn From Explanations In Context?</a> Andrew K. Lampinen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/madaan2022text/">Text And Patterns: For Effective Chain Of Thought, It Takes Two To Tango</a> Aman Madaan, Amir Yazdanbakhsh </li>
     
   
     
       <li> <a href="/publications/madaan2022language/">Language Models Of Code Are Few-shot Commonsense Learners</a> Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig </li>
     
   
     
   
     
       <li> <a href="/publications/talmor2022commonsenseqa/">Commonsenseqa 2.0: Exposing The Limits Of AI Through Gamification</a> Alon Talmor et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022mixture/">Adamix: Mixture-of-adaptations For Parameter-efficient Model Tuning</a> Yaqing Wang et al. </li>
     
   
     
       <li> <a href="/publications/razeghi2022impact/">Impact Of Pretraining Term Frequencies On Few-shot Reasoning</a> Yasaman Razeghi, Robert L. Iv Logan, Matt Gardner, Sameer Singh </li>
     
   
     
   
     
       <li> <a href="/publications/hao2022language/">Language Models Are General-purpose Interfaces</a> Yaru Hao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zang2022unified/">Unified Vision And Language Prompt Learning</a> Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, Chen Change Loy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chowdhery2022scaling/">Palm: Scaling Language Modeling With Pathways</a> Aakanksha Chowdhery et al. </li>
     
   
     
   
     
       <li> <a href="/publications/asai2022parameter/">ATTEMPT: Parameter-efficient Multi-task Tuning Via Attentional Mixtures Of Soft Prompts</a> Akari Asai, Mohammadreza Salehi, Matthew E. Peters, Hannaneh Hajishirzi </li>
     
   
     
   
     
       <li> <a href="/publications/ma2022prompt/">Prompt For Extraction? PAIE: Prompting Argument Interaction For Event Argument Extraction</a> Yubo Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/barei%C3%9F2022code/">Code Generation Tools (almost) For Free? A Study Of Few-shot, Pre-trained Language Models On Code</a> Patrick Barei√ü, Beatriz Souza, Marcelo D'amorim, Michael Pradel </li>
     
   
     
       <li> <a href="/publications/lu2022learn/">Learn To Explain: Multimodal Reasoning Via Thought Chains For Science Question Answering</a> Pan Lu et al. </li>
     
   
     
       <li> <a href="/publications/lu2022dynamic/">Dynamic Prompt Learning Via Policy Gradient For Semi-structured Mathematical Reasoning</a> Pan Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/arora2022ask/">Ask Me Anything: A Simple Strategy For Prompting Language Models</a> Simran Arora et al. </li>
     
   
     
       <li> <a href="/publications/mees2022grounding/">Grounding Language With Visual Affordances Over Unstructured Data</a> Oier Mees, Jessica Borja-diaz, Wolfram Burgard </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/black2022gpt/">Gpt-neox-20b: An Open-source Autoregressive Language Model</a> Sid Black et al. </li>
     
   
     
   
     
       <li> <a href="/publications/cohen2022is/">"this Is My Unicorn, Fluffy": Personalizing Frozen Vision-language Representations</a> Niv Cohen, Rinon Gal, Eli A. Meirom, Gal Chechik, Yuval Atzmon </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/taylor2022clinical/">Clinical Prompt Learning With Frozen Language Models</a> Niall Taylor, Yi Zhang, Dan Joyce, Alejo Nevado-holgado, Andrey Kormilitzin </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/iyer2022opt/">OPT-IML: Scaling Language Model Instruction Meta Learning Through The Lens Of Generalization</a> Srinivasan Iyer et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/varia2022instruction/">Instruction Tuning For Few-shot Aspect-based Sentiment Analysis</a> Siddharth Varia et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022language/">Language Models With Image Descriptors Are Strong Few-shot Video-language Learners</a> Zhenhailong Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/suzgun2022challenging/">Challenging Big-bench Tasks And Whether Chain-of-thought Can Solve Them</a> Mirac Suzgun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sivarajkumar2022zero/">Healthprompt: A Zero-shot Learning Paradigm For Clinical Natural Language Processing</a> Sonish Sivarajkumar, Yanshan Wang </li>
     
   
     
   
     
       <li> <a href="/publications/deng2022optimizing/">Rlprompt: Optimizing Discrete Text Prompts With Reinforcement Learning</a> Mingkai Deng et al. </li>
     
   
     
       <li> <a href="/publications/chan2022data/">Data Distributional Properties Drive Emergent In-context Learning In Transformers</a> Stephanie C. Y. Chan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gan2022vision/">Vision-language Pre-training: Basics, Recent Advances, And Future Trends</a> Zhe Gan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022open/">OPT: Open Pre-trained Transformer Language Models</a> Susan Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chaaben2022towards/">Towards Using Few-shot Prompt Learning For Automating Model Completion</a> Meriem Ben Chaaben, Lola Burgue√±o, Houari Sahraoui </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2022researching/">RARR: Researching And Revising What Language Models Say, Using Language Models</a> Luyu Gao et al. </li>
     
   
     
       <li> <a href="/publications/gao2022program/">PAL: Program-aided Language Models</a> Luyu Gao et al. </li>
     
   
     
       <li> <a href="/publications/bonifacio2022data/">Inpars: Data Augmentation For Information Retrieval Using Large Language Models</a> Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Rodrigo Nogueira </li>
     
   
     
       <li> <a href="/publications/wang2022super/">Super-naturalinstructions: Generalization Via Declarative Instructions On 1600+ NLP Tasks</a> Yizhong Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kojima2022large/">Large Language Models Are Zero-shot Reasoners</a> Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022multi/">Instructionner: A Multi-task Instruction-based Generative Framework For Few-shot NER</a> Liwen Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2022large/">Large Language Models Are Human-level Prompt Engineers</a> Yongchao Zhou et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xu2022exploring/">Exploring The Universal Vulnerability Of Prompt-based Learning Paradigm</a> Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu </li>
     
   
     
   
     
       <li> <a href="/publications/tunstall2022efficient/">Efficient Few-shot Learning Without Prompts</a> Lewis Tunstall et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/soltan2022alexatm/">Alexatm 20B: Few-shot Learning Using A Large-scale Multilingual Seq2seq Model</a> Saleh Soltan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shin2022effect/">On The Effect Of Pretraining Corpora On In-context Learning By A Large-scale Language Model</a> Seongjin Shin et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/smith2022using/">Using Deepspeed And Megatron To Train Megatron-turing NLG 530B, A Large-scale Generative Language Model</a> Shaden Smith et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2022multitask/">Multitask Vision-language Prompt Tuning</a> Sheng Shen et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geng2022recommendation/">Recommendation As Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)</a> Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/welleck2022generating/">Generating Sequences By Learning To Self-correct</a> Sean Welleck et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023then/">Prompt, Generate, Then Cache: Cascade Of Foundation Models Makes Strong Few-shot Learners</a> Renrui Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pires2023portuguese/">Sabi\'a: Portuguese Large Language Models</a> Ramon Pires, Hugo Abonizio, Thales Sales Almeida, Rodrigo Nogueira </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/abukhalaf2023codex/">On Codex Prompt Engineering For OCL Generation: An Empirical Study</a> Seif Abukhalaf, Mohammad Hamdaqa, Foutse Khomh </li>
     
   
     
       <li> <a href="/publications/kim2023cot/">The Cot Collection: Improving Zero-shot And Few-shot Learning Of Language Models Via Chain-of-thought Fine-tuning</a> Seungone Kim et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sanner2023large/">Large Language Models Are Competitive Near Cold-start Recommenders For Language- And Item-based Preferences</a> Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, Lucas Dixon </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023mixture/">Mixture-of-experts Meets Instruction Tuning:a Winning Combination For Large Language Models</a> Sheng Shen et al. </li>
     
   
     
       <li> <a href="/publications/mysore2023large/">Large Language Model Augmented Narrative Driven Recommendations</a> Sheshera Mysore, Andrew Mccallum, Hamed Zamani </li>
     
   
     
   
     
       <li> <a href="/publications/longpre2023flan/">The Flan Collection: Designing Data And Methods For Effective Instruction Tuning</a> Shayne Longpre et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023language/">Language Is Not All You Need: Aligning Perception With Language Models</a> Shaohan Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023label/">Label Supervised Llama Finetuning</a> Zongxi Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lyu2023faithful/">Faithful Chain-of-thought Reasoning</a> Qing Lyu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2023plug/">Chameleon: Plug-and-play Compositional Reasoning With Large Language Models</a> Pan Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khattab2023compiling/">Dspy: Compiling Declarative Language Model Calls Into Self-improving Pipelines</a> Omar Khattab et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ziems2023large/">Large Language Models Are Built-in Autoregressive Search Engines</a> Noah Ziems, Wenhao Yu, Zhihan Zhang, Meng Jiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/erik2023consistency/">Consistency Analysis Of Chatgpt</a> Myeongjun Erik Jang, Thomas Lukasiewicz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pourreza2023din/">DIN-SQL: Decomposed In-context Learning Of Text-to-sql With Self-correction</a> Mohammadreza Pourreza, Davood Rafiei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023time/">Time-llm: Time Series Forecasting By Reprogramming Large Language Models</a> Ming Jin et al. </li>
     
   
     
       <li> <a href="/publications/kwon2023reward/">Reward Design With Language Models</a> Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moor2023med/">Med-flamingo: A Multimodal Medical Few-shot Learner</a> Michael Moor et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sclar2023quantifying/">Quantifying Language Models' Sensitivity To Spurious Features In Prompt Design Or: How I Learned To Start Worrying About Prompt Formatting</a> Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr </li>
     
   
     
   
     
       <li> <a href="/publications/hasan2023zero/">Zero- And Few-shot Prompting With Llms: A Comparative Study With Fine-tuned Models For Bangla Sentiment Analysis</a> Md. Arid Hasan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maniparambil2023enhancing/">Enhancing CLIP With GPT-4: Harnessing Visual Descriptions As Prompts</a> Mayug Maniparambil et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sch%C3%A4fer2023empirical/">An Empirical Evaluation Of Using Large Language Models For Automated Unit Test Generation</a> Max Sch√§fer, Sarah Nadi, Aryaz Eghbali, Frank Tip </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mosbach2023few/">Few-shot Fine-tuning Vs. In-context Learning: A Fair Comparison And Evaluation</a> Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, Yanai Elazar </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singha2023visual/">Applenet: Visual Attention Parameterized Prompt Learning For Few-shot Remote Sensing Image Generalization Using CLIP</a> Mainak Singha, Ankit Jha, Bhupendra Solanki, Shirsha Bose, Biplab Banerjee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023query/">Query2doc: Query Expansion With Large Language Models</a> Liang Wang, Nan Yang, Furu Wei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023flexible/">Flexkbqa: A Flexible Llm-powered Framework For Few-shot Knowledge Base Question Answering</a> Zhenyu Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023aligning/">Aligning Instruction Tasks Unlocks Large Language Models As Zero-shot Relation Extractors</a> Kai Zhang, Bernal Jim√©nez Guti√©rrez, Yu Su </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023is/">Is Chatgpt A Good Recommender? A Preliminary Study</a> Junling Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ye2023comprehensive/">A Comprehensive Capability Analysis Of GPT-3 And GPT-3.5 Series Models</a> Junjie Ye et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chung2023increasing/">Increasing Diversity While Maintaining Accuracy: Text Data Generation With Large Language Models And Human Interventions</a> John Joon Young Chung, Ece Kamar, Saleema Amershi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023potential/">The Potential And Pitfalls Of Using A Large Language Model Such As Chatgpt Or GPT-4 As A Clinical Assistant</a> Jingqing Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023prompt/">Prompt-and-align: Prompt-based Social Alignment For Few-shot Fake News Detection</a> Jiaying Wu, Shen Li, Ailin Deng, Miao Xiong, Bryan Hooi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023retrieval/">Rella: Retrieval-enhanced Large Language Models For Lifelong Sequential Behavior Comprehension In Recommendation</a> Jianghao Lin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023empowering/">Empowering Molecule Discovery For Molecule-caption Translation With Large Language Models: A Chatgpt Perspective</a> Jiatong Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023memory/">Memory-efficient Fine-tuning Of Compressed Large Language Models Via Sub-4-bit Integer Quantization</a> Jeonghoon Kim et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/koco%C5%842023jack/">Chatgpt: Jack Of All Trades, Master Of None</a> Jan Koco≈Ñ et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jahan2023comprehensive/">A Comprehensive Evaluation Of Large Language Models On Benchmark Biomedical Text Processing Tasks</a> Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peters2023large/">Large Language Models Can Infer Psychological Dispositions Of Social Media Users</a> Heinrich Peters, Sandra Matz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/inan2023llama/">Llama Guard: Llm-based Input-output Safeguard For Human-ai Conversations</a> Hakan Inan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dai2023leveraging/">Auggpt: Leveraging Chatgpt For Text Data Augmentation</a> Haixing Dai et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2023applying/">Applying Large Language Models And Chain-of-thought For Automatic Scoring</a> Gyeong-geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu, Xiaoming Zhai </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meyerson2023language/">Language Model Crossover: Variation Through Few-shot Prompting</a> Elliot Meyerson et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kamalloo2023evaluating/">Evaluating Open-domain Question Answering In The Era Of Large Language Models</a> Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, Davood Rafiei </li>
     
   
     
       <li> <a href="/publications/lee2023read/">Read-only Prompt Optimization For Vision-language Few-shot Learning</a> Dongjun Lee et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ashok2023prompting/">Promptner: Prompting For Named Entity Recognition</a> Dhananjay Ashok, Zachary C. Lipton </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2023is/">Is Chatgpt A General-purpose Natural Language Processing Task Solver?</a> Chengwei Qin et al. </li>
     
   
     
       <li> <a href="/publications/peng2023model/">Model Tuning Or Prompt Tuning? A Study Of Large Language Models For Clinical Concept And Relation Extraction</a> Cheng Peng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hsieh2023distilling/">Distilling Step-by-step! Outperforming Larger Language Models With Less Training Data And Smaller Model Sizes</a> Cheng-yu Hsieh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023generative/">Generative Speech Recognition Error Correction With Large Language Models And Task-activating Prompting</a> Chao-han Huck Yang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/oh2023black/">Blackvip: Black-box Visual Prompting For Robust Transfer Learning</a> Changdae Oh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/paranjape2023automatic/">ART: Automatic Multi-step Reasoning And Tool-use For Large Language Models</a> Bhargavi Paranjape et al. </li>
     
   
     
       <li> <a href="/publications/clavi%C3%A92023large/">Large Language Models In The Workplace: A Case Study On Prompt Engineering For Job Type Classification</a> Benjamin Clavi√©, Alexandru Ciceu, Frederick Naylor, Guillaume Souli√©, Thomas Brightwell </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2023batch/">Batch Prompting: Efficient Inference With Large Language Model Apis</a> Zhoujun Cheng, Jungo Kasai, Tao Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023teaching/">Teaching Large Language Models To Self-debug</a> Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, Denny Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023how/">How To Unleash The Power Of Large Language Models For Few-shot Relation Extraction?</a> Xin Xu, Yuqi Zhu, Xiaohan Wang, Ningyu Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pu2023summarization/">Summarization Is (almost) Dead</a> Xiao Pu, Mingqi Gao, Xiaojun Wan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/garcia2023unreasonable/">The Unreasonable Effectiveness Of Few-shot Learning For Machine Translation</a> Xavier Garcia et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kang2023do/">Do Llms Understand User Preferences? Evaluating Llms On User Rating Prediction</a> Wang-cheng Kang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jeronymo2023inpars/">Inpars-v2: Large Language Models As Efficient Dataset Generators For Information Retrieval</a> Vitor Jeronymo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vu2023refreshing/">Freshllms: Refreshing Large Language Models With Search Engine Augmentation</a> Tu Vu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ahmed2023better/">Better Patching Using LLM Prompting, Via Self-consistency</a> Toufique Ahmed, Premkumar Devanbu </li>
     
   
     
       <li> <a href="/publications/ahmed2023automatic/">Automatic Semantic Augmentation Of Language Model Prompts (for Code Summarization)</a> Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, Earl T. Barr </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2023empirical/">Empirical Study Of Zero-shot NER With Chatgpt</a> Tingyu Xie et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023few/">Few-shot In-context Learning For Knowledge Base Question Answering</a> Tianle Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2023what/">What Can Large Language Models Do In Chemistry? A Comprehensive Benchmark On Eight Tasks</a> Taicheng Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2023is/">Is Chatgpt A Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation</a> Tao Fang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/coyne2023analyzing/">Analyzing The Performance Of GPT-3.5 And GPT-4 In Grammatical Error Correction</a> Steven Coyne, Keisuke Sakaguchi, Diana Galvan-sosa, Michael Zock, Kentaro Inui </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2023enhancing/">Enhancing Job Recommendation Through Llm-based Generative Adversarial Networks</a> Yingpeng Du et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023element/">Element-aware Summarization With Large Language Models: Expert-aligned Evaluation And Chain-of-thought Method</a> Yiming Wang, Zhuosheng Zhang, Rui Wang </li>
     
   
     
   
     
       <li> <a href="/publications/du2023improving/">Improving Factuality And Reasoning In Language Models Through Multiagent Debate</a> Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023human/">Human-centric Autonomous Systems With Llms For User Command Reasoning</a> Yi Yang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/moslem2023adaptive/">Adaptive Machine Translation With Large Language Models</a> Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023specializing/">Specializing Smaller Language Models Towards Multi-step Reasoning</a> Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023large/">Recmind: Large Language Model Powered Agent For Recommendation</a> Yancheng Wang et al. </li>
     
   
     
       <li> <a href="/publications/hu2023improving/">Improving Large Language Models For Clinical Named Entity Recognition Via Prompt Engineering</a> Yan Hu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023mental/">Mental-llm: Leveraging Large Language Models For Mental Health Prediction Via Online Text Data</a> Xuhai Xu et al. </li>
     
   
     
       <li> <a href="/publications/zhao2023chat/">Chat With The Environment: Interactive Multimodal Perception Using Large Language Models</a> Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023autoregressive/">Autotamp: Autoregressive Task And Motion Planning With Llms As Translators And Checkers</a> Yongchao Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2023sentence/">Sentence Simplification Via Large Language Models</a> Yutao Feng, Jipeng Qiang, Yun Li, Yunhao Yuan, Yi Zhu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wadhwa2023revisiting/">Revisiting Relation Extraction In The Era Of Large Language Models</a> Somin Wadhwa, Silvio Amir, Byron C. Wallace </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/biderman2023suite/">Pythia: A Suite For Analyzing Large Language Models Across Training And Scaling</a> Stella Biderman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2024large/">Large Language Models Meet Collaborative Filtering: An Efficient All-round Llm-based Recommender System</a> Sein Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chung2024large/">Large Language Model Capabilities In Perioperative Risk Prediction And Prognostication</a> Philip Chung et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bassner2024ai/">Iris: An Ai-driven Virtual Tutor For Computer Science Education</a> Patrick Bassner, Eduard Frankford, Stephan Krusche </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maharjan2024prompt/">Openmedlm: Prompt Engineering Can Out-perform Fine-tuning In Medical Question-answering With Open-source Large Language Models</a> Jenish Maharjan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yuan2024prompt/">Unist: A Prompt-empowered Universal Model For Urban Spatio-temporal Prediction</a> Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deldjoo2024understanding/">Understanding Biases In Chatgpt-based Recommender Systems: Provider Fairness, Temporal Stability, And Recency</a> Yashar Deldjoo </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2024data/">Data-efficient Fine-tuning For Llm-based Recommendation</a> Xinyu Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2024unsupervised/">Promptkd: Unsupervised Prompt Distillation For Vision-language Models</a> Zheng Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2024exploratory/">Llmparser: An Exploratory Study On Using Large Language Models For Log Parsing</a> Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-hsun Chen, Shaowei Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Fine-Tuning <a id="Fine-Tuning"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vijayakumar2016diverse/">Diverse Beam Search: Decoding Diverse Solutions From Neural Sequence Models</a> Ashwin K Vijayakumar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/johnson2016multilingual/">Google's Multilingual Neural Machine Translation System: Enabling Zero-shot Translation</a> Melvin Johnson et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sordoni2016iterative/">Iterative Alternating Neural Attention For Machine Reading</a> Alessandro Sordoni, Philip Bachman, Adam Trischler, Yoshua Bengio </li>
     
   
     
   
     
       <li> <a href="/publications/mo2017fine/">Fine Grained Knowledge Transfer For Personalized Task-oriented Dialogue Systems</a> Kaixiang Mo, Yu Zhang, Qiang Yang, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2017neural/">Neural Personalized Response Generation As Domain Adaptation</a> Weinan Zhang, Ting Liu, Yifa Wang, Qingfu Zhu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2017chinese/">Dureader: A Chinese Machine Reading Comprehension Dataset From Real-world Applications</a> Wei He et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guu2017from/">From Language To Programs: Bridging Reinforcement Learning And Maximum Marginal Likelihood</a> Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2017non/">Non-autoregressive Neural Machine Translation</a> Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, Richard Socher </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2018can/">Can You Tell Me How To Get Past Sesame Street? Sentence-level Pretraining Beyond Language Modeling</a> Alex Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schuster2018cross/">Cross-lingual Transfer Learning For Multilingual Task Oriented Dialog</a> Sebastian Schuster, Sonal Gupta, Rushin Shah, Mike Lewis </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ilievski2018goal/">Goal-oriented Chatbot Dialog Management Bootstrapping With Transfer Learning</a> Vladimir Ilievski, Claudiu Musat, Andreea Hossmann, Michael Baeriswyl </li>
     
   
     
   
     
       <li> <a href="/publications/lee2018zero/">Zero-shot Adaptive Transfer For Conversational Language Understanding</a> Sungjin Lee, Rahul Jha </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mazar%C3%A92018training/">Training Millions Of Personalized Dialogue Agents</a> Pierre-emmanuel Mazar√©, Samuel Humeau, Martin Raison, Antoine Bordes </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/niu2018polite/">Polite Dialogue Generation Without Parallel Data</a> Tong Niu, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kitaev2018multilingual/">Multilingual Constituency Parsing With Self-attention And Pre-training</a> Nikita Kitaev, Steven Cao, Dan Klein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2018fine/">Lingke: A Fine-grained Multi-turn Chatbot For Customer Service</a> Pengfei Zhu, Zhuosheng Zhang, Jiangtong Li, Yafang Huang, Hai Zhao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cer2018universal/">Universal Sentence Encoder</a> Daniel Cer et al. </li>
     
   
     
       <li> <a href="/publications/sun2018improving/">Improving Machine Reading Comprehension With General Reading Strategies</a> Kai Sun, Dian Yu, Dong Yu, Claire Cardie </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2018universal/">Universal Neural Machine Translation For Extremely Low Resource Languages</a> Jiatao Gu, Hany Hassan, Jacob Devlin, Victor O. K. Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/garg2019transfer/">TANDA: Transfer And Adapt Pre-trained Transformer Models For Answer Sentence Selection</a> Siddhant Garg, Thuy Vu, Alessandro Moschitti </li>
     
   
     
       <li> <a href="/publications/vanaken2019how/">How Does BERT Answer Questions? A Layer-wise Analysis Of Transformer Representations</a> Betty Van Aken, Benjamin Winter, Alexander L√∂ser, Felix A. Gers </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2019forget/">Forget Me Not: Reducing Catastrophic Forgetting For Domain Adaptation In Reading Comprehension</a> Y. Xu, X. Zhong, A. J. J. Yepes, J. H. Lau </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dathathri2019plug/">Plug And Play Language Models: A Simple Approach To Controlled Text Generation</a> Sumanth Dathathri et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kudugunta2019investigating/">Investigating Multilingual NMT Representations At Scale</a> Sneha Reddy Kudugunta, Ankur Bapna, Isaac Caswell, Naveen Arivazhagan, Orhan Firat </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/budzianowski2019gpt/">Hello, It's GPT-2 -- How Can I Help You? Towards The Use Of Pretrained Language Models For Task-oriented Dialogue Systems</a> Pawe≈Ç Budzianowski, Ivan Vuliƒá </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/eisenschlos2019efficient/">Multifit: Efficient Multi-lingual Language Model Fine-tuning</a> Julian Martin Eisenschlos et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019text/">Text Summarization With Pretrained Encoders</a> Yang Liu, Mirella Lapata </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/witteveen2019paraphrasing/">Paraphrasing With Large Language Models</a> Sam Witteveen, Martin Andrews </li>
     
   
     
       <li> <a href="/publications/wang2019incorporating/">Structbert: Incorporating Language Structures Into Pre-training For Deep Language Understanding</a> Wei Wang et al. </li>
     
   
     
       <li> <a href="/publications/xu2019review/">Review Conversational Reading Comprehension</a> Hu Xu, Bing Liu, Lei Shu, Philip S. Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cohan2019pretrained/">Pretrained Language Models For Sequential Sentence Classification</a> Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, Daniel S. Weld </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019sequence/">Sequence-to-sequence Pre-training With Data Augmentation For Sentence Rewriting</a> Yi Zhang, Tao Ge, Furu Wei, Ming Zhou, Xu Sun </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sanh2019distilled/">Distilbert, A Distilled Version Of BERT: Smaller, Faster, Cheaper And Lighter</a> Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf </li>
     
   
     
       <li> <a href="/publications/bapna2019scalable/">Simple, Scalable Adaptation For Neural Machine Translation</a> Ankur Bapna, Naveen Arivazhagan, Orhan Firat </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salazar2019masked/">Masked Language Model Scoring</a> Julian Salazar, Davis Liang, Toan Q. Nguyen, Katrin Kirchhoff </li>
     
   
     
       <li> <a href="/publications/song2019masked/">MASS: Masked Sequence To Sequence Pre-training For Language Generation</a> Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-yan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aliannejadi2019harnessing/">Harnessing Evolution Of Multi-turn Conversations For Effective Answer Retrieval</a> Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andr√©s R√≠ssola, Fabio Crestani </li>
     
   
     
       <li> <a href="/publications/turc2019well/">Well-read Students Learn Better: On The Importance Of Pre-training Compact Models</a> Iulia Turc, Ming-wei Chang, Kenton Lee, Kristina Toutanova </li>
     
   
     
   
     
       <li> <a href="/publications/yang2019towards/">Towards Making The Most Of BERT In Neural Machine Translation</a> Jiacheng Yang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019bert/">BERT For Joint Intent Classification And Slot Filling</a> Qian Chen, Zhu Zhuo, Wen Wang </li>
     
   
     
   
     
       <li> <a href="/publications/li2019say/">Don't Say That! Making Inconsistent Dialogue Unlikely With Unlikelihood Training</a> Margaret Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2019towards/">Towards Transfer Learning For End-to-end Speech Synthesis From Deep Pre-trained Language Models</a> Wei Fang, Yu-an Chung, James Glass </li>
     
   
     
       <li> <a href="/publications/henderson2019training/">Training Neural Response Selection For Task-oriented Dialogue Systems</a> Matthew Henderson et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/talmor2019olmpics/">Olmpics -- On What Language Model Pre-training Captures</a> Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2019what/">What Would Elsa Do? Freezing Layers During Transformer Fine-tuning</a> Jaejun Lee, Raphael Tang, Jimmy Lin </li>
     
   
     
   
     
       <li> <a href="/publications/yang2019data/">Data Augmentation For BERT Fine-tuning In Open-domain Question Answering</a> Wei Yang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/si2019what/">What Does BERT Learn From Multiple-choice Reading Comprehension Datasets?</a> Chenglei Si, Shuohang Wang, Min-yen Kan, Jing Jiang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2019mask/">Align, Mask And Select: A Simple Method For Incorporating Commonsense Knowledge Into Language Representation Models</a> Zhi-xiu Ye, Qian Chen, Wen Wang, Zhen-hua Ling </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2019enhanced/">Freelb: Enhanced Adversarial Training For Natural Language Understanding</a> Chen Zhu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/khandelwal2019sample/">Sample Efficient Text Summarization Using A Single Pre-trained Transformer</a> Urvashi Khandelwal, Kevin Clark, Dan Jurafsky, Lukasz Kaiser </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alberti2019synthetic/">Synthetic QA Corpora Generation With Roundtrip Consistency</a> Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, Michael Collins </li>
     
   
     
   
     
       <li> <a href="/publications/chen2019distilling/">Distilling Knowledge Learned In BERT For Text Generation</a> Yen-chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu </li>
     
   
     
   
     
       <li> <a href="/publications/donahue2019improving/">Lakhnes: Improving Multi-instrumental Music Generation With Cross-domain Pre-training</a> Chris Donahue, Huanru Henry Mao, Yiting Ethan Li, Garrison W. Cottrell, Julian Mcauley </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2019cross/">Cross-lingual Transfer Learning For Question Answering</a> Chia-hsuan Lee, Hung-yi Lee </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2019how/">How To Fine-tune BERT For Text Classification?</a> Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schmidt2019generalization/">Generalization In Generation: A Closer Look At Exposure Bias</a> Florian Schmidt </li>
     
   
     
       <li> <a href="/publications/clark2019exploring/">Boolq: Exploring The Surprising Difficulty Of Natural Yes/no Questions</a> Christopher Clark et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2019universal/">Unicoder: A Universal Language Encoder By Pre-training With Multiple Cross-lingual Tasks</a> Haoyang Huang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019structured/">Structured Pruning Of Large Language Models</a> Ziheng Wang, Jeremy Wohlwend, Tao Lei </li>
     
   
     
   
     
       <li> <a href="/publications/tan2019learning/">LXMERT: Learning Cross-modality Encoder Representations From Transformers</a> Hao Tan, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2019empathetic/">Caire: An Empathetic Neural Chatbot</a> Zhaojiang Lin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/junczysdowmunt2019microsoft/">Microsoft Translator At WMT 2019: Towards Large-scale Document-level Neural Machine Translation</a> Marcin Junczys-dowmunt </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/carrino2019automatic/">Automatic Spanish Translation Of The Squad Dataset For Multilingual Question Answering</a> Casimiro Pio Carrino, Marta R. Costa-juss√†, Jos√© A. R. Fonollosa </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sakaguchi2019adversarial/">Winogrande: An Adversarial Winograd Schema Challenge At Scale</a> Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi </li>
     
   
     
   
     
       <li> <a href="/publications/wolf2019transfer/">Transfertransfo: A Transfer Learning Approach For Neural Network Based Conversational Agents</a> Thomas Wolf, Victor Sanh, Julien Chaumond, Clement Delangue </li>
     
   
     
       <li> <a href="/publications/pan2019frustratingly/">Frustratingly Easy Natural Question Answering</a> Lin Pan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qian2019domain/">Domain Adaptive Dialog Generation Via Meta Learning</a> Kun Qian, Zhou Yu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/schwartz2019inducing/">Inducing Brain-relevant Bias In Natural Language Processing Models</a> Dan Schwartz, Mariya Toneva, Leila Wehbe </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019semantics/">Semantics-aware BERT For Language Understanding</a> Zhuosheng Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/htut2019do/">Do Attention Heads In BERT Track Syntactic Dependencies?</a> Phu Mon Htut, Jason Phang, Shikha Bordia, Samuel R. Bowman </li>
     
   
     
       <li> <a href="/publications/raffel2019exploring/">Exploring The Limits Of Transfer Learning With A Unified Text-to-text Transformer</a> Colin Raffel et al. </li>
     
   
     
   
     
       <li> <a href="/publications/richardson2019probing/">Probing Natural Language Inference Models Through Semantic Fragments</a> Kyle Richardson, Hai Hu, Lawrence S. Moss, Ashish Sabharwal </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/weng2019acquiring/">Acquiring Knowledge From Pre-trained Model To Neural Machine Translation</a> Rongxiang Weng, Heng Yu, Shujian Huang, Shanbo Cheng, Weihua Luo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ziegler2019fine/">Fine-tuning Language Models From Human Preferences</a> Daniel M. Ziegler et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/petroni2019language/">Language Models As Knowledge Bases?</a> Fabio Petroni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2019visualizing/">Visualizing And Understanding The Effectiveness Of BERT</a> Yaru Hao, Li Dong, Furu Wei, Ke Xu </li>
     
   
     
   
     
       <li> <a href="/publications/guo2019fine/">Fine-tuning By Curriculum Learning For Non-autoregressive Neural Machine Translation</a> Junliang Guo et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/edunov2019pre/">Pre-trained Language Model Representations For Language Generation</a> Sergey Edunov, Alexei Baevski, Michael Auli </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019tree/">Tree Transformer: Integrating Tree Structures Into Self-attention</a> Yau-shian Wang, Hung-yi Lee, Yun-nung Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chi2019just/">Just Ask:an Interactive Learning Framework For Vision And Language Navigation</a> Ta-chung Chi, Mihail Eric, Seokhwan Kim, Minmin Shen, Dilek Hakkani-tur </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kanade2019learning/">Learning And Evaluating Contextual Embedding Of Source Code</a> Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019story/">Story Ending Prediction By Transferable BERT</a> Zhongyang Li, Xiao Ding, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singh2019cross/">XLDA: Cross-lingual Data Augmentation For Natural Language Inference And Question Answering</a> Jasdeep Singh, Bryan Mccann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/keh2019myers/">Myers-briggs Personality Classification And Personality-specific Language Generation Using Pre-trained Language Models</a> Sedrick Scott Keh, I-tsun Cheng </li>
     
   
     
   
     
       <li> <a href="/publications/zafrir2019quantized/">Q8BERT: Quantized 8bit BERT</a> Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/qiao2019understanding/">Understanding The Behaviors Of BERT In Ranking</a> Yifan Qiao, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bansal2019learning/">Learning To Few-shot Learn Across Diverse Natural Language Classification Tasks</a> Trapit Bansal, Rishikesh Jha, Andrew Mccallum </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sundararaman2019syntax/">Syntax-infused Transformer And BERT Models For Machine Translation And Natural Language Understanding</a> Dhanasekar Sundararaman et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2019universal/">Universal Text Representation From BERT: An Empirical Study</a> Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, Bing Xiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mass2019study/">A Study Of BERT For Non-factoid Question-answering Under Passage Length Constraints</a> Yosi Mass et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pino2019harnessing/">Harnessing Indirect Training Data For End-to-end Automatic Speech Translation: Tricks Of The Trade</a> Juan Pino et al. </li>
     
   
     
       <li> <a href="/publications/lee2019patent/">Patent Claim Generation By Fine-tuning Openai GPT-2</a> Jieh-sheng Lee, Jieh Hsiang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liao2019gpt/">Gpt-based Generation For Classical Chinese Poetry</a> Yi Liao, Yasheng Wang, Qun Liu, Xin Jiang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2019model/">Model Compression With Two-stage Multi-teacher Knowledge Distillation For Web Question Answering System</a> Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, Daxin Jiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/press2019improving/">Improving Transformer Models By Reordering Their Sublayers</a> Ofir Press, Noah A. Smith, Omer Levy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/houlsby2019parameter/">Parameter-efficient Transfer Learning For NLP</a> Neil Houlsby et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kumar2019reinforcement/">Reinforcement Learning Based Curriculum Optimization For Neural Machine Translation</a> Gaurav Kumar, George Foster, Colin Cherry, Maxim Krikun </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mihaylova2019scheduled/">Scheduled Sampling For Transformers</a> Tsvetomila Mihaylova, Andr√© F. T. Martins </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2019reweighted/">Reweighted Proximal Pruning For Large-scale Language Representation</a> Fu-ming Guo, Sijia Liu, Finlay S. Mungall, Xue Lin, Yanzhi Wang </li>
     
   
     
   
     
       <li> <a href="/publications/yu2020fine/">Fine-tuning Pre-trained Language Model With Weak Supervision: A Contrastive-regularized Self-training Approach</a> Yue Yu et al. </li>
     
   
     
       <li> <a href="/publications/noever2020chess/">The Chess Transformer: Mastering Play Using Generative Language Models</a> David Noever, Matt Ciolino, Josh Kalin </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khashabi2020crossing/">Unifiedqa: Crossing Format Boundaries With A Single QA System</a> Daniel Khashabi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/saunders2020reducing/">Reducing Gender Bias In Neural Machine Translation As A Domain Adaptation Problem</a> Danielle Saunders, Bill Byrne </li>
     
   
     
       <li> <a href="/publications/guu2020retrieval/">REALM: Retrieval-augmented Language Model Pre-training</a> Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-wei Chang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lukovnikov2020pretrained/">Pretrained Transformers For Simple Question Answering Over Knowledge Graphs</a> D. Lukovnikov, A. Fischer, J. Lehmann </li>
     
   
     
   
     
       <li> <a href="/publications/cruzbenito2020automated/">Automated Source Code Generation And Auto-completion Using Deep Learning: Comparing And Discussing Current Language-model-related Approaches</a> Juan Cruz-benito, Sanjay Vishwakarma, Francisco Martin-fernandez, Ismael Faro </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shuster2020multi/">Multi-modal Open-domain Dialogue</a> Kurt Shuster, Eric Michael Smith, Da Ju, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jain2020indic/">Indic-transformers: An Analysis Of Transformer Language Models For Indian Languages</a> Kushal Jain, Adwait Deshpande, Kumar Shridhar, Felix Laumann, Ayushman Dash </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mehri2020natural/">Dialoglue: A Natural Language Understanding Benchmark For Task-oriented Dialogue</a> Shikib Mehri, Mihail Eric, Dilek Hakkani-tur </li>
     
   
     
       <li> <a href="/publications/donahue2020enabling/">Enabling Language Models To Fill In The Blanks</a> Chris Donahue, Mina Lee, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020multilingual/">Multilingual Speech Translation With Efficient Finetuning Of Pretrained Models</a> Xian Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/radiyadixit2020how/">How Fine Can Fine-tuning Be? Learning Efficient Language Models</a> Evani Radiya-dixit, Xin Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020when/">When Do You Need Billions Of Words Of Pretraining Data?</a> Yian Zhang, Alex Warstadt, Haau-sing Li, Samuel R. Bowman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2020incorporating/">Incorporating BERT Into Neural Machine Translation</a> Jinhua Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kale2020text/">Text-to-text Pre-training For Data-to-text Tasks</a> Mihir Kale, Abhinav Rastogi </li>
     
   
     
       <li> <a href="/publications/vu2020exploring/">Exploring And Predicting Transferability Across NLP Tasks</a> Tu Vu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2020compact/">Mobilebert: A Compact Task-agnostic BERT For Resource-limited Devices</a> Zhiqing Sun et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bugliarello2020multimodal/">Multimodal Pretraining Unmasked: A Meta-analysis And A Unified Framework Of Vision-and-language Berts</a> Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2020ernie/">ERNIE-GEN: An Enhanced Multi-flow Pre-training And Fine-tuning Framework For Natural Language Generation</a> Dongling Xiao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2020simple/">A Simple But Tough-to-beat Data Augmentation Approach For Natural Language Understanding And Generation</a> Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, Weizhu Chen </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020exploring/">Exploring Fine-tuning Techniques For Pre-trained Cross-lingual Models Via Continual Learning</a> Zihan Liu, Genta Indra Winata, Andrea Madotto, Pascale Fung </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2020simple/">A Simple Baseline To Semi-supervised Domain Adaptation For Machine Translation</a> Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2020discourse/">Dialogbert: Discourse-aware Response Generation Via Learning To Recover And Rank Utterances</a> Xiaodong Gu, Kang Min Yoo, Jung-woo Ha </li>
     
   
     
       <li> <a href="/publications/liu2020adversarial/">Adversarial Training For Large Neural Language Models</a> Xiaodong Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/koutsikakis2020greek/">GREEK-BERT: The Greeks Visiting Sesame Street</a> John Koutsikakis, Ilias Chalkidis, Prodromos Malakasiotis, Ion Androutsopoulos </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2020coreferential/">Coreferential Reasoning Learning For Language Representation</a> Deming Ye et al. </li>
     
   
     
       <li> <a href="/publications/pfeiffer2020framework/">Adapterhub: A Framework For Adapting Transformers</a> Jonas Pfeiffer et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020recall/">Recall And Learn: Fine-tuning Deep Pretrained Language Models With Less Forgetting</a> Sanyuan Chen et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gupta2020bert/">BERT Based Multilingual Machine Comprehension In English And Hindi</a> Somil Gupta, Nilesh Khade </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/penha2020what/">What Does BERT Know About Books, Movies And Music? Probing BERT For Conversational Recommendation</a> Gustavo Penha, Claudia Hauff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/heyman2020neural/">Neural Code Search Revisited: Enhancing Code Snippet Retrieval Through Natural Language Intent</a> Geert Heyman, Tom Van Cutsem </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2020pre/">Pre-training Via Paraphrasing</a> Mike Lewis et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pearce2020deriving/">DAVE: Deriving Automatically Verilog From English</a> Hammond Pearce, Benjamin Tan, Ramesh Karri </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020knowledge/">KGPT: Knowledge-grounded Pre-training For Data-to-text Generation</a> Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fei2020retrofitting/">Retrofitting Structure-aware Transformer Language Model For End Tasks</a> Hao Fei, Yafeng Ren, Donghong Ji </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2020minimalist/">Mintl: Minimalist Transfer Learning For Task-oriented Dialogue Systems</a> Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/si2020better/">Better Robustness By More Coverage: Adversarial Training With Mixup Augmentation For Robust Fine-tuning</a> Chenglei Si et al. </li>
     
   
     
       <li> <a href="/publications/gao2020dataset/">The Pile: An 800GB Dataset Of Diverse Text For Language Modeling</a> Leo Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2020how/">How Can We Know When Language Models Know? On The Calibration Of Language Models For Question Answering</a> Zhengbao Jiang, Jun Araki, Haibo Ding, Graham Neubig </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2020modifying/">Modifying Memories In Transformer Models</a> Chen Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2020retrieval/">Retrieval-augmented Generation For Knowledge-intensive NLP Tasks</a> Patrick Lewis et al. </li>
     
   
     
   
     
       <li> <a href="/publications/feng2020pre/">Codebert: A Pre-trained Model For Programming And Natural Languages</a> Zhangyin Feng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shleifer2020pre/">Pre-trained Summarization Distillation</a> Sam Shleifer, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2020detecting/">Detecting Hallucinated Content In Conditional Neural Sequence Generation</a> Chunting Zhou et al. </li>
     
   
     
       <li> <a href="/publications/wang2020fairseq/">Fairseq S2T: Fast Speech-to-text Modeling With Fairseq</a> Changhan Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bunk2020lightweight/">DIET: Lightweight Language Understanding For Dialogue Systems</a> Tanja Bunk, Daksh Varshneya, Vladimir Vlasov, Alan Nichol </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2020progressive/">Progressive Generation Of Long Text With Pretrained Language Models</a> Bowen Tan, Zichao Yang, Maruan Ai-shedivat, Eric P. Xing, Zhiting Hu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2020cosda/">Cosda-ml: Multi-lingual Code-switching Data Augmentation For Zero-shot Cross-lingual NLP</a> Libo Qin, Minheng Ni, Yue Zhang, Wanxiang Che </li>
     
   
     
       <li> <a href="/publications/pruksachatkun2020intermediate/">Intermediate-task Transfer Learning With Pretrained Models For Natural Language Understanding: When And Why Does It Work?</a> Yada Pruksachatkun et al. </li>
     
   
     
       <li> <a href="/publications/majumder2020like/">Like Hiking? You Probably Enjoy Nature: Persona-grounded Dialog With Commonsense Expansions</a> Bodhisattwa Prasad Majumder, Harsh Jhamtani, Taylor Berg-kirkpatrick, Julian Mcauley </li>
     
   
     
   
     
       <li> <a href="/publications/yang2020towards/">UBAR: Towards Fully End-to-end Task-oriented Dialog Systems With GPT-2</a> Yunyi Yang, Yunhao Li, Xiaojun Quan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bi2020pre/">PALM: Pre-training An Autoencoding&autoregressive Language Model For Context-conditioned Generation</a> Bin Bi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ranasinghe2020transquest/">Transquest At WMT2020: Sentence-level Direct Assessment</a> Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ruan2020fine/">Fine-tuning BERT For Schema-guided Zero-shot Dialogue State Tracking</a> Yu-ping Ruan, Zhen-hua Ling, Jia-chen Gu, Quan Liu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/muller2020when/">When Being Unseen From Mbert Is Just The Beginning: Handling New Languages With Multilingual Language Models</a> Benjamin Muller, Antonis Anastasopoulos, Beno√Æt Sagot, Djam√© Seddah </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2020building/">SOLOIST: Building Task Bots At Scale With Transfer Learning And Machine Teaching</a> Baolin Peng et al. </li>
     
   
     
       <li> <a href="/publications/ezencan2020comparison/">A Comparison Of LSTM And BERT For Small Corpus</a> Aysu Ezen-can </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aghajanyan2020better/">Better Fine-tuning By Reducing Representational Collapse</a> Armen Aghajanyan et al. </li>
     
   
     
       <li> <a href="/publications/majumdar2020improving/">Improving Vision-and-language Navigation With Image-text Pairs From The Web</a> Arjun Majumdar et al. </li>
     
   
     
       <li> <a href="/publications/cohan2020document/">SPECTER: Document-level Representation Learning Using Citation-informed Transformers</a> Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S. Weld </li>
     
   
     
   
     
       <li> <a href="/publications/chen2020facebook/">Facebook Ai's WMT20 News Translation Task Submission</a> Peng-jen Chen et al. </li>
     
   
     
       <li> <a href="/publications/bao2020plato/">PLATO-2: Towards Building An Open-domain Chatbot Via Curriculum Learning</a> Siqi Bao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lauscher2020from/">From Zero To Hero: On The Limitations Of Zero-shot Cross-lingual Transfer With Multilingual Transformers</a> Anne Lauscher, Vinit Ravishankar, Ivan Vuliƒá, Goran Glava≈° </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020unqovering/">Unqovering Stereotyping Biases Via Underspecified Questions</a> Tao Li, Tushar Khot, Daniel Khashabi, Ashish Sabharwal, Vivek Srikumar </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shakeri2020end/">End-to-end Synthetic Data Generation For Domain Adaptation Of Question Answering Systems</a> Siamak Shakeri et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/madotto2020language/">Language Models As Few-shot Learner For Task-oriented Dialogue Systems</a> Andrea Madotto, Zihan Liu, Zhaojiang Lin, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2020exploring/">Exploring Versatile Generative Language Model Via Parameter-efficient Transfer Learning</a> Zhaojiang Lin, Andrea Madotto, Pascale Fung </li>
     
   
     
   
     
       <li> <a href="/publications/li2020empirical/">An Empirical Investigation Of Pre-trained Transformer Language Models For Open-domain Dialogue Generation</a> Piji Li </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/merchant2020what/">What Happens To BERT Embeddings During Fine-tuning?</a> Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, Ian Tenney </li>
     
   
     
   
     
       <li> <a href="/publications/guo2020cross/">Multireqa: A Cross-domain Evaluation For Retrieval Question Answering Models</a> Mandy Guo, Yinfei Yang, Daniel Cer, Qinlan Shen, Noah Constant </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mosbach2020stability/">On The Stability Of Fine-tuning BERT: Misconceptions, Explanations, And Strong Baselines</a> Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow </li>
     
   
     
   
     
       <li> <a href="/publications/yu2020few/">Few-shot Generative Conversational Query Rewriting</a> Shi Yu et al. </li>
     
   
     
       <li> <a href="/publications/lopez2020simplifying/">Simplifying Paragraph-level Question Generation Via Transformer Language Models</a> Luis Enrico Lopez, Diane Kathryn Cruz, Jan Christian Blaise Cruz, Charibeth Cheng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mehri2020unsupervised/">Unsupervised Evaluation Of Interactive Dialog With Dialogpt</a> Shikib Mehri, Maxine Eskenazi </li>
     
   
     
       <li> <a href="/publications/fabbri2020template/">Template-based Question Generation From Retrieved Sentences For Improved Unsupervised Question Answering</a> Alexander R. Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, Bing Xiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/roberts2020how/">How Much Knowledge Can You Pack Into The Parameters Of A Language Model?</a> Adam Roberts, Colin Raffel, Noam Shazeer </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020efficient/">Earlybert: Efficient BERT Training Via Early-bird Lottery Tickets</a> Xiaohan Chen et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nie2020contextualized/">Coregen: Contextualized Code Representation Learning For Commit Message Generation</a> Lun Yiu Nie et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/han2020effective/">ECONET: Effective Continual Pretraining Of Language Models For Event Temporal Reasoning</a> Rujun Han, Xiang Ren, Nanyun Peng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2020data/">Genaug: Data Augmentation For Finetuning Text Generators</a> Steven Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, Eduard Hovy </li>
     
   
     
       <li> <a href="/publications/b2020language/">Language Models Are Few-shot Learners</a> Tom B. Brown et al. </li>
     
   
     
       <li> <a href="/publications/guo2020incorporating/">Incorporating BERT Into Parallel Sequence Decoding With Adapters</a> Junliang Guo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/beltagy2020long/">Longformer: The Long-document Transformer</a> Iz Beltagy, Matthew E. Peters, Arman Cohan </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2020pre/">Pre-training Text-to-text Transformers For Concept-centric Common Sense</a> Wangchunshu Zhou et al. </li>
     
   
     
       <li> <a href="/publications/casanueva2020efficient/">Efficient Intent Detection With Dual Sentence Encoders</a> I√±igo Casanueva, Tadas Temƒçinas, Daniela Gerz, Matthew Henderson, Ivan Vuliƒá </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020making/">Making Pre-trained Language Models Better Few-shot Learners</a> Tianyu Gao, Adam Fisch, Danqi Chen </li>
     
   
     
       <li> <a href="/publications/min2020syntactic/">Syntactic Data Augmentation Increases Robustness To Inference Heuristics</a> Junghyun Min, R. Thomas Mccoy, Dipanjan Das, Emily Pitler, Tal Linzen </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chung2020rethinking/">Rethinking Embedding Coupling In Pre-trained Language Models</a> Hyung Won Chung, Thibault F√©vry, Henry Tsai, Melvin Johnson, Sebastian Ruder </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2020multilingual/">Multilingual Translation With Extensible Multilingual Pretraining And Finetuning</a> Yuqing Tang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2020towards/">Towards Learning A Generic Agent For Vision-and-language Navigation Via Pre-training</a> Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2020improving/">Vokenization: Improving Language Understanding With Contextualized, Visual-grounded Supervision</a> Hao Tan, Mohit Bansal </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/guan2020knowledge/">A Knowledge-enhanced Pretraining Model For Commonsense Story Generation</a> Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, Minlie Huang </li>
     
   
     
   
     
       <li> <a href="/publications/bird2020chatbot/">Chatbot Interaction With Artificial Intelligence: Human Data Augmentation With T5 And Language Transformer Ensemble For Text Classification</a> Jordan J. Bird, Anik√≥ Ek√°rt, Diego R. Faria </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2020speaker/">Speaker-aware BERT For Multi-turn Response Selection In Retrieval-based Chatbots</a> Jia-chen Gu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dodge2020fine/">Fine-tuning Pretrained Language Models: Weight Initializations, Data Orders, And Early Stopping</a> Jesse Dodge et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2021towards/">Towards Few-shot Fact-checking Via Perplexity</a> Nayeon Lee, Yejin Bang, Andrea Madotto, Madian Khabsa, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2021learning/">Learning How To Ask: Querying Lms With Mixtures Of Soft Prompts</a> Guanghui Qin, Jason Eisner </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lourie2021unicorn/">UNICORN On RAINBOW: A Universal Commonsense Reasoning Model On A New Multitask Benchmark</a> Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi </li>
     
   
     
       <li> <a href="/publications/lin2021bilingual/">Bitod: A Bilingual Multi-domain Dataset For Task-oriented Dialogue Modeling</a> Zhaojiang Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2021tip/">Tip-adapter: Training-free Clip-adapter For Better Vision-language Modeling</a> Renrui Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2021prompt/">Prompt-learning For Fine-grained Entity Typing</a> Ning Ding et al. </li>
     
   
     
       <li> <a href="/publications/mitchell2021fast/">Fast Model Editing At Scale</a> Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D. Manning </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/reif2021recipe/">A Recipe For Arbitrary Text Style Transfer With Large Language Models</a> Emily Reif et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nagoudi2021text/">Arat5: Text-to-text Transformers For Arabic Language Generation</a> El Moatez Billah Nagoudi, Abdelrahim Elmadany, Muhammad Abdul-mageed </li>
     
   
     
       <li> <a href="/publications/zaken2021simple/">Bitfit: Simple Parameter-efficient Fine-tuning For Transformer-based Masked Language-models</a> Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg </li>
     
   
     
       <li> <a href="/publications/hu2021low/">Lora: Low-rank Adaptation Of Large Language Models</a> Edward J. Hu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nakano2021browser/">Webgpt: Browser-assisted Question-answering With Human Feedback</a> Reiichiro Nakano et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2021exploratory/">An Exploratory Study On Long Dialogue Summarization: What Works And What's Next</a> Yusen Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021explaining/">On Explaining Your Explanations Of BERT: An Empirical Study With Sequence Classification</a> Zhengxuan Wu, Desmond C. Ong </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021p/">P-tuning V2: Prompt Tuning Can Be Comparable To Fine-tuning Universally Across Scales And Tasks</a> Xiao Liu et al. </li>
     
   
     
       <li> <a href="/publications/tam2021improving/">Improving And Simplifying Pattern Exploiting Training</a> Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, Colin Raffel </li>
     
   
     
       <li> <a href="/publications/cai2021neural/">Neural Machine Translation With Monolingual Translation Memory</a> Deng Cai, Yan Wang, Huayang Li, Wai Lam, Lemao Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2021scale/">Scale Efficiently: Insights From Pre-training And Fine-tuning Transformers</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021entailment/">Entailment As Few-shot Learner</a> Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, Hao Ma </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pascual2021plug/">A Plug-and-play Method For Controlled Text Generation</a> Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell, Roger Wattenhofer </li>
     
   
     
   
     
       <li> <a href="/publications/dai2021knowledge/">Knowledge Neurons In Pretrained Transformers</a> Damai Dai et al. </li>
     
   
     
       <li> <a href="/publications/wang2021can/">Can Generative Pre-trained Language Models Serve As Knowledge Bases For Closed-book QA?</a> Cunxiang Wang, Pai Liu, Yue Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vu2021better/">Spot: Better Frozen Model Adaptation Through Soft Prompt Transfer</a> Tu Vu, Brian Lester, Noah Constant, Rami Al-rfou, Daniel Cer </li>
     
   
     
       <li> <a href="/publications/ding2021mastering/">Cogview: Mastering Text-to-image Generation Via Transformers</a> Ming Ding et al. </li>
     
   
     
       <li> <a href="/publications/artetxe2021efficient/">Efficient Large Scale Language Modeling With Mixtures Of Experts</a> Mikel Artetxe et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ni2021sentence/">Sentence-t5: Scalable Sentence Encoders From Pre-trained Text-to-text Models</a> Jianmo Ni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021revisiting/">Revisiting Self-training For Few-shot Learning Of Language Model</a> Yiming Chen et al. </li>
     
   
     
       <li> <a href="/publications/aribandi2021towards/">Ext5: Towards Extreme Multi-task Scaling For Transfer Learning</a> Vamsi Aribandi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sanh2021multitask/">Multitask Prompted Training Enables Zero-shot Task Generalization</a> Victor Sanh et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chen2021simple/">Hiddencut: Simple Data Augmentation For Natural Language Understanding With Better Generalization</a> Jiaao Chen, Dinghan Shen, Weizhu Chen, Diyi Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021recursively/">Recursively Summarizing Books With Human Feedback</a> Jeff Wu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2021finetuned/">Finetuned Language Models Are Zero-shot Learners</a> Jason Wei et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/austin2021program/">Program Synthesis With Large Language Models</a> Jacob Austin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chada2021simple/">Fewshotqa: A Simple Framework For Few-shot Learning Of Question Answering Tasks Using Pre-trained Text-to-text Models</a> Rakesh Chada, Pradeep Natarajan </li>
     
   
     
       <li> <a href="/publications/turc2021revisiting/">Revisiting The Primacy Of English In Zero-shot Cross-lingual Transfer</a> Iulia Turc, Kenton Lee, Jacob Eisenstein, Ming-wei Chang, Kristina Toutanova </li>
     
   
     
       <li> <a href="/publications/sun2021ernie/">ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training For Language Understanding And Generation</a> Yu Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gu2021pre/">PPT: Pre-trained Prompt Tuning For Few-shot Learning</a> Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang </li>
     
   
     
       <li> <a href="/publications/solaiman2021process/">Process For Adapting Language Models To Society (PALMS) With Values-targeted Datasets</a> Irene Openai Solaiman, Christy Openai Dennison </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/logan2021cutting/">Cutting Down On Prompts And Parameters: Simple Few-shot Learning With Language Models</a> Robert L. Iv Logan et al. </li>
     
   
     
       <li> <a href="/publications/tinn2021fine/">Fine-tuning Large Neural Language Models For Biomedical Natural Language Processing</a> Robert Tinn et al. </li>
     
   
     
       <li> <a href="/publications/lai2021thank/">Thank You BART! Rewarding Pre-trained Models Improves Formality Style Transfer</a> Huiyuan Lai, Antonio Toral, Malvina Nissim </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021prefix/">Prefix-tuning: Optimizing Continuous Prompts For Generation</a> Xiang Lisa Li, Percy Liang </li>
     
   
     
       <li> <a href="/publications/rothe2021simple/">A Simple Recipe For Multilingual Grammatical Error Correction</a> Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause, Aliaksei Severyn </li>
     
   
     
       <li> <a href="/publications/gheini2021cross/">Cross-attention Is All You Need: Adapting Pretrained Transformers For Machine Translation</a> Mozhdeh Gheini, Xiang Ren, Jonathan May </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2021colorful/">CPT: Colorful Prompt Tuning For Pre-trained Vision-language Models</a> Yuan Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021gradual/">Gradual Fine-tuning For Low-resource Domain Adaptation</a> Haoran Xu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sung2021vl/">Vl-adapter: Parameter-efficient Transfer Learning For Vision-and-language Tasks</a> Yi-lin Sung, Jaemin Cho, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021measuring/">Truthfulqa: Measuring How Models Mimic Human Falsehoods</a> Stephanie Lin, Jacob Hilton, Owain Evans </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2021using/">Using Prior Knowledge To Guide Bert's Attention In Semantic Textual Matching Tasks</a> Tingyu Xia, Yue Wang, Yuan Tian, Yi Chang </li>
     
   
     
       <li> <a href="/publications/borgeaud2021improving/">Improving Language Models By Retrieving From Trillions Of Tokens</a> Sebastian Borgeaud et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021pretrained/">Pretrained Language Models For Text Generation: A Survey</a> Junyi Li, Tianyi Tang, Wayne Xin Zhao, Ji-rong Wen </li>
     
   
     
   
     
       <li> <a href="/publications/kim2021scalable/">Scalable And Efficient Moe Training For Multitask Multilingual Models</a> Young Jin Kim et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singh2021nlp/">The NLP Cookbook: Modern Recipes For Transformer Based Deep Learning Architectures</a> Sushant Singh, Ausif Mahmood </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/p%C3%A9rez2021pre/">Robertuito: A Pre-trained Language Model For Social Media Text In Spanish</a> Juan Manuel P√©rez, Dami√°n A. Furman, Laura Alonso Alemany, Franco Luque </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zellers2021multimodal/">MERLOT: Multimodal Neural Script Knowledge Models</a> Rowan Zellers et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kalyan2021ammus/">AMMUS : A Survey Of Transformer-based Pretrained Models In Natural Language Processing</a> Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, Sivanesan Sangeetha </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hambardzumyan2021word/">WARP: Word-level Adversarial Reprogramming</a> Karen Hambardzumyan, Hrant Khachatrian, Jonathan May </li>
     
   
     
       <li> <a href="/publications/yoo2021leveraging/">Gpt3mix: Leveraging Large-scale Language Models For Text Augmentation</a> Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-woo Lee, Woomyeong Park </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/manakul2021long/">Long-span Summarization Via Local Attention And Content Selection</a> Potsawee Manakul, Mark J. F. Gales </li>
     
   
     
   
     
       <li> <a href="/publications/poth2021what/">What To Pre-train On? Efficient Intermediate Task Selection</a> Clifton Poth, Jonas Pfeiffer, Andreas R√ºckl√©, Iryna Gurevych </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dong2021how/">How Should Pre-trained Language Models Be Fine-tuned Towards Adversarial Robustness?</a> Xinhsuai Dong, Luu Anh Tuan, Min Lin, Shuicheng Yan, Hanwang Zhang </li>
     
   
     
       <li> <a href="/publications/li2021scheduled/">Scheduled Sampling In Vision-language Pretraining With Decoupled Encoder-decoder Network</a> Yehao Li, Yingwei Pan, Ting Yao, Jingwen Chen, Tao Mei </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ram2021few/">Few-shot Question Answering By Pretraining Span Selection</a> Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2021cpm/">CPM-2: Large-scale Cost-effective Pre-trained Language Models</a> Zhengyan Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mao2021unified/">Unipelt: A Unified Framework For Parameter-efficient Language Model Tuning</a> Yuning Mao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2021compressing/">Compressing Visual-linguistic Model Via Knowledge Distillation</a> Zhiyuan Fang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schramowski2021large/">Large Pre-trained Language Models Contain Human-like Biases Of What Is Right And Wrong To Do</a> Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A. Rothkopf, Kristian Kersting </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/min2021recent/">Recent Advances In Natural Language Processing Via Large Pre-trained Language Models: A Survey</a> Bonan Min et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2021transferability/">On Transferability Of Prompt Tuning For Natural Language Processing</a> Yusheng Su et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2021clip/">Clip-adapter: Better Vision-language Models With Feature Adapters</a> Peng Gao et al. </li>
     
   
     
       <li> <a href="/publications/chen2021meta/">Meta-learning Via Language Model In-context Tuning</a> Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, He He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aghajanyan2021hyper/">HTLM: Hyper-text Pre-training And Prompting Of Language Models</a> Armen Aghajanyan et al. </li>
     
   
     
       <li> <a href="/publications/aghajanyan2021massive/">Muppet: Massive Multi-task Representations With Pre-finetuning</a> Armen Aghajanyan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mahabadi2021variational/">Variational Information Bottleneck For Effective Low-resource Fine-tuning</a> Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mahabadi2021efficient/">Compacter: Efficient Low-rank Hypercomplex Adapter Layers</a> Rabeeh Karimi Mahabadi, James Henderson, Sebastian Ruder </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dabre2021pre/">Indicbart: A Pre-trained Model For Indic Natural Language Generation</a> Raj Dabre et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/narayan2021planning/">Planning With Learned Entity Prompts For Abstractive Summarization</a> Shashi Narayan et al. </li>
     
   
     
       <li> <a href="/publications/maneriker2021improving/">Urltran: Improving Phishing URL Detection Using Transformers</a> Pranav Maneriker et al. </li>
     
   
     
   
     
       <li> <a href="/publications/madotto2021few/">Few-shot Bot: Prompt-based Learning For Dialogue Systems</a> Andrea Madotto, Zhaojiang Lin, Genta Indra Winata, Pascale Fung </li>
     
   
     
   
     
       <li> <a href="/publications/min2021following/">FILM: Following Instructions In Language With Modular Methods</a> So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, Ruslan Salakhutdinov </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/atri2021leveraging/">See, Hear, Read: Leveraging Multimodality With Guided Attention For Abstractive Text Summarization</a> Yash Kumar Atri, Shraman Pramanick, Vikram Goyal, Tanmoy Chakraborty </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zafrir2021prune/">Prune Once For All: Sparse Pre-trained Language Models</a> Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, Moshe Wasserblat </li>
     
   
     
       <li> <a href="/publications/zhong2021adapting/">Adapting Language Models For Zero-shot Learning By Meta-tuning On Dataset And Prompt Collections</a> Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phan2021text/">Scifive: A Text-to-text Transformer Model For Biomedical Literature</a> Long N. Phan et al. </li>
     
   
     
       <li> <a href="/publications/xu2021raise/">Raise A Child In Large Language Model: Towards Effective And Generalizable Fine-tuning</a> Runxin Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2021improved/">Improved Text Classification Via Contrastive Adversarial Training</a> Lin Pan, Chung-wei Hang, Avirup Sil, Saloni Potdar </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/scao2021how/">How Many Data Points Is A Prompt Worth?</a> Teven Le Scao, Alexander M. Rush </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ribeiro2021structural/">Structural Adapters In Pretrained Language Models For Amr-to-text Generation</a> Leonardo F. R. Ribeiro, Yue Zhang, Iryna Gurevych </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ruder2021xtreme/">XTREME-R: Towards More Challenging And Nuanced Multilingual Evaluation</a> Sebastian Ruder et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021effectiveness/">On The Effectiveness Of Adapter-based Tuning For Pretrained Language Model Adaptation</a> Ruidan He et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2021unsupervised/">Unsupervised Corpus Aware Language Model Pre-training For Dense Passage Retrieval</a> Luyu Gao, Jamie Callan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021self/">Self-guided Contrastive Learning For BERT Sentence Representations</a> Taeuk Kim, Kang Min Yoo, Sang-goo Lee </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sohn2022visual/">Visual Prompt Tuning For Generative Transfer Learning</a> Kihyuk Sohn et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/levine2022standing/">Standing On The Shoulders Of Giant Frozen Language Models</a> Yoav Levine et al. </li>
     
   
     
       <li> <a href="/publications/sejnowski2022large/">Large Language Models And The Reverse Turing Test</a> Terrence Sejnowski </li>
     
   
     
       <li> <a href="/publications/chen2022think/">Think Global, Act Local: Dual-scale Graph Transformer For Vision-and-language Navigation</a> Shizhe Chen, Pierre-louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jeblick2022chatgpt/">Chatgpt Makes Medicine Easy To Swallow: An Exploratory Case Study On Simplified Radiology Reports</a> Katharina Jeblick et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chang2022exploration/">Speechprompt: An Exploration Of Prompt Tuning On Generative Spoken Language Model For Speech Processing Tasks</a> Kai-wei Chang, Wei-cheng Tseng, Shang-wen Li, Hung-yi Lee </li>
     
   
     
       <li> <a href="/publications/wang2022use/">On The Use Of BERT For Automated Essay Scoring: Joint Learning Of Multi-scale Essay Representation</a> Yongjie Wang, Chuan Wang, Ruobing Li, Hui Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hoffmann2022training/">Training Compute-optimal Large Language Models</a> Jordan Hoffmann et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schneider2022towards/">Towards Trustworthy Autograding Of Short, Multi-lingual, Multi-type Answers</a> Johannes Schneider, Robin Richner, Micha Riser </li>
     
   
     
       <li> <a href="/publications/zhang2022pretraining/">Coditt5: Pretraining For Source Code And Natural Language Editing</a> Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, Milos Gligoric </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qian2022controllable/">Controllable Natural Language Generation With Contrastive Prefixes</a> Jing Qian, Li Dong, Yelong Shen, Furu Wei, Weizhu Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022personalized/">Personalized Prompt For Sequential Recommendation</a> Yiqing Wu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/huang2022large/">Large Language Models Can Self-improve</a> Jiaxin Huang et al. </li>
     
   
     
       <li> <a href="/publications/dinh2022language/">LIFT: Language-interfaced Fine-tuning For Non-language Machine Learning Tasks</a> Tuan Dinh et al. </li>
     
   
     
       <li> <a href="/publications/chakrabarty2022help/">Help Me Write A Poem: Instruction Tuning As A Vehicle For Collaborative Poetry Writing</a> Tuhin Chakrabarty, Vishakh Padmakumar, He He </li>
     
   
     
   
     
       <li> <a href="/publications/lu2022unified/">Unified-io: A Unified Model For Vision, Language, And Multi-modal Tasks</a> Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022generative/">GIT: A Generative Image-to-text Transformer For Vision And Language</a> Jianfeng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022active/">Active Example Selection For In-context Learning</a> Yiming Zhang, Shi Feng, Chenhao Tan </li>
     
   
     
   
     
       <li> <a href="/publications/alabi2022adapting/">Adapting Pre-trained Language Models To African Languages Via Multilingual Adaptive Fine-tuning</a> Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, Dietrich Klakow </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022prompt/">Prompt Tuning For Discriminative Pre-trained Language Models</a> Yuan Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022generative/">M6-rec: Generative Pretrained Language Models Are Open-ended Recommender Systems</a> Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chung2022scaling/">Scaling Instruction-finetuned Language Models</a> Hyung Won Chung et al. </li>
     
   
     
       <li> <a href="/publications/le2022mastering/">Coderl: Mastering Code Generation Through Pretrained Models And Deep Reinforcement Learning</a> Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2022clip/">CLIP Models Are Few-shot Learners: Empirical Studies On VQA And Visual Entailment</a> Haoyu Song, Li Dong, Wei-nan Zhang, Ting Liu, Furu Wei </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2022few/">Few-shot Parameter-efficient Fine-tuning Is Better And Cheaper Than In-context Learning</a> Haokun Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2022rethinking/">Rethinking With Retrieval: Faithful Large Language Model Inference</a> Hangfeng He, Hongming Zhang, Dan Roth </li>
     
   
     
   
     
       <li> <a href="/publications/meng2022generating/">Generating Training Data With Language Models: Towards Zero-shot Language Understanding</a> Yu Meng, Jiaxin Huang, Yu Zhang, Jiawei Han </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022revisiting/">Revisiting Parameter-efficient Tuning: Are We Really There Yet?</a> Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, Shangsong Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sahu2022data/">Data Augmentation For Intent Classification With Off-the-shelf Large Language Models</a> Gaurav Sahu et al. </li>
     
   
     
       <li> <a href="/publications/li2022uni/">Uni-perceiver V2: A Generalist Model For Large-scale Vision And Vision-language Tasks</a> Hao Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/cui2022prototypical/">Prototypical Verbalizer For Prompt-based Few-shot Tuning</a> Ganqu Cui, Shengding Hu, Ning Ding, Longtao Huang, Zhiyuan Liu </li>
     
   
     
   
     
       <li> <a href="/publications/chen2022transferability/">On The Transferability Of Pre-trained Language Models For Low-resource Programming Languages</a> Fuxiang Chen, Fatemeh Fard, David Lo, Timofey Bryksin </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/poesia2022reliable/">Synchromesh: Reliable Code Generation From Pre-trained Language Models</a> Gabriel Poesia et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2022legal/">Legal Prompting: Teaching A Language Model To Think Like A Lawyer</a> Fangyi Yu, Lee Quartey, Frank Schilder </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zelikman2022bootstrapping/">Star: Bootstrapping Reasoning With Reasoning</a> Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D. Goodman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kurtic2022optimal/">The Optimal BERT Surgeon: Scalable And Accurate Second-order Pruning For Large Language Models</a> Eldar Kurtic et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/trautmann2022legal/">Legal Prompt Engineering For Multilingual Legal Judgement Prediction</a> Dietrich Trautmann, Alina Petrova, Frank Schilder </li>
     
   
     
       <li> <a href="/publications/chen2022exploring/">Convfinqa: Exploring The Chain Of Numerical Reasoning In Conversational Finance Question Answering</a> Zhiyu Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wan2022factuality/">Factpegasus: Factuality-aware Pre-training And Fine-tuning For Abstractive Summarization</a> David Wan, Mohit Bansal </li>
     
   
     
       <li> <a href="/publications/shah2022lm/">Lm-nav: Robotic Navigation With Large Pre-trained Models Of Language, Vision, And Action</a> Dhruv Shah, Blazej Osinski, Brian Ichter, Sergey Levine </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022benchmark/">ELEVATER: A Benchmark And Toolkit For Evaluating Language-augmented Visual Models</a> Chunyuan Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/colas2022language/">Language And Culture Internalisation For Human-like Autotelic AI</a> C√©dric Colas, Tristan Karch, Cl√©ment Moulin-frier, Pierre-yves Oudeyer </li>
     
   
     
       <li> <a href="/publications/lu2022controllable/">Quark: Controllable Text Generation With Reinforced Unlearning</a> Ximing Lu et al. </li>
     
   
     
       <li> <a href="/publications/wu2022little/">Noisytune: A Little Noise Can Help You Finetune Pretrained Language Models Better</a> Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, Xing Xie </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schuhmann2022laion/">LAION-5B: An Open Large-scale Dataset For Training Next Generation Image-text Models</a> Christoph Schuhmann et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tu2022visual/">Visual Query Tuning: Towards Effective Usage Of Intermediate Representations For Parameter And Memory Efficient Transfer Learning</a> Cheng-hao Tu, Zheda Mai, Wei-lun Chao </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2022visual/">Visual-language Navigation Pretraining Via Prompt-based Environmental Self-exploration</a> Xiwen Liang, Fengda Zhu, Lingling Li, Hang Xu, Xiaodan Liang </li>
     
   
     
   
     
       <li> <a href="/publications/tay2022unifying/">UL2: Unifying Language Learning Paradigms</a> Yi Tay et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022no/">No More Fine-tuning? An Experimental Evaluation Of Prompt Tuning In Code Intelligence</a> Chaozheng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guti%C3%A9rrez2022thinking/">Thinking About GPT-3 In-context Learning For Biomedical IE? Think Again</a> Bernal Jim√©nez Guti√©rrez et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2022prompt/">Prompt-aligned Gradient For Prompt Tuning</a> Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, Hanwang Zhang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zoph2022st/">St-moe: Designing Stable And Transferable Sparse Expert Models</a> Barret Zoph et al. </li>
     
   
     
       <li> <a href="/publications/ding2022multimodal/">Mukea: Multimodal Knowledge Extraction And Accumulation For Knowledge-based Visual Question Answering</a> Yang Ding et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2022large/">GODEL: Large-scale Pre-training For Goal-directed Dialog</a> Baolin Peng et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ushio2022t/">T-NER: An All-round Python Library For Transformer-based Named Entity Recognition</a> Asahi Ushio, Jose Camacho-collados </li>
     
   
     
   
     
       <li> <a href="/publications/ushio2022generative/">Generative Language Models For Paragraph-level Question Generation</a> Asahi Ushio, Fernando Alva-manchego, Jose Camacho-collados </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/creswell2022selection/">Selection-inference: Exploiting Large Language Models For Interpretable Logical Reasoning</a> Antonia Creswell, Murray Shanahan, Irina Higgins </li>
     
   
     
   
     
       <li> <a href="/publications/creswell2022faithful/">Faithful Reasoning Using Large Language Models</a> Antonia Creswell, Murray Shanahan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bapna2022massively/">Mslam: Massively Multilingual Joint Pre-training For Speech And Text</a> Ankur Bapna et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lazaridou2022internet/">Internet-augmented Language Models Through Few-shot Prompting For Open-domain Question Answering</a> Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022mixture/">Adamix: Mixture-of-adaptations For Parameter-efficient Model Tuning</a> Yaqing Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2022optimizing/">Optimizing Prompts For Text-to-image Generation</a> Yaru Hao, Zewen Chi, Li Dong, Furu Wei </li>
     
   
     
       <li> <a href="/publications/zang2022unified/">Unified Vision And Language Prompt Learning</a> Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, Chen Change Loy </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022instance/">IDPG: An Instance-dependent Prompt Generation Method</a> Zhuofeng Wu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maharana2022storydall/">Storydall-e: Adapting Pretrained Text-to-image Transformers For Story Continuation</a> Adyasha Maharana, Darryl Hannan, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/saparov2022language/">Language Models Are Greedy Reasoners: A Systematic Formal Analysis Of Chain-of-thought</a> Abulhair Saparov, He He </li>
     
   
     
       <li> <a href="/publications/asai2022parameter/">ATTEMPT: Parameter-efficient Multi-task Tuning Via Attentional Mixtures Of Soft Prompts</a> Akari Asai, Mohammadreza Salehi, Matthew E. Peters, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/diao2022black/">Black-box Prompt Learning For Pre-trained Language Models</a> Shizhe Diao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022faithful/">PINTO: Faithful Language Reasoning Using Prompt-generated Rationales</a> Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, Xiang Ren </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022delta/">Delta Tuning: A Comprehensive Study Of Parameter Efficient Methods For Pre-trained Language Models</a> Ning Ding et al. </li>
     
   
     
       <li> <a href="/publications/nllb2022no/">No Language Left Behind: Scaling Human-centered Machine Translation</a> Nllb Team et al. </li>
     
   
     
       <li> <a href="/publications/muennighoff2022crosslingual/">Crosslingual Generalization Through Multitask Finetuning</a> Niklas Muennighoff et al. </li>
     
   
     
       <li> <a href="/publications/long2022vision/">Vision-and-language Pretrained Models: A Survey</a> Siqu Long, Feiqi Cao, Soyeon Caren Han, Haiqin Yang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/muennighoff2022gpt/">SGPT: GPT Sentence Embeddings For Semantic Search</a> Niklas Muennighoff </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/taylor2022clinical/">Clinical Prompt Learning With Frozen Language Models</a> Niall Taylor, Yi Zhang, Dan Joyce, Alejo Nevado-holgado, Andrey Kormilitzin </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/iyer2022opt/">OPT-IML: Scaling Language Model Instruction Meta Learning Through The Lens Of Generalization</a> Srinivasan Iyer et al. </li>
     
   
     
       <li> <a href="/publications/ho2022large/">Large Language Models Are Reasoning Teachers</a> Namgyu Ho, Laura Schmid, Se-young Yun </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/varia2022instruction/">Instruction Tuning For Few-shot Aspect-based Sentiment Analysis</a> Siddharth Varia et al. </li>
     
   
     
   
     
       <li> <a href="/publications/valipour2022parameter/">Dylora: Parameter Efficient Tuning Of Pre-trained Models Using Dynamic Search-free Low-rank Adaptation</a> Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, Ali Ghodsi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kang2022knowledge/">KALA: Knowledge-augmented Language Model Adaptation</a> Minki Kang, Jinheon Baek, Sung Ju Hwang </li>
     
   
     
       <li> <a href="/publications/sivarajkumar2022zero/">Healthprompt: A Zero-shot Learning Paradigm For Clinical Natural Language Processing</a> Sonish Sivarajkumar, Yanshan Wang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022faster/">Cogview2: Faster And Better Text-to-image Generation Via Hierarchical Transformers</a> Ming Ding, Wendi Zheng, Wenyi Hong, Jie Tang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2022educational/">Educational Question Generation Of Children Storybooks Via Question Type Distribution Learning And Event-centric Summarization</a> Zhenjie Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bommarito2022gpt/">GPT Takes The Bar Exam</a> Michael Ii Bommarito, Daniel Martin Katz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jia2022visual/">Visual Prompt Tuning</a> Menglin Jia et al. </li>
     
   
     
   
     
       <li> <a href="/publications/cherti2022reproducible/">Reproducible Scaling Laws For Contrastive Language-image Learning</a> Mehdi Cherti et al. </li>
     
   
     
       <li> <a href="/publications/jia2022mner/">MNER-QG: An End-to-end MRC Framework For Multimodal Named Entity Recognition With Query Grounding</a> Meihuizi Jia et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/miotto2022who/">Who Is GPT-3? An Exploration Of Personality, Values And Demographics</a> Maril√π Miotto, Nicola Rossberg, Bennett Kleinberg </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hou2022learning/">Learning Vector-quantized Item Representation For Transferable Sequential Recommenders</a> Yupeng Hou, Zhankui He, Julian Mcauley, Wayne Xin Zhao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bonifacio2022data/">Inpars: Data Augmentation For Information Retrieval Using Large Language Models</a> Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Rodrigo Nogueira </li>
     
   
     
   
     
       <li> <a href="/publications/goyal2022news/">News Summarization And Evaluation In The Era Of GPT-3</a> Tanya Goyal, Junyi Jessy Li, Greg Durrett </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2022multimodality/">MVP: Multimodality-guided Visual Pre-training</a> Longhui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, Qi Tian </li>
     
   
     
       <li> <a href="/publications/ouyang2022training/">Training Language Models To Follow Instructions With Human Feedback</a> Long Ouyang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yu2022task/">Task Residual For Tuning Vision-language Models</a> Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, Xinchao Wang </li>
     
   
     
       <li> <a href="/publications/wang2022multi/">Instructionner: A Multi-task Instruction-based Generative Framework For Few-shot NER</a> Liwen Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dugan2022real/">Real Or Fake Text?: Investigating Human Ability To Detect Boundaries Between Human-written And Machine-generated Text</a> Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Sherry Shi, Chris Callison-burch </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2022exploring/">Exploring The Universal Vulnerability Of Prompt-based Learning Paradigm</a> Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu </li>
     
   
     
   
     
       <li> <a href="/publications/tunstall2022efficient/">Efficient Few-shot Learning Without Prompts</a> Lewis Tunstall et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ruis2022goldilocks/">The Goldilocks Of Pragmatic Understanding: Fine-tuning Strategy Matters For Implicature Resolution By Llms</a> Laura Ruis et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thakur2022benchmarking/">Benchmarking Large Language Models For Automated Verilog RTL Code Generation</a> Shailja Thakur et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/smith2022using/">Using Deepspeed And Megatron To Train Megatron-turing NLG 530B, A Large-scale Generative Language Model</a> Shaden Smith et al. </li>
     
   
     
       <li> <a href="/publications/thoppilan2022language/">Lamda: Language Models For Dialog Applications</a> Romal Thoppilan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geng2022recommendation/">Recommendation As Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)</a> Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chu2022meta/">Meta Policy Learning For Cold-start Conversational Recommendation</a> Zhendong Chu, Hongning Wang, Yun Xiao, Bo Long, Lingfei Wu </li>
     
   
     
   
     
       <li> <a href="/publications/yao2022towards/">Webshop: Towards Scalable Real-world Web Interaction With Grounded Language Agents</a> Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/siriwardhana2022improving/">Improving The Domain Adaptation Of Retrieval Augmented Generation (RAG) Models For Open Domain Question Answering</a> Shamane Siriwardhana et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rajbhandari2022deepspeed/">Deepspeed-moe: Advancing Mixture-of-experts Inference And Training To Power Next-generation AI Scale</a> Samyam Rajbhandari et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/malladi2023fine/">Fine-tuning Language Models With Just Forward Passes</a> Sadhika Malladi et al. </li>
     
   
     
       <li> <a href="/publications/bulathwela2023scalable/">Scalable Educational Question Generation With Pre-trained Language Models</a> Sahan Bulathwela, Hamze Muse, Emine Yilmaz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2023does/">Does Synthetic Data Generation Of Llms Help Clinical Text Mining?</a> Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, Xia Hu </li>
     
   
     
       <li> <a href="/publications/zheng2023secrets/">Secrets Of RLHF In Large Language Models Part I: PPO</a> Rui Zheng et al. </li>
     
   
     
       <li> <a href="/publications/yang2023teaching/">Gpt4tools: Teaching Large Language Model To Use Tools Via Self-instruction</a> Rui Yang et al. </li>
     
   
     
       <li> <a href="/publications/cao2023pro/">Pro-cap: Leveraging A Frozen Vision-language Model For Hateful Meme Detection</a> Rui Cao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/paiss2023teaching/">Teaching CLIP To Count To Ten</a> Roni Paiss et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schumann2023verbalization/">VELMA: Verbalization Embodiment Of LLM Agents For Vision And Language Navigation In Street View</a> Raphael Schumann et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023cot/">The Cot Collection: Improving Zero-shot And Few-shot Learning Of Language Models Via Chain-of-thought Fine-tuning</a> Seungone Kim et al. </li>
     
   
     
       <li> <a href="/publications/thakur2023large/">Verigen: A Large Language Model For Verilog Code Generation</a> Shailja Thakur et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/suh2023structured/">Luminate: Structured Generation And Exploration Of Design Space With Large Language Models For Human-ai Co-creation</a> Sangho Suh, Meng Chen, Bryan Min, Toby Jia-jun Li, Haijun Xia </li>
     
   
     
       <li> <a href="/publications/mitrovi%C4%872023chatgpt/">Chatgpt Or Human? Detect And Explain. Explaining Decisions Of Machine Learning Model For Detecting Short Chatgpt-generated Text</a> Sandra Mitroviƒá, Davide Andreoletti, Omran Ayoub </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023instruction/">Instruction Tuning For Large Language Models: A Survey</a> Shengyu Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zheng2023why/">Why Does Chatgpt Fall Short In Providing Truthful Answers?</a> Shen Zheng, Jie Huang, Kevin Chen-chuan Chang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2023sur/">Sur-adapter: Enhancing Text-to-image Pre-trained Diffusion Models With Large Language Models</a> Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Lin </li>
     
   
     
       <li> <a href="/publications/chen2023evaluation/">Evaluation Of Chatgpt Family Of Models For Biomedical Reasoning And Classification</a> Shan Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/geng2023towards/">VIP5: Towards Multimodal Foundation Models For Recommendation</a> Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, Yongfeng Zhang </li>
     
   
     
   
     
       <li> <a href="/publications/hao2023reasoning/">Reasoning With Language Model Is Planning With World Model</a> Shibo Hao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023label/">Label Supervised Llama Finetuning</a> Zongxi Li et al. </li>
     
   
     
       <li> <a href="/publications/lin2023joint/">SPHINX: The Joint Mixing Of Weights, Tasks, And Visual Embeddings For Multi-modal Large Language Models</a> Ziyi Lin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023llama/">Llama-adapter: Efficient Fine-tuning Of Language Models With Zero-init Attention</a> Renrui Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/rafailov2023direct/">Direct Preference Optimization: Your Language Model Is Secretly A Reward Model</a> Rafael Rafailov et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023lawyer/">Lawyer Llama Technical Report</a> Quzhe Huang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023mplug/">Mplug-owl: Modularization Empowers Large Language Models With Multimodality</a> Qinghao Ye et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023adaptive/">Adalora: Adaptive Budget Allocation For Parameter-efficient Fine-tuning</a> Qingru Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023pre/">3d-vista: Pre-trained Transformer For 3D Vision And Text Alignment</a> Ziyu Zhu et al. </li>
     
   
     
       <li> <a href="/publications/li2023masked/">Masked Vision And Language Pre-training With Unimodal And Multimodal Contrastive Losses For Medical Visual Question Answering</a> Pengfei Li, Gang Liu, Jinlong He, Zixu Zhao, Shenjun Zhong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023exploring/">Graphologue: Exploring Large Language Model Responses With Interactive Diagrams</a> Peiling Jiang, Jude Rayan, Steven P. Dow, Haijun Xia </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cascantebonilla2023going/">Going Beyond Nouns With Vision & Language Models Using Synthetic Data</a> Paola Cascante-bonilla et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ovadia2023fine/">Fine-tuning Or Retrieval? Comparing Knowledge Injection In Llms</a> Oded Ovadia, Menachem Brief, Moshik Mishaeli, Oren Elisha </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2023enhancing/">Enhancing Chat Language Models By Scaling High-quality Instructional Conversations</a> Ning Ding et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2023multimodal/">Openvivqa: Task, Dataset, And Multimodal Fusion Models For Visual Question Answering In Vietnamese</a> Nghia Hieu Nguyen, Duong T. D. Vo, Kiet Van Nguyen, Ngan Luu-thuy Nguyen </li>
     
   
     
   
     
       <li> <a href="/publications/sengupta2023jais/">Jais And Jais-chat: Arabic-centric Foundation And Instruction-tuned Open Generative Large Language Models</a> Neha Sengupta et al. </li>
     
   
     
   
     
       <li> <a href="/publications/shinn2023language/">Reflexion: Language Agents With Verbal Reinforcement Learning</a> Noah Shinn et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khattak2023self/">Self-regulating Prompts: Foundational Model Adaptation Without Forgetting</a> Muhammad Uzair Khattak et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sharma2023towards/">Towards Understanding Sycophancy In Language Models</a> Mrinank Sharma et al. </li>
     
   
     
       <li> <a href="/publications/siddiq2023using/">Using Large Language Models To Generate Junit Tests: An Empirical Study</a> Mohammed Latif Siddiq et al. </li>
     
   
     
   
     
       <li> <a href="/publications/reza2023rapid/">Abscribe: Rapid Exploration & Organization Of Multiple Writing Variations In Human-ai Co-writing Tasks Using Large Language Models</a> Mohi Reza et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023lamini/">Lamini-lm: A Diverse Herd Of Distilled Models From Large-scale Instructions</a> Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-mageed, Alham Fikri Aji </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parker2023large/">A Large Language Model Approach To Educational Survey Feedback Analysis</a> Michael J. Parker, Caitlin Anderson, Claire Stone, Yearim Oh </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023chatgpt/">Chatgpt For Vulnerability Detection, Classification, And Repair: How Far Are We?</a> Michael Fu, Chakkrit Tantithamthavorn, Van Nguyen, Trung Le </li>
     
   
     
       <li> <a href="/publications/xiong2023can/">Can Llms Express Their Uncertainty? An Empirical Evaluation Of Confidence Elicitation In Llms</a> Miao Xiong et al. </li>
     
   
     
       <li> <a href="/publications/xu2023interpretable/">Drivegpt4: Interpretable End-to-end Autonomous Driving Via Large Language Model</a> Zhenhua Xu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hasan2023zero/">Zero- And Few-shot Prompting With Llms: A Comparative Study With Fine-tuned Models For Bangla Sentiment Analysis</a> Md. Arid Hasan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nievas2023distilling/">Distilling Large Language Models For Matching Patients To Clinical Trials</a> Mauro Nievas, Aditya Basu, Yanshan Wang, Hrituraj Singh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/patil2023large/">Gorilla: Large Language Model Connected With Massive Apis</a> Shishir G. Patil, Tianjun Zhang, Xin Wang, Joseph E. Gonzalez </li>
     
   
     
   
     
       <li> <a href="/publications/mosbach2023few/">Few-shot Fine-tuning Vs. In-context Learning: A Fair Comparison And Evaluation</a> Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, Yanai Elazar </li>
     
   
     
       <li> <a href="/publications/binz2023turning/">Turning Large Language Models Into Cognitive Models</a> Marcel Binz, Eric Schulz </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/berglund2023reversal/">The Reversal Curse: Llms Trained On "A Is B" Fail To Learn "B Is A"</a> Lukas Berglund et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023driving/">Driving With Llms: Fusing Object-level Vector Modality For Explainable Autonomous Driving</a> Long Chen et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yan2023human/">Human-ai Collaboration In Thematic Analysis Using Chatgpt: A User Study And Design Recommendations</a> Lixiang Yan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023parameter/">Parameter-efficient Fine-tuning Methods For Pretrained Language Models: A Critical Review And Assessment</a> Lingling Xu, Haoran Xie, Si-zhao Joe Qin, Xiaohui Tao, Fu Lee Wang </li>
     
   
     
       <li> <a href="/publications/luo2023bilingual/">Taiyi: A Bilingual Fine-tuned Large Language Model For Diverse Biomedical Tasks</a> Ling Luo et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023survey/">A Survey On Large Language Models For Recommendation</a> Likang Wu et al. </li>
     
   
     
       <li> <a href="/publications/yu2023scaling/">Scaling Autoregressive Multi-modal Models: Pretraining And Instruction Tuning</a> Lili Yu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023improving/">Improving Text Embeddings With Large Language Models</a> Liang Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tunstall2023direct/">Zephyr: Direct Distillation Of LM Alignment</a> Lewis Tunstall et al. </li>
     
   
     
       <li> <a href="/publications/brocki2023deep/">Deep Learning Mental Health Dialogue System</a> Lennart Brocki, George C. Dyer, Anna G≈Çadka, Neo Christopher Chung </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xue2023ulip/">ULIP-2: Towards Scalable Multimodal Pre-training For 3D Understanding</a> Le Xue et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mahowald2023dissociating/">Dissociating Language And Thought In Large Language Models</a> Kyle Mahowald et al. </li>
     
   
     
       <li> <a href="/publications/li2023comprehensive/">Mvbench: A Comprehensive Multi-modal Video Understanding Benchmark</a> Kunchang Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yin2023language/">LAMM: Language-assisted Multi-modal Instruction-tuning Dataset, Framework, And Benchmark</a> Zhenfei Yin et al. </li>
     
   
     
       <li> <a href="/publications/kheiri2023exploiting/">Sentimentgpt: Exploiting GPT For Advanced Sentiment Analysis And Its Departure From Current Machine Learning</a> Kiana Kheiri, Hamid Karimi </li>
     
   
     
   
     
       <li> <a href="/publications/busch2023just/">Just Tell Me: Prompt Engineering In Business Process Management</a> Kiran Busch, Alexander Rochlitzer, Diana Sola, Henrik Leopold </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pandya2023automating/">Automating Customer Service Using Langchain: Building Custom Open-source GPT Chatbot For Organizations</a> Keivalya Pandya, Mehfuza Holia </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/soman2023biomedical/">Biomedical Knowledge Graph-optimized Prompt Generation For Large Language Models</a> Karthik Soman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023aligning/">Aligning Instruction Tasks Unlocks Large Language Models As Zero-shot Relation Extractors</a> Kai Zhang, Bernal Jim√©nez Guti√©rrez, Yu Su </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2023llama/">Llama-reviewer: Advancing Code Review Automation With Large Language Models Through Parameter-efficient Fine-tuning</a> Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, Chun Zuo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jang2023exploring/">Exploring The Benefits Of Training Expert Language Models Over Instruction Tuning</a> Joel Jang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023exploring/">Badgpt: Exploring Security Vulnerabilities Of Chatgpt Via Backdoor Attacks To Instructgpt</a> Jiawen Shi, Yixin Liu, Pan Zhou, Lichao Sun </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiang2023language/">Language Models Meet World Models: Embodied Experiences Enhance Language Models</a> Jiannan Xiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023universal/">Ureader: Universal Ocr-free Visually-situated Language Understanding With Multimodal Large Language Model</a> Jiabo Ye et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2023graph/">Graphgpt: Graph Instruction Tuning For Large Language Models</a> Jiabin Tang et al. </li>
     
   
     
       <li> <a href="/publications/lin2023pre/">VILA: On Pre-training For Visual Language Models</a> Ji Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023physically/">Physically Grounded Vision-language Models For Robotic Manipulation</a> Jensen Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023memory/">Memory-efficient Fine-tuning Of Compressed Large Language Models Via Sub-4-bit Integer Quantization</a> Jeonghoon Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jahan2023comprehensive/">A Comprehensive Evaluation Of Large Language Models On Benchmark Biomedical Text Processing Tasks</a> Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/arawjo2023visual/">Chainforge: A Visual Toolkit For Prompt Engineering And LLM Hypothesis Testing</a> Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, Elena Glassman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/touvron2023llama/">Llama 2: Open Foundation And Fine-tuned Chat Models</a> Hugo Touvron et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ye2023ip/">Ip-adapter: Text Compatible Image Prompt Adapter For Text-to-image Diffusion Models</a> Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, Wei Yang </li>
     
   
     
   
     
       <li> <a href="/publications/xiong2023fine/">Doctorglm: Fine-tuning Your Chinese Doctor Is Not A Herculean Task</a> Honglin Xiong et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023pre/">Missrec: Pre-training And Transferring Multi-modal Interest-aware Sequence Representation For Recommendation</a> Jinpeng Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/tran2023instruction/">Bioinstruct: Instruction Tuning Of Large Language Models For Biomedical Natural Language Processing</a> Hieu Tran, Zhichao Yang, Zonghai Yao, Hong Yu </li>
     
   
     
   
     
       <li> <a href="/publications/koziolek2023chatgpt/">Chatgpt For PLC/DCS Control Logic Generation</a> Heiko Koziolek, Sten Gruener, Virendra Ashiwal </li>
     
   
     
   
     
       <li> <a href="/publications/nori2023can/">Can Generalist Foundation Models Outcompete Special-purpose Tuning? Case Study In Medicine</a> Harsha Nori et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023generate/">Chatkbqa: A Generate-then-retrieve Framework For Knowledge Base Question Answering With Fine-tuned Large Language Models</a> Haoran Luo et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023extractive/">Extractive Summarization Via Chatgpt For Faithful Summary Generation</a> Haopeng Zhang, Xiao Liu, Jiawei Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wen2023llm/">Autodroid: Llm-powered Task Automation In Android</a> Hao Wen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fei2023unifying/">Lasuie: Unifying Information Extraction With Latent Adaptive Structure-aware Generative Language Model</a> Hao Fei et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kirk2023personalisation/">Personalisation Within Bounds: A Risk Taxonomy And Policy Framework For The Alignment Of Large Language Models With Personalised Feedback</a> Hannah Rose Kirk, Bertie Vidgen, Paul R√∂ttger, Scott A. Hale </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/inan2023llama/">Llama Guard: Llm-based Input-output Safeguard For Human-ai Conversations</a> Hakan Inan et al. </li>
     
   
     
       <li> <a href="/publications/zhao2023explainability/">Explainability For Large Language Models: A Survey</a> Haiyan Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023open/">Voyager: An Open-ended Embodied Agent With Large Language Models</a> Guanzhi Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chalvatzaki2023learning/">Learning To Reason Over Scene Graphs: A Case Study Of Finetuning GPT-2 Into A Robot Language Model For Grounded Task Planning</a> Georgia Chalvatzaki et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023explicit/">Navgpt: Explicit Reasoning In Vision-and-language Navigation With Large Language Models</a> Gengze Zhou, Yicong Hong, Qi Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023heavy/">H\(_2\)O: Heavy-hitter Oracle For Efficient Generative Inference Of Large Language Models</a> Zhenyu Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/song2023preference/">Preference Ranking Optimization For Human Alignment</a> Feifan Song et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2023llm/">Llm-adapters: An Adapter Family For Parameter-efficient Fine-tuning Of Large Language Models</a> Zhiqiang Hu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/nijkamp2023lessons/">Codegen2: Lessons For Training Llms On Programming And Natural Languages</a> Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou </li>
     
   
     
       <li> <a href="/publications/lehman2023do/">Do We Still Need Clinical Language Models?</a> Eric Lehman et al. </li>
     
   
     
   
     
       <li> <a href="/publications/shi2023towards/">Towards Efficient Fine-tuning Of Pre-trained Code Models: An Experimental Study And Beyond</a> Ensheng Shi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/latif2023fine/">Fine-tuning Chatgpt For Automatic Scoring</a> Ehsan Latif, Xiaoming Zhai </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023empowering/">Speechgpt: Empowering Large Language Models With Intrinsic Cross-modal Conversational Abilities</a> Dong Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mollo2023vector/">The Vector Grounding Problem</a> Dimitri Coelho Mollo, Rapha√´l Milli√®re </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023principle/">Principle-driven Self-alignment Of Language Models From Scratch With Minimal Human Supervision</a> Zhiqing Sun et al. </li>
     
   
     
       <li> <a href="/publications/ashok2023prompting/">Promptner: Prompting For Named Entity Recognition</a> Dhananjay Ashok, Zachary C. Lipton </li>
     
   
     
       <li> <a href="/publications/zhu2023minigpt/">Minigpt-4: Enhancing Vision-language Understanding With Advanced Large Language Models</a> Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023one/">One Adapter For All Programming Languages? Adapter Tuning For Code Search And Summarization</a> Deze Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xu2023large/">Large Language Models For Generative Information Extraction: A Survey</a> Derong Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023text/">Text-to-sql Empowered By Large Language Models: A Benchmark Evaluation</a> Dawei Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023solar/">SOLAR 10.7B: Scaling Large Language Models With Simple Yet Effective Depth Up-scaling</a> Dahyun Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023multimodal/">Multimodal Foundation Models: From Specialists To General-purpose Assistants</a> Chunyuan Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jeong2023study/">A Study On The Implementation Of Generative AI Services Using An Enterprise Data-based LLM Application Architecture</a> Cheonsu Jeong </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023iterative/">An Iterative Optimizing Framework For Radiology Report Summarization With Chatgpt</a> Chong Ma et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023model/">Model Tuning Or Prompt Tuning? A Study Of Large Language Models For Clinical Concept And Relation Extraction</a> Cheng Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2023foundation/">K2: A Foundation Language Model For Geoscience Knowledge Understanding And Utilization</a> Cheng Deng et al. </li>
     
   
     
       <li> <a href="/publications/han2023effective/">E^2VPT: An Effective And Efficient Approach For Visual Prompt Tuning</a> Cheng Han et al. </li>
     
   
     
       <li> <a href="/publications/whitehouse2023llm/">Llm-powered Data Augmentation For Enhanced Cross-lingual Performance</a> Chenxi Whitehouse, Monojit Choudhury, Alham Fikri Aji </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023pmc/">Pmc-llama: Towards Building Open-source Language Models For Medicine</a> Chaoyi Wu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023generative/">Generative Speech Recognition Error Correction With Large Language Models And Task-activating Prompting</a> Chao-han Huck Yang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mitra2023compositional/">Compositional Chain-of-thought Prompting For Large Multimodal Models</a> Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig </li>
     
   
     
       <li> <a href="/publications/oh2023black/">Blackvip: Black-box Visual Prompting For Robust Transfer Learning</a> Changdae Oh et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023empowering/">Wizardlm: Empowering Large Language Models To Follow Complex Instructions</a> Can Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023adapting/">Adapting Large Language Models By Integrating Collaborative Semantics For Recommendation</a> Bowen Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023prompting/">Prompting Or Fine-tuning? A Comparative Study Of Large Language Models For Taxonomy Construction</a> Boqi Chen, Fandi Yi, D√°niel Varr√≥ </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023instruction/">Instruction Tuning With GPT-4</a> Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao </li>
     
   
     
       <li> <a href="/publications/jiang2023human/">Motiongpt: Human Motion As A Foreign Language</a> Biao Jiang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/toma2023clinical/">Clinical Camel: An Open Expert-level Medical Language Model With Dialogue-based Knowledge Encoding</a> Augustin Toma et al. </li>
     
   
     
       <li> <a href="/publications/aberdam2023looking/">CLIPTER: Looking At The Bigger Picture In Scene Text Recognition</a> Aviad Aberdam et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023prompting/">Prompting Large Language Model For Machine Translation: A Case Study</a> Biao Zhang, Barry Haddow, Alexandra Birch </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023llm/">Expel: LLM Agents Are Experiential Learners</a> Andrew Zhao et al. </li>
     
   
     
       <li> <a href="/publications/gunjal2023detecting/">Detecting And Preventing Hallucinations In Large Vision Language Models</a> Anisha Gunjal, Jihan Yin, Erhan Bas </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/k%C3%B6pf2023openassistant/">Openassistant Conversations -- Democratizing Large Language Model Alignment</a> Andreas K√∂pf et al. </li>
     
   
     
       <li> <a href="/publications/liesenfeld2023opening/">Opening Up Chatgpt: Tracking Openness, Transparency, And Accountability In Instruction-tuned Text Generators</a> Andreas Liesenfeld, Alianda Lopez, Mark Dingemanse </li>
     
   
     
       <li> <a href="/publications/kucharavy2023fundamentals/">Fundamentals Of Generative Large Language Models And Perspectives In Cyber-defense</a> Andrei Kucharavy et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salemi2023when/">Lamp: When Large Language Models Meet Personalization</a> Alireza Salemi, Sheshera Mysore, Michael Bendersky, Hamed Zamani </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wan2023poisoning/">Poisoning Language Models During Instruction Tuning</a> Alexander Wan, Eric Wallace, Sheng Shen, Dan Klein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023multitask/">Multitask Prompt Tuning Enables Parameter-efficient Transfer Learning</a> Zhen Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023llm/">Llm-pruner: On The Structural Pruning Of Large Language Models</a> Xinyin Ma, Gongfan Fang, Xinchao Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lai2023reasoning/">LISA: Reasoning Segmentation Via Large Language Model</a> Xin Lai et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023pmc/">PMC-VQA: Visual Instruction Tuning For Medical Visual Question Answering</a> Xiaoman Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2023fine/">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</a> Xiangyu Qi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ding2023hpc/">HPC-GPT: Integrating Large Language Model For High-performance Computing</a> Xianzhong Ding et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/seo2023leveraging/">Chacha: Leveraging Large Language Models To Prompt Children To Share Their Emotions About Personal Events</a> Woosuk Seo, Chanmo Yang, Young-ho Kim </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gan2023large/">Large Language Models In Education: Vision And Opportunities</a> Wensheng Gan, Zhenlian Qi, Jiayang Wu, Jerry Chun-wei Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kang2023do/">Do Llms Understand User Preferences? Evaluating Llms On User Rating Prediction</a> Wang-cheng Kang et al. </li>
     
   
     
       <li> <a href="/publications/xiao2023supporting/">Supporting Qualitative Analysis With Large Language Models: Combining Codebook With GPT-3 For Deductive Coding</a> Ziang Xiao, Xingdi Yuan, Q. Vera Liao, Rania Abdelghani, Pierre-yves Oudeyer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/adlakha2023evaluating/">Evaluating Correctness And Faithfulness Of Instruction-following Models For Question Answering</a> Vaibhav Adlakha, Parishad Behnamghader, Xing Han Lu, Nicholas Meade, Siva Reddy </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lialin2023scaling/">Scaling Down To Scale Up: A Guide To Parameter-efficient Fine-tuning</a> Vladislav Lialin, Vijeta Deshpande, Xiaowei Yao, Anna Rumshisky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vansonsbeek2023open/">Open-ended Medical Visual Question Answering Through Prefix Tuning Of Language Models</a> Tom Van Sonsbeek, Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees G. M. Snoek, Marcel Worring </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dettmers2023efficient/">Qlora: Efficient Finetuning Of Quantized Llms</a> Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/han2023medalpaca/">Medalpaca -- An Open-source Collection Of Medical Conversational AI Models And Training Data</a> Tianyu Han et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023recommender/">Recommender Systems In The Era Of Large Language Models (llms)</a> Zihuai Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023large/">Large Language Model Alignment: A Survey</a> Tianhao Shen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gong2023multimodal/">Multimodal-gpt: A Vision And Language Model For Dialogue With Humans</a> Tao Gong et al. </li>
     
   
     
       <li> <a href="/publications/guo2023what/">What Can Large Language Models Do In Chemistry? A Comprehensive Benchmark On Eight Tasks</a> Taicheng Guo et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bubeck2023sparks/">Sparks Of Artificial General Intelligence: Early Experiments With GPT-4</a> S√©bastien Bubeck et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023element/">Element-aware Summarization With Large Language Models: Expert-aligned Evaluation And Chain-of-thought Method</a> Yiming Wang, Zhuosheng Zhang, Rui Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2023mol/">Mol-instructions: A Large-scale Biomolecular Instruction Dataset For Large Language Models</a> Yin Fang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2023summary/">Summary Of Chatgpt-related Research And Perspective Towards The Future Of Large Language Models</a> Yiheng Liu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xin2023mmap/">Mmap : Multi-modal Alignment Prompt For Cross-domain Multi-task Learning</a> Yi Xin, Junlong Du, Qiang Wang, Ke Yan, Shouhong Ding </li>
     
   
     
   
     
       <li> <a href="/publications/chia2023towards/">INSTRUCTEVAL: Towards Holistic Evaluation Of Instruction-tuned Large Language Models</a> Yew Ken Chia, Pengfei Hong, Lidong Bing, Soujanya Poria </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moslem2023adaptive/">Adaptive Machine Translation With Large Language Models</a> Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2023collaborative/">Collaborative Large Language Model For Recommender Systems</a> Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023improving/">Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback</a> Yao Fu, Hao Peng, Tushar Khot, Mirella Lapata </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dubois2023simulation/">Alpacafarm: A Simulation Framework For Methods That Learn From Human Feedback</a> Yann Dubois et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023mental/">Mental-llm: Leveraging Large Language Models For Mental Health Prediction Via Online Text Data</a> Xuhai Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023fine/">Fine-tuning Llama For Multi-stage Text Retrieval</a> Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, Jimmy Lin </li>
     
   
     
       <li> <a href="/publications/zhang2023xuanyuan/">Xuanyuan 2.0: A Large Chinese Financial Chat Model With Hundreds Of Billions Parameters</a> Xuanyu Zhang, Qing Yang, Dongliang Xu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023aligning/">Aligning Large Language Models With Human: A Survey</a> Yufei Wang et al. </li>
     
   
     
       <li> <a href="/publications/dan2023large/">Educhat: A Large-scale Language Model-based Chatbot System For Intelligent Education</a> Yuhao Dan et al. </li>
     
   
     
       <li> <a href="/publications/xie2023prompt/">A Prompt Log Analysis Of Text-to-image Generation Systems</a> Yutong Xie, Zhaoying Pan, Jinge Ma, Luo Jie, Qiaozhu Mei </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023low/">Low-rank Adaptation Of Large Language Model Rescoring For Parameter-efficient Speech Recognition</a> Yu Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023transforming/">NL2TL: Transforming Natural Languages To Temporal Logics Using Large Language Models</a> Yongchao Chen, Rujul Gandhi, Yang Zhang, Chuchu Fan </li>
     
   
     
       <li> <a href="/publications/fu2023towards/">Gpt4aigchip: Towards Next-generation AI Accelerator Design Automation Via Large Language Models</a> Yonggan Fu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/luo2023open/">Biomedgpt: Open Multimodal Generative Pre-trained Transformer For Biomedicine</a> Yizhen Luo et al. </li>
     
   
     
       <li> <a href="/publications/li2023medical/">Chatdoctor: A Medical Chat Model Fine-tuned On A Large Language Model Meta-ai (llama) Using Medical Domain Knowledge</a> Yunxiang Li et al. </li>
     
   
     
       <li> <a href="/publications/ji2023exploring/">Exploring The Impact Of Instruction Data Scaling On Large Language Models: An Empirical Study On Real-world Use Cases</a> Yunjie Ji et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2023facilitating/">Toolllm: Facilitating Large Language Models To Master 16000+ Real-world Apis</a> Yujia Qin et al. </li>
     
   
     
       <li> <a href="/publications/luo2023empirical/">An Empirical Study Of Catastrophic Forgetting In Large Language Models During Continual Fine-tuning</a> Yun Luo et al. </li>
     
   
     
       <li> <a href="/publications/du2023guiding/">Guiding Pretraining In Reinforcement Learning With Large Language Models</a> Yuqing Du et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2023editing/">Editing Large Language Models: Problems, Methods, And Opportunities</a> Yunzhi Yao et al. </li>
     
   
     
       <li> <a href="/publications/wang2023how/">How Far Can Camels Go? Exploring The State Of Instruction Tuning On Open Resources</a> Yizhong Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dai2023uncovering/">Uncovering Chatgpt's Capabilities In Recommender Systems</a> Sunhao Dai et al. </li>
     
   
     
       <li> <a href="/publications/brade2023text/">Promptify: Text-to-image Generation Through Interactive Prompt Exploration With Large Language Models</a> Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore, Tovi Grossman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wadhwa2023revisiting/">Revisiting Relation Extraction In The Era Of Large Language Models</a> Somin Wadhwa, Silvio Amir, Byron C. Wallace </li>
     
   
     
   
     
       <li> <a href="/publications/yang2023enhancing/">Zhongjing: Enhancing The Chinese Medical Capabilities Of Large Language Model Through Expert Feedback And Real-world Multi-turn Dialogue</a> Songhua Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023llm/">Llm-empowered Chatbots For Psychiatrist And Patient Simulation: Application And Evaluation</a> Siyuan Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sudhakaran2023open/">Mariogpt: Open-ended Text2level Generation Through Large Language Models</a> Shyam Sudhakaran et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yao2023tree/">Tree Of Thoughts: Deliberate Problem Solving With Large Language Models</a> Shunyu Yao et al. </li>
     
   
     
       <li> <a href="/publications/gholami2023can/">Can A Student Large Language Model Perform As Well As It's Teacher?</a> Sia Gholami, Marwan Omar </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2023time/">Timechat: A Time-sensitive Multimodal Large Language Model For Long Video Understanding</a> Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou </li>
     
   
     
       <li> <a href="/publications/bai2023prompt/">Prompt-based Distribution Alignment For Unsupervised Domain Adaptation</a> Shuanghao Bai et al. </li>
     
   
     
       <li> <a href="/publications/yu2023self/">Self-chained Image-language Model For Video Localization And Question Answering</a> Shoubin Yu, Jaemin Cho, Prateek Yadav, Mohit Bansal </li>
     
   
     
       <li> <a href="/publications/chen2023extending/">Extending Context Window Of Large Language Models Via Positional Interpolation</a> Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian </li>
     
   
     
       <li> <a href="/publications/pramanick2023egocentric/">Egovlpv2: Egocentric Video-language Pre-training With Fusion In The Backbone</a> Shraman Pramanick et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023plan/">Describe, Explain, Plan And Select: Interactive Planning With Large Language Models Enables Open-world Multi-task Agents</a> Zihao Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2023large/">Large Language Models As Zero-shot Conversational Recommenders</a> Zhankui He et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023pushing/">Pushing Large Language Models To The 6G Edge: Vision, Challenges, And Opportunities</a> Zheng Lin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nazi2023large/">Large Language Models In Healthcare And Medical Domain: A Review</a> Zabir Al Nazi, Wei Peng </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2023eva/">EVA-02: A Visual Representation For Neon Genesis</a> Yuxin Fang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2023guiding/">Guiding Large Language Models Via Directional Stimulus Prompting</a> Zekun Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2024large/">Large Language Models Meet Collaborative Filtering: An Efficient All-round Llm-based Recommender System</a> Sein Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lieber2024hybrid/">Jamba: A Hybrid Transformer-mamba Language Model</a> Opher Lieber et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gruver2024fine/">Fine-tuned Language Models Generate Stable Inorganic Materials As Text</a> Nate Gruver et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gekhman2024does/">Does Fine-tuning Llms On New Knowledge Encourage Hallucinations?</a> Zorik Gekhman et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gero2024supporting/">Supporting Sensemaking Of Large Language Model Outputs At Scale</a> Katy Ilonka Gero, Chelse Swoopes, Ziwei Gu, Jonathan K. Kummerfeld, Elena L. Glassman </li>
     
   
     
       <li> <a href="/publications/chang2024data/">Data Is All You Need: Finetuning Llms For Chip Design Via An Automated Design-data Augmentation Framework</a> Kaiyan Chang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2024monolithic/">ORPO: Monolithic Preference Optimization Without Reference Model</a> Jiwoo Hong, Noah Lee, James Thorne </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/maharjan2024prompt/">Openmedlm: Prompt Engineering Can Out-perform Fine-tuning In Medical Question-answering With Open-source Large Language Models</a> Jenish Maharjan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hartsock2024vision/">Vision-language Models For Medical Report Generation And Visual Question Answering: A Review</a> Iryna Hartsock, Ghulam Rasool </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2024closing/">Closing The Gap Between Open-source And Commercial Large Language Models For Medical Evidence Summarization</a> Gongbo Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lei2024materials/">Materials Science In The Era Of Large Language Models: A Perspective</a> Ge Lei, Ronan Docherty, Samuel J. Cooper </li>
     
   
     
       <li> <a href="/publications/gemmateam2024open/">Gemma: Open Models Based On Gemini Research And Technology</a> Gemma Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bozkir2024embedding/">Embedding Large Language Models Into Extended Reality: Opportunities And Challenges For Inclusion, Engagement, And Privacy</a> Efe Bozkir et al. </li>
     
   
     
       <li> <a href="/publications/frankford2024ai/">Ai-tutoring In Software Engineering Education</a> Eduard Frankford, Clemens Sauerwein, Patrick Bassner, Stephan Krusche, Ruth Breu </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2024chemical/">Chemllm: A Chemical Large Language Model</a> Di Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/deepseekai2024deepseek/">Deepseek-v2: A Strong, Economical, And Efficient Mixture-of-experts Language Model</a> Deepseek-ai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2024understanding/">Understanding Large-language Model (llm)-powered Human-robot Interaction</a> Callie Y. Kim, Christine P. Lee, Bilge Mutlu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/balaguer2024rag/">RAG Vs Fine-tuning: Pipelines, Tradeoffs, And A Case Study On Agriculture</a> Angels Balaguer et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ai2024open/">Yi: Open Foundation Models By 01.AI</a> 01. Ai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2024understanding/">Understanding Llms: A Comprehensive Overview From Training To Inference</a> Yiheng Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2024unified/">Llamafactory: Unified Efficient Fine-tuning Of 100+ Language Models</a> Yaowei Zheng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2024datasets/">Datasets For Large Language Models: A Comprehensive Survey</a> Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin </li>
     
   
     
       <li> <a href="/publications/lin2024data/">Data-efficient Fine-tuning For Llm-based Recommendation</a> Xinyu Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parthasarathy2024ultimate/">The Ultimate Guide To Fine-tuning Llms From Basics To Breakthroughs: An Exhaustive Review Of Technologies, Research, Best Practices, Applied Research Challenges And Opportunities</a> Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/glm2024family/">Chatglm: A Family Of Large Language Models From GLM-130B To GLM-4 All Tools</a> Team Glm et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lankford2024fine/">Adaptmllm: Fine-tuning Multilingual Language Models On Low-resource Languages With Integrated LLM Playgrounds</a> S√©amus Lankford, Haithem Afli, Andy Way </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2024harnessing/">Harnessing Large Language Models For Text-rich Sequential Recommendation</a> Zhi Zheng, Wenshuo Chao, Zhaopeng Qiu, Hengshu Zhu, Hui Xiong </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2024exploratory/">Llmparser: An Exploratory Study On Using Large Language Models For Log Parsing</a> Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-hsun Chen, Shaowei Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alaofi2025can/">Can Generative Llms Create Query Variants For Test Collections? An Exploratory Study</a> Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, Paul Thomas </li>
     
   
     
       <li> <a href="/publications/deepseekai2025deepseek/">Deepseek-r1: Incentivizing Reasoning Capability In Llms Via Reinforcement Learning</a> Deepseek-ai et al. </li>
     
   
     
   
   </ul>

   <h3>üè∑ GPT <a id="GPT"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fedus2018better/">Maskgan: Better Text Generation Via Filling In The______</a> William Fedus, Ian Goodfellow, Andrew M. Dai </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2018robust/">Robust Text-to-sql Generation With Execution-guided Decoding</a> Chenglong Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rothe2019leveraging/">Leveraging Pre-trained Checkpoints For Sequence Generation Tasks</a> Sascha Rothe, Shashi Narayan, Aliaksei Severyn </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2019evaluating/">Evaluating Commonsense In Pre-trained Language Models</a> Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang </li>
     
   
     
   
     
       <li> <a href="/publications/shoeybi2019megatron/">Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism</a> Mohammad Shoeybi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/budzianowski2019gpt/">Hello, It's GPT-2 -- How Can I Help You? Towards The Use Of Pretrained Language Models For Task-oriented Dialogue Systems</a> Pawe≈Ç Budzianowski, Ivan Vuliƒá </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/witteveen2019paraphrasing/">Paraphrasing With Large Language Models</a> Sam Witteveen, Martin Andrews </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salazar2019masked/">Masked Language Model Scoring</a> Julian Salazar, Davis Liang, Toan Q. Nguyen, Katrin Kirchhoff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/klein2019learning/">Learning To Answer By Learning To Ask: Getting The Best Of GPT-2 And BERT Worlds</a> Tassilo Klein, Moin Nabi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vig2019multiscale/">A Multiscale Visualization Of Attention In The Transformer Model</a> Jesse Vig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schmidt2019generalization/">Generalization In Generation: A Closer Look At Exposure Bias</a> Florian Schmidt </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2019insertion/">Insertion-based Decoding With Automatically Inferred Generation Order</a> Jiatao Gu, Qi Liu, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rajbhandari2019memory/">Zero: Memory Optimizations Toward Training Trillion Parameter Models</a> Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2019denoising/">BART: Denoising Sequence-to-sequence Pre-training For Natural Language Generation, Translation, And Comprehension</a> Mike Lewis et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/irie2019language/">Language Modeling With Deep Transformers</a> Kazuki Irie, Albert Zeyer, Ralf Schl√ºter, Hermann Ney </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vig2019visualizing/">Visualizing Attention In Transformer-based Language Representation Models</a> Jesse Vig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/see2019do/">Do Massively Pretrained Language Models Make Better Storytellers?</a> Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, Christopher D. Manning </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2019large/">Dialogpt: Large-scale Generative Pre-training For Conversational Response Generation</a> Yizhe Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2019patent/">Patent Claim Generation By Fine-tuning Openai GPT-2</a> Jieh-sheng Lee, Jieh Hsiang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liao2019gpt/">Gpt-based Generation For Classical Chinese Poetry</a> Yi Liao, Yasheng Wang, Qun Liu, Xin Jiang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2019model/">Model Compression With Two-stage Multi-teacher Knowledge Distillation For Web Question Answering System</a> Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, Daxin Jiang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mansimov2019generalized/">A Generalized Framework Of Sequence Generation With Application To Undirected Sequence Models</a> Elman Mansimov, Alex Wang, Sean Welleck, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wallace2019universal/">Universal Adversarial Triggers For Attacking And Analyzing NLP</a> Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/malmi2019high/">Encode, Tag, Realize: High-precision Text Editing</a> Eric Malmi, Sebastian Krause, Sascha Rothe, Daniil Mirylenka, Aliaksei Severyn </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/noever2020chess/">The Chess Transformer: Mastering Play Using Generative Language Models</a> David Noever, Matt Ciolino, Josh Kalin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rosset2020knowledge/">Knowledge-aware Language Model Pretraining</a> Corby Rosset et al. </li>
     
   
     
   
     
       <li> <a href="/publications/clement2020multi/">Pymt5: Multi-mode Translation Of Natural Language And Python Code With Transformers</a> Colin B. Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, Neel Sundaresan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020organizing/">Optimus: Organizing Sentences Via Pre-trained Modeling Of A Latent Space</a> Chunyuan Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2020large/">CPM: A Large-scale Generative Chinese Pre-trained Language Model</a> Zhengyan Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gehman2020evaluating/">Realtoxicityprompts: Evaluating Neural Toxic Degeneration In Language Models</a> Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, Noah A. Smith </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/k%C3%B6bis2020artificial/">Artificial Intelligence Versus Maya Angelou: Experimental Evidence That People Cannot Differentiate Ai-generated From Human-written Poetry</a> Nils K√∂bis, Luca Mossink </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2020variational/">Variational Transformers For Diverse Response Generation</a> Zhaojiang Lin, Genta Indra Winata, Peng Xu, Zihan Liu, Pascale Fung </li>
     
   
     
       <li> <a href="/publications/kale2020text/">Text-to-text Pre-training For Data-to-text Tasks</a> Mihir Kale, Abhinav Rastogi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ekstedt2020transformer/">Turngpt: A Transformer-based Language Model For Predicting Turn-taking In Spoken Dialog</a> Erik Ekstedt, Gabriel Skantze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hosseiniasl2020simple/">A Simple Language Model For Task-oriented Dialogue</a> Ehsan Hosseini-asl, Bryan Mccann, Chien-sheng Wu, Semih Yavuz, Richard Socher </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/puri2020training/">Training Question Answering Models From Synthetic Data</a> Raul Puri, Ryan Spring, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zandie2020multi/">Emptransfo: A Multi-head Transformer Architecture For Creating Empathetic Dialog Systems</a> Rohola Zandie, Mohammad H. Mahoor </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pang2020text/">Text Generation By Learning From Demonstrations</a> Richard Yuanzhe Pang, He He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020narrative/">Narrative Interpolation For Generating And Understanding Stories</a> Su Wang, Greg Durrett, Katrin Erk </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020large/">A Large-scale Chinese Short-text Conversation Dataset</a> Yida Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/devries2020as/">As Good As New. How To Successfully Recycle English GPT-2 To Make Models For Other Languages</a> Wietse De Vries, Malvina Nissim </li>
     
   
     
   
     
       <li> <a href="/publications/antoun2020pre/">Aragpt2: Pre-trained Transformer For Arabic Language Generation</a> Wissam Antoun, Fady Baly, Hazem Hajj </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2020bridging/">BANG: Bridging Autoregressive And Non-autoregressive Generation With Large Scale Pretraining</a> Weizhen Qi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/pearce2020deriving/">DAVE: Deriving Automatically Verilog From English</a> Hammond Pearce, Benjamin Tan, Ramesh Karri </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rashkin2020outline/">Plotmachines: Outline-conditioned Generation With Dynamic Plot State Tracking</a> Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, Jianfeng Gao </li>
     
   
     
       <li> <a href="/publications/chen2020knowledge/">KGPT: Knowledge-grounded Pre-training For Data-to-text Generation</a> Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020dataset/">The Pile: An 800GB Dataset Of Diverse Text For Language Modeling</a> Leo Gao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2020trojaning/">Trojaning Language Models For Fun And Profit</a> Xinyang Zhang, Zheng Zhang, Shouling Ji, Ting Wang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2020how/">How Can We Know When Language Models Know? On The Calibration Of Language Models For Question Answering</a> Zhengbao Jiang, Jun Araki, Haibo Ding, Graham Neubig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hegde2020unsupervised/">Unsupervised Paraphrase Generation Using Pre-trained Language Models</a> Chaitra Hegde, Shrikumar Patil </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2020progressive/">Progressive Generation Of Long Text With Pretrained Language Models</a> Bowen Tan, Zichao Yang, Maruan Ai-shedivat, Eric P. Xing, Zhiting Hu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2020towards/">UBAR: Towards Fully End-to-end Task-oriented Dialog Systems With GPT-2</a> Yunyi Yang, Yunhao Li, Xiaojun Quan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bi2020pre/">PALM: Pre-training An Autoencoding&autoregressive Language Model For Context-conditioned Generation</a> Bin Bi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/krause2020generative/">Gedi: Generative Discriminator Guided Sequence Generation</a> Ben Krause et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020constrained/">POINTER: Constrained Progressive Text Generation Via Insertion-based Generative Pre-training</a> Yizhe Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2020few/">Few-shot Natural Language Generation For Task-oriented Dialog</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/madotto2020language/">Language Models As Few-shot Learner For Task-oriented Dialogue Systems</a> Andrea Madotto, Zihan Liu, Zhaojiang Lin, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2020few/">Few-shot Generative Conversational Query Rewriting</a> Shi Yu et al. </li>
     
   
     
       <li> <a href="/publications/lopez2020simplifying/">Simplifying Paragraph-level Question Generation Via Transformer Language Models</a> Luis Enrico Lopez, Diane Kathryn Cruz, Jan Christian Blaise Cruz, Charibeth Cheng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mehri2020unsupervised/">Unsupervised Evaluation Of Interactive Dialog With Dialogpt</a> Shikib Mehri, Maxine Eskenazi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2020cross/">XGPT: Cross-modal Generative Pre-training For Image Captioning</a> Qiaolin Xia et al. </li>
     
   
     
       <li> <a href="/publications/mager2020gpt/">Gpt-too: A Language-model-first Approach For Amr-to-text Generation</a> Manuel Mager et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2020data/">Genaug: Data Augmentation For Finetuning Text Generators</a> Steven Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, Eduard Hovy </li>
     
   
     
       <li> <a href="/publications/b2020language/">Language Models Are Few-shot Learners</a> Tom B. Brown et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020making/">Making Pre-trained Language Models Better Few-shot Learners</a> Tianyu Gao, Adam Fisch, Danqi Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mcguffie2020radicalization/">The Radicalization Risks Of GPT-3 And Advanced Neural Language Models</a> Kris Mcguffie, Alex Newhouse </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guan2020knowledge/">A Knowledge-enhanced Pretraining Model For Commonsense Story Generation</a> Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, Minlie Huang </li>
     
   
     
       <li> <a href="/publications/schick2020not/">It's Not Just Size That Matters: Small Language Models Are Also Few-shot Learners</a> Timo Schick, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2020data/">Data Augmentation Using Pre-trained Transformer Models</a> Varun Kumar, Ashutosh Choudhary, Eunah Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kirk2021bias/">Bias Out-of-the-box: An Empirical Analysis Of Intersectional Occupational Biases In Popular Generative Language Models</a> Hannah Kirk et al. </li>
     
   
     
       <li> <a href="/publications/alex2021real/">RAFT: A Real-world Few-shot Text Classification Benchmark</a> Neel Alex et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/betz2021thinking/">Thinking Aloud: Dynamic Context Generation Improves Zero-shot Reasoning Performance Of GPT-2</a> Gregor Betz, Kyle Richardson, Christian Voigt </li>
     
   
     
   
     
       <li> <a href="/publications/winata2021language/">Language Models Are Few-shot Multilingual Learners</a> Genta Indra Winata et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2021efficient/">Glam: Efficient Scaling Of Language Models With Mixture-of-experts</a> Nan Du et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021few/">Few-shot Learning With Multilingual Language Models</a> Xi Victoria Lin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/feng2021language/">Language Model As An Annotator: Exploring Dialogpt For Dialogue Summarization</a> Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mitchell2021fast/">Fast Model Editing At Scale</a> Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D. Manning </li>
     
   
     
       <li> <a href="/publications/kharitonov2021text/">Text-free Prosody-aware Generative Spoken Language Modeling</a> Eugene Kharitonov et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sheng2021revealing/">Revealing Persona Biases In Dialogue Systems</a> Emily Sheng, Josh Arnold, Zhou Yu, Kai-wei Chang, Nanyun Peng </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/clark2021all/">All That's 'human' Is Not Gold: Evaluating Human Evaluation Of Generated Text</a> Elizabeth Clark et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2021low/">Lora: Low-rank Adaptation Of Large Language Models</a> Edward J. Hu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nakano2021browser/">Webgpt: Browser-assisted Question-answering With Human Feedback</a> Reiichiro Nakano et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021gpt/">GPT Understands, Too</a> Xiao Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021entailment/">Entailment As Few-shot Learner</a> Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, Hao Ma </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/so2021searching/">Primer: Searching For Efficient Transformers For Language Modeling</a> David R. So et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/pascual2021plug/">A Plug-and-play Method For Controlled Text Generation</a> Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell, Roger Wattenhofer </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021stability/">The Stability-efficiency Dilemma: Investigating Sequence Length Warmup For Training GPT Models</a> Conglong Li, Minjia Zhang, Yuxiong He </li>
     
   
     
       <li> <a href="/publications/eichenberg2021magma/">MAGMA -- Multimodal Augmentation Of Generative Models Through Adapter-based Finetuning</a> Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, Anette Frank </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/artetxe2021efficient/">Efficient Large Scale Language Modeling With Mixtures Of Experts</a> Mikel Artetxe et al. </li>
     
   
     
       <li> <a href="/publications/moradi2021gpt/">GPT-3 Models Are Poor Few-shot Learners In The Biomedical Domain</a> Milad Moradi, Kathrin Blagec, Florian Haberl, Matthias Samwald </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021what/">What Makes Good In-context Examples For GPT-\(3\)?</a> Jiachang Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021recursively/">Recursively Summarizing Books With Human Feedback</a> Jeff Wu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2021finetuned/">Finetuned Language Models Are Zero-shot Learners</a> Jason Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2021ernie/">ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training For Language Understanding And Generation</a> Yu Sun et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/solaiman2021process/">Process For Adapting Language Models To Society (PALMS) With Values-targeted Datasets</a> Irene Openai Solaiman, Christy Openai Dennison </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lai2021thank/">Thank You BART! Rewarding Pre-trained Models Improves Formality Style Transfer</a> Huiyuan Lai, Antonio Toral, Malvina Nissim </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021prefix/">Prefix-tuning: Optimizing Continuous Prompts For Generation</a> Xiang Lisa Li, Percy Liang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2021pangu/">Pangu-\(Œ±\): Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation</a> Wei Zeng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mishra2021reframing/">Reframing Instructional Prompts To Gptk's Language</a> Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, Hannaneh Hajishirzi </li>
     
   
     
       <li> <a href="/publications/lin2021measuring/">Truthfulqa: Measuring How Models Mimic Human Falsehoods</a> Stephanie Lin, Jacob Hilton, Owain Evans </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/borgeaud2021improving/">Improving Language Models By Retrieving From Trillions Of Tokens</a> Sebastian Borgeaud et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singh2021nlp/">The NLP Cookbook: Modern Recipes For Transformer Based Deep Learning Architectures</a> Sushant Singh, Ausif Mahmood </li>
     
   
     
       <li> <a href="/publications/chen2021data/">Visualgpt: Data-efficient Adaptation Of Pretrained Language Models For Image Captioning</a> Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2021calibrate/">Calibrate Before Use: Improving Few-shot Performance Of Language Models</a> Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kalyan2021ammus/">AMMUS : A Survey Of Transformer-based Pretrained Models In Natural Language Processing</a> Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, Sivanesan Sangeetha </li>
     
   
     
       <li> <a href="/publications/liu2021token/">A Token-level Reference-free Hallucination Detection Benchmark For Free-form Text Generation</a> Tianyu Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hambardzumyan2021word/">WARP: Word-level Adversarial Reprogramming</a> Karen Hambardzumyan, Hrant Khachatrian, Jonathan May </li>
     
   
     
       <li> <a href="/publications/yoo2021leveraging/">Gpt3mix: Leveraging Large-scale Language Models For Text Augmentation</a> Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-woo Lee, Woomyeong Park </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2021empirical/">An Empirical Study Of GPT-3 For Few-shot Knowledge-based VQA</a> Zhengyuan Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/buschek2021impact/">The Impact Of Multiple Parallel Phrase Suggestions On Email Input And Composition Behaviour Of Native And Non-native English Writers</a> Daniel Buschek, Martin Z√ºrn, Malin Eiband </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tafjord2021general/">General-purpose Question-answering With Macaw</a> Oyvind Tafjord, Peter Clark </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lester2021power/">The Power Of Scale For Parameter-efficient Prompt Tuning</a> Brian Lester, Rami Al-rfou, Noah Constant </li>
     
   
     
   
     
       <li> <a href="/publications/schramowski2021large/">Large Pre-trained Language Models Contain Human-like Biases Of What Is Right And Wrong To Do</a> Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A. Rothkopf, Kristian Kersting </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021token/">Terapipe: Token-level Pipeline Parallelism For Training Large-scale Language Models</a> Zhuohan Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mccoy2021how/">How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty In Text Generation Using RAVEN</a> R. Thomas Mccoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz </li>
     
   
     
       <li> <a href="/publications/kim2021what/">What Changes Can Large-scale Language Models Bring? Intensive Study On Hyperclova: Billions-scale Korean Generative Pretrained Transformers</a> Boseop Kim et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chintagunta2021medically/">Medically Aware GPT-3 As A Data Generator For Medical Dialogue Summarization</a> Bharath Chintagunta, Namit Katariya, Xavier Amatriain, Anitha Kannan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sharma2021towards/">Towards Facilitating Empathic Conversations In Online Mental Health Support: A Reinforcement Learning Approach</a> Ashish Sharma, Inna W. Lin, Adam S. Miner, David C. Atkins, Tim Althoff </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/guti%C3%A9rrezfandi%C3%B1o2021spanish/">Maria: Spanish Language Models</a> Asier Guti√©rrez-fandi√±o et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lu2021machine/">Codexglue: A Machine Learning Benchmark Dataset For Code Understanding And Generation</a> Shuai Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dou2021is/">Is GPT-3 Text Indistinguishable From Human Text? Scarecrow: A Framework For Scrutinizing Machine Text</a> Yao Dou, Maxwell Forbes, Rik Koncel-kedziorski, Noah A. Smith, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/madotto2021few/">Few-shot Bot: Prompt-based Learning For Dialogue Systems</a> Andrea Madotto, Zhaojiang Lin, Genta Indra Winata, Pascale Fung </li>
     
   
     
       <li> <a href="/publications/lu2021fantastically/">Fantastically Ordered Prompts And Where To Find Them: Overcoming Few-shot Prompt Order Sensitivity</a> Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cahyawijaya2021benchmark/">Indonlg: Benchmark And Resources For Evaluating Indonesian Natural Language Generation</a> Samuel Cahyawijaya et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2021ernie/">ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training For Language Understanding And Generation</a> Shuohuan Wang et al. </li>
     
   
     
       <li> <a href="/publications/liu2021decoding/">Dexperts: Decoding-time Controlled Text Generation With Experts And Anti-experts</a> Alisa Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2021identifier/">Codet5: Identifier-aware Unified Pre-trained Encoder-decoder Models For Code Understanding And Generation</a> Yue Wang, Weishi Wang, Shafiq Joty, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tamkin2021understanding/">Understanding The Capabilities, Limitations, And Societal Impact Of Large Language Models</a> Alex Tamkin, Miles Brundage, Jack Clark, Deep Ganguli </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/asai2021one/">One Question Answering Model For Many Languages With Cross-lingual Dense Passage Retrieval</a> Akari Asai, Xinyan Yu, Jungo Kasai, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/uchendu2021benchmark/">TURINGBENCH: A Benchmark Environment For Turing Test In The Age Of Neural Text Generation</a> Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, Dongwon Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/west2021symbolic/">Symbolic Knowledge Distillation: From General Language Models To Commonsense Models</a> Peter West et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhong2021adapting/">Adapting Language Models For Zero-shot Learning By Meta-tuning On Dataset And Prompt Collections</a> Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wiegreffe2021reframing/">Reframing Human-ai Collaboration For Generating Free-text Explanations</a> Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shleifer2021improved/">Normformer: Improved Transformer Pretraining With Extra Normalization</a> Sam Shleifer, Jason Weston, Myle Ott </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2021explanation/">An Explanation Of In-context Learning As Implicit Bayesian Inference</a> Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma </li>
     
   
     
   
     
       <li> <a href="/publications/fang2021transformer/">Transformer-based Conditional Variational Autoencoder For Controllable Story Generation</a> Le Fang et al. </li>
     
   
     
       <li> <a href="/publications/du2021general/">GLM: General Language Model Pretraining With Autoregressive Blank Infilling</a> Zhengxiao Du et al. </li>
     
   
     
       <li> <a href="/publications/reynolds2021prompt/">Prompt Programming For Large Language Models: Beyond The Few-shot Paradigm</a> Laria Reynolds, Kyle Mcdonell </li>
     
   
     
   
     
       <li> <a href="/publications/barikeri2021real/">Redditbias: A Real-world Resource For Bias Evaluation And Debiasing Of Conversational Language Models</a> Soumya Barikeri, Anne Lauscher, Ivan Vuliƒá, Goran Glava≈° </li>
     
   
     
   
     
       <li> <a href="/publications/liu2021mitigating/">Mitigating Political Bias In Language Models Through Reinforced Calibration</a> Ruibo Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/topal2021exploring/">Exploring Transformers In Natural Language Generation: GPT, BERT, And Xlnet</a> M. Onat Topal, Anil Bas, Imke Van Heerden </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021evaluating/">Evaluating Large Language Models Trained On Code</a> Mark Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kayser2021e/">E-vil: A Dataset And Benchmark For Natural Language Explanations In Vision-language Tasks</a> Maxime Kayser et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/meng2022mass/">Mass-editing Memory In A Transformer</a> Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David Bau </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sejnowski2022large/">Large Language Models And The Reverse Turing Test</a> Terrence Sejnowski </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jeblick2022chatgpt/">Chatgpt Makes Medicine Easy To Swallow: An Exploratory Case Study On Simplified Radiology Reports</a> Katharina Jeblick et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022what/">What Language Model Architecture And Pretraining Objective Work Best For Zero-shot Generalization?</a> Thomas Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2022contrastive/">Contrastive Search Is What You Need For Neural Text Generation</a> Yixuan Su, Nigel Collier </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2022language/">Language Models Can See: Plugging Visual Controls In Text Generation</a> Yixuan Su et al. </li>
     
   
     
   
     
       <li> <a href="/publications/robinson2022leveraging/">Leveraging Large Language Models For Multiple Choice Question Answering</a> Joshua Robinson, Christopher Michael Rytting, David Wingate </li>
     
   
     
       <li> <a href="/publications/hoffmann2022training/">Training Compute-optimal Large Language Models</a> Jordan Hoffmann et al. </li>
     
   
     
       <li> <a href="/publications/lee2022do/">Do Language Models Plagiarize?</a> Jooyoung Lee, Thai Le, Jinghui Chen, Dongwon Lee </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2022fast/">Flashattention: Fast And Memory-efficient Exact Attention With Io-awareness</a> Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R√© </li>
     
   
     
   
     
       <li> <a href="/publications/jang2022can/">Can Large Language Models Truly Understand Prompts? A Case Study With Negated Prompts</a> Joel Jang, Seonghyeon Ye, Minjoon Seo </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022efficient/">Zeroquant: Efficient And Affordable Post-training Quantization For Large-scale Transformers</a> Zhewei Yao et al. </li>
     
   
     
       <li> <a href="/publications/xie2022unifying/">Unifiedskg: Unifying And Multi-tasking Structured Knowledge Grounding With Text-to-text Language Models</a> Tianbao Xie et al. </li>
     
   
     
       <li> <a href="/publications/khot2022decomposed/">Decomposed Prompting: A Modular Approach For Solving Complex Tasks</a> Tushar Khot et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qian2022controllable/">Controllable Natural Language Generation With Contrastive Prefixes</a> Jing Qian, Li Dong, Yelong Shen, Furu Wei, Weizhu Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahmed2022few/">Few-shot Training Llms For Project-specific Code-summarization</a> Toufique Ahmed, Premkumar Devanbu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chakrabarty2022help/">Help Me Write A Poem: Instruction Tuning As A Vehicle For Collaborative Poetry Writing</a> Tuhin Chakrabarty, Vishakh Padmakumar, He He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2022efficient/">Zerogen: Efficient Zero-shot Learning Via Dataset Generation</a> Jiacheng Ye et al. </li>
     
   
     
       <li> <a href="/publications/yu2022scaling/">Scaling Autoregressive Models For Content-rich Text-to-image Generation</a> Jiahui Yu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li%C3%A9vin2022can/">Can Large Language Models Reason About Medical Questions?</a> Valentin Li√©vin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, Ole Winther </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2022active/">Active Example Selection For In-context Learning</a> Yiming Zhang, Shi Feng, Chenhao Tan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2022black/">Black-box Tuning For Language-model-as-a-service</a> Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2022chain/">Chain-of-thought Prompting Elicits Reasoning In Large Language Models</a> Jason Wei et al. </li>
     
   
     
   
     
       <li> <a href="/publications/pereira2022multi/">Visconde: Multi-document QA With GPT-3 And Neural Reranking</a> Jayr Pereira, Robson Fidalgo, Roberto Lotufo, Rodrigo Nogueira </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2022integrating/">3DALL-E: Integrating Text-to-image AI In 3D Design Workflows</a> Vivian Liu, Jo Vermeulen, George Fitzmaurice, Justin Matejka </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sekuli%C4%872022evaluating/">Evaluating Mixed-initiative Conversational Search Systems Via User Simulation</a> Ivan Sekuliƒá, Mohammad Aliannejadi, Fabio Crestani </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022generative/">M6-rec: Generative Pretrained Language Models Are Open-ended Recommender Systems</a> Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang </li>
     
   
     
       <li> <a href="/publications/cao2022model/">A Model-agnostic Data Manipulation Method For Persona-based Dialogue Generation</a> Yu Cao, Wei Bi, Meng Fang, Shuming Shi, Dacheng Tao </li>
     
   
     
       <li> <a href="/publications/weng2022large/">Large Language Models Are Better Reasoners With Self-verification</a> Yixuan Weng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yuan2022pretraining/">Biobart: Pretraining And Evaluation Of A Biomedical Generative Language Model</a> Hongyi Yuan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gonen2022demystifying/">Demystifying Prompts In Language Models Via Perplexity Estimation</a> Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, Luke Zettlemoyer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/trivedi2022interleaving/">Interleaving Retrieval With Chain-of-thought Reasoning For Knowledge-intensive Multi-step Questions</a> Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2022rethinking/">Rethinking With Retrieval: Faithful Large Language Model Inference</a> Hangfeng He, Hongming Zhang, Dan Roth </li>
     
   
     
   
     
       <li> <a href="/publications/meng2022generating/">Generating Training Data With Language Models: Towards Zero-shot Language Understanding</a> Yu Meng, Jiaxin Huang, Yu Zhang, Jiawei Han </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022making/">Making Large Language Models Better Reasoners With Step-aware Verifier</a> Yifei Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022diffusion/">Diffusion-lm Improves Controllable Text Generation</a> Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, Tatsunori B. Hashimoto </li>
     
   
     
   
     
       <li> <a href="/publications/sahu2022data/">Data Augmentation For Intent Classification With Off-the-shelf Large Language Models</a> Gaurav Sahu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/aher2022using/">Using Large Language Models To Simulate Multiple Humans And Replicate Human Subject Studies</a> Gati Aher, Rosa I. Arriaga, Adam Tauman Kalai </li>
     
   
     
   
     
       <li> <a href="/publications/perez2022ignore/">Ignore Previous Prompt: Attack Techniques For Language Models</a> F√°bio Perez, Ian Ribeiro </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/poesia2022reliable/">Synchromesh: Reliable Code Generation From Pre-trained Language Models</a> Gabriel Poesia et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sammani2022nlx/">NLX-GPT: A Model For Natural Language Explanations In Vision And Vision-language Tasks</a> Fawaz Sammani, Tanmoy Mukherjee, Nikos Deligiannis </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frantar2022accurate/">GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers</a> Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022contrastive/">Contrastive Decoding: Open-ended Text Generation As Optimization</a> Xiang Lisa Li et al. </li>
     
   
     
       <li> <a href="/publications/hosseiniasl2022generative/">A Generative Language Model For Few-shot Aspect-based Sentiment Analysis</a> Ehsan Hosseini-asl, Wenhao Liu, Caiming Xiong </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2022large/">Cogvideo: Large-scale Pretraining For Text-to-video Generation Via Transformers</a> Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang </li>
     
   
     
       <li> <a href="/publications/ye2022unreliability/">The Unreliability Of Explanations In Few-shot Prompting For Textual Reasoning</a> Xi Ye, Greg Durrett </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2022least/">Least-to-most Prompting Enables Complex Reasoning In Large Language Models</a> Denny Zhou et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shah2022lm/">Lm-nav: Robotic Navigation With Large Pre-trained Models Of Language, Vision, And Action</a> Dhruv Shah, Blazej Osinski, Brian Ichter, Sergey Levine </li>
     
   
     
       <li> <a href="/publications/xu2022systematic/">A Systematic Evaluation Of Large Language Models Of Code</a> Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zan2022continual/">CERT: Continual Pre-training On Sketches For Library-oriented Code Generation</a> Daoguang Zan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dai2022why/">Why Can GPT Learn In-context? Language Models Implicitly Perform Gradient Descent As Meta-optimizers</a> Damai Dai et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2022binding/">Binding Language Models In Symbolic Languages</a> Zhoujun Cheng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/stevenson2022putting/">Putting Gpt-3's Creativity To The (alternative Uses) Test</a> Claire Stevenson, Iris Smal, Matthijs Baas, Raoul Grasman, Han Van Der Maas </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/si2022prompting/">Prompting GPT-3 To Be Reliable</a> Chenglei Si et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022automatic/">Automatic Chain Of Thought Prompting In Large Language Models</a> Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola </li>
     
   
     
       <li> <a href="/publications/tay2022unifying/">UL2: Unifying Language Learning Paradigms</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/oh2022why/">Why Does Surprisal From Larger Transformer-based Language Models Provide A Poorer Fit To Human Reading Times?</a> Byung-doh Oh, William Schuler </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022exploring/">Exploring The Limits Of Domain-adaptive Training For Detoxifying Large-scale Language Models</a> Boxin Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ding2022is/">Is GPT-3 A Good Data Annotator?</a> Bosheng Ding et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bhavya2022analogy/">Analogy Generation By Prompting Large Language Models: A Case Study Of Instructgpt</a> Bhavya Bhavya, Jinjun Xiong, Chengxiang Zhai </li>
     
   
     
   
     
       <li> <a href="/publications/guti%C3%A9rrez2022thinking/">Thinking About GPT-3 In-context Learning For Biomedical IE? Think Again</a> Bernal Jim√©nez Guti√©rrez et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2022large/">GODEL: Large-scale Pre-training For Goal-directed Dialog</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2022glm/">GLM-130B: An Open Bilingual Pre-trained Model</a> Aohan Zeng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2022complexity/">Complexity-based Prompting For Multi-step Reasoning</a> Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tack2022ai/">The AI Teacher Test: Measuring The Pedagogical Ability Of Blender And GPT-3 In Educational Dialogues</a> Ana√Øs Tack, Chris Piech </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2022prompt/">Promptcap: Prompt-guided Task-aware Image Captioning</a> Yushi Hu et al. </li>
     
   
     
       <li> <a href="/publications/leviathan2022fast/">Fast Inference From Transformers Via Speculative Decoding</a> Yaniv Leviathan, Matan Kalman, Yossi Matias </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/madaan2022memory/">Memory-assisted Prompt Editing To Improve GPT-3 After Deployment</a> Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang </li>
     
   
     
       <li> <a href="/publications/madaan2022text/">Text And Patterns: For Effective Chain Of Thought, It Takes Two To Tango</a> Aman Madaan, Amir Yazdanbakhsh </li>
     
   
     
       <li> <a href="/publications/madaan2022language/">Language Models Of Code Are Few-shot Commonsense Learners</a> Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2022worker/">WANLI: Worker And AI Collaboration For Natural Language Inference Dataset Creation</a> Alisa Liu, Swabha Swayamdipta, Noah A. Smith, Yejin Choi </li>
     
   
     
   
     
       <li> <a href="/publications/razeghi2022impact/">Impact Of Pretraining Term Frequencies On Few-shot Reasoning</a> Yasaman Razeghi, Robert L. Iv Logan, Matt Gardner, Sameer Singh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/prasad2022gradient/">Grips: Gradient-free, Edit-based Instruction Search For Prompting Large Language Models</a> Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/haviv2022transformer/">Transformer Language Models Without Positional Encodings Still Learn Positional Information</a> Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/roberts2022scaling/">Scaling Up Models And Data With \(\texttt{t5x}\) And \(\texttt{seqio}\)</a> Adam Roberts et al. </li>
     
   
     
       <li> <a href="/publications/srivastava2022beyond/">Beyond The Imitation Game: Quantifying And Extrapolating The Capabilities Of Language Models</a> Aarohi Shammie Srivastava et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/saparov2022language/">Language Models Are Greedy Reasoners: A Systematic Formal Analysis Of Chain-of-thought</a> Abulhair Saparov, He He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/diao2022black/">Black-box Prompt Learning For Pre-trained Language Models</a> Shizhe Diao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022learn/">Learn To Explain: Multimodal Reasoning Via Thought Chains For Science Question Answering</a> Pan Lu et al. </li>
     
   
     
       <li> <a href="/publications/lu2022dynamic/">Dynamic Prompt Learning Via Policy Gradient For Semi-structured Mathematical Reasoning</a> Pan Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khattab2022demonstrate/">Demonstrate-search-predict: Composing Retrieval And Language Models For Knowledge-intensive NLP</a> Omar Khattab et al. </li>
     
   
     
   
     
       <li> <a href="/publications/arora2022ask/">Ask Me Anything: A Simple Strategy For Prompting Language Models</a> Simran Arora et al. </li>
     
   
     
   
     
       <li> <a href="/publications/press2022measuring/">Measuring And Narrowing The Compositionality Gap In Language Models</a> Ofir Press et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/black2022gpt/">Gpt-neox-20b: An Open-source Autoregressive Language Model</a> Sid Black et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/muennighoff2022gpt/">SGPT: GPT Sentence Embeddings For Semantic Search</a> Niklas Muennighoff </li>
     
   
     
   
     
       <li> <a href="/publications/li2022explanations/">Explanations From Large Language Models Make Small Reasoners Better</a> Shiyang Li et al. </li>
     
   
     
       <li> <a href="/publications/taylor2022clinical/">Clinical Prompt Learning With Frozen Language Models</a> Niall Taylor, Yi Zhang, Dan Joyce, Alejo Nevado-holgado, Andrey Kormilitzin </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ho2022large/">Large Language Models Are Reasoning Teachers</a> Namgyu Ho, Laura Schmid, Se-young Yun </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022accelerating/">Accelerating Attention Through Gradient-based Learned Runtime Pruning</a> Zheng Li, Soroush Ghodrati, Amir Yazdanbakhsh, Hadi Esmaeilzadeh, Mingu Kang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/valipour2022parameter/">Dylora: Parameter Efficient Tuning Of Pre-trained Models Using Dynamic Search-free Low-rank Adaptation</a> Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, Ali Ghodsi </li>
     
   
     
       <li> <a href="/publications/geva2022transformer/">Transformer Feed-forward Layers Build Predictions By Promoting Concepts In The Vocabulary Space</a> Mor Geva, Avi Caciularu, Kevin Ro Wang, Yoav Goldberg </li>
     
   
     
   
     
       <li> <a href="/publications/lin2022teaching/">Teaching Models To Express Their Uncertainty In Words</a> Stephanie Lin, Jacob Hilton, Owain Evans </li>
     
   
     
   
     
       <li> <a href="/publications/bavarian2022efficient/">Efficient Training Of Language Models To Fill In The Middle</a> Mohammad Bavarian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2022optimizing/">Rlprompt: Optimizing Discrete Text Prompts With Reinforcement Learning</a> Mingkai Deng et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2022evaluating/">Evaluating Human-language Model Interaction</a> Mina Lee et al. </li>
     
   
     
       <li> <a href="/publications/lee2022designing/">Coauthor: Designing A Human-ai Collaborative Writing Dataset For Exploring Language Model Capabilities</a> Mina Lee, Percy Liang, Qian Yang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2022can/">CREPE: Can Vision-language Foundation Models Reason Compositionally?</a> Zixian Ma et al. </li>
     
   
     
       <li> <a href="/publications/bommarito2022gpt/">GPT Takes The Bar Exam</a> Michael Ii Bommarito, Daniel Martin Katz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022open/">OPT: Open Pre-trained Transformer Language Models</a> Susan Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/miotto2022who/">Who Is GPT-3? An Exploration Of Personality, Values And Demographics</a> Maril√π Miotto, Nicola Rossberg, Bennett Kleinberg </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sap2022neural/">Neural Theory-of-mind? On The Limits Of Social Intelligence In Large Lms</a> Maarten Sap, Ronan Lebras, Daniel Fried, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022super/">Super-naturalinstructions: Generalization Via Declarative Instructions On 1600+ NLP Tasks</a> Yizhong Wang et al. </li>
     
   
     
       <li> <a href="/publications/goyal2022news/">News Summarization And Evaluation In The Era Of GPT-3</a> Tanya Goyal, Junyi Jessy Li, Greg Durrett </li>
     
   
     
       <li> <a href="/publications/kojima2022large/">Large Language Models Are Zero-shot Reasoners</a> Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ouyang2022training/">Training Language Models To Follow Instructions With Human Feedback</a> Long Ouyang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/webb2022emergent/">Emergent Analogical Reasoning In Large Language Models</a> Taylor Webb, Keith J. Holyoak, Hongjing Lu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/magee2022structured/">Structured Like A Language Model: Analysing AI As An Automated Subject</a> Liam Magee, Vanicka Arora, Luke Munn </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/susnjak2022end/">Chatgpt: The End Of Online Exam Integrity?</a> Teo Susnjak </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shuster2022language/">Language Models That Seek For Knowledge: Modular Search & Generation For Dialogue And Prompt Completion</a> Kurt Shuster et al. </li>
     
   
     
       <li> <a href="/publications/shridhar2022distilling/">Distilling Reasoning Capabilities Into Smaller Language Models</a> Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan </li>
     
   
     
   
     
       <li> <a href="/publications/hagendorff2022thinking/">Thinking Fast And Slow In Large Language Models</a> Thilo Hagendorff, Sarah Fabi, Michal Kosinski </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/soltan2022alexatm/">Alexatm 20B: Few-shot Learning Using A Large-scale Multilingual Seq2seq Model</a> Saleh Soltan et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kalakonda2022action/">Action-gpt: Leveraging Large-scale Language Models For Improved And Generalized Action Generation</a> Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla </li>
     
   
     
       <li> <a href="/publications/shin2022effect/">On The Effect Of Pretraining Corpora On In-context Learning By A Large-scale Language Model</a> Seongjin Shin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/taylor2022large/">Galactica: A Large Language Model For Science</a> Ross Taylor et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/strudel2022self/">Self-conditioned Embedding Diffusion For Text Generation</a> Robin Strudel et al. </li>
     
   
     
   
     
       <li> <a href="/publications/min2022rethinking/">Rethinking The Role Of Demonstrations: What Makes In-context Learning Work?</a> Sewon Min et al. </li>
     
   
     
       <li> <a href="/publications/ramos2022lightweight/">Smallcap: Lightweight Image Captioning Prompted With Retrieval Augmentation</a> Rita Ramos, Bruno Martins, Desmond Elliott, Yova Kementchedjhieva </li>
     
   
     
   
     
       <li> <a href="/publications/luo2022generative/">Biogpt: Generative Pre-trained Transformer For Biomedical Text Generation And Mining</a> Renqian Luo et al. </li>
     
   
     
       <li> <a href="/publications/gong2022sequence/">Diffuseq: Sequence To Sequence Text Generation With Diffusion Models</a> Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong </li>
     
   
     
       <li> <a href="/publications/abdelghani2022gpt/">Gpt-3-driven Pedagogical Agents For Training Children's Curious Question-asking Skills</a> Rania Abdelghani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/trott2022do/">Do Large Language Models Know What Humans Know?</a> Sean Trott, Cameron Jones, Tyler Chang, James Michaelov, Benjamin Bergen </li>
     
   
     
   
     
       <li> <a href="/publications/shahriar2023have/">Let's Have A Chat! A Conversation With Chatgpt: Technology, Applications, And Limitations</a> Sakib Shahriar, Kadhim Hayawi </li>
     
   
     
       <li> <a href="/publications/malladi2023fine/">Fine-tuning Language Models With Just Forward Passes</a> Sadhika Malladi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/poldrack2023ai/">Ai-assisted Coding: Experiments With GPT-4</a> Russell A Poldrack, Thomas Lu, Ga≈°per Begu≈° </li>
     
   
     
   
     
       <li> <a href="/publications/zhao2023verify/">Verify-and-edit: A Knowledge-enhanced Chain-of-thought Framework</a> Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, Lidong Bing </li>
     
   
     
       <li> <a href="/publications/schaeffer2023are/">Are Emergent Abilities Of Large Language Models A Mirage?</a> Rylan Schaeffer, Brando Miranda, Sanmi Koyejo </li>
     
   
     
   
     
       <li> <a href="/publications/tang2023does/">Does Synthetic Data Generation Of Llms Help Clinical Text Mining?</a> Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, Xia Hu </li>
     
   
     
       <li> <a href="/publications/zheng2023secrets/">Secrets Of RLHF In Large Language Models Part I: PPO</a> Rui Zheng et al. </li>
     
   
     
       <li> <a href="/publications/yang2023teaching/">Gpt4tools: Teaching Large Language Model To Use Tools Via Self-instruction</a> Rui Yang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mao2023survey/">Gpteval: A Survey On Assessments Of Chatgpt And GPT-4</a> Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, Erik Cambria </li>
     
   
     
       <li> <a href="/publications/xu2023chatgpt/">Chatgpt Vs. Google: A Comparative Study Of Search Performance And User Experience</a> Ruiyun Rayna Xu, Yue Katherine Feng, Hailiang Chen </li>
     
   
     
       <li> <a href="/publications/huang2023understanding/">Audiogpt: Understanding And Generating Speech, Music, Sound, And Talking Head</a> Rongjie Huang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/eldan2023how/">Tinystories: How Small Can Language Models Be And Still Speak Coherent English?</a> Ronen Eldan, Yuanzhi Li </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chew2023llm/">Llm-assisted Content Analysis: Using Large Language Models To Support Deductive Coding</a> Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, Annice Kim </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023then/">Prompt, Generate, Then Cache: Cascade Of Foundation Models Makes Strong Few-shot Learners</a> Renrui Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/omar2023chatgpt/">Chatgpt Versus Traditional Question Answering For Knowledge Graphs: Current Status And Future Directions Towards Knowledge Graph Chatbots</a> Reham Omar, Omij Mangukiya, Panos Kalnis, Essam Mansour </li>
     
   
     
       <li> <a href="/publications/khoury2023how/">How Secure Is Code Generated By Chatgpt?</a> Rapha√´l Khoury, Anderson R. Avila, Jacob Brunelle, Baba Mamadou Camara </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/pires2023portuguese/">Sabi\'a: Portuguese Large Language Models</a> Ramon Pires, Hugo Abonizio, Thales Sales Almeida, Rodrigo Nogueira </li>
     
   
     
       <li> <a href="/publications/sohail2023decoding/">Decoding Chatgpt: A Taxonomy Of Existing Research, Current Challenges, And Possible Future Directions</a> Shahab Saquib Sohail et al. </li>
     
   
     
       <li> <a href="/publications/min2023fine/">Factscore: Fine-grained Atomic Evaluation Of Factual Precision In Long Form Text Generation</a> Sewon Min et al. </li>
     
   
     
   
     
       <li> <a href="/publications/abukhalaf2023codex/">On Codex Prompt Engineering For OCL Generation: An Empirical Study</a> Seif Abukhalaf, Mohammad Hamdaqa, Foutse Khomh </li>
     
   
     
       <li> <a href="/publications/kim2023cot/">The Cot Collection: Improving Zero-shot And Few-shot Learning Of Language Models Via Chain-of-thought Fine-tuning</a> Seungone Kim et al. </li>
     
   
     
       <li> <a href="/publications/thakur2023large/">Verigen: A Large Language Model For Verilog Code Generation</a> Shailja Thakur et al. </li>
     
   
     
       <li> <a href="/publications/wu2023comparative/">A Comparative Study Of Open-source Large Language Models, GPT-4 And Claude 2: Multiple-choice Test Taking In Nephrology</a> Sean Wu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fleming2023clinician/">Medalign: A Clinician-generated Dataset For Instruction Following With Electronic Medical Records</a> Scott L. Fleming et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kr%C3%BCgel2023moral/">The Moral Authority Of Chatgpt</a> Sebastian Kr√ºgel, Andreas Ostermaier, Matthias Uhl </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mitrovi%C4%872023chatgpt/">Chatgpt Or Human? Detect And Explain. Explaining Decisions Of Machine Learning Model For Detecting Short Chatgpt-generated Text</a> Sandra Mitroviƒá, Davide Andreoletti, Omran Ayoub </li>
     
   
     
       <li> <a href="/publications/roy2023generating/">Generating Phishing Attacks Using Chatgpt</a> Sayak Saha Roy, Krishna Vamsi Naragam, Shirin Nilizadeh </li>
     
   
     
       <li> <a href="/publications/wu2023next/">Next-gpt: Any-to-any Multimodal LLM</a> Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-seng Chua </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023why/">Why Does Chatgpt Fall Short In Providing Truthful Answers?</a> Shen Zheng, Jie Huang, Kevin Chen-chuan Chang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/rajput2023recommender/">Recommender Systems With Generative Retrieval</a> Shashank Rajput et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023evaluation/">Evaluation Of Chatgpt Family Of Models For Biomedical Reasoning And Classification</a> Shan Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2023augmenting/">Toolkengpt: Augmenting Frozen Language Models With Massive Tools Via Tool Embeddings</a> Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu </li>
     
   
     
       <li> <a href="/publications/hao2023reasoning/">Reasoning With Language Model Is Planning With World Model</a> Shibo Hao et al. </li>
     
   
     
       <li> <a href="/publications/dai2023llm/">Llm-in-the-loop: Leveraging Large Language Model For Thematic Analysis</a> Shih-chieh Dai, Aiping Xiong, Lun-wei Ku </li>
     
   
     
       <li> <a href="/publications/imani2023mathematical/">Mathprompter: Mathematical Reasoning Using Large Language Models</a> Shima Imani, Liang Du, Harsh Shrivastava </li>
     
   
     
       <li> <a href="/publications/moghaddam2023boosting/">Boosting Theory-of-mind Performance In Large Language Models Via Prompting</a> Shima Rahimi Moghaddam, Christopher J. Honey </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gozalobrizuela2023chatgpt/">Chatgpt Is Not All You Need. A State Of The Art Review Of Large Generative AI Models</a> Roberto Gozalo-brizuela, Eduardo C. Garrido-merchan </li>
     
   
     
   
     
       <li> <a href="/publications/aiyappa2023can/">Can We Trust The Evaluation On Chatgpt?</a> Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-yeol Ahn </li>
     
   
     
       <li> <a href="/publications/mccoy2023embers/">Embers Of Autoregression: Understanding Large Language Models Through The Problem They Are Trained To Solve</a> R. Thomas Mccoy, Shunyu Yao, Dan Friedman, Matthew Hardy, Thomas L. Griffiths </li>
     
   
     
       <li> <a href="/publications/marjieh2023large/">Large Language Models Predict Human Sensory Judgments Across Six Modalities</a> Raja Marjieh, Ilia Sucholutsky, Pol Van Rijn, Nori Jacoby, Thomas L. Griffiths </li>
     
   
     
   
     
       <li> <a href="/publications/khraisha2023can/">Can Large Language Models Replace Humans In The Systematic Review Process? Evaluating Gpt-4's Efficacy In Screening And Extracting Data From Peer-reviewed And Grey Literature In Multiple Languages</a> Qusai Khraisha, Sophie Put, Johanna Kappenberg, Azza Warraitch, Kristin Hadfield </li>
     
   
     
       <li> <a href="/publications/huang2023lawyer/">Lawyer Llama Technical Report</a> Quzhe Huang et al. </li>
     
   
     
       <li> <a href="/publications/wei2023evaluation/">Evaluation Of Chatgpt-generated Medical Responses: A Systematic Review And Meta-analysis</a> Qiuhong Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lyu2023translating/">Translating Radiology Reports Into Plain Language Using Chatgpt And GPT-4 With Prompt Learning: Promising Results, Limitations, And Potential</a> Qing Lyu et al. </li>
     
   
     
       <li> <a href="/publications/jin2023augmenting/">Genegpt: Augmenting Large Language Models With Domain Tools For Improved Access To Biomedical Information</a> Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu </li>
     
   
     
       <li> <a href="/publications/jin2023contrastive/">Medcpt: Contrastive Pre-trained Transformers With Large-scale Pubmed Search Logs For Zero-shot Biomedical Information Retrieval</a> Qiao Jin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lyu2023faithful/">Faithful Chain-of-thought Reasoning</a> Qing Lyu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sridhar2023harnessing/">Harnessing Llms In Curricular Design: Using GPT-4 To Support Authoring Of Learning Objectives</a> Pragnya Sridhar et al. </li>
     
   
     
       <li> <a href="/publications/bhandari2023are/">Are Large Language Models Geospatially Knowledgeable?</a> Prabin Bhandari, Antonios Anastasopoulos, Dieter Pfoser </li>
     
   
     
       <li> <a href="/publications/manakul2023zero/">Selfcheckgpt: Zero-resource Black-box Hallucination Detection For Generative Large Language Models</a> Potsawee Manakul, Adian Liusie, Mark J. F. Gales </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023llama/">Llama-adapter V2: Parameter-efficient Visual Instruction Model</a> Peng Gao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hacker2023regulating/">Regulating Chatgpt And Other Large Generative AI Models</a> Philipp Hacker, Andreas Engel, Marco Mauer </li>
     
   
     
       <li> <a href="/publications/cai2023do/">Do Large Language Models Resemble Humans In Language Use?</a> Zhenguang G. Cai, Xufeng Duan, David A. Haslett, Shuqi Wang, Martin J. Pickering </li>
     
   
     
       <li> <a href="/publications/niszczota2023gpt/">GPT Has Become Financially Literate: Insights From Financial Literacy Tests Of GPT And A Preliminary Test Of How People Use It As A Source Of Advice</a> Pawe≈Ç Niszczota, Sami Abbas </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023exploring/">Graphologue: Exploring Large Language Model Responses With Interactive Diagrams</a> Peiling Jiang, Jude Rayan, Steven P. Dow, Haijun Xia </li>
     
   
     
       <li> <a href="/publications/lu2023plug/">Chameleon: Plug-and-play Compositional Reasoning With Large Language Models</a> Pan Lu et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023internlm/">Internlm-xcomposer: A Vision-language Large Model For Advanced Text-image Comprehension And Composition</a> Pan Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/openai2023gpt/">GPT-4 Technical Report</a> Openai et al. </li>
     
   
     
       <li> <a href="/publications/khattab2023compiling/">Dspy: Compiling Declarative Language Model Calls Into Self-improving Pipelines</a> Omar Khattab et al. </li>
     
   
     
   
     
       <li> <a href="/publications/guerreiro2023hallucinations/">Hallucinations In Large Multilingual Translation Models</a> Nuno M. Guerreiro et al. </li>
     
   
     
       <li> <a href="/publications/dziri2023faith/">Faith And Fate: Limits Of Transformers On Compositionality</a> Nouha Dziri et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ziems2023large/">Large Language Models Are Built-in Autoregressive Search Engines</a> Noah Ziems, Wenhao Yu, Zhihan Zhang, Meng Jiang </li>
     
   
     
       <li> <a href="/publications/palagin2023ontochatgpt/">Ontochatgpt Information System: Ontology-driven Structured Prompts For Chatgpt Meta-learning</a> Oleksandr Palagin, Vladislav Kaverinskiy, Anna Litvin, Kyrylo Malakhov </li>
     
   
     
       <li> <a href="/publications/bian2023chatgpt/">Chatgpt Is A Knowledgeable But Inexperienced Solver: An Investigation Of Commonsense Problem In Large Language Models</a> Ning Bian et al. </li>
     
   
     
       <li> <a href="/publications/rao2023cat/">CAT-LM: Training Language Models On Aligned Code And Tests</a> Nikitha Rao, Kush Jain, Uri Alon, Claire Le Goues, Vincent J. Hellendoorn </li>
     
   
     
       <li> <a href="/publications/m%C3%BCndler2023self/">Self-contradictory Hallucinations Of Large Language Models: Evaluation, Detection And Mitigation</a> Niels M√ºndler, Jingxuan He, Slobodan Jenko, Martin Vechev </li>
     
   
     
       <li> <a href="/publications/ding2023enhancing/">Enhancing Chat Language Models By Scaling High-quality Instructional Conversations</a> Ning Ding et al. </li>
     
   
     
       <li> <a href="/publications/mckenna2023sources/">Sources Of Hallucination By Large Language Models On Inference Tasks</a> Nick Mckenna et al. </li>
     
   
     
   
     
       <li> <a href="/publications/pangakis2023automated/">Automated Annotation With Generative AI Requires Validation</a> Nicholas Pangakis, Samuel Wolken, Neil Fasching </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sengupta2023jais/">Jais And Jais-chat: Arabic-centric Foundation And Instruction-tuned Open Generative Large Language Models</a> Neha Sengupta et al. </li>
     
   
     
   
     
       <li> <a href="/publications/shinn2023language/">Reflexion: Language Agents With Verbal Reinforcement Learning</a> Noah Shinn et al. </li>
     
   
     
       <li> <a href="/publications/robinson2023chatgpt/">Chatgpt MT: Competitive For High- (but Not Low-) Resource Languages</a> Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig </li>
     
   
     
       <li> <a href="/publications/gruver2023large/">Large Language Models Are Zero-shot Time Series Forecasters</a> Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson </li>
     
   
     
       <li> <a href="/publications/kiesler2023exploring/">Exploring The Potential Of Large Language Models To Generate Formative Programming Feedback</a> Natalie Kiesler, Dominic Lohr, Hieke Keuning </li>
     
   
     
       <li> <a href="/publications/shapira2023clever/">Clever Hans Or Neural Theory Of Mind? Stress Testing Social Reasoning In Large Language Models</a> Natalie Shapira et al. </li>
     
   
     
       <li> <a href="/publications/varshney2023stitch/">A Stitch In Time Saves Nine: Detecting And Mitigating Hallucinations Of Llms By Validating Low-confidence Generation</a> Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu </li>
     
   
     
   
     
       <li> <a href="/publications/erik2023consistency/">Consistency Analysis Of Chatgpt</a> Myeongjun Erik Jang, Thomas Lukasiewicz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/siddiq2023using/">Using Large Language Models To Generate Junit Tests: An Empirical Study</a> Mohammed Latif Siddiq et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fraiwan2023review/">A Review Of Chatgpt Applications In Education, Marketing, Software Engineering, And Healthcare: Benefits, Drawbacks, And Research Directions</a> Mohammad Fraiwan, Natheer Khasawneh </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023api/">Api-bank: A Comprehensive Benchmark For Tool-augmented Llms</a> Minghao Li et al. </li>
     
   
     
       <li> <a href="/publications/wu2023lamini/">Lamini-lm: A Diverse Herd Of Distilled Models From Large-scale Instructions</a> Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-mageed, Alham Fikri Aji </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kwon2023reward/">Reward Design With Language Models</a> Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh </li>
     
   
     
       <li> <a href="/publications/kosinski2023evaluating/">Evaluating Large Language Models In Theory Of Mind Tasks</a> Michal Kosinski </li>
     
   
     
       <li> <a href="/publications/nasr2023scalable/">Scalable Extraction Of Training Data From (production) Language Models</a> Milad Nasr et al. </li>
     
   
     
   
     
       <li> <a href="/publications/parker2023large/">A Large Language Model Approach To Educational Survey Feedback Analysis</a> Michael J. Parker, Caitlin Anderson, Claire Stone, Yearim Oh </li>
     
   
     
   
     
       <li> <a href="/publications/orenstrakh2023detecting/">Detecting Llm-generated Text In Computing Education: A Comparative Study For Chatgpt Cases</a> Michael Sheinman Orenstrakh, Oscar Karnalim, Carlos Anibal Suarez, Michael Liut </li>
     
   
     
       <li> <a href="/publications/fu2023chatgpt/">Chatgpt For Vulnerability Detection, Classification, And Repair: How Far Are We?</a> Michael Fu, Chakkrit Tantithamthavorn, Van Nguyen, Trung Le </li>
     
   
     
       <li> <a href="/publications/xiong2023can/">Can Llms Express Their Uncertainty? An Empirical Evaluation Of Confidence Elicitation In Llms</a> Miao Xiong et al. </li>
     
   
     
       <li> <a href="/publications/xu2023interpretable/">Drivegpt4: Interpretable End-to-end Autonomous Driving Via Large Language Model</a> Zhenhua Xu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hasan2023zero/">Zero- And Few-shot Prompting With Llms: A Comparative Study With Fine-tuned Models For Bangla Sentiment Analysis</a> Md. Arid Hasan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/laskar2023systematic/">A Systematic Study And Comprehensive Evaluation Of Chatgpt On Benchmark Datasets</a> Md Tahmid Rahman Laskar et al. </li>
     
   
     
       <li> <a href="/publications/khondaker2023comprehensive/">Gptaraeval: A Comprehensive Evaluation Of Chatgpt On Arabic NLP</a> Md Tawkat Islam Khondaker, Abdul Waheed, El Moatez Billah Nagoudi, Muhammad Abdul-mageed </li>
     
   
     
   
     
       <li> <a href="/publications/maniparambil2023enhancing/">Enhancing CLIP With GPT-4: Harnessing Visual Descriptions As Prompts</a> Mayug Maniparambil et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023unleashing/">Unleashing The Emergent Cognitive Synergy In Large Language Models: A Task-solving Agent Through Multi-persona Self-collaboration</a> Zhenhailong Wang et al. </li>
     
   
     
       <li> <a href="/publications/sch%C3%A4fer2023empirical/">An Empirical Evaluation Of Using Large Language Models For Automated Unit Test Generation</a> Max Sch√§fer, Sarah Nadi, Aryaz Eghbali, Frank Tip </li>
     
   
     
       <li> <a href="/publications/nievas2023distilling/">Distilling Large Language Models For Matching Patients To Clinical Trials</a> Mauro Nievas, Aditya Basu, Yanshan Wang, Hrituraj Singh </li>
     
   
     
       <li> <a href="/publications/jakesch2023co/">Co-writing With Opinionated Language Models Affects Users' Views</a> Maurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalmanson, Mor Naaman </li>
     
   
     
   
     
       <li> <a href="/publications/le2023text/">Voicebox: Text-guided Multilingual Universal Speech Generation At Scale</a> Matthew Le et al. </li>
     
   
     
       <li> <a href="/publications/karpinska2023large/">Large Language Models Effectively Leverage Document-level Context For Literary Translation, But Critical Errors Persist</a> Marzena Karpinska, Mohit Iyyer </li>
     
   
     
   
     
       <li> <a href="/publications/patil2023large/">Gorilla: Large Language Model Connected With Massive Apis</a> Shishir G. Patil, Tianjun Zhang, Xin Wang, Joseph E. Gonzalez </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phute2023llm/">LLM Self Defense: By Self Examination, Llms Know They Are Being Tricked</a> Mansi Phute et al. </li>
     
   
     
   
     
       <li> <a href="/publications/berglund2023reversal/">The Reversal Curse: Llms Trained On "A Is B" Fail To Learn "B Is A"</a> Lukas Berglund et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023document/">Document-level Machine Translation With Large Language Models</a> Longyue Wang et al. </li>
     
   
     
       <li> <a href="/publications/chen2023driving/">Driving With Llms: Fusing Object-level Vector Modality For Explainable Autonomous Driving</a> Long Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yan2023generative/">Generative Artificial Intelligence In Learning Analytics: Contextualising Opportunities And Challenges Through The Learning Analytics Cycle</a> Lixiang Yan, Roberto Martinez-maldonado, Dragan Ga≈°eviƒá </li>
     
   
     
       <li> <a href="/publications/yan2023human/">Human-ai Collaboration In Thematic Analysis Using Chatgpt: A User Study And Design Recommendations</a> Lixiang Yan et al. </li>
     
   
     
       <li> <a href="/publications/fu2023comparing/">Comparing Sentence-level Suggestions To Message-level Suggestions In Ai-mediated Communication</a> Liye Fu, Benjamin Newman, Maurice Jakesch, Sarah Kreps </li>
     
   
     
       <li> <a href="/publications/yan2023practical/">Practical And Ethical Challenges Of Large Language Models In Education: A Systematic Scoping Review</a> Lixiang Yan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yang2023give/">Give Us The Facts: Enhancing Large Language Models With Knowledge Graphs For Fact-aware Language Modeling</a> Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, Xindong Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023scaling/">Scaling Autoregressive Multi-modal Models: Pretraining And Instruction Tuning</a> Lili Yu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zheng2023judging/">Judging Llm-as-a-judge With Mt-bench And Chatbot Arena</a> Lianmin Zheng et al. </li>
     
   
     
       <li> <a href="/publications/xu2023comprehensive/">Superclue: A Comprehensive Chinese Large Language Model Benchmark</a> Liang Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guan2023leveraging/">Leveraging Pre-trained Large Language Models To Construct And Utilize World Models For Model-based Task Planning</a> Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023zero/">Zero-shot Next-item Recommendation Using Large Pretrained Language Models</a> Lei Wang, Ee-peng Lim </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/seenivasan2023end/">Surgicalgpt: End-to-end Language-vision GPT For Visual Question Answering In Surgery</a> Lalithkumar Seenivasan, Mobarakol Islam, Gokul Kannan, Hongliang Ren </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maaz2023video/">Video-chatgpt: Towards Detailed Video Understanding Via Large Vision And Language Models</a> Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023human/">VISAR: A Human-ai Argumentative Writing Assistant With Visual Programming And Rapid Draft Prototyping</a> Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, Toby Jia-jun Li </li>
     
   
     
   
     
       <li> <a href="/publications/kheiri2023exploiting/">Sentimentgpt: Exploiting GPT For Advanced Sentiment Analysis And Its Departure From Current Machine Learning</a> Kiana Kheiri, Hamid Karimi </li>
     
   
     
       <li> <a href="/publications/caramancion2023news/">News Verifiers Showdown: A Comparative Performance Evaluation Of Chatgpt 3.5, Chatgpt 4.0, Bing AI, And Bard In News Fact-checking</a> Kevin Matthe Caramancion </li>
     
   
     
       <li> <a href="/publications/busch2023just/">Just Tell Me: Prompt Engineering In Business Process Management</a> Kiran Busch, Alexander Rochlitzer, Diana Sola, Henrik Leopold </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chang2023archaeology/">Speak, Memory: An Archaeology Of Books Known To Chatgpt/gpt-4</a> Kent K. Chang, Mackenzie Cramer, Sandeep Soni, David Bamman </li>
     
   
     
   
     
       <li> <a href="/publications/jablonka2023examples/">14 Examples Of How Llms Can Transform Materials Science And Chemistry: A Reflection On A Large Language Model Hackathon</a> Kevin Maik Jablonka et al. </li>
     
   
     
   
     
       <li> <a href="/publications/pandya2023automating/">Automating Customer Service Using Langchain: Building Custom Open-source GPT Chatbot For Organizations</a> Keivalya Pandya, Mehfuza Holia </li>
     
   
     
       <li> <a href="/publications/tian2023just/">Just Ask For Calibration: Strategies For Eliciting Calibrated Confidence Scores From Language Models Fine-tuned With Human Feedback</a> Katherine Tian et al. </li>
     
   
     
       <li> <a href="/publications/collins2023evaluating/">Evaluating Language Models For Mathematics Through Interactions</a> Katherine M. Collins et al. </li>
     
   
     
       <li> <a href="/publications/kalyan2023survey/">A Survey Of GPT-3 Family Large Language Models Including Chatgpt And GPT-4</a> Katikapalli Subramanyam Kalyan </li>
     
   
     
   
     
       <li> <a href="/publications/roth2023waffling/">Waffling Around For Performance: Visual Classification With Random Words And Broad Concepts</a> Karsten Roth et al. </li>
     
   
     
   
     
       <li> <a href="/publications/soman2023biomedical/">Biomedical Knowledge Graph-optimized Prompt Generation For Large Language Models</a> Karthik Soman et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chang2023how/">Chipgpt: How Far Are We From Natural Language Hardware Design</a> Kaiyan Chang et al. </li>
     
   
     
       <li> <a href="/publications/benharrak2023writer/">Writer-defined AI Personas For On-demand Feedback Generation</a> Karim Benharrak, Tim Zindulka, Florian Lehmann, Hendrik Heuer, Daniel Buschek </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hayawi2023imitation/">The Imitation Game: Detecting Human And Ai-generated Texts In The Era Of Chatgpt And BARD</a> Kadhim Hayawi, Sakib Shahriar, Sujith Samuel Mathew </li>
     
   
     
       <li> <a href="/publications/greshake2023not/">Not What You've Signed Up For: Compromising Real-world Llm-integrated Applications With Indirect Prompt Injection</a> Kai Greshake et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ahuja2023multilingual/">MEGA: Multilingual Evaluation Of Generative AI</a> Kabir Ahuja et al. </li>
     
   
     
       <li> <a href="/publications/wang2023evaluation/">Evaluation And Analysis Of Hallucination In Large Vision-language Models</a> Junyang Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023is/">Is Chatgpt A Good Recommender? A Preliminary Study</a> Junling Liu et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023recommendation/">Recommendation As Instruction Following: A Large Language Model Empowered Recommendation Approach</a> Junjie Zhang et al. </li>
     
   
     
       <li> <a href="/publications/ye2023comprehensive/">A Comprehensive Capability Analysis Of GPT-3 And GPT-3.5 Series Models</a> Junjie Ye et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kasai2023evaluating/">Evaluating GPT-4 And Chatgpt On Japanese Medical Licensing Examinations</a> Jungo Kasai, Yuhei Kasai, Keisuke Sakaguchi, Yutaro Yamada, Dragomir Radev </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023minigpt/">Minigpt-v2: Large Language Model As A Unified Interface For Vision-language Multi-task Learning</a> Jun Chen et al. </li>
     
   
     
       <li> <a href="/publications/hazell2023spear/">Spear Phishing With Large Language Models</a> Julian Hazell </li>
     
   
     
   
     
       <li> <a href="/publications/liu2023large/">Chatcounselor: A Large Language Models For Mental Health Support</a> June M. Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/schneider2023towards/">Towards Llm-based Autograding For Short Textual Answers</a> Johannes Schneider, Bernd Schenk, Christina Niklaus </li>
     
   
     
       <li> <a href="/publications/hartmann2023political/">The Political Ideology Of Conversational AI: Converging Evidence On Chatgpt's Pro-environmental, Left-libertarian Orientation</a> Jochen Hartmann, Jasper Schwenzow, Maximilian Witte </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023is/">Is Chatgpt Fair For Recommendation? Evaluating Fairness In Large Language Model Recommendation</a> Jizhi Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023general/">Structgpt: A General Framework For Large Language Model To Reason Over Structured Data</a> Jinhao Jiang et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023potential/">The Potential And Pitfalls Of Using A Large Language Model Such As Chatgpt Or GPT-4 As A Clinical Assistant</a> Jingqing Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023robustness/">On The Robustness Of Chatgpt: An Adversarial And Out-of-distribution Perspective</a> Jindong Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023exploring/">Badgpt: Exploring Security Vulnerabilities Of Chatgpt Via Backdoor Attacks To Instructgpt</a> Jiawen Shi, Yixin Liu, Pan Zhou, Lichao Sun </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023think/">Think-on-graph: Deep And Responsible Reasoning Of Large Language Model On Knowledge Graph</a> Jiashuo Sun et al. </li>
     
   
     
       <li> <a href="/publications/xiang2023language/">Language Models Meet World Models: Embodied Experiences Enhance Language Models</a> Jiannan Xiang et al. </li>
     
   
     
       <li> <a href="/publications/zhou2023ethical/">Ethical Chatgpt: Concerns, Challenges, And Commandments</a> Jianlong Zhou, Heimo M√ºller, Andreas Holzinger, Fang Chen </li>
     
   
     
       <li> <a href="/publications/yang2023set/">Set-of-mark Prompting Unleashes Extraordinary Visual Grounding In GPT-4V</a> Jianwei Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023empowering/">Empowering Molecule Discovery For Molecule-caption Translation With Large Language Models: A Chatgpt Perspective</a> Jiatong Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2023icl/">ICL-D3IE: In-context Learning With Diverse Demonstrations Updating For Document Information Extraction</a> Jiabang He et al. </li>
     
   
     
   
     
       <li> <a href="/publications/tang2023graph/">Graphgpt: Graph Instruction Tuning For Large Language Models</a> Jiabin Tang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/espejel2023gpt/">GPT-3.5, GPT-4, Or BARD? Evaluating Llms Reasoning Ability In Zero-shot Setting And Performance Boosting Through Prompts</a> Jessica L√≥pez Espejel, El Hassane Ettifouri, Mahaman Sanoussi Yahaya Alassan, El Mehdi Chouham, Walid Dahhane </li>
     
   
     
   
     
       <li> <a href="/publications/wei2023larger/">Larger Language Models Do In-context Learning Differently</a> Jerry Wei et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yao2023llm/">LLM Lies: Hallucinations Are Not Bugs, But Features As Adversarial Examples</a> Jia-yu Yao et al. </li>
     
   
     
       <li> <a href="/publications/yang2023impact/">The Impact Of Chatgpt And Llms On Medical Imaging Stakeholders: Perspectives And Use Cases</a> Jiancheng Yang, Hongwei Bran Li, Donglai Wei </li>
     
   
     
       <li> <a href="/publications/fu2023evaluate/">Gptscore: Evaluate As You Desire</a> Jinlan Fu, See-kiong Ng, Zhengbao Jiang, Pengfei Liu </li>
     
   
     
   
     
       <li> <a href="/publications/haase2023artificial/">Artificial Muses: Generative Artificial Intelligence Chatbots Have Risen To Human-level Creativity</a> Jennifer Haase, Paul H. P. Hanel </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/blocklove2023chip/">Chip-chat: Challenges And Opportunities In Conversational Hardware Design</a> Jason Blocklove, Siddharth Garg, Ramesh Karri, Hammond Pearce </li>
     
   
     
       <li> <a href="/publications/savelka2023thrilled/">Thrilled By Your Progress! Large Language Models (GPT-4) No Longer Struggle To Pass Assessments In Higher Education Programming Courses</a> Jaromir Savelka, Arav Agarwal, Marshall An, Chris Bogart, Majd Sakr </li>
     
   
     
       <li> <a href="/publications/holmes2023evaluating/">Evaluating Large Language Models On A Highly-specialized Topic, Radiation Oncology Physics</a> Jason Holmes et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/koco%C5%842023jack/">Chatgpt: Jack Of All Trades, Master Of None</a> Jan Koco≈Ñ et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jahan2023evaluation/">Evaluation Of Chatgpt On Biomedical Tasks: A Zero-shot Comparison With Fine-tuned Generative Transformers</a> Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang </li>
     
   
     
       <li> <a href="/publications/joshi2023not/">"it's Not Like Jarvis, But It's Pretty Close!" -- Examining Chatgpt's Usage Among Undergraduate Students In Computer Science</a> Ishika Joshi, Ritvik Budhiraja, Harshal D Akolekar, Jagat Sesh Challa, Dhruv Kumar </li>
     
   
     
       <li> <a href="/publications/joshi2023chatgpt/">Chatgpt In The Classroom: An Analysis Of Its Strengths And Weaknesses For Solving Undergraduate Computer Science Questions</a> Ishika Joshi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/doughty2023comparative/">A Comparative Study Of Ai-generated (GPT-4) And Human-crafted Mcqs In Programming Education</a> Jacob Doughty et al. </li>
     
   
     
       <li> <a href="/publications/savelka2023large/">Large Language Models (GPT) Struggle To Answer Multiple-choice Questions About Code</a> Jaromir Savelka, Arav Agarwal, Christopher Bogart, Majd Sakr </li>
     
   
     
   
     
       <li> <a href="/publications/shumailov2023curse/">The Curse Of Recursion: Training On Generated Data Makes Models Forget</a> Ilia Shumailov et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023compressing/">Llmlingua: Compressing Prompts For Accelerated Inference Of Large Language Models</a> Huiqiang Jiang, Qianhui Wu, Chin-yew Lin, Yuqing Yang, Lili Qiu </li>
     
   
     
       <li> <a href="/publications/touvron2023open/">Llama: Open And Efficient Foundation Language Models</a> Hugo Touvron et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023building/">Building Cooperative Embodied Agents Modularly With Large Language Models</a> Hongxin Zhang et al. </li>
     
   
     
       <li> <a href="/publications/xiong2023fine/">Doctorglm: Fine-tuning Your Chinese Doctor Is Not A Herculean Task</a> Honglin Xiong et al. </li>
     
   
     
       <li> <a href="/publications/yang2023open/">Fingpt: Open-source Financial Large Language Models</a> Hongyang Yang, Xiao-yang Liu, Christina Dan Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tran2023instruction/">Bioinstruct: Instruction Tuning Of Large Language Models For Biomedical Natural Language Processing</a> Hieu Tran, Zhichao Yang, Zonghai Yao, Hong Yu </li>
     
   
     
       <li> <a href="/publications/gilbert2023semantic/">Semantic Compression With Large Language Models</a> Henry Gilbert, Michael Sandborn, Douglas C. Schmidt, Jesse Spencer-smith, Jules White </li>
     
   
     
       <li> <a href="/publications/koziolek2023chatgpt/">Chatgpt For PLC/DCS Control Logic Generation</a> Heiko Koziolek, Sten Gruener, Virendra Ashiwal </li>
     
   
     
       <li> <a href="/publications/nori2023capabilities/">Capabilities Of GPT-4 On Medical Challenge Problems</a> Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz </li>
     
   
     
       <li> <a href="/publications/nori2023can/">Can Generalist Foundation Models Outcompete Special-purpose Tuning? Case Study In Medicine</a> Harsha Nori et al. </li>
     
   
     
       <li> <a href="/publications/peters2023large/">Large Language Models Can Infer Psychological Dispositions Of Social Media Users</a> Heinrich Peters, Sandra Matz </li>
     
   
     
       <li> <a href="/publications/tian2023is/">Is Chatgpt The Ultimate Programming Assistant -- How Far Is It?</a> Haoye Tian et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023visual/">Visual Instruction Tuning</a> Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee </li>
     
   
     
   
     
       <li> <a href="/publications/wu2023chatgpt/">Chatgpt Or Grammarly? Evaluating Chatgpt On Grammatical Error Correction Benchmark</a> Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, Michael Lyu </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023extractive/">Extractive Summarization Via Chatgpt For Faithful Summary Generation</a> Haopeng Zhang, Xiao Liu, Jiawei Zhang </li>
     
   
     
   
     
       <li> <a href="/publications/wu2023q/">Q-instruct: Improving Low-level Visual Abilities For Multi-modality Foundation Models</a> Haoning Wu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/rao2023can/">Can Chatgpt Assess Human Personalities? A General Evaluation Framework</a> Haocong Rao, Cyril Leung, Chunyan Miao </li>
     
   
     
       <li> <a href="/publications/liu2023audioldm/">Audioldm 2: Learning Holistic Audio Generation With Self-supervised Pretraining</a> Haohe Liu et al. </li>
     
   
     
       <li> <a href="/publications/wen2023llm/">Autodroid: Llm-powered Task Automation In Android</a> Hao Wen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sun2023safety/">Safety Assessment Of Chinese Large Language Models</a> Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang </li>
     
   
     
       <li> <a href="/publications/kumar2023geotechnical/">Geotechnical Parrot Tales (GPT): Harnessing Large Language Models In Geotechnical Engineering</a> Krishna Kumar </li>
     
   
     
       <li> <a href="/publications/chen2023democratizing/">Phoenix: Democratizing Chatgpt Across Languages</a> Zhihong Chen et al. </li>
     
   
     
       <li> <a href="/publications/fei2023reasoning/">Reasoning Implicit Sentiment With Chain-of-thought Prompting</a> Hao Fei et al. </li>
     
   
     
       <li> <a href="/publications/huang2023chatgpt/">Chatgpt For Shaping The Future Of Dentistry: The Potential Of Multi-modal Large Language Model</a> Hanyao Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023investigating/">Personallm: Investigating The Ability Of Large Language Models To Express Personality Traits</a> Hang Jiang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2023leveraging/">Auggpt: Leveraging Chatgpt For Text Data Augmentation</a> Haixing Dai et al. </li>
     
   
     
       <li> <a href="/publications/liao2023gpt/">GPT-4 Enhanced Multimodal Grounding For Autonomous Driving: Leveraging Cross-modal Attention With Large Language Models</a> Haicheng Liao et al. </li>
     
   
     
       <li> <a href="/publications/dang2023choice/">Choice Over Control: How Users Write With Large Language Models Using Diegetic And Non-diegetic Prompting</a> Hai Dang, Sven Goller, Florian Lehmann, Daniel Buschek </li>
     
   
     
   
     
       <li> <a href="/publications/lee2023applying/">Applying Large Language Models And Chain-of-thought For Automatic Scoring</a> Gyeong-geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu, Xiaoming Zhai </li>
     
   
     
       <li> <a href="/publications/li2023revisiting/">Revisiting Large Language Models As Zero-shot Relation Extractors</a> Guozheng Li, Peng Wang, Wenjun Ke </li>
     
   
     
       <li> <a href="/publications/almeida2023exploring/">Exploring The Psychology Of Llms' Moral And Legal Reasoning</a> Guilherme F. C. F. Almeida, Jos√© Luiz Nunes, Neele Engelmann, Alex Wiegmann, Marcelo De Ara√∫jo </li>
     
   
     
       <li> <a href="/publications/zuccon2023dr/">Dr Chatgpt, Tell Me What I Want To Hear: How Prompt Knowledge Impacts Health Answer Correctness</a> Guido Zuccon, Bevan Koopman </li>
     
   
     
       <li> <a href="/publications/faggioli2023perspectives/">Perspectives On Large Language Models For Relevance Judgment</a> Guglielmo Faggioli et al. </li>
     
   
     
       <li> <a href="/publications/zuccon2023chatgpt/">Chatgpt Hallucinates When Attributing Answers</a> Guido Zuccon, Bevan Koopman, Razia Shaik </li>
     
   
     
       <li> <a href="/publications/penedo2023refinedweb/">The Refinedweb Dataset For Falcon LLM: Outperforming Curated Corpora With Web Data, And Web Data Only</a> Guilherme Penedo et al. </li>
     
   
     
       <li> <a href="/publications/wang2023open/">Voyager: An Open-ended Embodied Agent With Large Language Models</a> Guanzhi Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023fair/">"it's A Fair Game", Or Is It? Examining How Users Navigate Disclosure Risks And Benefits When Using Llm-based Conversational Agents</a> Zhiping Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chalvatzaki2023learning/">Learning To Reason Over Scene Graphs: A Case Study Of Finetuning GPT-2 Into A Robot Language Model For Grounded Task Planning</a> Georgia Chalvatzaki et al. </li>
     
   
     
       <li> <a href="/publications/kortemeyer2023performance/">Performance Of The Pre-trained Large Language Model GPT-4 On Automated Short Answer Grading</a> Gerd Kortemeyer </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2023explicit/">Navgpt: Explicit Reasoning In Vision-and-language Navigation With Large Language Models</a> Gengze Zhou, Yicong Hong, Qi Wu </li>
     
   
     
       <li> <a href="/publications/kim2023language/">Language Models Can Solve Computer Tasks</a> Geunwoo Kim, Pierre Baldi, Stephen Mcaleer </li>
     
   
     
       <li> <a href="/publications/sun2023aligning/">Aligning Large Multimodal Models With Factually Augmented RLHF</a> Zhiqing Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/suri2023do/">Do Large Language Models Show Decision Heuristics Similar To Humans? A Case Study Using GPT-3.5</a> Gaurav Suri, Lily R. Slater, Ali Ziaee, Morgan Nguyen </li>
     
   
     
       <li> <a href="/publications/nicholas2023lost/">Lost In Translation: Large Language Models In Non-english Content Analysis</a> Gabriel Nicholas, Aliya Bhatia </li>
     
   
     
   
     
       <li> <a href="/publications/zheng2023chatgpt/">Chatgpt Chemistry Assistant For Text Mining And Prediction Of MOF Synthesis</a> Zhiling Zheng, Oufan Zhang, Christian Borgs, Jennifer T. Chayes, Omar M. Yaghi </li>
     
   
     
       <li> <a href="/publications/yan2023multimodal/">Multimodal Chatgpt For Medical Applications: An Experimental Study Of GPT-4V</a> Zhiling Yan et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023heavy/">H\(_2\)O: Heavy-hitter Oracle For Efficient Generative Inference Of Large Language Models</a> Zhenyu Zhang et al. </li>
     
   
     
       <li> <a href="/publications/delatorre2023real/">LLMR: Real-time Prompting Of Interactive Worlds Using Large Language Models</a> Fernanda De La Torre et al. </li>
     
   
     
   
     
       <li> <a href="/publications/song2023preference/">Preference Ranking Optimization For Human Alignment</a> Feifan Song et al. </li>
     
   
     
       <li> <a href="/publications/gilardi2023chatgpt/">Chatgpt Outperforms Crowd-workers For Text-annotation Tasks</a> Fabrizio Gilardi, Meysam Alizadeh, Ma√´l Kubli </li>
     
   
     
       <li> <a href="/publications/huang2023is/">Is Chatgpt Better Than Human Annotators? Potential And Limitations Of Chatgpt In Explaining Implicit Hate Speech</a> Fan Huang, Haewoon Kwak, Jisun An </li>
     
   
     
       <li> <a href="/publications/hu2023llm/">Llm-adapters: An Adapter Family For Parameter-efficient Fine-tuning Of Large Language Models</a> Zhiqiang Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shayegani2023survey/">Survey Of Vulnerabilities In Large Language Models Revealed By Adversarial Attacks</a> Erfan Shayegani et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/latif2023fine/">Fine-tuning Chatgpt For Automatic Scoring</a> Ehsan Latif, Xiaoming Zhai </li>
     
   
     
       <li> <a href="/publications/garridomerch%C3%A1n2023simulating/">Simulating H.P. Lovecraft Horror Literature With The Chatgpt Large Language Model</a> Eduardo C. Garrido-merch√°n, Jos√© Luis Arroyo-barrig√ºete, Roberto Gozalo-brizuela </li>
     
   
     
       <li> <a href="/publications/frantar2023massive/">Sparsegpt: Massive Language Models Can Be Accurately Pruned In One-shot</a> Elias Frantar, Dan Alistarh </li>
     
   
     
       <li> <a href="/publications/chen2023chatgpt/">Gptutor: A Chatgpt-powered Programming Tool For Code Explanation</a> Eason Chen, Ray Huang, Han-shin Chen, Yuen-hsien Tseng, Liang-yi Li </li>
     
   
     
       <li> <a href="/publications/sur%C3%ADs2023visual/">Vipergpt: Visual Inference Via Python Execution For Reasoning</a> D√≠dac Sur√≠s, Sachit Menon, Carl Vondrick </li>
     
   
     
       <li> <a href="/publications/almazrouei2023falcon/">The Falcon Series Of Open Language Models</a> Ebtesam Almazrouei et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kamalloo2023evaluating/">Evaluating Open-domain Question Answering In The Era Of Large Language Models</a> Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, Davood Rafiei </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023empowering/">Speechgpt: Empowering Large Language Models With Intrinsic Cross-modal Conversational Abilities</a> Dong Zhang et al. </li>
     
   
     
       <li> <a href="/publications/jiang2023llm/">Llm-blender: Ensembling Large Language Models With Pairwise Ranking And Generative Fusion</a> Dongfu Jiang, Xiang Ren, Bill Yuchen Lin </li>
     
   
     
   
     
       <li> <a href="/publications/jang2023gpt/">GPT-4 Can Pass The Korean National Licensing Examination For Korean Medicine Doctors</a> Dongyeop Jang, Tae-rim Yun, Choong-yeol Lee, Young-kyu Kwon, Chang-eop Kim </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023minigpt/">Minigpt-4: Enhancing Vision-language Understanding With Advanced Large Language Models</a> Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny </li>
     
   
     
       <li> <a href="/publications/zhu2023chatgpt/">Chatgpt Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions</a> Deyao Zhu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/nunes2023evaluating/">Evaluating GPT-3.5 And GPT-4 Models On Brazilian University Admission Exams</a> Desnes Nunes, Ricardo Primi, Ramon Pires, Roberto Lotufo, Rodrigo Nogueira </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/paul2023reasoning/">REFINER: Reasoning Feedback On Intermediate Representations</a> Debjit Paul et al. </li>
     
   
     
       <li> <a href="/publications/nam2023using/">Using An LLM To Help With Code Understanding</a> Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, Brad Myers </li>
     
   
     
   
     
       <li> <a href="/publications/yang2023mm/">MM-REACT: Prompting Chatgpt For Multimodal Reasoning And Action</a> Zhengyuan Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hodel2023emergent/">Response: Emergent Analogical Reasoning In Large Language Models</a> Damian Hodel, Jevin West </li>
     
   
     
   
     
       <li> <a href="/publications/arora2023have/">Have Llms Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models</a> Daman Arora, Himanshu Gaurav Singh, Mausam </li>
     
   
     
   
     
       <li> <a href="/publications/west2023ai/">AI And The FCI: Can Chatgpt Project An Understanding Of Introductory Physics?</a> Colin G. West </li>
     
   
     
       <li> <a href="/publications/burns2023weak/">Weak-to-strong Generalization: Eliciting Strong Capabilities With Weak Supervision</a> Collin Burns et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2023llava/">Llava-med: Training A Large Language-and-vision Assistant For Biomedicine In One Day</a> Chunyuan Li et al. </li>
     
   
     
       <li> <a href="/publications/chan2023chatgpt/">Chatgpt Evaluation On Sentence Level Relations: A Focus On Temporal, Causal, And Discourse Relations</a> Chunkit Chan et al. </li>
     
   
     
       <li> <a href="/publications/zheng2023progressive/">Progressive-hint Prompting Improves Reasoning In Large Language Models</a> Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li </li>
     
   
     
   
     
       <li> <a href="/publications/xia2023conversational/">Conversational Automated Program Repair</a> Chunqiu Steven Xia, Lingming Zhang </li>
     
   
     
   
     
       <li> <a href="/publications/leiter2023meta/">Chatgpt: A Meta-analysis After 2.5 Months</a> Christoph Leiter et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2023distilled/">Distilled GPT For Source Code Summarization</a> Chia-yi Su, Collin Mcmillan </li>
     
   
     
       <li> <a href="/publications/ma2023iterative/">An Iterative Optimizing Framework For Radiology Report Summarization With Chatgpt</a> Chong Ma et al. </li>
     
   
     
       <li> <a href="/publications/zhou2023less/">LIMA: Less Is More For Alignment</a> Chunting Zhou et al. </li>
     
   
     
       <li> <a href="/publications/qin2023is/">Is Chatgpt A General-purpose Natural Language Processing Task Solver?</a> Chengwei Qin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/peng2023study/">A Study Of Generative Large Language Model For Medical Research And Healthcare</a> Cheng Peng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023visual/">Visual Chatgpt: Talking, Drawing And Editing With Visual Foundation Models</a> Chenfei Wu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/whitehouse2023llm/">Llm-powered Data Augmentation For Enhanced Cross-lingual Performance</a> Chenxi Whitehouse, Monojit Choudhury, Alham Fikri Aji </li>
     
   
     
       <li> <a href="/publications/rastogi2023supporting/">Supporting Human-ai Collaboration In Auditing Llms With Llms</a> Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, Harsha Nori, Saleema Amershi </li>
     
   
     
       <li> <a href="/publications/packer2023towards/">Memgpt: Towards Llms As Operating Systems</a> Charles Packer et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023pmc/">Pmc-llama: Towards Building Open-source Language Models For Medicine</a> Chaoyi Wu et al. </li>
     
   
     
       <li> <a href="/publications/jiang2023hallucination/">Hallucination Augmented Contrastive Learning For Multimodal Large Language Model</a> Chaoya Jiang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023one/">One Small Step For Generative AI, One Giant Leap For AGI: A Complete Survey On Chatgpt In AIGC Era</a> Chaoning Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023empowering/">Wizardlm: Empowering Large Language Models To Follow Complex Instructions</a> Can Xu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jones2023does/">Does GPT-4 Pass The Turing Test?</a> Cameron R. Jones, Benjamin K. Bergen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/soong2023improving/">Improving Accuracy Of GPT-3/4 Results On Biomedical Data Using A Retrieval-augmented Language Model</a> David Soong et al. </li>
     
   
     
       <li> <a href="/publications/lund2023chatgpt/">Chatgpt And A New Academic Reality: Artificial Intelligence-written Research Papers And The Ethics Of The Large Language Models In Scholarly Publishing</a> Brady Lund et al. </li>
     
   
     
       <li> <a href="/publications/jin2023large/">Large Language Models On Graphs: A Comprehensive Survey</a> Bowen Jin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2023seed/">Seed-bench-2: Benchmarking Multimodal Large Language Models</a> Bohao Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2023mimic/">MIMIC-IT: Multi-modal In-context Instruction Tuning</a> Bo Li et al. </li>
     
   
     
       <li> <a href="/publications/guo2023how/">How Close Is Chatgpt To Human Experts? Comparison Corpus, Evaluation, And Detection</a> Biyang Guo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chen2023prompting/">Prompting Or Fine-tuning? A Comparative Study Of Large Language Models For Taxonomy Construction</a> Boqi Chen, Fandi Yi, D√°niel Varr√≥ </li>
     
   
     
   
     
       <li> <a href="/publications/lamichhane2023evaluation/">Evaluation Of Chatgpt For Nlp-based Mental Health Applications</a> Bishal Lamichhane </li>
     
   
     
       <li> <a href="/publications/lin2023generative/">Swiftsage: A Generative Agent With Fast And Slow Thinking For Complex Interactive Tasks</a> Bill Yuchen Lin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/clavi%C3%A92023large/">Large Language Models In The Workplace: A Case Study On Prompt Engineering For Job Type Classification</a> Benjamin Clavi√©, Alexandru Ciceu, Frederick Naylor, Guillaume Souli√©, Thomas Brightwell </li>
     
   
     
   
     
       <li> <a href="/publications/xu2023instructing/">Expertprompting: Instructing Large Language Models To Be Distinguished Experts</a> Benfeng Xu et al. </li>
     
   
     
       <li> <a href="/publications/hu2023bad/">Bad Actor, Good Advisor: Exploring The Role Of Large Language Models In Fake News Detection</a> Beizhe Hu et al. </li>
     
   
     
       <li> <a href="/publications/rozi%C3%A8re2023code/">Code Llama: Open Foundation Models For Code</a> Baptiste Rozi√®re et al. </li>
     
   
     
       <li> <a href="/publications/peng2023instruction/">Instruction Tuning With GPT-4</a> Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao </li>
     
   
     
       <li> <a href="/publications/jiang2023human/">Motiongpt: Human Motion As A Foreign Language</a> Biao Jiang et al. </li>
     
   
     
       <li> <a href="/publications/cheng2023batch/">Batch Prompting: Efficient Inference With Large Language Model Apis</a> Zhoujun Cheng, Jungo Kasai, Tao Yu </li>
     
   
     
       <li> <a href="/publications/peng2023check/">Check Your Facts And Try Again: Improving Large Language Models With External Knowledge And Automated Feedback</a> Baolin Peng et al. </li>
     
   
     
   
     
       <li> <a href="/publications/toma2023clinical/">Clinical Camel: An Open Expert-level Medical Language Model With Dialogue-based Knowledge Encoding</a> Augustin Toma et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hellas2023exploring/">Exploring The Responses Of Large Language Models To Beginner Programmers' Help Requests</a> Arto Hellas et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gudibande2023false/">The False Promise Of Imitating Proprietary Llms</a> Arnav Gudibande et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kong2023better/">Better Zero-shot Reasoning With Role-play Prompting</a> Aobo Kong et al. </li>
     
   
     
   
     
       <li> <a href="/publications/pal2023med/">Med-halt: Medical Domain Hallucination Test For Large Language Models</a> Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu </li>
     
   
     
       <li> <a href="/publications/gu2023knowledge/">Minillm: Knowledge Distillation Of Large Language Models</a> Yuxian Gu, Li Dong, Furu Wei, Minlie Huang </li>
     
   
     
       <li> <a href="/publications/zou2023universal/">Universal And Transferable Adversarial Attacks On Aligned Language Models</a> Andy Zou et al. </li>
     
   
     
       <li> <a href="/publications/zhao2023llm/">Expel: LLM Agents Are Experiential Learners</a> Andrew Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/caines2023application/">On The Application Of Large Language Models For Language Teaching And Assessment Technology</a> Andrew Caines et al. </li>
     
   
     
       <li> <a href="/publications/bran2023augmenting/">Chemcrow: Augmenting Large-language Models With Chemistry Tools</a> Andres M Bran et al. </li>
     
   
     
       <li> <a href="/publications/k%C3%B6pf2023openassistant/">Openassistant Conversations -- Democratizing Large Language Model Alignment</a> Andreas K√∂pf et al. </li>
     
   
     
       <li> <a href="/publications/liesenfeld2023opening/">Opening Up Chatgpt: Tracking Openness, Transparency, And Accountability In Instruction-tuned Text Generators</a> Andreas Liesenfeld, Alianda Lopez, Mark Dingemanse </li>
     
   
     
       <li> <a href="/publications/kucharavy2023fundamentals/">Fundamentals Of Generative Large Language Models And Perspectives In Cyber-defense</a> Andrei Kucharavy et al. </li>
     
   
     
       <li> <a href="/publications/olga2023generative/">Generative AI: Implications And Applications For Education</a> Anastasia Olnancy Olga et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hendy2023how/">How Good Are GPT Models At Machine Translation? A Comprehensive Evaluation</a> Amr Hendy et al. </li>
     
   
     
   
     
       <li> <a href="/publications/azaria2023chatgpt/">Chatgpt Is A Remarkable Tool -- For Experts</a> Amos Azaria, Rina Azoulay, Shulamit Reches </li>
     
   
     
       <li> <a href="/publications/bhattacharjee2023fighting/">Fighting Fire With Fire: Can Chatgpt Detect Ai-generated Text?</a> Amrita Bhattacharjee, Huan Liu </li>
     
   
     
       <li> <a href="/publications/awadalla2023open/">Openflamingo: An Open-source Framework For Training Large Autoregressive Vision-language Models</a> Anas Awadalla et al. </li>
     
   
     
       <li> <a href="/publications/bahrini2023threats/">Chatgpt: Applications, Opportunities, And Threats</a> Aram Bahrini et al. </li>
     
   
     
   
     
       <li> <a href="/publications/deshpande2023toxicity/">Toxicity In Chatgpt: Analyzing Persona-assigned Language Models</a> Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan </li>
     
   
     
       <li> <a href="/publications/madaan2023self/">Self-refine: Iterative Refinement With Self-feedback</a> Aman Madaan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/borji2023categorical/">A Categorical Archive Of Chatgpt Failures</a> Ali Borji </li>
     
   
     
       <li> <a href="/publications/wei2023how/">Jailbroken: How Does LLM Safety Training Fail?</a> Alexander Wei, Nika Haghtalab, Jacob Steinhardt </li>
     
   
     
       <li> <a href="/publications/robey2023defending/">Smoothllm: Defending Large Language Models Against Jailbreaking Attacks</a> Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas </li>
     
   
     
       <li> <a href="/publications/wan2023poisoning/">Poisoning Language Models During Instruction Tuning</a> Alexander Wan, Eric Wallace, Sheng Shen, Dan Klein </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sison2023more/">Chatgpt: More Than A Weapon Of Mass Deception, Ethical Challenges And Responses From The Human-centered Artificial Intelligence (HCAI) Perspective</a> Alejo Jose G. Sison, Marco Tulio Daza, Roberto Gozalo-brizuela, Eduardo C. Garrido-merch√°n </li>
     
   
     
       <li> <a href="/publications/lopezlira2023can/">Can Chatgpt Forecast Stock Price Movements? Return Predictability And Large Language Models</a> Alejandro Lopez-lira, Yuehua Tang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/asai2023self/">Self-rag: Learning To Retrieve, Generate, And Critique Through Self-reflection</a> Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khademi2023can/">Can Chatgpt And Bard Generate Aligned Assessment Items? A Reliability Analysis Against Human Performance</a> Abdolvahab Khademi </li>
     
   
     
       <li> <a href="/publications/kocaballi2023conversational/">Conversational Ai-powered Design: Chatgpt As Designer, User, And Product</a> A. Baki Kocaballi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023chatgpt/">In Chatgpt We Trust? Measuring And Characterizing The Reliability Of Chatgpt</a> Xinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang </li>
     
   
     
   
     
       <li> <a href="/publications/shen2023anything/">"do Anything Now": Characterizing And Evaluating In-the-wild Jailbreak Prompts On Large Language Models</a> Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang </li>
     
   
     
   
     
       <li> <a href="/publications/mei2023chatgpt/">Wavcaps: A Chatgpt-assisted Weakly-labelled Audio Captioning Dataset For Audio-language Multimodal Research</a> Xinhao Mei et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023how/">How To Unleash The Power Of Large Language Models For Few-shot Relation Extraction?</a> Xin Xu, Yuqi Zhu, Xiaohan Wang, Ningyu Zhang </li>
     
   
     
       <li> <a href="/publications/wang2023rethinking/">Rethinking The Evaluation For Conversational Recommendation In The Era Of Large Language Models</a> Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, Ji-rong Wen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023unveiling/">Unveiling Security, Privacy, And Ethical Concerns Of Chatgpt</a> Xiaodong Wu, Ran Duan, Jianbing Ni </li>
     
   
     
   
     
       <li> <a href="/publications/qi2023fine/">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</a> Xiangyu Qi et al. </li>
     
   
     
       <li> <a href="/publications/qi2023visual/">Visual Adversarial Examples Jailbreak Aligned Large Language Models</a> Xiangyu Qi et al. </li>
     
   
     
       <li> <a href="/publications/ding2023hpc/">HPC-GPT: Integrating Large Language Model For High-performance Computing</a> Xianzhong Ding et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023trust/">Don't Trust Chatgpt When Your Question Is Not In English: A Study Of Multilingual Abilities And Types Of Llms</a> Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, Grzegorz Kondrak </li>
     
   
     
       <li> <a href="/publications/yue2023massive/">MMMU: A Massive Multi-discipline Multimodal Understanding And Reasoning Benchmark For Expert AGI</a> Xiang Yue et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fang2023bias/">Bias Of Ai-generated Content: An Examination Of News Produced By Large Language Models</a> Xiao Fang et al. </li>
     
   
     
       <li> <a href="/publications/zhan2023deceptive/">Deceptive AI Ecosystems: The Case Of Chatgpt</a> Xiao Zhan, Yifan Xu, Stefan Sarkadi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiao2023is/">Is Chatgpt A Good Translator? Yes With GPT-4 As The Engine</a> Wenxiang Jiao et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023multilevel/">M3exam: A Multilingual, Multimodal, Multilevel Benchmark For Examining Large Language Models</a> Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, Lidong Bing </li>
     
   
     
       <li> <a href="/publications/hong2023visual/">Cogagent: A Visual Language Model For GUI Agents</a> Wenyi Hong et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023multilingual/">Multilingual Machine Translation With Large Language Models: Empirical Results And Analysis</a> Wenhao Zhu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/pan2023preliminary/">A Preliminary Evaluation Of Chatgpt For Zero-shot Dialogue Understanding</a> Wenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che, Libo Qin </li>
     
   
     
   
     
       <li> <a href="/publications/liang2023can/">Can Large Language Models Provide Useful Feedback On Research Papers? A Large-scale Empirical Analysis</a> Weixin Liang et al. </li>
     
   
     
       <li> <a href="/publications/zhao2023is/">Is Chatgpt Equipped With Emotional Dialogue Capabilities?</a> Weixiang Zhao et al. </li>
     
   
     
       <li> <a href="/publications/feng2023compositional/">Layoutgpt: Compositional Visual Planning And Generation With Large Language Models</a> Weixi Feng et al. </li>
     
   
     
       <li> <a href="/publications/liang2023gpt/">GPT Detectors Are Biased Against Non-native English Writers</a> Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou </li>
     
   
     
       <li> <a href="/publications/li2023dark/">The Dark Side Of Chatgpt: Legal And Ethical Challenges From Stochastic Parrots And Hallucination</a> Zihao Li </li>
     
   
     
   
     
       <li> <a href="/publications/shi2023trusting/">Trusting Your Evidence: Hallucinate Less With Context-aware Decoding</a> Weijia Shi et al. </li>
     
   
     
       <li> <a href="/publications/shi2023retrieval/">REPLUG: Retrieval-augmented Black-box Language Models</a> Weijia Shi et al. </li>
     
   
     
       <li> <a href="/publications/sun2023is/">Is Chatgpt Good At Search? Investigating Large Language Models As Re-ranking Agents</a> Weiwei Sun et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xin2023survey/">A Survey Of Large Language Models</a> Wayne Xin Zhao et al. </li>
     
   
     
       <li> <a href="/publications/zhong2023enhancing/">Memorybank: Enhancing Large Language Models With Long-term Memory</a> Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang </li>
     
   
     
   
     
       <li> <a href="/publications/xiao2023supporting/">Supporting Qualitative Analysis With Large Language Models: Combining Codebook With GPT-3 For Deductive Coding</a> Ziang Xiao, Xingdi Yuan, Q. Vera Liao, Rania Abdelghani, Pierre-yves Oudeyer </li>
     
   
     
       <li> <a href="/publications/jeronymo2023inpars/">Inpars-v2: Large Language Models As Efficient Dataset Generators For Information Retrieval</a> Vitor Jeronymo et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dibia2023tool/">LIDA: A Tool For Automatic Generation Of Grammar-agnostic Visualizations And Infographics Using Large Language Models</a> Victor Dibia </li>
     
   
     
       <li> <a href="/publications/hackl2023is/">Is GPT-4 A Reliable Rater? Evaluating Consistency In GPT-4 Text Ratings</a> Veronika Hackl, Alexandra Elena M√ºller, Michael Granitzer, Maximilian Sailer </li>
     
   
     
       <li> <a href="/publications/lai2023chatgpt/">Chatgpt Beyond English: Towards A Comprehensive Evaluation Of Large Language Models In Multilingual Learning</a> Viet Dac Lai et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bezirhan2023automated/">Automated Reading Passage Generation With Openai's Large Language Model</a> Ummugul Bezirhan, Matthias Von Davier </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phung2023automating/">Automating Human Tutor-style Programming Feedback: Leveraging GPT-4 Tutor Model For Hint Generation And GPT-3.5 Student Model For Hint Validation</a> Tung Phung et al. </li>
     
   
     
   
     
       <li> <a href="/publications/phung2023generative/">Generative AI For Programming Education: Benchmarking Chatgpt, GPT-4, And Human Tutors</a> Tung Phung et al. </li>
     
   
     
       <li> <a href="/publications/dao2023flashattention/">Flashattention-2: Faster Attention With Better Parallelism And Work Partitioning</a> Tri Dao </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hariri2023unlocking/">Unlocking The Potential Of Chatgpt: A Comprehensive Exploration Of Its Applications, Advantages, Limitations, And Future Directions In Natural Language Processing</a> Walid Hariri </li>
     
   
     
       <li> <a href="/publications/zhou2023chinese/">Chinese Intermediate English Learners Outdid Chatgpt In Deep Cohesion: Evidence From English Narrative Writing</a> Tongquan Zhou, Siyi Cao, Siruo Zhou, Yao Zhang, Aijing He </li>
     
   
     
   
     
       <li> <a href="/publications/silver2023generalized/">Generalized Planning In PDDL Domains With Pretrained Large Language Models</a> Tom Silver et al. </li>
     
   
     
       <li> <a href="/publications/kocmi2023large/">Large Language Models Are State-of-the-art Evaluators Of Translation Quality</a> Tom Kocmi, Christian Federmann </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2023empirical/">Empirical Study Of Zero-shot NER With Chatgpt</a> Tingyu Xie et al. </li>
     
   
     
       <li> <a href="/publications/dettmers2023efficient/">Qlora: Efficient Finetuning Of Quantized Llms</a> Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer </li>
     
   
     
       <li> <a href="/publications/yu2023rlhf/">RLHF-V: Towards Trustworthy Mllms Via Behavior Alignment From Fine-grained Correctional Human Feedback</a> Tianyu Yu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/han2023medalpaca/">Medalpaca -- An Open-source Collection Of Medical Conversational AI Models And Training Data</a> Tianyu Han et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023recommender/">Recommender Systems In The Era Of Large Language Models (llms)</a> Zihuai Zhao et al. </li>
     
   
     
       <li> <a href="/publications/liang2023encouraging/">Encouraging Divergent Thinking In Large Language Models Through Multi-agent Debate</a> Tian Liang et al. </li>
     
   
     
       <li> <a href="/publications/savage2023diagnostic/">Diagnostic Reasoning Prompts Reveal The Potential For Large Language Model Interpretability In Medicine</a> Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, Jonathan H Chen </li>
     
   
     
       <li> <a href="/publications/hagendorff2023deception/">Deception Abilities Emerged In Large Language Models</a> Thilo Hagendorff </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhuo2023red/">Red Teaming Chatgpt Via Jailbreaking: Bias, Robustness, Reliability And Toxicity</a> Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gong2023multimodal/">Multimodal-gpt: A Vision And Language Model For Dialogue With Humans</a> Tao Gong et al. </li>
     
   
     
       <li> <a href="/publications/guo2023what/">What Can Large Language Models Do In Chemistry? A Comprehensive Benchmark On Eight Tasks</a> Taicheng Guo et al. </li>
     
   
     
       <li> <a href="/publications/kuzman2023beginning/">Chatgpt: Beginning Of An End Of Manual Linguistic Data Annotation? Use Case Of Automatic Genre Identification</a> Taja Kuzman, Igor Mozetiƒç, Nikola Ljube≈°iƒá </li>
     
   
     
   
     
       <li> <a href="/publications/bubeck2023sparks/">Sparks Of Artificial General Intelligence: Early Experiments With GPT-4</a> S√©bastien Bubeck et al. </li>
     
   
     
       <li> <a href="/publications/mirchandani2023large/">Large Language Models As General Pattern Machines</a> Suvir Mirchandani et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fang2023is/">Is Chatgpt A Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation</a> Tao Fang et al. </li>
     
   
     
       <li> <a href="/publications/soman2023observations/">Observations On Llms For Telecom Domain: Capabilities And Limitations</a> Sumit Soman, Ranjani H G </li>
     
   
     
       <li> <a href="/publications/gill2023transformative/">Transformative Effects Of Chatgpt On Modern Education: Emerging Era Of AI Chatbots</a> Sukhpal Singh Gill et al. </li>
     
   
     
       <li> <a href="/publications/coyne2023analyzing/">Analyzing The Performance Of GPT-3.5 And GPT-4 In Grammatical Error Correction</a> Steven Coyne, Keisuke Sakaguchi, Diana Galvan-sosa, Michael Zock, Kentaro Inui </li>
     
   
     
       <li> <a href="/publications/mukherjee2023progressive/">Orca: Progressive Learning From Complex Explanation Traces Of GPT-4</a> Subhabrata Mukherjee et al. </li>
     
   
     
       <li> <a href="/publications/wan2023is/">"kelly Is A Warm Person, Joseph Is A Role Model": Gender Biases In Llm-generated Reference Letters</a> Yixin Wan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/su2023one/">Pandagpt: One Model To Instruction-follow Them All</a> Yixuan Su et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023analyzing/">Analyzing And Mitigating Object Hallucination In Large Vision-language Models</a> Yiyang Zhou et al. </li>
     
   
     
       <li> <a href="/publications/tan2023can/">Can Chatgpt Replace Traditional KBQA Models? An In-depth Analysis Of The Question Answering Performance Of The GPT LLM Family</a> Yiming Tan et al. </li>
     
   
     
       <li> <a href="/publications/cui2023efficient/">Efficient And Effective Text Encoding For Chinese Llama And Alpaca</a> Yiming Cui, Ziqing Yang, Xin Yao </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2023can/">Can Chatgpt Reproduce Human-generated Labels? A Study Of Social Computing Tasks</a> Yiming Zhu, Peixian Zhang, Ehsan-ul Haq, Pan Hui, Gareth Tyson </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2023comprehensive/">A Comprehensive Survey Of Ai-generated Content (AIGC): A History Of Generative AI From GAN To Chatgpt</a> Yihan Cao et al. </li>
     
   
     
       <li> <a href="/publications/liu2023summary/">Summary Of Chatgpt-related Research And Perspective Towards The Future Of Large Language Models</a> Yiheng Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chia2023towards/">INSTRUCTEVAL: Towards Holistic Evaluation Of Instruction-tuned Large Language Models</a> Yew Ken Chia, Pengfei Hong, Lidong Bing, Soujanya Poria </li>
     
   
     
       <li> <a href="/publications/liu2023jailbreaking/">Jailbreaking Chatgpt Via Prompt Engineering: An Empirical Study</a> Yi Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bang2023multimodal/">A Multitask, Multilingual, Multimodal Evaluation Of Chatgpt On Reasoning, Hallucination, And Interactivity</a> Yejin Bang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2023translating/">Translating Natural Language To Planning Goals With Large-language Models</a> Yaqi Xie et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mu2023vision/">Embodiedgpt: Vision-language Pre-training Via Embodied Chain Of Thought</a> Yao Mu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fu2023specializing/">Specializing Smaller Language Models Towards Multi-step Reasoning</a> Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot </li>
     
   
     
       <li> <a href="/publications/fu2023improving/">Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback</a> Yao Fu, Hao Peng, Tushar Khot, Mirella Lapata </li>
     
   
     
       <li> <a href="/publications/lu2023open/">RTLLM: An Open-source Benchmark For Design RTL Generation With Large Language Model</a> Yao Lu, Shang Liu, Qijun Zhang, Zhiyao Xie </li>
     
   
     
       <li> <a href="/publications/zhang2023enhanced/">Llavar: Enhanced Visual Instruction Tuning For Text-rich Image Understanding</a> Yanzhe Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dubois2023simulation/">Alpacafarm: A Simulation Framework For Methods That Learn From Human Feedback</a> Yann Dubois et al. </li>
     
   
     
       <li> <a href="/publications/zhao2023enabling/">Bubogpt: Enabling Visual Grounding In Multi-modal Llms</a> Yang Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2023g/">G-eval: NLG Evaluation Using GPT-4 With Better Human Alignment</a> Yang Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2023improving/">Improving Large Language Models For Clinical Named Entity Recognition Via Prompt Engineering</a> Yan Hu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023mental/">Mental-llm: Leveraging Large Language Models For Mental Health Prediction Via Online Text Data</a> Xuhai Xu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023emotional/">Emotional Intelligence Of Large Language Models</a> Xuena Wang, Xueting Li, Zi Yin, Yue Wu, Liu Jia </li>
     
   
     
       <li> <a href="/publications/du2023manually/">Classeval: A Manually-crafted Benchmark For Evaluating Llms On Class-level Code Generation</a> Xueying Du et al. </li>
     
   
     
       <li> <a href="/publications/ma2023fine/">Fine-tuning Llama For Multi-stage Text Retrieval</a> Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, Jimmy Lin </li>
     
   
     
   
     
       <li> <a href="/publications/dao2023can/">Can Chatgpt Pass The Vietnamese National High School Graduation Examination?</a> Xuan-quy Dao, Ngoc-bich Le, Xuan-dung Phan, Bac-bien Ngo </li>
     
   
     
       <li> <a href="/publications/chen2023how/">How Robust Is GPT-3.5 To Predecessors? A Comprehensive Study On Language Understanding Tasks</a> Xuanting Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yu2023large/">Large Language Model As Attributed Training Data Generator: A Tale Of Diversity And Bias</a> Yue Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023interpretable/">Chatgraph: Interpretable Text Classification By Converting Chatgpt Knowledge To Graphs</a> Yucheng Shi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wolf2023fundamental/">Fundamental Limitations Of Alignment In Large Language Models</a> Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua </li>
     
   
     
       <li> <a href="/publications/shen2023solving/">Hugginggpt: Solving AI Tasks With Chatgpt And Its Friends In Hugging Face</a> Yongliang Shen et al. </li>
     
   
     
       <li> <a href="/publications/chen2023autoregressive/">Autotamp: Autoregressive Task And Motion Planning With Llms As Translators And Checkers</a> Yongchao Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fu2023towards/">Gpt4aigchip: Towards Next-generation AI Accelerator Design Automation Via Large Language Models</a> Yonggan Fu et al. </li>
     
   
     
       <li> <a href="/publications/cao2023assessing/">Assessing Cross-cultural Alignment Between Chatgpt And Human Societies: An Empirical Study</a> Yong Cao et al. </li>
     
   
     
       <li> <a href="/publications/luo2023open/">Biomedgpt: Open Multimodal Generative Pre-trained Transformer For Biomedicine</a> Yizhen Luo et al. </li>
     
   
     
       <li> <a href="/publications/li2023medical/">Chatdoctor: A Medical Chat Model Fine-tuned On A Large Language Model Meta-ai (llama) Using Medical Domain Knowledge</a> Yunxiang Li et al. </li>
     
   
     
       <li> <a href="/publications/ji2023exploring/">Exploring The Impact Of Instruction Data Scaling On Large Language Models: An Empirical Study On Real-world Use Cases</a> Yunjie Ji et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shao2023character/">Character-llm: A Trainable Agent For Role-playing</a> Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu </li>
     
   
     
       <li> <a href="/publications/gao2023chat/">Chat-rec: Towards Interactive And Explainable Llms-augmented Recommender System</a> Yunfan Gao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qin2023facilitating/">Toolllm: Facilitating Large Language Models To Master 16000+ Real-world Apis</a> Yujia Qin et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hou2023large/">Large Language Models Are Zero-shot Rankers For Recommender Systems</a> Yupeng Hou et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023how/">How Far Can Camels Go? Exploring The State Of Instruction Tuning On Open Resources</a> Yizhong Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dai2023uncovering/">Uncovering Chatgpt's Capabilities In Recommender Systems</a> Sunhao Dai et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ghosh2023chatgpt/">Chatgpt Perpetuates Gender Bias In Machine Translation And Ignores Non-gendered Pronouns: Findings Across Bengali And Five Other Low-resource Languages</a> Sourojit Ghosh, Aylin Caliskan </li>
     
   
     
       <li> <a href="/publications/chakraborty2023possibilities/">On The Possibilities Of Ai-generated Text Detection</a> Souradip Chakraborty et al. </li>
     
   
     
       <li> <a href="/publications/herbold2023write/">AI, Write An Essay For Me: A Large-scale Comparison Of Human-written Versus Chatgpt-generated Essays</a> Steffen Herbold, Annette Hautli-janisz, Ute Heuer, Zlata Kikteva, Alexander Trautsch </li>
     
   
     
   
     
       <li> <a href="/publications/jentzsch2023chatgpt/">Chatgpt Is Fun, But It Is Not Funny! Humor Is Still Challenging Large Language Models</a> Sophie Jentzsch, Kristian Kersting </li>
     
   
     
       <li> <a href="/publications/wadhwa2023revisiting/">Revisiting Relation Extraction In The Era Of Large Language Models</a> Somin Wadhwa, Silvio Amir, Byron C. Wallace </li>
     
   
     
       <li> <a href="/publications/bsharat2023principled/">Principled Instructions Are All You Need For Questioning Llama-1/2, GPT-3.5/4</a> Sondos Mahmoud Bsharat, Aidar Myrzakhan, Zhiqiang Shen </li>
     
   
     
       <li> <a href="/publications/yang2023enhancing/">Zhongjing: Enhancing The Chinese Medical Capabilities Of Large Language Model Through Expert Feedback And Real-world Multi-turn Dialogue</a> Songhua Yang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dao2023performance/">Performance Comparison Of Large Language Models On VNHSGE English Dataset: Openai Chatgpt, Microsoft Bing Chat, And Google Bard</a> Xuan-quy Dao </li>
     
   
     
       <li> <a href="/publications/hong2023meta/">Metagpt: Meta Programming For A Multi-agent Collaborative Framework</a> Sirui Hong et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chen2023llm/">Llm-empowered Chatbots For Psychiatrist And Patient Simulation: Application And Evaluation</a> Siyuan Chen et al. </li>
     
   
     
       <li> <a href="/publications/ott2023central/">Thoughtsource: A Central Hub For Large Language Model Reasoning Data</a> Simon Ott et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dhingra2023mind/">Mind Meets Machine: Unravelling Gpt-4's Cognitive Psychology</a> Sifatkaur Dhingra, Manmeet Singh, Vaisakh Sb, Neetiraj Malviya, Sukhpal Singh Gill </li>
     
   
     
       <li> <a href="/publications/samsi2023from/">From Words To Watts: Benchmarking The Energy Costs Of Large Language Model Inference</a> Siddharth Samsi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sudhakaran2023open/">Mariogpt: Open-ended Text2level Generation Through Large Language Models</a> Shyam Sudhakaran et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yao2023tree/">Tree Of Thoughts: Deliberate Problem Solving With Large Language Models</a> Shunyu Yao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yin2023survey/">A Survey On Multimodal Large Language Models</a> Shukang Yin et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023automl/">Automl-gpt: Automatic Machine Learning With GPT</a> Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, Mingyuan Zhou </li>
     
   
     
       <li> <a href="/publications/tian2023opportunities/">Opportunities And Challenges For Chatgpt And Large Language Models In Biomedicine And Health</a> Shubo Tian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gunasekar2023textbooks/">Textbooks Are All You Need</a> Suriya Gunasekar et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023is/">Is Chatgpt A Good Sentiment Analyzer? A Preliminary Study</a> Zengzhi Wang et al. </li>
     
   
     
       <li> <a href="/publications/chen2023meditron/">MEDITRON-70B: Scaling Medical Pretraining For Large Language Models</a> Zeming Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023radiology/">R2gengpt: Radiology Report Generation With Frozen Llms</a> Zhanyu Wang, Lingqiao Liu, Lei Wang, Luping Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023c/">C-eval: A Multi-level Multi-discipline Chinese Evaluation Suite For Foundation Models</a> Yuzhen Huang et al. </li>
     
   
     
       <li> <a href="/publications/pardos2023learning/">Learning Gain Differences Between Chatgpt And Human Tutor Generated Algebra Hints</a> Zachary A. Pardos, Shreya Bhandari </li>
     
   
     
       <li> <a href="/publications/abbasiantaeb2023let/">Let The Llms Talk: Simulating Human-to-human Conversational QA Via Zero-shot Llm-to-llm Interactions</a> Zahra Abbasiantaeb, Yifei Yuan, Evangelos Kanoulas, Mohammad Aliannejadi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2023copiloting/">Copiloting The Copilots: Fusing Large Language Models With Completion Engines For Automated Program Repair</a> Yuxiang Wei, Chunqiu Steven Xia, Lingming Zhang </li>
     
   
     
       <li> <a href="/publications/li2023guiding/">Guiding Large Language Models Via Directional Stimulus Prompting</a> Zekun Li et al. </li>
     
   
     
       <li> <a href="/publications/qin2023large/">Large Language Models Are Effective Text Rankers With Pairwise Ranking Prompting</a> Zhen Qin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/luo2023chatgpt/">Chatgpt As A Factual Inconsistency Evaluator For Text Summarization</a> Zheheng Luo, Qianqian Xie, Sophia Ananiadou </li>
     
   
     
       <li> <a href="/publications/wan2023gpt/">GPT-RE: In-context Learning For Relation Extraction Using Large Language Models</a> Zhen Wan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/tong2024eyes/">Eyes Wide Shut? Exploring The Visual Shortcomings Of Multimodal Llms</a> Shengbang Tong et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khojah2024beyond/">Beyond Code Generation: An Observational Study Of Chatgpt Usage In Software Engineering Practice</a> Ranim Khojah, Mazen Mohamad, Philipp Leitner, Francisco Gomes De Oliveira Neto </li>
     
   
     
       <li> <a href="/publications/jin2024hidden/">Hidden Flaws Behind Expert-level Accuracy Of Multimodal GPT-4 Vision In Medicine</a> Qiao Jin et al. </li>
     
   
     
       <li> <a href="/publications/kaur2024from/">From Text To Transformation: A Comprehensive Review Of Large Language Models' Versatility</a> Pravneet Kaur et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chung2024large/">Large Language Model Capabilities In Perioperative Risk Prediction And Prognostication</a> Philip Chung et al. </li>
     
   
     
       <li> <a href="/publications/qi2024multimodal/">SNIFFER: Multimodal Large Language Model For Explainable Out-of-context Misinformation Detection</a> Peng Qi, Zehong Yan, Wynne Hsu, Mong Li Lee </li>
     
   
     
       <li> <a href="/publications/bassner2024ai/">Iris: An Ai-driven Virtual Tutor For Computer Science Education</a> Patrick Bassner, Eduard Frankford, Stephan Krusche </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/haque2024exploring/">Exploring Chatgpt And Its Impact On Society</a> Md. Asraful Haque, Shuai Li </li>
     
   
     
   
     
       <li> <a href="/publications/alamin2024history/">History Of Generative Artificial Intelligence (AI) Chatbots: Past, Present, And Future Development</a> Md. Al-amin et al. </li>
     
   
     
       <li> <a href="/publications/dahl2024large/">Large Legal Fictions: Profiling Legal Hallucinations In Large Language Models</a> Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E. Ho </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kazemitabaar2024evaluating/">Codeaid: Evaluating A Classroom Deployment Of An Llm-based Programming Assistant That Balances Student And Educator Needs</a> Majeed Kazemitabaar et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/saab2024capabilities/">Capabilities Of Gemini Models In Medicine</a> Khaled Saab et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chang2024data/">Data Is All You Need: Finetuning Llms For Chip Design Via An Automated Design-data Augmentation Framework</a> Kaiyan Chang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ha2024understanding/">Clochat: Understanding How People Customize, Interact, And Experience Personas In Large Language Models</a> Juhye Ha, Hyeon Jeon, Daeun Han, Jinwook Seo, Changhoon Oh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2024revolutionizing/">Revolutionizing Finance With Llms: An Overview Of Applications And Insights</a> Huaqin Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xiong2024benchmarking/">Benchmarking Retrieval-augmented Generation For Medicine</a> Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang </li>
     
   
     
       <li> <a href="/publications/zhang2024closing/">Closing The Gap Between Open-source And Commercial Large Language Models For Medical Evidence Summarization</a> Gongbo Zhang et al. </li>
     
   
     
       <li> <a href="/publications/geminiteam2024gemini/">Gemini 1.5: Unlocking Multimodal Understanding Across Millions Of Tokens Of Context</a> Gemini Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frankford2024ai/">Ai-tutoring In Software Engineering Education</a> Eduard Frankford, Clemens Sauerwein, Patrick Bassner, Stephan Krusche, Ruth Breu </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2024chemical/">Chemllm: A Chemical Large Language Model</a> Di Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2024deepseek/">Deepseek-coder: When The Large Language Model Meets Programming -- The Rise Of Code Intelligence</a> Daya Guo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/novelli2024generative/">Generative AI In EU Law: Liability, Privacy, Intellectual Property, And Cybersecurity</a> Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato, Luciano Floridi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/anderson2024homogenization/">Homogenization Effects Of Large Language Models On Human Creative Ideation</a> Barrett R. Anderson, Jash Hemant Shah, Max Kreminski </li>
     
   
     
   
     
       <li> <a href="/publications/bewersdorff2024taking/">Taking The Next Step With Generative Artificial Intelligence: The Transformative Role Of Multimodal Large Language Models In Science Education</a> Arne Bewersdorff et al. </li>
     
   
     
       <li> <a href="/publications/khurana2024why/">Why And When Llm-based Assistants Can Go Wrong: Investigating The Effectiveness Of Prompt-based Interactions For Software Help-seeking</a> Anjali Khurana, Hari Subramonyam, Parmit K Chilana </li>
     
   
     
       <li> <a href="/publications/balaguer2024rag/">RAG Vs Fine-tuning: Pipelines, Tradeoffs, And A Case Study On Agriculture</a> Angels Balaguer et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2024financial/">Financial Statement Analysis With Large Language Models</a> Alex Kim, Maximilian Muhn, Valeri Nikolaev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2024understanding/">Understanding Llms: A Comprehensive Overview From Training To Inference</a> Yiheng Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/deldjoo2024understanding/">Understanding Biases In Chatgpt-based Recommender Systems: Provider Fairness, Temporal Stability, And Recency</a> Yashar Deldjoo </li>
     
   
     
   
     
       <li> <a href="/publications/zeng2024how/">How Johnny Can Persuade Llms To Jailbreak Them: Rethinking Persuasion To Challenge AI Safety By Humanizing Llms</a> Yi Zeng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2024monitoring/">Monitoring Ai-modified Content At Scale: A Case Study On The Impact Of Chatgpt On AI Conference Peer Reviews</a> Weixin Liang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2024assessing/">Assessing AI Detectors In Identifying Ai-generated Code: Implications For Education</a> Wei Hung Pan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2024universal/">Earthgpt: A Universal Multi-modal Large Language Model For Multi-sensor Image Comprehension In Remote Sensing Domain</a> Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, Xuerui Mao </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/glm2024family/">Chatglm: A Family Of Large Language Models From GLM-130B To GLM-4 All Tools</a> Team Glm et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lehr2024chatgpt/">Chatgpt As Research Scientist: Probing Gpt's Capabilities As A Research Librarian, Research Ethicist, Data Generator And Data Predictor</a> Steven A. Lehr, Aylin Caliskan, Suneragiri Liyanage, Mahzarin R. Banaji </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2024large/">Large Language Models For Data Annotation And Synthesis: A Survey</a> Zhen Tan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/he2024quality/">Quality Of Answers Of Generative Large Language Models Vs Peer Patients For Interpreting Lab Test Results For Lay Patients: Evaluation Study</a> Zhe He et al. </li>
     
   
     
       <li> <a href="/publications/chen2024how/">How Far Are We To GPT-4V? Closing The Gap To Commercial Multimodal Models With Open-source Suites</a> Zhe Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Has Code <a id="Has Code"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2020length/">Length-adaptive Transformer: Train Once With Length Drop, Use Anytime With Search</a> Gyuwan Kim, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2020sub/">Sub-instruction Aware Vision-and-language Navigation</a> Yicong Hong, Cristian Rodriguez-opazo, Qi Wu, Stephen Gould </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mehta2020deep/">Delight: Deep And Light-weight Transformer</a> Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kong2021bidirectional/">BLT: Bidirectional Layout Transformer For Controllable Layout Generation</a> Xiang Kong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021simple/">Hiddencut: Simple Data Augmentation For Natural Language Understanding With Better Generalization</a> Jiaao Chen, Dinghan Shen, Weizhu Chen, Diyi Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shleifer2021improved/">Normformer: Improved Transformer Pretraining With Extra Normalization</a> Sam Shleifer, Jason Weston, Myle Ott </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ In-Context Learning <a id="In-Context Learning"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2021efficient/">Glam: Efficient Scaling Of Language Models With Mixture-of-experts</a> Nan Du et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rubin2021learning/">Learning To Retrieve Prompts For In-context Learning</a> Ohad Rubin, Jonathan Herzig, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/eichenberg2021magma/">MAGMA -- Multimodal Augmentation Of Generative Models Through Adapter-based Finetuning</a> Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, Anette Frank </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/min2021learning/">Metaicl: Learning To Learn In Context</a> Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021what/">What Changes Can Large-scale Language Models Bring? Intensive Study On Hyperclova: Billions-scale Korean Generative Pretrained Transformers</a> Boseop Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021meta/">Meta-learning Via Language Model In-context Tuning</a> Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, He He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2021explanation/">An Explanation Of In-context Learning As Implicit Bayesian Inference</a> Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2022learning/">In-context Learning For Few-shot Dialogue State Tracking</a> Yushi Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khot2022decomposed/">Decomposed Prompting: A Modular Approach For Solving Complex Tasks</a> Tushar Khot et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022active/">Active Example Selection For In-context Learning</a> Yiming Zhang, Shi Feng, Chenhao Tan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2022black/">Black-box Tuning For Language-model-as-a-service</a> Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/levy2022diverse/">Diverse Demonstrations Improve In-context Compositional Generalization</a> Itay Levy, Ben Bogin, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2022teaching/">Teaching Algorithmic Reasoning Via In-context Learning</a> Hattie Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2022few/">Few-shot Parameter-efficient Fine-tuning Is Better And Cheaper Than In-context Learning</a> Haokun Liu et al. </li>
     
   
     
       <li> <a href="/publications/su2022selective/">Selective Annotation Makes Language Models Better Few-shot Learners</a> Hongjin Su et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022large/">Large Language Models Are Few(1)-shot Table Reasoners</a> Wenhu Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2022legal/">Legal Prompting: Teaching A Language Model To Think Like A Lawyer</a> Fangyi Yu, Lee Quartey, Frank Schilder </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022self/">Self-adaptive In-context Learning: An Information Compression Perspective For In-context Example Selection And Ordering</a> Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2022unreliability/">The Unreliability Of Explanations In Few-shot Prompting For Textual Reasoning</a> Xi Ye, Greg Durrett </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vilar2022prompting/">Prompting Palm For Translation: Assessing Strategies And Performance</a> David Vilar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2022why/">Why Can GPT Learn In-context? Language Models Implicitly Perform Gradient Descent As Meta-optimizers</a> Damai Dai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2022unifying/">UL2: Unifying Language Learning Paradigms</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2022proposal/">Don't Generate, Discriminate: A Proposal For Grounding Language Models To Real-world Environments</a> Yu Gu, Xiang Deng, Yu Su </li>
     
   
     
       <li> <a href="/publications/olsson2022learning/">In-context Learning And Induction Heads</a> Catherine Olsson et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/anil2022exploring/">Exploring Length Generalization In Large Language Models</a> Cem Anil et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guti%C3%A9rrez2022thinking/">Thinking About GPT-3 In-context Learning For Biomedical IE? Think Again</a> Bernal Jim√©nez Guti√©rrez et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022rationale/">Rationale-augmented Ensembles In Language Models</a> Xuezhi Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/athiwaratkun2022multi/">Multi-lingual Evaluation Of Code Generation Models</a> Ben Athiwaratkun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lazaridou2022internet/">Internet-augmented Language Models Through Few-shot Prompting For Open-domain Question Answering</a> Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/k2022can/">Can Language Models Learn From Explanations In Context?</a> Andrew K. Lampinen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/madaan2022text/">Text And Patterns: For Effective Chain Of Thought, It Takes Two To Tango</a> Aman Madaan, Amir Yazdanbakhsh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2022language/">Language Models Are General-purpose Interfaces</a> Yaru Hao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khattab2022demonstrate/">Demonstrate-search-predict: Composing Retrieval And Language Models For Knowledge-intensive NLP</a> Omar Khattab et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mees2022grounding/">Grounding Language With Visual Affordances Over Unstructured Data</a> Oier Mees, Jessica Borja-diaz, Wolfram Burgard </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ratner2022parallel/">Parallel Context Windows For Large Language Models</a> Nir Ratner et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022explanations/">Explanations From Large Language Models Make Small Reasoners Better</a> Shiyang Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/suzgun2022challenging/">Challenging Big-bench Tasks And Whether Chain-of-thought Can Solve Them</a> Mirac Suzgun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chan2022data/">Data Distributional Properties Drive Emergent In-context Learning In Transformers</a> Stephanie C. Y. Chan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yasunaga2022retrieval/">Retrieval-augmented Multimodal Language Modeling</a> Michihiro Yasunaga et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gupta2022visual/">Visual Programming: Compositional Visual Reasoning Without Training</a> Tanmay Gupta, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2022program/">PAL: Program-aided Language Models</a> Luyu Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2022large/">Large Language Models Are Human-level Prompt Engineers</a> Yongchao Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/agrawal2022examples/">In-context Examples Selection For Machine Translation</a> Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, Marjan Ghazvininejad </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shin2022effect/">On The Effect Of Pretraining Corpora On In-context Learning By A Large-scale Language Model</a> Seongjin Shin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/min2022rethinking/">Rethinking The Role Of Demonstrations: What Makes In-context Learning Work?</a> Sewon Min et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2023augmenting/">Toolkengpt: Augmenting Frozen Language Models With Massive Tools Via Tool Embeddings</a> Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu </li>
     
   
     
   
     
       <li> <a href="/publications/dai2023llm/">Llm-in-the-loop: Leveraging Large Language Model For Thematic Analysis</a> Shih-chieh Dai, Aiping Xiong, Lun-wei Ku </li>
     
   
     
   
     
       <li> <a href="/publications/moghaddam2023boosting/">Boosting Theory-of-mind Performance In Large Language Models Via Prompting</a> Shima Rahimi Moghaddam, Christopher J. Honey </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023augmenting/">Genegpt: Augmenting Large Language Models With Domain Tools For Improved Access To Biomedical Information</a> Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023speak/">Speak Foreign Languages With Your Own Voice: Cross-lingual Neural Codec Language Modeling</a> Ziqiang Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pourreza2023din/">DIN-SQL: Decomposed In-context Learning Of Text-to-sql With Self-correction</a> Mohammadreza Pourreza, Davood Rafiei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hasan2023zero/">Zero- And Few-shot Prompting With Llms: A Comparative Study With Fine-tuned Models For Bangla Sentiment Analysis</a> Md. Arid Hasan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/le2023text/">Voicebox: Text-guided Multilingual Universal Speech Generation At Scale</a> Matthew Le et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mosbach2023few/">Few-shot Fine-tuning Vs. In-context Learning: A Fair Comparison And Evaluation</a> Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, Yanai Elazar </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2023improving/">Improving CLIP Training With Language Rewrites</a> Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, Yonglong Tian </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qu2023layoutllm/">Layoutllm-t2i: Eliciting Layout Guidance From LLM For Text-to-image Generation</a> Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, Tat-seng Chua </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2023effective/">Tallrec: An Effective And Efficient Tuning Framework To Align Large Language Model With Recommendation</a> Keqin Bao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023is/">Is Chatgpt A Good Recommender? A Preliminary Study</a> Junling Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/koh2023grounding/">Grounding Language Models To Images For Multimodal Inputs And Outputs</a> Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023empowering/">Empowering Molecule Discovery For Molecule-caption Translation With Large Language Models: A Chatgpt Perspective</a> Jiatong Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023compositional/">Compositional Exemplars For In-context Learning</a> Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/he2023icl/">ICL-D3IE: In-context Learning With Diverse Demonstrations Updating For Document Information Extraction</a> Jiabang He et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023pre/">VILA: On Pre-training For Visual Language Models</a> Ji Lin et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2023larger/">Larger Language Models Do In-context Learning Differently</a> Jerry Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2023symbol/">Symbol Tuning Improves In-context Learning In Language Models</a> Jerry Wei et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jiang2023compressing/">Llmlingua: Compressing Prompts For Accelerated Inference Of Large Language Models</a> Huiqiang Jiang, Qianhui Wu, Chin-yew Lin, Yuqing Yang, Lili Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023audioldm/">Audioldm 2: Learning Holistic Audio Generation With Self-supervised Pretraining</a> Haohe Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lehman2023do/">Do We Still Need Clinical Language Models?</a> Eric Lehman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meyerson2023language/">Language Model Crossover: Variation Through Few-shot Prompting</a> Elliot Meyerson et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023principle/">Principle-driven Self-alignment Of Language Models From Scratch With Minimal Human Supervision</a> Zhiqing Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chan2023chatgpt/">Chatgpt Evaluation On Sentence Level Relations: A Focus On Temporal, Causal, And Discourse Relations</a> Chunkit Chan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023iterative/">An Iterative Optimizing Framework For Radiology Report Summarization With Chatgpt</a> Chong Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023generative/">Generative Speech Recognition Error Correction With Large Language Models And Task-activating Prompting</a> Chao-han Huck Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023mimic/">MIMIC-IT: Multi-modal In-context Instruction Tuning</a> Bo Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/paranjape2023automatic/">ART: Automatic Multi-step Reasoning And Tool-use For Large Language Models</a> Bhargavi Paranjape et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023instructing/">Expertprompting: Instructing Large Language Models To Be Distinguished Experts</a> Benfeng Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023kosmos/">Kosmos-2: Grounding Multimodal Large Language Models To The World</a> Zhiliang Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023how/">How To Unleash The Power Of Large Language Models For Few-shot Relation Extraction?</a> Xin Xu, Yuqi Zhu, Xiaohan Wang, Ningyu Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023few/">Few-shot In-context Learning For Knowledge Base Question Answering</a> Tianle Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2023what/">What Can Large Language Models Do In Chemistry? A Comprehensive Benchmark On Eight Tasks</a> Taicheng Guo et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mirchandani2023large/">Large Language Models As General Pattern Machines</a> Suvir Mirchandani et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fang2023is/">Is Chatgpt A Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation</a> Tao Fang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023making/">Making Large Language Models Perform Better In Knowledge Graph Completion</a> Yichi Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moslem2023adaptive/">Adaptive Machine Translation With Large Language Models</a> Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023improving/">Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback</a> Yao Fu, Hao Peng, Tushar Khot, Mirella Lapata </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023textbooks/">Textbooks Are All You Need II: Phi-1.5 Technical Report</a> Yuanzhi Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023towards/">Gpt4aigchip: Towards Next-generation AI Accelerator Design Automation Via Large Language Models</a> Yonggan Fu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wadhwa2023revisiting/">Revisiting Relation Extraction In The Era Of Large Language Models</a> Somin Wadhwa, Silvio Amir, Byron C. Wallace </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wan2023gpt/">GPT-RE: In-context Learning For Relation Extraction Using Large Language Models</a> Zhen Wan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/saab2024capabilities/">Capabilities Of Gemini Models In Medicine</a> Khaled Saab et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deldjoo2024understanding/">Understanding Biases In Chatgpt-based Recommender Systems: Provider Fairness, Temporal Stability, And Recency</a> Yashar Deldjoo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Interpretability and Explainability <a id="Interpretability and Explainability"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alvarezmelis2017causal/">A Causal Framework For Explaining The Predictions Of Black-box Sequence-to-sequence Models</a> David Alvarez-melis, Tommi S. Jaakkola </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/raghu2018disentangling/">Disentangling Language And Knowledge In Task-oriented Dialogs</a> Dinesh Raghu, Nikhil Gupta, Mausam </li>
     
   
     
       <li> <a href="/publications/park2018multimodal/">Multimodal Explanations: Justifying Decisions And Pointing To The Evidence</a> Dong Huk Park et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2018multi/">Multi-cast Attention Networks For Retrieval-based Question Answering And Response Prediction</a> Yi Tay, Luu Anh Tuan, Siu Cheung Hui </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2018explicit/">Explicit State Tracking With Semi-supervision For Neural Dialogue Generation</a> Xisen Jin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2018faithful/">Faithful Multimodal Explanation For Visual Question Answering</a> Jialin Wu, Raymond J. Mooney </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cornia2019training/">Smart: Training Shallow Memory-aware Transformers For Robotic Explainability</a> Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019learning/">Learning From Explanations With Neural Execution Tree</a> Ziqi Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2019answering/">Answering Complex Open-domain Questions Through Iterative Query Generation</a> Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, Christopher D. Manning </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vashishth2019attention/">Attention Interpretability Across NLP Tasks</a> Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, Manaal Faruqui </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jain2019attention/">Attention Is Not Explanation</a> Sarthak Jain, Byron C. Wallace </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rajani2019explain/">Explain Yourself! Leveraging Language Models For Commonsense Reasoning</a> Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ehsan2019automated/">Automated Rationale Generation: A Technique For Explainable AI And Its Effects On Human Perceptions</a> Upol Ehsan, Pradyumna Tambwekar, Larry Chan, Brent Harrison, Mark Riedl </li>
     
   
     
       <li> <a href="/publications/bhagavatula2019abductive/">Abductive Commonsense Reasoning</a> Chandra Bhagavatula et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pruthi2019learning/">Learning To Deceive With Attention-based Explanations</a> Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, Zachary C. Lipton </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kovaleva2019revealing/">Revealing The Dark Secrets Of BERT</a> Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wallace2019allennlp/">Allennlp Interpret: A Framework For Explaining Predictions Of NLP Models</a> Eric Wallace et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019detecting/">Recosa: Detecting The Relevant Contexts With Self-attention For Multi-turn Dialogue Generation</a> Hainan Zhang, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng </li>
     
   
     
       <li> <a href="/publications/xie2019visual/">Visual Entailment: A Novel Task For Fine-grained Image Understanding</a> Ning Xie, Farley Lai, Derek Doran, Asim Kadav </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/narang2020training/">WT5?! Training Text-to-text Models To Explain Their Predictions</a> Sharan Narang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ho2020constructing/">Constructing A Multi-hop QA Dataset For Comprehensive Evaluation Of Reasoning Steps</a> Xanh Ho, Anh-khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020learning/">Learning Modality Interaction For Temporal Sentence Localization And Event Captioning In Videos</a> Shaoxiang Chen, Wenhao Jiang, Wei Liu, Yu-gang Jiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parcalabescu2020seeing/">Seeing Past Words: Testing The Cross-modal Capabilities Of Pretrained V&L Models On Counting Tasks</a> Letitia Parcalabescu, Albert Gatt, Anette Frank, Iacer Calixto </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wiegreffe2020measuring/">Measuring Association Between Labels And Free-text Rationales</a> Sarah Wiegreffe, Ana Marasoviƒá, Noah A. Smith </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vanaken2020hidden/">Visbert: Hidden-state Visualizations For Transformers</a> Betty Van Aken, Benjamin Winter, Alexander L√∂ser, Felix A. Gers </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2020efficient/">SPARTA: Efficient Open-domain Question Answering Via Sparse Transformer Matching Retrieval</a> Tiancheng Zhao, Xiaopeng Lu, Kyusong Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2020segment/">SEAL: Segment-wise Extractive-abstractive Long-form Text Summarization</a> Yao Zhao, Mohammad Saleh, Peter J. Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clark2020transformers/">Transformers As Soft Reasoners Over Language</a> Peter Clark, Oyvind Tafjord, Kyle Richardson </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/marasovi%C4%872020natural/">Natural Language Rationales With Full-stack Visual Reasoning: From Pixels To Semantic Frames To Commonsense Graphs</a> Ana Marasoviƒá et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jain2020learning/">Learning To Faithfully Rationalize By Construction</a> Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, Byron C. Wallace </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mosbach2020stability/">On The Stability Of Fine-tuning BERT: Misconceptions, Explanations, And Strong Baselines</a> Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ross2020explaining/">Explaining NLP Models Via Minimal Contrastive Editing (mice)</a> Alexis Ross, Ana Marasoviƒá, Matthew E. Peters </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mohankumar2020towards/">Towards Transparent And Explainable Attention Models</a> Akash Kumar Mohankumar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/do2020e/">E-snli-ve: Corrected Visual-textual Entailment With Natural Language Explanations</a> Virginie Do, Oana-maria Camburu, Zeynep Akata, Thomas Lukasiewicz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bastings2020elephant/">The Elephant In The Interpretability Room: Why Use Attention As Explanation When We Have Saliency Methods?</a> Jasmijn Bastings, Katja Filippova </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tenney2020language/">The Language Interpretability Tool: Extensible, Interactive Visualizations And Analysis For NLP Models</a> Ian Tenney et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021generate/">Generate Natural Language Explanations For Recommendation</a> Hanxiong Chen, Xu Chen, Shaoyun Shi, Yongfeng Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chrysostomou2021improving/">Improving The Faithfulness Of Attention-based Explanations With Task-specific Information For Text Classification</a> George Chrysostomou, Nikolaos Aletras </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/flemotomos2021automated/">Automated Quality Assessment Of Cognitive Behavioral Therapy Sessions Through Highly Contextualized Language Representations</a> Nikolaos Flemotomos et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rahimi2021explaining/">Explaining Documents' Relevance To Search Queries</a> Razieh Rahimi, Youngwoo Kim, Hamed Zamani, James Allan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021explaining/">On Explaining Your Explanations Of BERT: An Empirical Study With Sequence Classification</a> Zhengxuan Wu, Desmond C. Ong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021good/">Good For Misconceived Reasons: An Empirical Revisiting On The Need For Visual Context In Multimodal Machine Translation</a> Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mao2021dynamic/">DYLE: Dynamic Latent Extraction For Abstractive Long-input Summarization</a> Ziming Mao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chefer2021generic/">Generic Attention-model Explainability For Interpreting Bi-modal And Encoder-decoder Transformers</a> Hila Chefer, Shir Gur, Lior Wolf </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2021hallucination/">On Hallucination And Predictive Uncertainty In Conditional Language Generation</a> Yijun Xiao, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2021towards/">Towards Interpreting And Mitigating Shortcut Learning Behavior Of NLU Models</a> Mengnan Du et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khare2021multimodal/">MMBERT: Multimodal BERT Pretraining For Improved Medical VQA</a> Yash Khare et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wiegreffe2021reframing/">Reframing Human-ai Collaboration For Generating Free-text Explanations</a> Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gui2021knowledge/">KAT: A Knowledge Augmented Transformer For Vision-and-language</a> Liangke Gui et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021personalized/">Personalized Transformer For Explainable Recommendation</a> Lei Li, Yongfeng Zhang, Li Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kayser2021e/">E-vil: A Dataset And Benchmark For Natural Language Explanations In Vision-language Tasks</a> Maxime Kayser et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2022investigating/">Investigating Explainability Of Generative AI For Code Through Scenario-based Design</a> Jiao Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pereira2022multi/">Visconde: Multi-document QA With GPT-3 And Neural Reranking</a> Jayr Pereira, Robson Fidalgo, Roberto Lotufo, Rodrigo Nogueira </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jung2022maieutic/">Maieutic Prompting: Logically Consistent Reasoning With Recursive Explanations</a> Jaehun Jung et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022paradox/">On The Paradox Of Learning To Reason From Data</a> Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-wei Chang, Guy Van Den Broeck </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2022rethinking/">Rethinking With Retrieval: Faithful Large Language Model Inference</a> Hangfeng He, Hongming Zhang, Dan Roth </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sammani2022nlx/">NLX-GPT: A Model For Natural Language Explanations In Vision And Vision-language Tasks</a> Fawaz Sammani, Tanmoy Mukherjee, Nikos Deligiannis </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2022legal/">Legal Prompting: Teaching A Language Model To Think Like A Lawyer</a> Fangyi Yu, Lee Quartey, Frank Schilder </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/aflalo2022vl/">Vl-interpret: An Interactive Visualization Tool For Interpreting Vision-language Transformers</a> Estelle Aflalo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2022unreliability/">The Unreliability Of Explanations In Few-shot Prompting For Textual Reasoning</a> Xi Ye, Greg Durrett </li>
     
   
     
   
     
       <li> <a href="/publications/gao2022multi/">MIST: Multi-modal Iterative Spatial-temporal Transformer For Long-form Video Question Answering</a> Difei Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hernandez2022scaling/">Scaling Laws And Interpretability Of Learning From Repeated Data</a> Danny Hernandez et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2022binding/">Binding Language Models In Symbolic Languages</a> Zhoujun Cheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bhavya2022analogy/">Analogy Generation By Prompting Large Language Models: A Case Study Of Instructgpt</a> Bhavya Bhavya, Jinjun Xiong, Chengxiang Zhai </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022rationale/">Rationale-augmented Ensembles In Language Models</a> Xuezhi Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022end/">End-to-end Multimodal Fact-checking And Explanation Generation: A Challenging Dataset And Models</a> Barry Menglong Virginia Tech Yao, Aditya Virginia Tech Shah, Lichao Lehigh University Sun, Jin-hee Virginia Tech Cho, Lifu Virginia Tech Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/k2022can/">Can Language Models Learn From Explanations In Context?</a> Andrew K. Lampinen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wan2022what/">What Do They Capture? -- A Structural Analysis Of Pre-trained Language Models For Source Code</a> Yao Wan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/diao2022black/">Black-box Prompt Learning For Pre-trained Language Models</a> Shizhe Diao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022learn/">Learn To Explain: Multimodal Reasoning Via Thought Chains For Science Question Answering</a> Pan Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/golovneva2022suite/">ROSCOE: A Suite Of Metrics For Scoring Step-by-step Reasoning</a> Olga Golovneva et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022explanations/">Explanations From Large Language Models Make Small Reasoners Better</a> Shiyang Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022generating/">Emphi: Generating Empathetic Responses With Human-like Intents</a> Mao Yan Chen, Siheng Li, Yujiu Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022personalized/">Personalized Prompt Learning For Explainable Recommendation</a> Lei Li, Yongfeng Zhang, Li Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2022language/">Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey</a> Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos, Yulia Tsvetkov </li>
     
   
     
       <li> <a href="/publications/sarsa2022automatic/">Automatic Generation Of Programming Exercises And Code Explanations Using Large Language Models</a> Sami Sarsa, Paul Denny, Arto Hellas, Juho Leinonen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022synergizing/">React: Synergizing Reasoning And Acting In Language Models</a> Shunyu Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023retrieving/">Retrieving Multimodal Information For Augmented Generation: A Survey</a> Ruochen Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/schaeffer2023are/">Are Emergent Abilities Of Large Language Models A Mirage?</a> Rylan Schaeffer, Brando Miranda, Sanmi Koyejo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mandi2023dialectic/">Roco: Dialectic Multi-robot Collaboration With Large Language Models</a> Zhao Mandi, Shreeya Jain, Shuran Song </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mitrovi%C4%872023chatgpt/">Chatgpt Or Human? Detect And Explain. Explaining Decisions Of Machine Learning Model For Detecting Short Chatgpt-generated Text</a> Sandra Mitroviƒá, Davide Andreoletti, Omran Ayoub </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023scaling/">Scaling Vision-language Models With Sparse Mixture Of Experts</a> Sheng Shen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lyu2023faithful/">Faithful Chain-of-thought Reasoning</a> Qing Lyu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liao2023ai/">AI Transparency In The Age Of Llms: A Human-centered Research Roadmap</a> Q. Vera Liao, Jennifer Wortman Vaughan </li>
     
   
     
   
     
       <li> <a href="/publications/liao2023designerly/">Designerly Understanding: Information Needs For Model Transparency To Support Design Ideation For Ai-powered User Experience</a> Q. Vera Liao, Hariharan Subramonyam, Jennifer Wang, Jennifer Wortman Vaughan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023driving/">Driving With Llms: Fusing Object-level Vector Modality For Explainable Autonomous Driving</a> Long Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023is/">Is Chatgpt A Good Recommender? A Preliminary Study</a> Junling Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xue2023bias/">Bias And Fairness In Chatbots: An Overview</a> Jintang Xue et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/savelka2023thrilled/">Thrilled By Your Progress! Large Language Models (GPT-4) No Longer Struggle To Pass Assessments In Higher Education Programming Courses</a> Jaromir Savelka, Arav Agarwal, Marshall An, Chris Bogart, Majd Sakr </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/l%C3%A1la2023retrieval/">Paperqa: Retrieval-augmented Generative Agent For Scientific Research</a> Jakub L√°la et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/savelka2023large/">Large Language Models (GPT) Struggle To Answer Multiple-choice Questions About Code</a> Jaromir Savelka, Arav Agarwal, Christopher Bogart, Majd Sakr </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nori2023capabilities/">Capabilities Of GPT-4 On Medical Challenge Problems</a> Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023explainability/">Explainability For Large Language Models: A Survey</a> Haiyan Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2023applying/">Applying Large Language Models And Chain-of-thought For Automatic Scoring</a> Gyeong-geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu, Xiaoming Zhai </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kotek2023gender/">Gender Bias And Stereotypes In Large Language Models</a> Hadas Kotek, Rikker Dockum, David Q. Sun </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mialon2023augmented/">Augmented Language Models: A Survey</a> Gr√©goire Mialon et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nicholas2023lost/">Lost In Translation: Large Language Models In Non-english Content Analysis</a> Gabriel Nicholas, Aliya Bhatia </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023is/">Is Chatgpt Better Than Human Annotators? Potential And Limitations Of Chatgpt In Explaining Implicit Hate Speech</a> Fan Huang, Haewoon Kwak, Jisun An </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023chatgpt/">Gptutor: A Chatgpt-powered Programming Tool For Code Explanation</a> Eason Chen, Ray Huang, Han-shin Chen, Yuen-hsien Tseng, Liang-yi Li </li>
     
   
     
       <li> <a href="/publications/sur%C3%ADs2023visual/">Vipergpt: Visual Inference Via Python Execution For Reasoning</a> D√≠dac Sur√≠s, Sachit Menon, Carl Vondrick </li>
     
   
     
   
     
       <li> <a href="/publications/lenat2023getting/">Getting From Generative AI To Trustworthy AI: What Llms Might Learn From Cyc</a> Doug Lenat, Gary Marcus </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mollo2023vector/">The Vector Grounding Problem</a> Dimitri Coelho Mollo, Rapha√´l Milli√®re </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nunes2023evaluating/">Evaluating GPT-3.5 And GPT-4 Models On Brazilian University Admission Exams</a> Desnes Nunes, Ricardo Primi, Ramon Pires, Roberto Lotufo, Rodrigo Nogueira </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ziems2023can/">Can Large Language Models Transform Computational Social Science?</a> Caleb Ziems et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023action/">ADAPT: Action-aware Driving Caption Transformer</a> Bu Jin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sha2023large/">Languagempc: Large Language Models As Decision Makers For Autonomous Driving</a> Hao Sha et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lopezlira2023can/">Can Chatgpt Forecast Stock Price Movements? Return Predictability And Large Language Models</a> Alejandro Lopez-lira, Yuehua Tang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023teaching/">Teaching Large Language Models To Self-debug</a> Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, Denny Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023rethinking/">Rethinking The Evaluation For Conversational Recommendation In The Era Of Large Language Models</a> Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, Ji-rong Wen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahmed2023better/">Better Patching Using LLM Prompting, Via Self-consistency</a> Toufique Ahmed, Premkumar Devanbu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/savage2023diagnostic/">Diagnostic Reasoning Prompts Reveal The Potential For Large Language Model Interpretability In Medicine</a> Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, Jonathan H Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023large/">Large Language Model Alignment: A Survey</a> Tianhao Shen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mukherjee2023progressive/">Orca: Progressive Learning From Complex Explanation Traces Of GPT-4</a> Subhabrata Mukherjee et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023trustworthy/">Trustworthy Llms: A Survey And Guideline For Evaluating Large Language Models' Alignment</a> Yang Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023interpretable/">Chatgraph: Interpretable Text Classification By Converting Chatgpt Knowledge To Graphs</a> Yucheng Shi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023chat/">Chat-rec: Towards Interactive And Explainable Llms-augmented Recommender System</a> Yunfan Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jentzsch2023chatgpt/">Chatgpt Is Fun, But It Is Not Funny! Humor Is Still Challenging Large Language Models</a> Sophie Jentzsch, Kristian Kersting </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tong2024eyes/">Eyes Wide Shut? Exploring The Visual Shortcomings Of Multimodal Llms</a> Shengbang Tong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chung2024large/">Large Language Model Capabilities In Perioperative Risk Prediction And Prognostication</a> Philip Chung et al. </li>
     
   
     
       <li> <a href="/publications/qi2024multimodal/">SNIFFER: Multimodal Large Language Model For Explainable Out-of-context Misinformation Detection</a> Peng Qi, Zehong Yan, Wynne Hsu, Mong Li Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ramos2024review/">A Review Of Large Language Models And Autonomous Agents In Chemistry</a> Mayk Caldas Ramos, Christopher J. Collison, Andrew D. White </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kazemitabaar2024evaluating/">Codeaid: Evaluating A Classroom Deployment Of An Llm-based Programming Assistant That Balances Student And Educator Needs</a> Majeed Kazemitabaar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singh2024rethinking/">Rethinking Interpretability In The Era Of Large Language Models</a> Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, Jianfeng Gao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Language Modeling <a id="Language Modeling"></a></h3>
   <ul>
   
     
   
     
   
     
       <li> <a href="/publications/bhoopchand2016learning/">Learning Python Code Suggestion With A Sparse Pointer Network</a> Avishkar Bhoopchand, Tim Rockt√§schel, Earl Barr, Sebastian Riedel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lebret2016neural/">Neural Text Generation From Structured Data With Application To The Biography Domain</a> Remi Lebret, David Grangier, Michael Auli </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bahdanau2016actor/">An Actor-critic Algorithm For Sequence Prediction</a> Dzmitry Bahdanau et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wiseman2016sequence/">Sequence-to-sequence Learning As Beam-search Optimization</a> Sam Wiseman, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2016chinese/">Chinese Song Iambics Generation With Neural Attention-based Model</a> Qixin Wang, Tianyi Luo, Dong Wang, Chao Xing </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2017table/">Table-to-text Generation By Structure-aware Seq2seq Learning</a> Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, Zhifang Sui </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2017long/">Long Text Generation Via Adversarial Training With Leaked Information</a> Jiaxian Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahmed2017weighted/">Weighted Transformer Network For Machine Translation</a> Karim Ahmed, Nitish Shirish Keskar, Richard Socher </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/daniluk2017frustratingly/">Frustratingly Short Attention Spans In Neural Language Modeling</a> Micha≈Ç Daniluk, Tim Rockt√§schel, Johannes Welbl, Sebastian Riedel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vaswani2017attention/">Attention Is All You Need</a> Ashish Vaswani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2017non/">Non-autoregressive Neural Machine Translation</a> Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, Richard Socher </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2017neural/">Neural Text Generation: A Practical Guide</a> Ziang Xie </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2018dp/">DP-GAN: Diversity-promoting Generative Adversarial Network For Generating Informative And Diversified Text</a> Jingjing Xu, Xuancheng Ren, Junyang Lin, Xu Sun </li>
     
   
     
       <li> <a href="/publications/fedus2018better/">Maskgan: Better Text Generation Via Filling In The______</a> William Fedus, Ian Goodfellow, Andrew M. Dai </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2018can/">Can You Tell Me How To Get Past Sesame Street? Sentence-level Pretraining Beyond Language Modeling</a> Alex Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2018language/">Language Modeling Teaches You More Syntax Than Translation Does: Lessons Learned Through Auxiliary Task Analysis</a> Kelly W. Zhang, Samuel R. Bowman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gehrmann2018end/">End-to-end Content And Plan Selection For Data-to-text Generation</a> Sebastian Gehrmann, Falcon Z. Dai, Henry Elder, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nie2018operations/">Operations Guided Neural Networks For High Fidelity Data-to-text Generation</a> Feng Nie, Jinpeng Wang, Jin-ge Yao, Rong Pan, Chin-yew Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wiseman2018learning/">Learning Neural Templates For Text Generation</a> Sam Wiseman, Stuart M. Shieber, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tevet2018evaluating/">Evaluating Text Gans As Language Models</a> Guy Tevet, Gavriel Habib, Vered Shwartz, Jonathan Berant </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/phang2018sentence/">Sentence Encoders On Stilts: Supplementary Training On Intermediate Labeled-data Tasks</a> Jason Phang, Thibault F√©vry, Samuel R. Bowman </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2018toward/">Toward Diverse Text Generation With Inverse Reinforcement Learning</a> Zhan Shi, Xinchi Chen, Xipeng Qiu, Xuanjing Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/winata2018learn/">Learn To Code-switch: Data Augmentation Using Copy Mechanism On Language Modeling</a> Genta Indra Winata, Andrea Madotto, Chien-sheng Wu, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alrfou2018character/">Character-level Language Modeling With Deeper Self-attention</a> Rami Al-rfou, Dokook Choe, Noah Constant, Mandy Guo, Llion Jones </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/strobelt2018visual/">Seq2seq-vis: A Visual Debugging Tool For Sequence-to-sequence Models</a> Hendrik Strobelt et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/merity2018analysis/">An Analysis Of Neural Language Modeling At Multiple Scales</a> Stephen Merity, Nitish Shirish Keskar, Richard Socher </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dathathri2019plug/">Plug And Play Language Models: A Simple Approach To Controlled Text Generation</a> Sumanth Dathathri et al. </li>
     
   
     
       <li> <a href="/publications/guo2019conditional/">Conditional Text Generation For Harmonious Human-machine Interaction</a> Bin Guo et al. </li>
     
   
     
       <li> <a href="/publications/lin2019constrained/">Commongen: A Constrained Text Generation Challenge For Generative Commonsense Reasoning</a> Bill Yuchen Lin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2019evaluating/">Evaluating Commonsense In Pre-trained Language Models</a> Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang </li>
     
   
     
   
     
       <li> <a href="/publications/shoeybi2019megatron/">Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism</a> Mohammad Shoeybi et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2019text/">Moverscore: Text Generation Evaluating With Contextualized Embeddings And Earth Mover Distance</a> Wei Zhao et al. </li>
     
   
     
       <li> <a href="/publications/budzianowski2019gpt/">Hello, It's GPT-2 -- How Can I Help You? Towards The Use Of Pretrained Language Models For Task-oriented Dialogue Systems</a> Pawe≈Ç Budzianowski, Ivan Vuliƒá </li>
     
   
     
   
     
       <li> <a href="/publications/ferreira2019neural/">Neural Data-to-text Generation: A Comparison Between Pipeline And End-to-end Architectures</a> Thiago Castro Ferreira, Chris Van Der Lee, Emiel Van Miltenburg, Emiel Krahmer </li>
     
   
     
   
     
       <li> <a href="/publications/qin2019counterfactual/">Counterfactual Story Reasoning And Generation</a> Lianhui Qin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019pretrained/">Pretrained Language Models For Document-level Neural Machine Translation</a> Liangyou Li, Xin Jiang, Qun Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sukhbaatar2019adaptive/">Adaptive Attention Span In Transformers</a> Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/witteveen2019paraphrasing/">Paraphrasing With Large Language Models</a> Sam Witteveen, Martin Andrews </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/neelakantan2019neural/">Neural Assistant: Joint Action Prediction, Response Generation, And Latent Knowledge Reasoning</a> Arvind Neelakantan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/cohan2019pretrained/">Pretrained Language Models For Sequential Sentence Classification</a> Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, Daniel S. Weld </li>
     
   
     
       <li> <a href="/publications/holtzman2019curious/">The Curious Case Of Neural Text Degeneration</a> Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sukhbaatar2019augmenting/">Augmenting Self-attention With Persistent Memory</a> Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, Armand Joulin </li>
     
   
     
       <li> <a href="/publications/dai2019transformer/">Transformer-xl: Attentive Language Models Beyond A Fixed-length Context</a> Zihang Dai et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sanh2019distilled/">Distilbert, A Distilled Version Of BERT: Smaller, Faster, Cheaper And Lighter</a> Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2019text/">Text Infilling</a> Wanrong Zhu, Zhiting Hu, Eric Xing </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2019long/">ELI5: Long Form Question Answering</a> Angela Fan et al. </li>
     
   
     
       <li> <a href="/publications/yavuz2019grounded/">Deepcopy: Grounded Response Generation With Hierarchical Pointer Networks</a> Semih Yavuz, Abhinav Rastogi, Guan-lin Chao, Dilek Hakkani-tur </li>
     
   
     
   
     
       <li> <a href="/publications/fan2019reducing/">Reducing Transformer Depth On Demand With Structured Dropout</a> Angela Fan, Edouard Grave, Armand Joulin </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019evaluating/">Bertscore: Evaluating Text Generation With BERT</a> Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, Yoav Artzi </li>
     
   
     
   
     
       <li> <a href="/publications/tenney2019what/">What Do You Learn From Context? Probing For Sentence Structure In Contextualized Word Representations</a> Ian Tenney et al. </li>
     
   
     
       <li> <a href="/publications/kumar2019cross/">Cross-lingual Training For Automatic Question Generation</a> Vishwajeet Kumar, Nitish Joshi, Arijit Mukherjee, Ganesh Ramakrishnan, Preethi Jyothi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moryossef2019step/">Step-by-step: Separating Planning From Realization In Neural Data-to-text Generation</a> Amit Moryossef, Yoav Goldberg, Ido Dagan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019say/">Don't Say That! Making Inconsistent Dialogue Unlikely With Unlikelihood Training</a> Margaret Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rae2019compressive/">Compressive Transformers For Long-range Sequence Modelling</a> Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shao2019long/">Long And Diverse Text Generation With Planning-based Hierarchical Variational Model</a> Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2019levenshtein/">Levenshtein Transformer</a> Jiatao Gu, Changhan Wang, Jake Zhao </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hua2019sentence/">Sentence-level Content Planning And Style Specification For Neural Text Generation</a> Xinyu Hua, Lu Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019distilling/">Distilling Knowledge Learned In BERT For Text Generation</a> Yen-chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/r%C3%B6nnqvist2019is/">Is Multilingual BERT Fluent In Language Generation?</a> Samuel R√∂nnqvist, Jenna Kanerva, Tapio Salakoski, Filip Ginter </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/schmidt2019generalization/">Generalization In Generation: A Closer Look At Exposure Bias</a> Florian Schmidt </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2019pay/">Pay Less Attention With Lightweight And Dynamic Convolutions</a> Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, Michael Auli </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019structured/">Structured Pruning Of Large Language Models</a> Ziheng Wang, Jeremy Wohlwend, Tao Lei </li>
     
   
     
   
     
       <li> <a href="/publications/tan2019learning/">LXMERT: Learning Cross-modality Encoder Representations From Transformers</a> Hao Tan, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2019empathetic/">Caire: An Empathetic Neural Chatbot</a> Zhaojiang Lin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ott2019extensible/">Fairseq: A Fast, Extensible Toolkit For Sequence Modeling</a> Myle Ott et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019few/">Few-shot NLG With Pre-trained Language Model</a> Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tian2019sticking/">Sticking To The Facts: Confident Decoding For Faithful Data-to-text Generation</a> Ran Tian, Shashi Narayan, Thibault Sellam, Ankur P. Parikh </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gehrmann2019statistical/">GLTR: Statistical Detection And Visualization Of Generated Text</a> Sebastian Gehrmann, Hendrik Strobelt, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dong2019unified/">Unified Language Model Pre-training For Natural Language Understanding And Generation</a> Li Dong et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2019bp/">Bp-transformer: Modelling Long-range Context Via Binary Partitioning</a> Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, Zheng Zhang </li>
     
   
     
   
     
       <li> <a href="/publications/gavrilov2019self/">Self-attentive Model For Headline Generation</a> Daniil Gavrilov, Pavel Kalaidin, Valentin Malykh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2019language/">LAMOL: Language Modeling For Lifelong Language Learning</a> Fan-keng Sun, Cheng-hao Ho, Hung-yi Lee </li>
     
   
     
       <li> <a href="/publications/lewis2019denoising/">BART: Denoising Sequence-to-sequence Pre-training For Natural Language Generation, Translation, And Comprehension</a> Mike Lewis et al. </li>
     
   
     
       <li> <a href="/publications/petroni2019language/">Language Models As Knowledge Bases?</a> Fabio Petroni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/edunov2019pre/">Pre-trained Language Model Representations For Language Generation</a> Sergey Edunov, Alexei Baevski, Michael Auli </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/irie2019language/">Language Modeling With Deep Transformers</a> Kazuki Irie, Albert Zeyer, Ralf Schl√ºter, Hermann Ney </li>
     
   
     
       <li> <a href="/publications/stern2019insertion/">Insertion Transformer: Flexible Sequence Generation Via Insertion Operations</a> Mitchell Stern, William Chan, Jamie Kiros, Jakob Uszkoreit </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019bert/">BERT Has A Mouth, And It Must Speak: BERT As A Markov Random Field Language Model</a> Alex Wang, Kyunghyun Cho </li>
     
   
     
   
     
       <li> <a href="/publications/wang2019tree/">Tree Transformer: Integrating Tree Structures Into Self-attention</a> Yau-shian Wang, Hung-yi Lee, Yun-nung Chen </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/welleck2019non/">Non-monotonic Sequential Text Generation</a> Sean Welleck, Kiant√© Brantley, Hal Iii Daum√©, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nogueira2019passage/">Passage Re-ranking With BERT</a> Rodrigo Nogueira, Kyunghyun Cho </li>
     
   
     
       <li> <a href="/publications/see2019do/">Do Massively Pretrained Language Models Make Better Storytellers?</a> Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, Christopher D. Manning </li>
     
   
     
       <li> <a href="/publications/see2019what/">What Makes A Good Conversation? How Controllable Attributes Affect Human Judgments</a> Abigail See, Stephen Roller, Douwe Kiela, Jason Weston </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2019non/">Non-autoregressive Transformer By Position Learning</a> Yu Bao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/logan2019wife/">Barack's Wife Hillary: Using Knowledge-graphs For Fact-aware Language Modeling</a> Robert L. Iv Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, Sameer Singh </li>
     
   
     
       <li> <a href="/publications/merity2019single/">Single Headed Attention RNN: Stop Thinking With Your Head</a> Stephen Merity </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019unicoder/">Unicoder-vl: A Universal Encoder For Vision And Language By Cross-modal Pre-training</a> Gen Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/puduppully2019data/">Data-to-text Generation With Entity Modeling</a> Ratish Puduppully, Li Dong, Mirella Lapata </li>
     
   
     
   
     
       <li> <a href="/publications/zhao2019explicit/">Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection</a> Guangxiang Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/cai2019graph/">Graph Transformer For Graph-to-sequence Learning</a> Deng Cai, Wai Lam </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/santhanam2019survey/">A Survey Of Natural Language Generation Techniques With A Focus On Dialogue Systems - Past, Present And Future Directions</a> Sashank Santhanam, Samira Shaikh </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiong2019pretrained/">Pretrained Encyclopedia: Weakly Supervised Knowledge-pretrained Language Model</a> Wenhan Xiong, Jingfei Du, William Yang Wang, Veselin Stoyanov </li>
     
   
     
       <li> <a href="/publications/kim2019probing/">Probing What Different NLP Tasks Teach Machines About Function Word Comprehension</a> Najoung Kim et al. </li>
     
   
     
       <li> <a href="/publications/zhu2019modeling/">Modeling Graph Structure In Transformer For Better Amr-to-text Generation</a> Jie Zhu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/voita2019bottom/">The Bottom-up Evolution Of Representations In The Transformer: A Study With Machine Translation And Language Modeling Objectives</a> Elena Voita, Rico Sennrich, Ivan Titov </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2019patent/">Patent Claim Generation By Fine-tuning Openai GPT-2</a> Jieh-sheng Lee, Jieh Hsiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2019tensorized/">A Tensorized Transformer For Language Modeling</a> Xindian Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mansimov2019generalized/">A Generalized Framework Of Sequence Generation With Application To Undirected Sequence Models</a> Elman Mansimov, Alex Wang, Sean Welleck, Kyunghyun Cho </li>
     
   
     
       <li> <a href="/publications/kassner2019negated/">Negated And Misprimed Probes For Pretrained Language Models: Birds Can Talk, But Cannot Fly</a> Nora Kassner, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/press2019improving/">Improving Transformer Models By Reordering Their Sublayers</a> Ofir Press, Noah A. Smith, Omer Levy </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nangia2019human/">Human Vs. Muppet: A Conservative Estimate Of Human Performance On The GLUE Benchmark</a> Nikita Nangia, Samuel R. Bowman </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ziegler2019encoder/">Encoder-agnostic Adaptation For Conditional Language Generation</a> Zachary M. Ziegler, Luke Melas-kyriazi, Sebastian Gehrmann, Alexander M. Rush </li>
     
   
     
   
     
       <li> <a href="/publications/wallace2019universal/">Universal Adversarial Triggers For Attacking And Analyzing NLP</a> Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh </li>
     
   
     
       <li> <a href="/publications/wallace2019allennlp/">Allennlp Interpret: A Framework For Explaining Predictions Of NLP Models</a> Eric Wallace et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/keskar2019conditional/">CTRL: A Conditional Transformer Language Model For Controllable Generation</a> Nitish Shirish Keskar, Bryan Mccann, Lav R. Varshney, Caiming Xiong, Richard Socher </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khashabi2020crossing/">Unifiedqa: Crossing Format Boundaries With A Single QA System</a> Daniel Khashabi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/guu2020retrieval/">REALM: Retrieval-augmented Language Model Pre-training</a> Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-wei Chang </li>
     
   
     
   
     
       <li> <a href="/publications/adiwardana2020towards/">Towards A Human-like Open-domain Chatbot</a> Daniel Adiwardana et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rosset2020knowledge/">Knowledge-aware Language Model Pretraining</a> Corby Rosset et al. </li>
     
   
     
       <li> <a href="/publications/xia2020cg/">CG-BERT: Conditional Text Generation With BERT For Generalized Few-shot Intent Detection</a> Congying Xia, Chenwei Zhang, Hoang Nguyen, Jiawei Zhang, Philip Yu </li>
     
   
     
   
     
       <li> <a href="/publications/meister2020if/">If Beam Search Is The Answer, What Was The Question?</a> Clara Meister, Tim Vieira, Ryan Cotterell </li>
     
   
     
   
     
       <li> <a href="/publications/li2020organizing/">Optimus: Organizing Sentences Via Pre-trained Modeling Of A Latent Space</a> Chunyuan Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2020assessing/">Assessing Phrasal Representation And Composition In Transformers</a> Lang Yu, Allyson Ettinger </li>
     
   
     
       <li> <a href="/publications/gao2020meaningful/">Meaningful Answer Generation Of E-commerce Question-answering</a> Shen Gao, Xiuying Chen, Zhaochun Ren, Dongyan Zhao, Rui Yan </li>
     
   
     
   
     
       <li> <a href="/publications/gehman2020evaluating/">Realtoxicityprompts: Evaluating Neural Toxic Degeneration In Language Models</a> Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, Noah A. Smith </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/donahue2020enabling/">Enabling Language Models To Fill In The Blanks</a> Chris Donahue, Mina Lee, Percy Liang </li>
     
   
     
       <li> <a href="/publications/saharia2020non/">Non-autoregressive Machine Translation With Latent Alignments</a> Chitwan Saharia, William Chan, Saurabh Saxena, Mohammad Norouzi </li>
     
   
     
       <li> <a href="/publications/wu2020tod/">TOD-BERT: Pre-trained Natural Language Understanding For Task-oriented Dialogue</a> Chien-sheng Wu, Steven Hoi, Richard Socher, Caiming Xiong </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020multilingual/">Multilingual Speech Translation With Efficient Finetuning Of Pretrained Models</a> Xian Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kale2020text/">Text-to-text Pre-training For Data-to-text Tasks</a> Mihir Kale, Abhinav Rastogi </li>
     
   
     
       <li> <a href="/publications/vu2020exploring/">Exploring And Predicting Transferability Across NLP Tasks</a> Tu Vu et al. </li>
     
   
     
       <li> <a href="/publications/nichols2020collaborative/">Collaborative Storytelling With Large-scale Neural Language Models</a> Eric Nichols, Leo Gao, Randy Gomez </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maynez2020faithfulness/">On Faithfulness And Factuality In Abstractive Summarization</a> Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald </li>
     
   
     
       <li> <a href="/publications/sinha2020unnatural/">Unnatural Language Inference</a> Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, Adina Williams </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hosseiniasl2020simple/">A Simple Language Model For Task-oriented Dialogue</a> Ehsan Hosseini-asl, Bryan Mccann, Chien-sheng Wu, Semih Yavuz, Richard Socher </li>
     
   
     
       <li> <a href="/publications/shazeer2020talking/">Talking-heads Attention</a> Noam Shazeer, Zhenzhong Lan, Youlong Cheng, Nan Ding, Le Hou </li>
     
   
     
       <li> <a href="/publications/xiao2020ernie/">ERNIE-GEN: An Enhanced Multi-flow Pre-training And Fine-tuning Framework For Natural Language Generation</a> Dongling Xiao et al. </li>
     
   
     
       <li> <a href="/publications/tay2020rethinking/">Synthesizer: Rethinking Self-attention In Transformer Models</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020kg/">KG-BART: Knowledge Graph-augmented BART For Generative Commonsense Reasoning</a> Ye Liu, Yao Wan, Lifang He, Hao Peng, Philip S. Yu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2020cross/">Imagebert: Cross-modal Pre-training With Large-scale Weak-supervised Image-text Data</a> Di Qi et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clark2020pre/">ELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators</a> Kevin Clark, Minh-thang Luong, Quoc V. Le, Christopher D. Manning </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2020text/">TAP: Text-aware Pre-training For Text-vqa And Text-caption</a> Zhengyuan Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pang2020text/">Text Generation By Learning From Demonstrations</a> Richard Yuanzhe Pang, He He </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2020variational/">Variational Template Machine For Data-to-text Generation</a> Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, Lei Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/antoun2020pre/">Aragpt2: Pre-trained Transformer For Arabic Language Generation</a> Wissam Antoun, Fady Baly, Hazem Hajj </li>
     
   
     
   
     
       <li> <a href="/publications/lewis2020pre/">Pre-training Via Paraphrasing</a> Mike Lewis et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ding2020ernie/">Ernie-doc: A Retrospective Long-document Modeling Transformer</a> Siyu Ding et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2020survey/">A Survey Of Knowledge-enhanced Text Generation</a> Wenhao Yu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2020pseudo/">Unilmv2: Pseudo-masked Language Models For Unified Language Model Pre-training</a> Hangbo Bao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/harkous2020have/">Have Your Text And Use It Too! End-to-end Neural Data-to-text Generation With Semantic Fidelity</a> Hamza Harkous, Isabel Groves, Amir Saffari </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rashkin2020outline/">Plotmachines: Outline-conditioned Generation With Dynamic Plot State Tracking</a> Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, Jianfeng Gao </li>
     
   
     
       <li> <a href="/publications/chen2020knowledge/">KGPT: Knowledge-grounded Pre-training For Data-to-text Generation</a> Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2020residual/">Residual Energy-based Models For Text Generation</a> Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, Marc'aurelio Ranzato </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020pretrain/">To Pretrain Or Not To Pretrain: Examining The Benefits Of Pretraining On Resource Rich Tasks</a> Sinong Wang, Madian Khabsa, Hao Ma </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020dataset/">The Pile: An 800GB Dataset Of Diverse Text For Language Modeling</a> Leo Gao et al. </li>
     
   
     
       <li> <a href="/publications/wiegreffe2020measuring/">Measuring Association Between Labels And Free-text Rationales</a> Sarah Wiegreffe, Ana Marasoviƒá, Noah A. Smith </li>
     
   
     
   
     
       <li> <a href="/publications/rabe2020mathematical/">Mathematical Reasoning Via Self-supervised Skip-tree Training</a> Markus N. Rabe, Dennis Lee, Kshitij Bansal, Christian Szegedy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ribeiro2020investigating/">Investigating Pretrained Language Models For Graph-to-text Generation</a> Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Sch√ºtze, Iryna Gurevych </li>
     
   
     
   
     
       <li> <a href="/publications/rust2020how/">How Good Is Your Tokenizer? On The Monolingual Performance Of Multilingual Language Models</a> Phillip Rust, Jonas Pfeiffer, Ivan Vuliƒá, Sebastian Ruder, Iryna Gurevych </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sellam2020learning/">BLEURT: Learning Robust Metrics For Text Generation</a> Thibault Sellam, Dipanjan Das, Ankur P. Parikh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2020megatron/">MEGATRON-CNTRL: Controllable Story Generation With External Knowledge Using Large-scale Language Models</a> Peng Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dhoffschmidt2020french/">Fquad: French Question Answering Dataset</a> Martin D'hoffschmidt, Wacim Belblidia, Tom Brendl√©, Quentin Heinrich, Maxime Vidal </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ezencan2020comparison/">A Comparison Of LSTM And BERT For Small Corpus</a> Aysu Ezen-can </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020constrained/">POINTER: Constrained Progressive Text Generation Via Insertion-based Generative Pre-training</a> Yizhe Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2020multi/">Layoutlmv2: Multi-modal Pre-training For Visually-rich Document Understanding</a> Yang Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2020plato/">PLATO-2: Towards Building An Open-domain Chatbot Via Curriculum Learning</a> Siqi Bao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2020code/">Code Prediction By Feeding Trees To Transformers</a> Seohyun Kim, Jinman Zhao, Yuchi Tian, Satish Chandra </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lauscher2020from/">From Zero To Hero: On The Limitations Of Zero-shot Cross-lingual Transfer With Multilingual Transformers</a> Anne Lauscher, Vinit Ravishankar, Ivan Vuliƒá, Goran Glava≈° </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/parikh2020controlled/">Totto: A Controlled Table-to-text Generation Dataset</a> Ankur P. Parikh et al. </li>
     
   
     
       <li> <a href="/publications/gupta2020global/">GMAT: Global Memory Augmentation For Transformers</a> Ankit Gupta, Jonathan Berant </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2020addressing/">Addressing Some Limitations Of Transformers With Feedback Memory</a> Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2020contrastive/">Contrastive Learning With Adversarial Perturbations For Conditional Text Generation</a> Seanie Lee, Dong Bok Lee, Sung Ju Hwang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mao2020generation/">Generation-augmented Retrieval For Open-domain Question Answering</a> Yuning Mao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020empirical/">An Empirical Investigation Of Pre-trained Transformer Language Models For Open-domain Dialogue Generation</a> Piji Li </li>
     
   
     
       <li> <a href="/publications/mehta2020deep/">Delight: Deep And Light-weight Transformer</a> Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chan2020self/">Cocon: A Self-supervised Approach For Controlled Text Generation</a> Alvin Chan, Yew-soon Ong, Bill Pung, Aston Zhang, Jie Fu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fabbri2020template/">Template-based Question Generation From Retrieved Sentences For Improved Unsupervised Question Answering</a> Alexander R. Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, Bing Xiang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2020cross/">XGPT: Cross-modal Generative Pre-training For Image Captioning</a> Qiaolin Xia et al. </li>
     
   
     
       <li> <a href="/publications/mager2020gpt/">Gpt-too: A Language-model-first Approach For Amr-to-text Generation</a> Manuel Mager et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wu2020lite/">Lite Transformer With Long-short Range Attention</a> Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, Song Han </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2020are/">Are All Languages Created Equal In Multilingual BERT?</a> Shijie Wu, Mark Dredze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nogueira2020document/">Document Ranking With A Pretrained Sequence-to-sequence Model</a> Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vuli%C4%872020probing/">Probing Pretrained Language Models For Lexical Semantics</a> Ivan Vuliƒá, Edoardo Maria Ponti, Robert Litschko, Goran Glava≈°, Anna Korhonen </li>
     
   
     
   
     
       <li> <a href="/publications/feng2020data/">Genaug: Data Augmentation For Finetuning Text Generators</a> Steven Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, Eduard Hovy </li>
     
   
     
   
     
       <li> <a href="/publications/guo2020incorporating/">Incorporating BERT Into Parallel Sequence Decoding With Adapters</a> Junliang Guo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/beltagy2020long/">Longformer: The Long-document Transformer</a> Iz Beltagy, Matthew E. Peters, Arman Cohan </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020accelerating/">Accelerating Training Of Transformer-based Language Models With Progressive Layer Dropping</a> Minjia Zhang, Yuxiong He </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tenney2020language/">The Language Interpretability Tool: Extensible, Interactive Visualizations And Analysis For NLP Models</a> Ian Tenney et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/filippova2020controlled/">Controlled Hallucinations: Learning To Generate Faithfully From Noisy Data</a> Katja Filippova </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/schick2020few/">Few-shot Text Generation With Pattern-exploiting Training</a> Timo Schick, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ji2020language/">Language Generation With Multi-hop Reasoning On Commonsense Knowledge Graph</a> Haozhe Ji et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rei2020neural/">COMET: A Neural Framework For MT Evaluation</a> Ricardo Rei, Craig Stewart, Ana C Farinha, Alon Lavie </li>
     
   
     
       <li> <a href="/publications/bostrom2020byte/">Byte Pair Encoding Is Suboptimal For Language Model Pretraining</a> Kaj Bostrom, Greg Durrett </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kale2020template/">Template Guided Text Generation For Task-oriented Dialogue</a> Mihir Kale, Abhinav Rastogi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2020knowledge/">Knowledge-driven Data Construction For Zero-shot Evaluation In Commonsense Question Answering</a> Kaixin Ma et al. </li>
     
   
     
       <li> <a href="/publications/latcinnik2020explaining/">Explaining Question Answering Models Through Text Generation</a> Veronica Latcinnik, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020robust/">Robust Conversational AI With Grounded Text Generation</a> Jianfeng Gao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/goyal2021larger/">Larger-scale Transformers For Multilingual Masked Language Modeling</a> Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau </li>
     
   
     
       <li> <a href="/publications/yuan2021evaluating/">Bartscore: Evaluating Generated Text As Text Generation</a> Weizhe Yuan, Graham Neubig, Pengfei Liu </li>
     
   
     
   
     
       <li> <a href="/publications/kirk2021bias/">Bias Out-of-the-box: An Empirical Analysis Of Intersectional Occupational Biases In Popular Generative Language Models</a> Hannah Kirk et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2021ernie/">Ernie-vilg: Unified Generative Pre-training For Bidirectional Vision-language Generation</a> Han Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/betz2021thinking/">Thinking Aloud: Dynamic Context Generation Improves Zero-shot Reasoning Performance Of GPT-2</a> Gregor Betz, Kyle Richardson, Christian Voigt </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2021good/">A Good Prompt Is Worth Millions Of Parameters: Low-resource Prompt-based Learning For Vision-language Models</a> Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, Xiang Ren </li>
     
   
     
       <li> <a href="/publications/zhang2021tip/">Tip-adapter: Training-free Clip-adapter For Better Vision-language Modeling</a> Renrui Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2021data/">MELM: Data Augmentation With Masked Entity Language Modeling For Low-resource NER</a> Ran Zhou et al. </li>
     
   
     
       <li> <a href="/publications/ding2021open/">Openprompt: An Open-source Framework For Prompt-learning</a> Ning Ding et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2021prompt/">Prompt-learning For Fine-grained Entity Typing</a> Ning Ding et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kharitonov2021text/">Text-free Prosody-aware Generative Spoken Language Modeling</a> Eugene Kharitonov et al. </li>
     
   
     
       <li> <a href="/publications/chen2021knowledge/">Knowprompt: Knowledge-aware Prompt-tuning With Synergistic Optimization For Relation Extraction</a> Xiang Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nagoudi2021text/">Arat5: Text-to-text Transformers For Arabic Language Generation</a> El Moatez Billah Nagoudi, Abdelrahim Elmadany, Muhammad Abdul-mageed </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pillutla2021measuring/">MAUVE: Measuring The Gap Between Neural Text And Human Text Using Divergence Frontiers</a> Krishna Pillutla et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/so2021searching/">Primer: Searching For Efficient Transformers For Language Modeling</a> David R. So et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yogatama2021adaptive/">Adaptive Semiparametric Language Models</a> Dani Yogatama, Cyprien De Masson D'autume, Lingpeng Kong </li>
     
   
     
       <li> <a href="/publications/pascual2021plug/">A Plug-and-play Method For Controlled Text Generation</a> Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell, Roger Wattenhofer </li>
     
   
     
       <li> <a href="/publications/sileo2021zero/">Zero-shot Recommendation As Language Modeling</a> Damien Sileo, Wout Vossen, Robbe Raymaekers </li>
     
   
     
   
     
       <li> <a href="/publications/wang2021can/">Can Generative Pre-trained Language Models Serve As Knowledge Bases For Closed-book QA?</a> Cunxiang Wang, Pai Liu, Yue Zhang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/eichenberg2021magma/">MAGMA -- Multimodal Augmentation Of Generative Models Through Adapter-based Finetuning</a> Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, Anette Frank </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/artetxe2021efficient/">Efficient Large Scale Language Modeling With Mixtures Of Experts</a> Mikel Artetxe et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021unified/">UFO: A Unified Transformer For Vision-language Representation Learning</a> Jianfeng Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guan2021long/">Long Text Generation By Modeling Sentence-level And Discourse-level Coherence</a> Jian Guan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021simple/">Simvlm: Simple Visual Language Model Pretraining With Weak Supervision</a> Zirui Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2021universal/">UC2: Universal Cross-lingual Cross-modal Vision-and-language Pre-training</a> Mingyang Zhou et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cho2021unifying/">Unifying Vision-and-language Tasks Via Text Generation</a> Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal </li>
     
   
     
       <li> <a href="/publications/tang2021clip/">Clip4caption: CLIP For Video Caption</a> Mingkang Tang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2021unifying/">Unitab: Unifying Text And Box Outputs For Grounded Vision-language Modeling</a> Zhengyuan Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahmad2021unified/">Unified Pre-training For Program Understanding And Generation</a> Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-wei Chang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021prefix/">Prefix-tuning: Optimizing Continuous Prompts For Generation</a> Xiang Lisa Li, Percy Liang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2021pangu/">Pangu-\(Œ±\): Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation</a> Wei Zeng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021math/">Math Word Problem Generation With Mathematical Consistency And Problem Context Constraints</a> Zichao Wang, Andrew S. Lan, Richard G. Baraniuk </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zou2021controllable/">Controllable Generation From Pre-trained Language Models Via Inverse Prompting</a> Xu Zou et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xiao2021hallucination/">On Hallucination And Predictive Uncertainty In Conditional Language Generation</a> Yijun Xiao, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021few/">Few-shot Knowledge Graph-to-text Generation With Pretrained Language Models</a> Junyi Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021pretrained/">Pretrained Language Models For Text Generation: A Survey</a> Junyi Li, Tianyi Tang, Wayne Xin Zhao, Ji-rong Wen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021token/">A Token-level Reference-free Hallucination Detection Benchmark For Free-form Text Generation</a> Tianyu Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yu2021vision/">Vision Guided Generative Pre-trained Language Models For Multimodal Abstractive Summarization</a> Tiezheng Yu, Wenliang Dai, Zihan Liu, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2021plan/">Plan-then-generate: Controlled Data-to-text Generation Via Planning</a> Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, Nigel Collier </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mishra2021cross/">Cross-task Generalization Via Natural Language Crowdsourcing Instructions</a> Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi </li>
     
   
     
       <li> <a href="/publications/castellon2021codified/">Codified Audio Language Modeling Learns Useful Representations For Music Information Retrieval</a> Rodrigo Castellon, Chris Donahue, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meister2021language/">Language Model Evaluation Beyond Perplexity</a> Clara Meister, Ryan Cotterell </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021parallel/">Parallel Refinements For Lexically Constrained Text Generation With BART</a> Xingwei He </li>
     
   
     
       <li> <a href="/publications/lee2021dialogue/">Dialogue State Tracking With A Language Model Using Schema-driven Prompting</a> Chia-hsuan Lee, Hao Cheng, Mari Ostendorf </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2021pre/">Lawformer: A Pre-trained Language Model For Chinese Legal Long Documents</a> Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, Maosong Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021self/">Self-supervised Dialogue Learning For Spoken Conversational Question Answering</a> Nuo Chen, Chenyu You, Yuexian Zou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/min2021recent/">Recent Advances In Natural Language Processing Via Large Pre-trained Language Models: A Survey</a> Bonan Min et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mccoy2021how/">How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty In Text Generation Using RAVEN</a> R. Thomas Mccoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021linear/">Luna: Linear Unified Nested Attention</a> Xuezhe Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/prabhumoye2021focused/">Focused Attention Improves Document-grounded Generation</a> Shrimai Prabhumoye, Kazuma Hashimoto, Yingbo Zhou, Alan W Black, Ruslan Salakhutdinov </li>
     
   
     
   
     
       <li> <a href="/publications/caciularu2021cross/">CDLM: Cross-document Language Modeling</a> Avi Caciularu et al. </li>
     
   
     
       <li> <a href="/publications/he2021improving/">Debertav3: Improving Deberta Using Electra-style Pre-training With Gradient-disentangled Embedding Sharing</a> Pengcheng He, Jianfeng Gao, Weizhu Chen </li>
     
   
     
       <li> <a href="/publications/paranjape2021posterior/">Hindsight: Posterior-guided Training Of Retrievers For Improved Open-ended Generation</a> Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, Christopher D. Manning </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guti%C3%A9rrezfandi%C3%B1o2021spanish/">Maria: Spanish Language Models</a> Asier Guti√©rrez-fandi√±o et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lauscher2021sustainable/">Sustainable Modular Debiasing Of Language Models</a> Anne Lauscher, Tobias L√ºken, Goran Glava≈° </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cahyawijaya2021benchmark/">Indonlg: Benchmark And Resources For Evaluating Indonesian Natural Language Generation</a> Samuel Cahyawijaya et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2021ernie/">ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training For Language Understanding And Generation</a> Shuohuan Wang et al. </li>
     
   
     
       <li> <a href="/publications/liu2021decoding/">Dexperts: Decoding-time Controlled Text Generation With Experts And Anti-experts</a> Alisa Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021encoder/">Deltalm: Encoder-decoder Pre-training For Language Generation And Translation By Augmenting Pretrained Multilingual Encoders</a> Shuming Ma et al. </li>
     
   
     
       <li> <a href="/publications/khare2021multimodal/">MMBERT: Multimodal BERT Pretraining For Improved Medical VQA</a> Yash Khare et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/uchendu2021benchmark/">TURINGBENCH: A Benchmark Environment For Turing Test In The Age Of Neural Text Generation</a> Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, Dongwon Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lei2021when/">When Attention Meets Fast Recurrence: Training Language Models With Reduced Compute</a> Tao Lei </li>
     
   
     
   
     
       <li> <a href="/publications/phan2021text/">Scifive: A Text-to-text Transformer Model For Biomedical Literature</a> Long N. Phan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2021effective/">An Effective Non-autoregressive Model For Spoken Language Understanding</a> Lizhi Cheng, Weijia Jia, Wenmian Yang </li>
     
   
     
   
     
       <li> <a href="/publications/huang2021unifying/">Unifying Multimodal Transformer For Bi-directional Image And Text Generation</a> Yupan Huang, Hongwei Xue, Bei Liu, Yutong Lu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shleifer2021improved/">Normformer: Improved Transformer Pretraining With Extra Normalization</a> Sam Shleifer, Jason Weston, Myle Ott </li>
     
   
     
   
     
       <li> <a href="/publications/ribeiro2021structural/">Structural Adapters In Pretrained Language Models For Amr-to-text Generation</a> Leonardo F. R. Ribeiro, Yue Zhang, Iryna Gurevych </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2021general/">GLM: General Language Model Pretraining With Autoregressive Blank Infilling</a> Zhengxiao Du et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/topal2021exploring/">Exploring Transformers In Natural Language Generation: GPT, BERT, And Xlnet</a> M. Onat Topal, Anil Bas, Imke Van Heerden </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kayser2021e/">E-vil: A Dataset And Benchmark For Natural Language Explanations In Vision-language Tasks</a> Maxime Kayser et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yao2022position/">PEVL: Position-enhanced Pre-training And Prompt Tuning For Vision-language Models</a> Yuan Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2022screenshot/">Pix2struct: Screenshot Parsing As Pretraining For Visual Language Understanding</a> Kenton Lee et al. </li>
     
   
     
       <li> <a href="/publications/sun2022length/">A Length-extrapolatable Transformer</a> Yutao Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/misra2022enabling/">Minicons: Enabling Flexible Behavioral And Representational Analyses Of Transformer Language Models</a> Kanishka Misra </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022what/">What Language Model Architecture And Pretraining Objective Work Best For Zero-shot Generalization?</a> Thomas Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2022contrastive/">Contrastive Search Is What You Need For Neural Text Generation</a> Yixuan Su, Nigel Collier </li>
     
   
     
       <li> <a href="/publications/wang2022foundation/">Omnivl:one Foundation Model For Image-language And Video-language Tasks</a> Junke Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/su2022language/">Language Models Can See: Plugging Visual Controls In Text Generation</a> Yixuan Su et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geiping2022training/">Cramming: Training A Language Model On A Single GPU In One Day</a> Jonas Geiping, Tom Goldstein </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022pretraining/">Coditt5: Pretraining For Source Code And Natural Language Editing</a> Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, Milos Gligoric </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2022generative/">Generative Spoken Dialogue Language Modeling</a> Tu Anh Nguyen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022generative/">GIT: A Generative Image-to-text Transformer For Vision And Language</a> Jianfeng Wang et al. </li>
     
   
     
       <li> <a href="/publications/borisov2022language/">Language Models Are Realistic Tabular Data Generators</a> Vadim Borisov, Kathrin Se√üler, Tobias Leemann, Martin Pawelczyk, Gjergji Kasneci </li>
     
   
     
   
     
       <li> <a href="/publications/ye2022efficient/">Zerogen: Efficient Zero-shot Learning Via Dataset Generation</a> Jiacheng Ye et al. </li>
     
   
     
       <li> <a href="/publications/yu2022scaling/">Scaling Autoregressive Models For Content-rich Text-to-image Generation</a> Jiahui Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2022emergent/">Emergent Abilities Of Large Language Models</a> Jason Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022prompt/">Prompt Tuning For Discriminative Pre-trained Language Models</a> Yuan Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alabdulmohsin2022revisiting/">Revisiting Neural Scaling Laws In Language And Vision</a> Ibrahim Alabdulmohsin, Behnam Neyshabur, Xiaohua Zhai </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022survey/">A Survey On Retrieval-augmented Text Generation</a> Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2022one/">One Embedder, Any Task: Instruction-finetuned Text Embeddings</a> Hongjin Su et al. </li>
     
   
     
       <li> <a href="/publications/lu2022collaborative/">COTS: Collaborative Two-stream Vision-language Pre-training Model For Cross-modal Retrieval</a> Haoyu Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hua2022transformer/">Transformer Quality In Linear Time</a> Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc V. Le </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2022survey/">A Survey Of Controllable Text Generation Using Transformer-based Pre-trained Language Models</a> Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song </li>
     
   
     
   
     
       <li> <a href="/publications/bao2022vl/">Vl-beit: Generative Vision-language Pretraining</a> Hangbo Bao, Wenhui Wang, Li Dong, Furu Wei </li>
     
   
     
       <li> <a href="/publications/meng2022generating/">Generating Training Data With Language Models: Towards Zero-shot Language Understanding</a> Yu Meng, Jiaxin Huang, Yu Zhang, Jiawei Han </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dang2022beyond/">Beyond Text Generation: Supporting Writers With Continuous Automatic Text Summaries</a> Hai Dang, Karim Benharrak, Florian Lehmann, Daniel Buschek </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2022evaluating/">Evaluating And Inducing Personality In Pre-trained Language Models</a> Guangyuan Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022diffusion/">Diffusion-lm Improves Controllable Text Generation</a> Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, Tatsunori B. Hashimoto </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/christopoulou2022pangu/">Pangu-coder: Program Synthesis With Function-level Language Modeling</a> Fenia Christopoulou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nijkamp2022open/">Codegen: An Open Large Language Model For Code With Multi-turn Program Synthesis</a> Erik Nijkamp et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022contrastive/">Contrastive Decoding: Open-ended Text Generation As Optimization</a> Xiang Lisa Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/he2022space/">SPACE-3: Unified Dialog Model Pre-training For Task-oriented Dialog Understanding And Generation</a> Wanwei He et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sachan2022improving/">Improving Passage Retrieval With Zero-shot Question Generation</a> Devendra Singh Sachan et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hutchins2022block/">Block-recurrent Transformers</a> Delesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, Behnam Neyshabur </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shah2022lm/">Lm-nav: Robotic Navigation With Large Pre-trained Models Of Language, Vision, And Action</a> Dhruv Shah, Blazej Osinski, Brian Ichter, Sergey Levine </li>
     
   
     
       <li> <a href="/publications/xu2022systematic/">A Systematic Evaluation Of Large Language Models Of Code</a> Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2022hungry/">Hungry Hungry Hippos: Towards Language Modeling With State Space Models</a> Daniel Y. Fu et al. </li>
     
   
     
       <li> <a href="/publications/fried2022generative/">Incoder: A Generative Model For Code Infilling And Synthesis</a> Daniel Fried et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022graph/">Greaselm: Graph Reasoning Enhanced Language Models For Question Answering</a> Xikun Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022controllable/">Quark: Controllable Text Generation With Reinforced Unlearning</a> Ximing Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/an2022contrastive/">Cont: Contrastive Neural Text Generation</a> Chenxin An et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022adaptive/">Adaprompt: Adaptive Model Training For Prompt-based NLP</a> Yulong Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bulatov2022recurrent/">Recurrent Memory Transformer</a> Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2022pre/">Layoutlmv3: Pre-training For Document AI With Unified Text And Image Masking</a> Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2022zero/">Zero-shot Video Question Answering Via Frozen Bidirectional Language Models</a> Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid </li>
     
   
     
       <li> <a href="/publications/creswell2022faithful/">Faithful Reasoning Using Large Language Models</a> Antonia Creswell, Murray Shanahan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2022calibrating/">Calibrating Sequence Likelihood Improves Conditional Language Generation</a> Yao Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/leviathan2022fast/">Fast Inference From Transformers Via Speculative Decoding</a> Yaniv Leviathan, Matan Kalman, Yossi Matias </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/madaan2022text/">Text And Patterns: For Effective Chain Of Thought, It Takes Two To Tango</a> Aman Madaan, Amir Yazdanbakhsh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wettig2022should/">Should You Mask 15% In Masked Language Modeling?</a> Alexander Wettig, Tianyu Gao, Zexuan Zhong, Danqi Chen </li>
     
   
     
       <li> <a href="/publications/hao2022language/">Language Models Are General-purpose Interfaces</a> Yaru Hao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewkowycz2022solving/">Solving Quantitative Reasoning Problems With Language Models</a> Aitor Lewkowycz et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chowdhery2022scaling/">Palm: Scaling Language Modeling With Pathways</a> Aakanksha Chowdhery et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022memorizing/">Memorizing Transformers</a> Yuhuai Wu, Markus N. Rabe, Delesley Hutchins, Christian Szegedy </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ji2022survey/">Survey Of Hallucination In Natural Language Generation</a> Ziwei Ji et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022unifying/">OFA: Unifying Architectures, Tasks, And Modalities Through A Simple Sequence-to-sequence Learning Framework</a> Peng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/golovneva2022suite/">ROSCOE: A Suite Of Metrics For Scoring Step-by-step Reasoning</a> Olga Golovneva et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/black2022gpt/">Gpt-neox-20b: An Open-source Autoregressive Language Model</a> Sid Black et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/borsos2022language/">Audiolm: A Language Modeling Approach To Audio Generation</a> Zal√°n Borsos et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2022teaching/">Teaching Models To Express Their Uncertainty In Words</a> Stephanie Lin, Jacob Hilton, Owain Evans </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2022towards/">Towards A Unified Multi-dimensional Evaluator For Text Generation</a> Ming Zhong et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yasunaga2022deep/">Deep Bidirectional Language-knowledge Graph Pretraining</a> Michihiro Yasunaga et al. </li>
     
   
     
       <li> <a href="/publications/yasunaga2022retrieval/">Retrieval-augmented Multimodal Language Modeling</a> Michihiro Yasunaga et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2022pre/">Retromae: Pre-training Retrieval-oriented Language Models Via Masked Auto-encoder</a> Shitao Xiao, Zheng Liu, Yingxia Shao, Zhao Cao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schuster2022confident/">Confident Adaptive Language Modeling</a> Tal Schuster et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gao2022researching/">RARR: Researching And Revising What Language Models Say, Using Language Models</a> Luyu Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phan2022pretrained/">Vit5: Pretrained Text-to-text Transformer For Vietnamese Language Generation</a> Long Phan, Hieu Tran, Hieu Nguyen, Trieu H. Trinh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/webb2022emergent/">Emergent Analogical Reasoning In Large Language Models</a> Taylor Webb, Keith J. Holyoak, Hongjing Lu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tirumala2022memorization/">Memorization Without Overfitting: Analyzing The Training Dynamics Of Large Language Models</a> Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, Armen Aghajanyan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/soltan2022alexatm/">Alexatm 20B: Few-shot Learning Using A Large-scale Multilingual Seq2seq Model</a> Saleh Soltan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shin2022effect/">On The Effect Of Pretraining Corpora On In-context Learning By A Large-scale Language Model</a> Seongjin Shin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/strudel2022self/">Self-conditioned Embedding Diffusion For Text Generation</a> Robin Strudel et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2022generative/">Biogpt: Generative Pre-trained Transformer For Biomedical Text Generation And Mining</a> Renqian Luo et al. </li>
     
   
     
       <li> <a href="/publications/gong2022sequence/">Diffuseq: Sequence To Sequence Text Generation With Diffusion Models</a> Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022retrieval/">Reacc: A Retrieval-augmented Code Completion Framework</a> Shuai Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mirowski2022co/">Co-writing Screenplays And Theatre Scripts With Language Models: An Evaluation By Industry Professionals</a> Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, Richard Evans </li>
     
   
     
       <li> <a href="/publications/ramamurthy2022is/">Is Reinforcement Learning (not) For Natural Language Processing: Benchmarks, Baselines, And Building Blocks For Natural Language Policy Optimization</a> Rajkumar Ramamurthy et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/elkins2023how/">How Useful Are Educational Questions Generated By Large Language Models?</a> Sabina Elkins, Ekaterina Kochmar, Jackie C. K. Cheung, Iulian Serban </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/eldan2023how/">Tinystories: How Small Can Language Models Be And Still Speak Coherent English?</a> Ronen Eldan, Yuanzhi Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/omar2023universal/">A Universal Question-answering Platform For Knowledge Graphs</a> Reham Omar, Ishika Dhall, Panos Kalnis, Essam Mansour </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/min2023fine/">Factscore: Fine-grained Atomic Evaluation Of Factual Precision In Long Form Text Generation</a> Sewon Min et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thakur2023large/">Verigen: A Large Language Model For Verilog Code Generation</a> Shailja Thakur et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fleming2023clinician/">Medalign: A Clinician-generated Dataset For Instruction Following With Electronic Medical Records</a> Scott L. Fleming et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dhuliawala2023chain/">Chain-of-verification Reduces Hallucination In Large Language Models</a> Shehzaad Dhuliawala et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mccoy2023embers/">Embers Of Autoregression: Understanding Large Language Models Through The Problem They Are Trained To Solve</a> R. Thomas Mccoy, Shunyu Yao, Dan Friedman, Matthew Hardy, Thomas L. Griffiths </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2023unifying/">Unifying Large Language Models And Knowledge Graphs: A Roadmap</a> Shirui Pan et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023speak/">Speak Foreign Languages With Your Own Voice: Cross-lingual Neural Codec Language Modeling</a> Ziqiang Zhang et al. </li>
     
   
     
       <li> <a href="/publications/zhu2023pre/">3d-vista: Pre-trained Transformer For 3D Vision And Text Alignment</a> Ziyu Zhu et al. </li>
     
   
     
       <li> <a href="/publications/li2023masked/">Masked Vision And Language Pre-training With Unimodal And Multimodal Contrastive Losses For Medical Visual Question Answering</a> Pengfei Li, Gang Liu, Jinlong He, Zixu Zhao, Shenjun Zhong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ram2023retrieval/">In-context Retrieval-augmented Language Models</a> Ori Ram et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023lost/">Lost In The Middle: How Language Models Use Long Contexts</a> Nelson F. Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sultanum2023investigating/">Datatales: Investigating The Use Of Large Language Models For Authoring Data-driven Articles</a> Nicole Sultanum, Arjun Srinivasan </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/siddiq2023using/">Using Large Language Models To Generate Junit Tests: An Empirical Study</a> Mohammed Latif Siddiq et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phute2023llm/">LLM Self Defense: By Self Examination, Llms Know They Are Being Tricked</a> Mansi Phute et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023give/">Give Us The Facts: Enhancing Large Language Models With Knowledge Graphs For Fact-aware Language Modeling</a> Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, Xindong Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023scaling/">Scaling Autoregressive Multi-modal Models: Pretraining And Instruction Tuning</a> Lili Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023human/">VISAR: A Human-ai Argumentative Writing Assistant With Visual Programming And Rapid Draft Prototyping</a> Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, Toby Jia-jun Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023blip/">BLIP-2: Bootstrapping Language-image Pre-training With Frozen Image Encoders And Large Language Models</a> Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fei2023transferable/">Transferable Decoding With Visual Entities For Zero-shot Image Captioning</a> Junjie Fei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chung2023increasing/">Increasing Diversity While Maintaining Accuracy: Text Data Generation With Large Language Models And Human Interventions</a> John Joon Young Chung, Ece Kamar, Saleema Amershi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023large/">Large Language Models Cannot Self-correct Reasoning Yet</a> Jie Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2023prompting/">Prompting Is Not A Substitute For Probability Measurements In Large Language Models</a> Jennifer Hu, Roger Levy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023memory/">Memory-efficient Fine-tuning Of Compressed Large Language Models Via Sub-4-bit Integer Quantization</a> Jeonghoon Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jahan2023comprehensive/">A Comprehensive Evaluation Of Large Language Models On Benchmark Biomedical Text Processing Tasks</a> Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chang2023text/">Muse: Text-to-image Generation Via Masked Generative Transformers</a> Huiwen Chang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023cognitive/">Cognitive Mirage: A Review Of Hallucinations In Large Language Models</a> Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, Weiqiang Jia </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023video/">Video-llama: An Instruction-tuned Audio-visual Language Model For Video Understanding</a> Hang Zhang, Xin Li, Lidong Bing </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/penedo2023refinedweb/">The Refinedweb Dataset For Falcon LLM: Outperforming Curated Corpora With Web Data, And Web Data Only</a> Guilherme Penedo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mialon2023augmented/">Augmented Language Models: A Survey</a> Gr√©goire Mialon et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nijkamp2023lessons/">Codegen2: Lessons For Training Llms On Programming And Natural Languages</a> Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hodel2023emergent/">Response: Emergent Analogical Reasoning In Large Language Models</a> Damian Hodel, Jevin West </li>
     
   
     
       <li> <a href="/publications/kim2023solar/">SOLAR 10.7B: Scaling Large Language Models With Simple Yet Effective Depth Up-scaling</a> Dahyun Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lyu2023macaw/">Macaw-llm: Multi-modal Language Modeling With Image, Audio, Video, And Text Integration</a> Chenyang Lyu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2023is/">Is Chatgpt A General-purpose Natural Language Processing Task Solver?</a> Chengwei Qin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gulcehre2023reinforced/">Reinforced Self-training (rest) For Language Modeling</a> Caglar Gulcehre et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rozi%C3%A8re2023code/">Code Llama: Open Foundation Models For Code</a> Baptiste Rozi√®re et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jiang2023human/">Motiongpt: Human Motion As A Foreign Language</a> Biao Jiang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bulatov2023scaling/">Scaling Transformer To 1M Tokens And Beyond With RMT</a> Aydar Bulatov, Yuri Kuratov, Yermek Kapushev, Mikhail S. Burtsev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2023knowledge/">Minillm: Knowledge Distillation Of Large Language Models</a> Yuxian Gu, Li Dong, Furu Wei, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caines2023application/">On The Application Of Large Language Models For Language Teaching And Assessment Technology</a> Andrew Caines et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liesenfeld2023opening/">Opening Up Chatgpt: Tracking Openness, Transparency, And Accountability In Instruction-tuned Text Generators</a> Andreas Liesenfeld, Alianda Lopez, Mark Dingemanse </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salemi2023when/">Lamp: When Large Language Models Meet Personalization</a> Alireza Salemi, Sheshera Mysore, Michael Bendersky, Hamed Zamani </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ghosh2023clip/">Clipsyntel: CLIP And LLM Synergy For Multimodal Question Summarization In Healthcare</a> Akash Ghosh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pu2023summarization/">Summarization Is (almost) Dead</a> Xiao Pu, Mingqi Gao, Xiaojun Wan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023multilingual/">Multilingual Machine Translation With Large Language Models: Empirical Results And Analysis</a> Wenhao Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023retrieval/">REPLUG: Retrieval-augmented Black-box Language Models</a> Weijia Shi et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xin2023survey/">A Survey Of Large Language Models</a> Wayne Xin Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2023flashattention/">Flashattention-2: Faster Attention With Better Parallelism And Work Partitioning</a> Tri Dao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schick2023language/">Toolformer: Language Models Can Teach Themselves To Use Tools</a> Timo Schick et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023element/">Element-aware Summarization With Large Language Models: Expert-aligned Evaluation And Chain-of-thought Method</a> Yiming Wang, Zhuosheng Zhang, Rui Wang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tian2023graph/">Graph Neural Prompting With Large Language Models</a> Yijun Tian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moslem2023adaptive/">Adaptive Machine Translation With Large Language Models</a> Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2023collaborative/">Collaborative Large Language Model For Recommender Systems</a> Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023fine/">Fine-tuning Llama For Multi-stage Text Retrieval</a> Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023low/">Low-rank Adaptation Of Large Language Model Rescoring For Parameter-efficient Speech Recognition</a> Yu Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023retentive/">Retentive Network: A Successor To Transformer For Large Language Models</a> Yutao Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ge2023expressive/">Expressive Text-to-image Generation With Rich Text</a> Songwei Ge, Taesung Park, Jun-yan Zhu, Jia-bin Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gholami2023do/">Do Generative Large Language Models Need Billions Of Parameters?</a> Sia Gholami, Marwan Omar </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tian2023opportunities/">Opportunities And Challenges For Chatgpt And Large Language Models In Biomedicine And Health</a> Shubo Tian et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023extending/">Extending Context Window Of Large Language Models Via Positional Interpolation</a> Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023fine/">Fine-grained Human Feedback Gives Better Rewards For Language Model Training</a> Zeqiu Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023chatgpt/">Chatgpt As A Factual Inconsistency Evaluator For Text Summarization</a> Zheheng Luo, Qianqian Xie, Sophia Ananiadou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dhillon2024shaping/">Shaping Human-ai Collaboration: Varied Scaffolding Levels In Co-writing With Language Models</a> Paramveer S. Dhillon et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maleki2024ai/">AI Hallucinations: A Misnomer Worth Clarifying</a> Negar Maleki, Balaji Padmanabhan, Kaushik Dutta </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2024findings/">Findings Of The Second Babylm Challenge: Sample-efficient Pretraining On Developmentally Plausible Corpora</a> Michael Y. Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/beck2024extended/">Xlstm: Extended Long Short-term Memory</a> Maximilian Beck et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gemmateam2024open/">Gemma: Open Models Based On Gemini Research And Technology</a> Gemma Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mo2024large/">Large Language Model (LLM) AI Text Generation Detection Based On Transformer Deep Learning Algorithm</a> Yuhong Mo, Hao Qin, Yushan Dong, Ziyi Zhu, Zhenglin Li </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2024unified/">Llamafactory: Unified Efficient Fine-tuning Of 100+ Language Models</a> Yaowei Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2024transformers/">Transformers Are Ssms: Generalized Models And Efficient Algorithms Through Structured State Space Duality</a> Tri Dao, Albert Gu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Large-Scale Training <a id="Large-Scale Training"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/smith2022using/">Using Deepspeed And Megatron To Train Megatron-turing NLG 530B, A Large-scale Generative Language Model</a> Shaden Smith et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Masked Language Model <a id="Masked Language Model"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salazar2019masked/">Masked Language Model Scoring</a> Julian Salazar, Davis Liang, Toan Q. Nguyen, Katrin Kirchhoff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/conneau2019unsupervised/">Unsupervised Cross-lingual Representation Learning At Scale</a> Alexis Conneau et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019unicoder/">Unicoder-vl: A Universal Encoder For Vision And Language By Cross-modal Pre-training</a> Gen Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2020learning/">On Learning Universal Representations Across Languages</a> Xiangpeng Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2020contextualized/">Colake: Contextualized Language And Knowledge Embedding</a> Tianxiang Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shin2020eliciting/">Autoprompt: Eliciting Knowledge From Language Models With Automatically Generated Prompts</a> Taylor Shin, Yasaman Razeghi, Robert L. Iv Logan, Eric Wallace, Sameer Singh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2020masking/">Masking As An Efficient Alternative To Finetuning For Pretrained Language Models</a> Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/webersinke2021pretrained/">Climatebert: A Pretrained Language Model For Climate-related Text</a> Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler, Markus Leippold </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2021good/">A Good Prompt Is Worth Millions Of Parameters: Low-resource Prompt-based Learning For Vision-language Models</a> Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, Xiang Ren </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021study/">BERT, Mbert, Or Bibert? A Study On Contextualized Embeddings For Neural Machine Translation</a> Haoran Xu, Benjamin Van Durme, Kenton Murray </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frank2021vision/">Vision-and-language Or Vision-for-language? On Cross-modal Influence In Multimodal Transformers</a> Stella Frank, Emanuele Bugliarello, Desmond Elliott </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caciularu2021cross/">CDLM: Cross-document Language Modeling</a> Avi Caciularu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khare2021multimodal/">MMBERT: Multimodal BERT Pretraining For Improved Medical VQA</a> Yash Khare et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geiping2022training/">Cramming: Training A Language Model On A Single GPU In One Day</a> Jonas Geiping, Tom Goldstein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022linguistically/">LERT: A Linguistically-motivated Pre-trained Language Model</a> Yiming Cui, Wanxiang Che, Shijin Wang, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/christopoulou2022pangu/">Pangu-coder: Program Synthesis With Function-level Language Modeling</a> Fenia Christopoulou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2022zero/">Zero-shot Video Question Answering Via Frozen Bidirectional Language Models</a> Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yasunaga2022deep/">Deep Bidirectional Language-knowledge Graph Pretraining</a> Michihiro Yasunaga et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023masked/">Masked Vision And Language Pre-training With Unimodal And Multimodal Contrastive Losses For Medical Visual Question Answering</a> Pengfei Li, Gang Liu, Jinlong He, Zixu Zhao, Shenjun Zhong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Model Architecture <a id="Model Architecture"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2016latent/">Latent Attention For If-then Program Synthesis</a> Xinyun Chen, Chang Liu, Richard Shin, Dawn Song, Mingcheng Chen </li>
     
   
     
       <li> <a href="/publications/serban2016generative/">Generative Deep Neural Networks For Dialogue: A Short Review</a> Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, Joelle Pineau </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meng2016interactive/">Interactive Attention For Neural Machine Translation</a> Fandong Meng, Zhengdong Lu, Hang Li, Qun Liu </li>
     
   
     
   
     
       <li> <a href="/publications/yao2016attentional/">An Attentional Neural Conversation Model With Improved Specificity</a> Kaisheng Yao, Baolin Peng, Geoffrey Zweig, Kam-fai Wong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wiseman2016sequence/">Sequence-to-sequence Learning As Beam-search Optimization</a> Sam Wiseman, Alexander M. Rush </li>
     
   
     
   
     
       <li> <a href="/publications/firat2016multi/">Multi-way, Multilingual Neural Machine Translation With A Shared Attention Mechanism</a> Orhan Firat, Kyunghyun Cho, Yoshua Bengio </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2016neural/">Neural Machine Translation Advised By Statistical Machine Translation</a> Xing Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2016context/">A Context-aware Attention Network For Interactive Question Answering</a> Huayu Li, Martin Renqiang Min, Yong Ge, Asim Kadav </li>
     
   
     
       <li> <a href="/publications/liu2016neural/">Neural Machine Translation With Supervised Attention</a> Lemao Liu, Masao Utiyama, Andrew Finch, Eiichiro Sumita </li>
     
   
     
   
     
       <li> <a href="/publications/sordoni2016iterative/">Iterative Alternating Neural Attention For Machine Reading</a> Alessandro Sordoni, Philip Bachman, Adam Trischler, Yoshua Bengio </li>
     
   
     
       <li> <a href="/publications/eriguchi2016tree/">Tree-to-sequence Attentional Neural Machine Translation</a> Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/eric2017copy/">A Copy-augmented Sequence-to-sequence Architecture Gives Good Performance On Task-oriented Dialogue</a> Mihail Eric, Christopher D. Manning </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2017beyond/">Beyond Bilinear: Generalized Multimodal Factorized High-order Pooling For Visual Question Answering</a> Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, Dacheng Tao </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2017table/">Table-to-text Generation By Structure-aware Seq2seq Learning</a> Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, Zhifang Sui </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2017phase/">Phase Conductor On Multi-layered Attentions For Machine Comprehension</a> Rui Liu, Wei Wei, Weiguang Mao, Maria Chikina </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2017unified/">A Unified Query-based Generative Model For Question Generation And Question Answering</a> Linfeng Song, Zhiguo Wang, Wael Hamza </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2017syntax/">Syntax-directed Attention For Neural Machine Translation</a> Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2017best/">Best Of Both Worlds: Transferring Knowledge From Discriminative Learning To A Generative Visual Dialog Model</a> Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/calixto2017incorporating/">Incorporating Global Visual Features Into Attention-based Neural Machine Translation</a> Iacer Calixto, Qun Liu, Nick Campbell </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2017learning/">Learning To Ask: Neural Question Generation For Reading Comprehension</a> Xinya Du, Junru Shao, Claire Cardie </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dziri2018augmenting/">Augmenting Neural Response Generation With Context-aware Topical Attention</a> Nouha Dziri, Ehsan Kamalloo, Kory W. Mathewson, Osmar Zaiane </li>
     
   
     
   
     
       <li> <a href="/publications/alon2018generating/">Code2seq: Generating Sequences From Structured Representations Of Code</a> Uri Alon, Shaked Brody, Omer Levy, Eran Yahav </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2018hierarchical/">Hierarchical Neural Story Generation</a> Angela Fan, Mike Lewis, Yann Dauphin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2018contextualized/">Sdnet: Contextualized Attention-based Deep Network For Conversational Question Answering</a> Chenguang Zhu, Michael Zeng, Xuedong Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2018combining/">Qanet: Combining Local Convolution With Global Self-attention For Reading Comprehension</a> Adams Wei Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peters2018dissecting/">Dissecting Contextual Word Embeddings: Architecture And Representation</a> Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, Wen-tau Yih </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2018dual/">Dual Ask-answer Network For Machine Reading Comprehension</a> Han Xiao, Feng Wang, Jianfeng Yan, Jingyao Zheng </li>
     
   
     
       <li> <a href="/publications/stahlberg2018simple/">Simple Fusion: Return Of The Language Model</a> Felix Stahlberg, James Cross, Veselin Stoyanov </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2018multi/">Multi-granularity Hierarchical Attention Fusion Networks For Reading Comprehension And Question Answering</a> Wei Wang, Ming Yan, Chen Wu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2018deriving/">Deriving Machine Attention From Human Rationales</a> Yujia Bao, Shiyu Chang, Mo Yu, Regina Barzilay </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2018explicit/">Explicit State Tracking With Semi-supervision For Neural Dialogue Generation</a> Xisen Jin et al. </li>
     
   
     
       <li> <a href="/publications/choi2018fine/">Fine-grained Attention Mechanism For Neural Machine Translation</a> Heeyoul Choi, Kyunghyun Cho, Yoshua Bengio </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wen2018sequence/">Sequence-to-sequence Learning For Task-oriented Dialogue With Dialogue State Representation</a> Haoyang Wen, Yijia Liu, Wanxiang Che, Libo Qin, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2019pretraining/">Vilbert: Pretraining Task-agnostic Visiolinguistic Representations For Vision-and-language Tasks</a> Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee </li>
     
   
     
   
     
       <li> <a href="/publications/pei2019modular/">A Modular Task-oriented Dialogue System Using A Neural Mixture-of-experts</a> Jiahuan Pei, Pengjie Ren, Maarten De Rijke </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2019cosmos/">Cosmos QA: Machine Reading Comprehension With Contextual Commonsense Reasoning</a> Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi </li>
     
   
     
       <li> <a href="/publications/qin2019entity/">Entity-consistent End-to-end Task-oriented Dialogue System With KB Retriever</a> Libo Qin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2019transferring/">Macnet: Transferring Knowledge From Machine Comprehension To Sequence-to-sequence Models</a> Boyuan Pan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019incorporating/">Structbert: Incorporating Language Structures Into Pre-training For Deep Language Understanding</a> Wei Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019improving/">Improving Knowledge-aware Dialogue Generation Via Knowledge Base Question Answering</a> Jian Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2019dynamic/">Dynamic Past And Future For Neural Machine Translation</a> Zaixiang Zheng, Shujian Huang, Zhaopeng Tu, Xin-yu Dai, Jiajun Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2019transformers/">Transformers Without Tears: Improving The Normalization Of Self-attention</a> Toan Q. Nguyen, Julian Salazar </li>
     
   
     
       <li> <a href="/publications/zha2019context/">Context-aware Visual Policy Network For Fine-grained Image Captioning</a> Zheng-jun Zha, Daqing Liu, Hanwang Zhang, Yongdong Zhang, Feng Wu </li>
     
   
     
   
     
       <li> <a href="/publications/song2019masked/">MASS: Masked Sequence To Sequence Pre-training For Language Generation</a> Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-yan Liu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2019efficient/">Efficient Attention Mechanism For Visual Dialog That Can Handle All The Interactions Between Multiple Inputs</a> Van-quang Nguyen, Masanori Suganuma, Takayuki Okatani </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2019coarse/">Coarse-grain Fine-grain Coattention Network For Multi-evidence Question Answering</a> Victor Zhong, Caiming Xiong, Nitish Shirish Keskar, Richard Socher </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nema2019ask/">Let's Ask Again: Refine Network For Automatic Question Generation</a> Preksha Nema, Akash Kumar Mohankumar, Mitesh M. Khapra, Balaji Vasan Srinivasan, Balaraman Ravindran </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maruf2019selective/">Selective Attention For Context-aware Neural Machine Translation</a> Sameen Maruf, Andr√© F. T. Martins, Gholamreza Haffari </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2019simple/">Simple And Effective Curriculum Pointer-generator Networks For Reading Comprehension Over Long Narratives</a> Yi Tay et al. </li>
     
   
     
       <li> <a href="/publications/gu2019levenshtein/">Levenshtein Transformer</a> Jiatao Gu, Changhan Wang, Jake Zhao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/trivedi2019repurposing/">Repurposing Entailment For Multi-hop Question Answering Tasks</a> Harsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, Niranjan Balasubramanian </li>
     
   
     
       <li> <a href="/publications/alberti2019bert/">A BERT Baseline For The Natural Questions</a> Chris Alberti, Kenton Lee, Michael Collins </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zenkel2019adding/">Adding Interpretable Attention To Neural Translation Models Improves Word Alignment</a> Thomas Zenkel, Joern Wuebker, John Denero </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qu2019attentive/">Attentive History Selection For Conversational Question Answering</a> Chen Qu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cornia2019meshed/">Meshed-memory Transformer For Image Captioning</a> Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara </li>
     
   
     
   
     
       <li> <a href="/publications/hao2019modeling/">Modeling Recurrence For Transformer</a> Jie Hao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2019pre/">A Pre-training Based Personalized Dialogue Generation Model With Persona-sparse Data</a> Yinhe Zheng, Rongsheng Zhang, Xiaoxi Mao, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019semantics/">Semantics-aware BERT For Language Understanding</a> Zhuosheng Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2019bp/">Bp-transformer: Modelling Long-range Context Via Binary Partitioning</a> Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, Zheng Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2019denoising/">BART: Denoising Sequence-to-sequence Pre-training For Natural Language Generation, Translation, And Comprehension</a> Mike Lewis et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/svyatkovskiy2019ai/">Pythia: Ai-assisted Code Completion System</a> Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, Neel Sundaresan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019tree/">Tree Transformer: Integrating Tree Structures Into Self-attention</a> Yau-shian Wang, Hung-yi Lee, Yun-nung Chen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lan2019lite/">ALBERT: A Lite BERT For Self-supervised Learning Of Language Representations</a> Zhenzhong Lan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nogueira2019passage/">Passage Re-ranking With BERT</a> Rodrigo Nogueira, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/merity2019single/">Single Headed Attention RNN: Stop Thinking With Your Head</a> Stephen Merity </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2019multi/">MMM: Multi-stage Multi-task Learning For Multi-choice Reading Comprehension</a> Di Jin, Shuyang Gao, Jiun-yu Kao, Tagyoung Chung, Dilek Hakkani-tur </li>
     
   
     
   
     
       <li> <a href="/publications/li2019unicoder/">Unicoder-vl: A Universal Encoder For Vision And Language By Cross-modal Pre-training</a> Gen Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qiu2019blockwise/">Blockwise Self-attention For Long Document Understanding</a> Jiezhong Qiu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/puduppully2019data/">Data-to-text Generation With Entity Modeling</a> Ratish Puduppully, Li Dong, Mirella Lapata </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2019parallel/">MUSE: Parallel Multi-scale Attention For Sequence To Sequence Learning</a> Guangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, Liangchen Luo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shazeer2019fast/">Fast Transformer Decoding: One Write-head Is All You Need</a> Noam Shazeer </li>
     
   
     
       <li> <a href="/publications/mansimov2019generalized/">A Generalized Framework Of Sequence Generation With Application To Undirected Sequence Models</a> Elman Mansimov, Alex Wang, Sean Welleck, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pfeiffer2020mad/">MAD-X: An Adapter-based Framework For Multi-task Cross-lingual Transfer</a> Jonas Pfeiffer, Ivan Vuliƒá, Iryna Gurevych, Sebastian Ruder </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2020long/">Long Range Arena: A Benchmark For Efficient Transformers</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/smith2020can/">Can You Put It All Together: Evaluating Conversational Agents' Ability To Blend Skills</a> Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, Y-lan Boureau </li>
     
   
     
       <li> <a href="/publications/smith2020controlling/">Controlling Style In Generated Dialogue</a> Eric Michael Smith, Diana Gonzalez-rico, Emily Dinan, Y-lan Boureau </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sood2020improving/">Improving Natural Language Processing Tasks With Human Gaze-guided Neural Attention</a> Ekta Sood, Simon Tannert, Philipp Mueller, Andreas Bulling </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lepikhin2020scaling/">Gshard: Scaling Giant Models With Conditional Computation And Automatic Sharding</a> Dmitry Lepikhin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2020kg/">KG-BART: Knowledge Graph-augmented BART For Generative Commonsense Reasoning</a> Ye Liu, Yao Wan, Lifang He, Hao Peng, Philip S. Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clark2020pre/">ELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators</a> Kevin Clark, Minh-thang Luong, Quoc V. Le, Christopher D. Manning </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kolluru2020iterative/">Imojie: Iterative Memory-based Joint Open Information Extraction</a> Keshav Kolluru, Samarth Aggarwal, Vipul Rathore, Mausam, Soumen Chakrabarti </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2020character/">Charbert: Character-aware Pre-trained Language Model</a> Wentao Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2020length/">Length-adaptive Transformer: Train Once With Length Drop, Use Anytime With Search</a> Gyuwan Kim, Kyunghyun Cho </li>
     
   
     
   
     
       <li> <a href="/publications/ye2020variational/">Variational Template Machine For Data-to-text Generation</a> Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, Lei Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2020sub/">Sub-instruction Aware Vision-and-language Navigation</a> Yicong Hong, Cristian Rodriguez-opazo, Qi Wu, Stephen Gould </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020logical/">Logical Natural Language Generation From Open-domain Tables</a> Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bhojanapalli2020low/">Low-rank Bottleneck In Multi-head Attention Models</a> Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar </li>
     
   
     
       <li> <a href="/publications/lei2020memory/">MART: Memory-augmented Recurrent Transformer For Coherent Video Paragraph Captioning</a> Jie Lei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2020retrieval/">Retrieval-augmented Generation For Knowledge-intensive NLP Tasks</a> Patrick Lewis et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/muller2020when/">When Being Unseen From Mbert Is Just The Beginning: Handling New Languages With Multilingual Language Models</a> Benjamin Muller, Antonis Anastasopoulos, Beno√Æt Sagot, Djam√© Seddah </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2020reading/">DUMA: Reading Comprehension With Transposition Thinking</a> Pengfei Zhu, Hai Zhao, Xiaoguang Li </li>
     
   
     
       <li> <a href="/publications/yang2020intent/">IART: Intent-aware Response Ranking With Transformers In Information-seeking Conversation Systems</a> Liu Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020machine/">Machine Reading Comprehension: The Role Of Contextualized Language Models And Beyond</a> Zhuosheng Zhang, Hai Zhao, Rui Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020self/">Linformer: Self-attention With Linear Complexity</a> Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma </li>
     
   
     
   
     
       <li> <a href="/publications/rahman2020improved/">An Improved Attention For Visual Question Answering</a> Tanzila Rahman, Shih-han Chou, Leonid Sigal, Giuseppe Carenini </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2020incorporating/">Incorporating BERT Into Parallel Sequence Decoding With Adapters</a> Junliang Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kasai2020non/">Non-autoregressive Machine Translation With Disentangled Context Transformer</a> Jungo Kasai, James Cross, Marjan Ghazvininejad, Jiatao Gu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2020inducing/">Inducing Language-agnostic Multilingual Representations</a> Wei Zhao, Steffen Eger, Johannes Bjerva, Isabelle Augenstein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020comparison/">A Comparison Of Pre-trained Vision-and-language Models For Multimodal Representation Learning Across Medical Images And Reports</a> Yikuan Li, Hanyin Wang, Yuan Luo </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/you2020hard/">Hard-coded Gaussian Attention For Neural Machine Translation</a> Weiqiu You, Simeng Sun, Mohit Iyyer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2021efficient/">Glam: Efficient Scaling Of Language Models With Mixture-of-experts</a> Nan Du et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kharitonov2021text/">Text-free Prosody-aware Generative Spoken Language Modeling</a> Eugene Kharitonov et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kong2021bidirectional/">BLT: Bidirectional Layout Transformer For Controllable Layout Generation</a> Xiang Kong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2021pre/">Dialoglm: Pre-trained Model For Long Dialogue Understanding And Summarization</a> Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021fast/">Fastmoe: A Fast Mixture-of-expert Training System</a> Jiaao He et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dejong2021mention/">Mention Memory: Incorporating Textual Knowledge Into Transformers Through Entity Mention Attention</a> Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, William Cohen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guhur2021pretraining/">Airbert: In-domain Pretraining For Vision-and-language Navigation</a> Pierre-louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, Cordelia Schmid </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2021ernie/">ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training For Language Understanding And Generation</a> Yu Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2021emotion/">Emotion-aware Chat Machine: Automatic Emotional Response Generation For Human-like Emotional Interaction</a> Wei Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021study/">BERT, Mbert, Or Bibert? A Study On Contextualized Embeddings For Neural Machine Translation</a> Haoran Xu, Benjamin Van Durme, Kenton Murray </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021scalable/">Scalable And Efficient Moe Training For Multitask Multilingual Models</a> Young Jin Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021data/">Visualgpt: Data-efficient Adaptation Of Pretrained Language Models For Image Captioning</a> Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny </li>
     
   
     
       <li> <a href="/publications/eisenschlos2021multi/">MATE: Multi-view Attention For Table Transformer Efficiency</a> Julian Martin Eisenschlos, Maharshi Gor, Thomas M√ºller, William W. Cohen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021end/">Swinbert: End-to-end Transformers With Sparse Attention For Video Captioning</a> Kevin Lin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/santhanam2021effective/">Colbertv2: Effective And Efficient Retrieval Via Lightweight Late Interaction</a> Keshav Santhanam, Omar Khattab, Jon Saad-falcon, Christopher Potts, Matei Zaharia </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meister2021language/">Language Model Evaluation Beyond Perplexity</a> Clara Meister, Ryan Cotterell </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tafjord2021general/">General-purpose Question-answering With Macaw</a> Oyvind Tafjord, Peter Clark </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2021code/">Code Structure Guided Transformer For Source Code Summarization</a> Shuzheng Gao et al. </li>
     
   
     
       <li> <a href="/publications/caciularu2021cross/">CDLM: Cross-document Language Modeling</a> Avi Caciularu et al. </li>
     
   
     
       <li> <a href="/publications/he2021improving/">Debertav3: Improving Deberta Using Electra-style Pre-training With Gradient-disentangled Embedding Sharing</a> Pengcheng He, Jianfeng Gao, Weizhu Chen </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guti%C3%A9rrezfandi%C3%B1o2021spanish/">Maria: Spanish Language Models</a> Asier Guti√©rrez-fandi√±o et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2021less/">Less Is More: Pre-train A Strong Text Encoder For Dense Retrieval Using A Weak Decoder</a> Shuqi Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/suglia2021embodied/">Embodied BERT: A Transformer Model For Embodied, Language-guided Visual Task Completion</a> Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2021effective/">An Effective Non-autoregressive Model For Spoken Language Understanding</a> Lizhi Cheng, Weijia Jia, Wenmian Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2021general/">GLM: General Language Model Pretraining With Autoregressive Blank Infilling</a> Zhengxiao Du et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2021intent/">Intent Detection And Slot Filling For Vietnamese</a> Mai Hoang Dao, Thinh Hung Truong, Dat Quoc Nguyen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021text/">Text Compression-aided Transformer Encoding</a> Zuchao Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2022screenshot/">Pix2struct: Screenshot Parsing As Pretraining For Visual Language Understanding</a> Kenton Lee et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022what/">What Language Model Architecture And Pretraining Objective Work Best For Zero-shot Generalization?</a> Thomas Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2022adaptive/">Adamct: Adaptive Mixture Of Cnn-transformer For Sequential Recommendation</a> Juyong Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2022fast/">Flashattention: Fast And Memory-efficient Exact Attention With Io-awareness</a> Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R√© </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022simple/">Lilt: A Simple Yet Effective Language-independent Layout Transformer For Structured Document Understanding</a> Jiapeng Wang, Lianwen Jin, Kai Ding </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022generative/">M6-rec: Generative Pretrained Language Models Are Open-ended Recommender Systems</a> Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/le2022mastering/">Coderl: Mastering Code Generation Through Pretrained Models And Deep Reinforcement Learning</a> Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ke2022hierarchical/">Hitskt: A Hierarchical Transformer Model For Session-aware Knowledge Tracing</a> Fucai Ke et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2022multi/">MIST: Multi-modal Iterative Spatial-temporal Transformer For Long-form Video Question Answering</a> Difei Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022prototypical/">Protoclip: Prototypical Contrastive Language Image Pretraining</a> Delong Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gong2022future/">Future Transformer For Long-term Action Anticipation</a> Dayoung Gong, Joonseok Lee, Manjin Kim, Seong Jong Ha, Minsu Cho </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2022systematic/">A Systematic Evaluation Of Large Language Models Of Code</a> Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2022hungry/">Hungry Hungry Hippos: Towards Language Modeling With State Space Models</a> Daniel Y. Fu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schuhmann2022laion/">LAION-5B: An Open Large-scale Dataset For Training Next Generation Image-text Models</a> Christoph Schuhmann et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2022unifying/">UL2: Unifying Language Learning Paradigms</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bohnet2022attributed/">Attributed Question Answering: Evaluation And Modeling For Attributed Large Language Models</a> Bernd Bohnet et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bulatov2022recurrent/">Recurrent Memory Transformer</a> Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022democratizing/">Democratizing Contrastive Language-image Pre-training: A CLIP Benchmark Of Data, Model, And Supervision</a> Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, Jing Shao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2022investigating/">EVA2.0: Investigating Open-domain Chinese Dialogue Systems With Large-scale Pre-training</a> Yuxian Gu et al. </li>
     
   
     
       <li> <a href="/publications/petrov2022systematic/">A Systematic Review And Replicability Study Of Bert4rec For Sequential Recommendation</a> Aleksandr Petrov, Craig Macdonald </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/roberts2022scaling/">Scaling Up Models And Data With \(\texttt{t5x}\) And \(\texttt{seqio}\)</a> Adam Roberts et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022unifying/">OFA: Unifying Architectures, Tasks, And Modalities Through A Simple Sequence-to-sequence Learning Framework</a> Peng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sap2022neural/">Neural Theory-of-mind? On The Limits Of Social Intelligence In Large Lms</a> Maarten Sap, Ronan Lebras, Daniel Fried, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shuster2022blenderbot/">Blenderbot 3: A Deployed Conversational Agent That Continually Learns To Responsibly Engage</a> Kurt Shuster et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rajbhandari2022deepspeed/">Deepspeed-moe: Advancing Mixture-of-experts Inference And Training To Power Next-generation AI Scale</a> Samyam Rajbhandari et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023may/">Starcoder: May The Source Be With You!</a> Raymond Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023llama/">Llama-adapter V2: Parameter-efficient Visual Instruction Model</a> Peng Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rao2023cat/">CAT-LM: Training Language Models On Aligned Code And Tests</a> Nikitha Rao, Kush Jain, Uri Alon, Claire Le Goues, Vincent J. Hellendoorn </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dehghani2023scaling/">Scaling Vision Transformers To 22 Billion Parameters</a> Mostafa Dehghani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/poli2023hyena/">Hyena Hierarchy: Towards Larger Convolutional Language Models</a> Michael Poli et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023recommendation/">Recommendation As Instruction Following: A Large Language Model Empowered Recommendation Approach</a> Junjie Zhang et al. </li>
     
   
     
       <li> <a href="/publications/ye2023comprehensive/">A Comprehensive Capability Analysis Of GPT-3 And GPT-3.5 Series Models</a> Junjie Ye et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023graphix/">Graphix-t5: Mixing Pre-trained Transformers With Graph-aware Layers For Text-to-sql Parsing</a> Jinyang Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023retrieval/">Rella: Retrieval-enhanced Large Language Models For Lifelong Sequential Behavior Comprehension In Recommendation</a> Jianghao Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023decoder/">On Decoder-only Architecture For Speech-to-text And Large Language Model Integration</a> Jian Wu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023universal/">Ureader: Universal Ocr-free Visually-situated Language Understanding With Multimodal Large Language Model</a> Jiabo Ye et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023ip/">Ip-adapter: Text Compatible Image Prompt Adapter For Text-to-image Diffusion Models</a> Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, Wei Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023audioldm/">Audioldm 2: Learning Holistic Audio Generation With Self-supervised Pretraining</a> Haohe Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2023efficient/">Efficient Streaming Language Models With Attention Sinks</a> Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nijkamp2023lessons/">Codegen2: Lessons For Training Llms On Programming And Natural Languages</a> Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/garridomerch%C3%A1n2023simulating/">Simulating H.P. Lovecraft Horror Literature With The Chatgpt Large Language Model</a> Eduardo C. Garrido-merch√°n, Jos√© Luis Arroyo-barrig√ºete, Roberto Gozalo-brizuela </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023llm/">Llm-blender: Ensembling Large Language Models With Pairwise Ranking And Generative Fusion</a> Dongfu Jiang, Xiang Ren, Bill Yuchen Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023show/">Show-1: Marrying Pixel And Latent Diffusion Models For Text-to-video Generation</a> David Junhao Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023reinventing/">RWKV: Reinventing Rnns For The Transformer Era</a> Bo Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/klenitskiy2023turning/">Turning Dross Into Gold Loss: Is Bert4rec Really Better Than Sasrec?</a> Anton Klenitskiy, Alexey Vasilev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kazemnejad2023impact/">The Impact Of Positional Encoding On Length Generalization In Transformers</a> Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, Siva Reddy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2023linear/">Mamba: Linear-time Sequence Modeling With Selective State Spaces</a> Albert Gu, Tri Dao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2023flashattention/">Flashattention-2: Faster Attention With Better Parallelism And Work Partitioning</a> Tri Dao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/carta2023grounding/">Grounding Large Language Models In Interactive Environments With Online Reinforcement Learning</a> Thomas Carta et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sheng2023high/">Flexgen: High-throughput Generative Inference Of Large Language Models With A Single GPU</a> Ying Sheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023specializing/">Specializing Smaller Language Models Towards Multi-step Reasoning</a> Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023training/">Controlvideo: Training-free Controllable Text-to-video Generation</a> Yabo Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023xuanyuan/">Xuanyuan 2.0: A Large Chinese Financial Chat Model With Hundreds Of Billions Parameters</a> Xuanyu Zhang, Qing Yang, Dongliang Xu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2023representation/">Representation Learning With Large Language Models For Recommendation</a> Xubin Ren et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dhingra2023mind/">Mind Meets Machine: Unravelling Gpt-4's Cognitive Psychology</a> Sifatkaur Dhingra, Manmeet Singh, Vaisakh Sb, Neetiraj Malviya, Sukhpal Singh Gill </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023extending/">Extending Context Window Of Large Language Models Via Positional Interpolation</a> Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian </li>
     
   
     
   
     
       <li> <a href="/publications/gunasekar2023textbooks/">Textbooks Are All You Need</a> Suriya Gunasekar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2024findings/">Findings Of The Second Babylm Challenge: Sample-efficient Pretraining On Developmentally Plausible Corpora</a> Michael Y. Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/beck2024extended/">Xlstm: Extended Long Short-term Memory</a> Maximilian Beck et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2024pixart/">Pixart-\sigma: Weak-to-strong Training Of Diffusion Transformer For 4K Text-to-image Generation</a> Junsong Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/groeneveld2024accelerating/">Olmo: Accelerating The Science Of Language Models</a> Dirk Groeneveld et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/deepseekai2024deepseek/">Deepseek-v2: A Strong, Economical, And Efficient Mixture-of-experts Language Model</a> Deepseek-ai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gholami2024ai/">AI And Memory Wall</a> Amir Gholami et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2024trustworthiness/">Trustllm: Trustworthiness In Large Language Models</a> Yue Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2024transformers/">Transformers Are Ssms: Generalized Models And Efficient Algorithms Through Structured State Space Duality</a> Tri Dao, Albert Gu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/glm2024family/">Chatglm: A Family Of Large Language Models From GLM-130B To GLM-4 All Tools</a> Team Glm et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Multimodal Models <a id="Multimodal Models"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2016multimodal/">Multimodal Memory Modelling For Video Captioning</a> Junbo Wang, Wei Wang, Yan Huang, Liang Wang, Tieniu Tan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caglayan2016multimodal/">Multimodal Attention For Neural Machine Translation</a> Ozan Caglayan, Lo√Øc Barrault, Fethi Bougares </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fukui2016multimodal/">Multimodal Compact Bilinear Pooling For Visual Question Answering And Visual Grounding</a> Akira Fukui et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2017beyond/">Beyond Bilinear: Generalized Multimodal Factorized High-order Pooling For Visual Question Answering</a> Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, Dacheng Tao </li>
     
   
     
       <li> <a href="/publications/agrawal2017just/">Don't Just Assume; Look And Answer: Overcoming Priors For Visual Question Answering</a> Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/elliott2017imagination/">Imagination Improves Multimodal Translation</a> Desmond Elliott, √Åkos K√°d√°r </li>
     
   
     
       <li> <a href="/publications/caglayan2017lium/">LIUM-CVC Submissions For WMT17 Multimodal Translation Task</a> Ozan Caglayan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/delbrouck2017multimodal/">Multimodal Compact Bilinear Pooling For Multimodal Neural Machine Translation</a> Jean-benoit Delbrouck, Stephane Dupont </li>
     
   
     
   
     
       <li> <a href="/publications/mostafazadeh2017image/">Image-grounded Conversations: Multimodal Context For Natural Question And Response Generation</a> Nasrin Mostafazadeh et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/calixto2017incorporating/">Incorporating Global Visual Features Into Attention-based Neural Machine Translation</a> Iacer Calixto, Qun Liu, Nick Campbell </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2017visual/">Memexqa: Visual Memex Question Answering</a> Lu Jiang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/libovick%C3%BD2017attention/">Attention Strategies For Multi-source Sequence-to-sequence Learning</a> Jind≈ôich Libovick√Ω, Jind≈ôich Helcl </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2018unsupervised/">Unsupervised Multi-modal Neural Machine Translation</a> Yuanhang Su, Kai Fan, Nguyen Bach, C. -c. Jay Kuo, Fei Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2018dynamic/">Dynamic Fusion With Intra- And Inter- Modality Attention Flow For Visual Question Answering</a> Gao Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moon2018multimodal/">Multimodal Named Entity Recognition For Short Social Media Posts</a> Seungwhan Moon, Leonardo Neves, Vitor Carvalho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gr%C3%B6nroos2018memad/">The Memad Submission To The WMT18 Multimodal Translation Task</a> Stig-arne Gr√∂nroos et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/agarwal2018knowledge/">A Knowledge-grounded Multimodal Search-based Conversational Agent</a> Shubham Agarwal, Ondrej Dusek, Ioannis Konstas, Verena Rieser </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2018multimodal/">Multimodal Dual Attention Memory For Video Story Question Answering</a> Kyung-min Kim, Seong-ho Choi, Jin-hwa Kim, Byoung-tak Zhang </li>
     
   
     
   
     
       <li> <a href="/publications/yagcioglu2018challenge/">Recipeqa: A Challenge Dataset For Multimodal Comprehension Of Cooking Recipes</a> Semih Yagcioglu, Aykut Erdem, Erkut Erdem, Nazli Ikizler-cinbis </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2018visual/">A Visual Attention Grounding Neural Model For Multimodal Machine Translation</a> Mingyang Zhou, Runxiang Cheng, Yong Jae Lee, Zhou Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2018reinforced/">Reinforced Cross-modal Matching And Self-supervised Imitation Learning For Vision-language Navigation</a> Xin Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2018sentiment/">Sentiment Adaptive End-to-end Dialog Systems</a> Weiyan Shi, Zhou Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/park2018multimodal/">Multimodal Explanations: Justifying Decisions And Pointing To The Evidence</a> Dong Huk Park et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2018visual/">Visual Question Answering As Reading Comprehension</a> Hui Li, Peng Wang, Chunhua Shen, Anton Van Den Hengel </li>
     
   
     
   
     
       <li> <a href="/publications/helcl2018cuni/">CUNI System For The WMT18 Multimodal Translation Task</a> Jind≈ôich Helcl, Jind≈ôich Libovick√Ω, Du≈°an Vari≈° </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2018dual/">Dual Ask-answer Network For Machine Reading Comprehension</a> Han Xiao, Feng Wang, Jianfeng Yan, Jingyao Zheng </li>
     
   
     
   
     
       <li> <a href="/publications/liu2018context/">Context-aware Visual Policy Network For Sequence-level Image Captioning</a> Daqing Liu, Zheng-jun Zha, Hanwang Zhang, Yongdong Zhang, Feng Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2018vision/">Vision-based Navigation With Language-based Assistance Via Imitation Learning With Indirect Intervention</a> Khanh Nguyen, Debadeepta Dey, Chris Brockett, Bill Dolan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/niu2018recursive/">Recursive Visual Attention In Visual Dialog</a> Yulei Niu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vu2018grounded/">Grounded Textual Entailment</a> Hoa Trong Vu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2018faithful/">Faithful Multimodal Explanation For Visual Question Answering</a> Jialin Wu, Raymond J. Mooney </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2019multi/">Multi-modality Latent Interaction Network For Visual Question Answering</a> Peng Gao, Haoxuan You, Zhanpeng Zhang, Xiaogang Wang, Hongsheng Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2019iterative/">Iterative Answer Prediction With Pointer-augmented Multimodal Transformers For Textvqa</a> Ronghang Hu, Amanpreet Singh, Trevor Darrell, Marcus Rohrbach </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fayek2019temporal/">Temporal Reasoning Via Audio Question Answering</a> Haytham M. Fayek, Justin Johnson </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2019vl/">VL-BERT: Pre-training Of Generic Visual-linguistic Representations</a> Weijie Su et al. </li>
     
   
     
       <li> <a href="/publications/sulubacak2019multimodal/">Multimodal Machine Translation Through Visuals And Speech</a> Umut Sulubacak et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/le2019multimodal/">Multimodal Transformer Networks For End-to-end Video-grounded Dialogue Systems</a> Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019universal/">UNITER: Universal Image-text Representation Learning</a> Yen-chun Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2019multimodal/">Multimodal Unified Attention Networks For Vision-and-language Interactions</a> Zhou Yu, Yuhao Cui, Jun Yu, Dacheng Tao, Qi Tian </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/magassouba2019understanding/">Understanding Natural Language Instructions For Fetching Daily Objects Using Gan-based Multimodal Target-source Classification</a> Aly Magassouba, Komei Sugiura, Anh Trinh Quoc, Hisashi Kawai </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2019transferable/">Transferable Representation Learning In Vision-and-language Navigation</a> Haoshuo Huang et al. </li>
     
   
     
       <li> <a href="/publications/nguyen2019visual/">Help, Anna! Visual Navigation With Natural Multimodal Assistance Via Retrospective Curiosity-encouraging Imitation Learning</a> Khanh Nguyen, Hal Iii Daum√© </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alberti2019fusion/">Fusion Of Detected Objects In Text For Visual Question Answering</a> Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/caglayan2019probing/">Probing The Need For Visual Context In Multimodal Machine Translation</a> Ozan Caglayan, Pranava Madhyastha, Lucia Specia, Lo√Øc Barrault </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2019heterogeneous/">Heterogeneous Memory Enhanced Multimodal Attention Model For Video Question Answering</a> Chenyou Fan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2019learning/">LXMERT: Learning Cross-modality Encoder Representations From Transformers</a> Hao Tan, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahuja2019natural/">Language2pose: Natural Language Grounded Pose Forecasting</a> Chaitanya Ahuja, Louis-philippe Morency </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kottur2019clevr/">Clevr-dialog: A Diagnostic Dataset For Multi-round Reasoning In Visual Dialog</a> Satwik Kottur, Jos√© M. F. Moura, Devi Parikh, Dhruv Batra, Marcus Rohrbach </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2019vision/">Vision-language Navigation With Self-supervised Auxiliary Reasoning Tasks</a> Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/landi2019multimodal/">Multimodal Attention Networks For Low-level Vision-and-language Navigation</a> Federico Landi, Lorenzo Baraldi, Marcella Cornia, Massimiliano Corsini, Rita Cucchiara </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019dual/">DMRM: A Dual-channel Multi-hop Reasoning Model For Visual Dialog</a> Feilong Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cadene2019multimodal/">MUREL: Multimodal Relational Reasoning For Visual Question Answering</a> Remi Cadene, Hedi Ben-younes, Matthieu Cord, Nicolas Thome </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/murahari2019large/">Large-scale Pretraining For Visual Dialog: A Simple State-of-the-art Baseline</a> Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2019unified/">Unified Vision-language Pre-training For Image Captioning And VQA</a> Luowei Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019unicoder/">Unicoder-vl: A Universal Encoder For Vision And Language By Cross-modal Pre-training</a> Gen Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2019making/">Making History Matter: History-advantage Sequence Training For Visual Dialog</a> Tianhao Yang, Zheng-jun Zha, Hanwang Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2019visual/">Visual Entailment: A Novel Task For Fine-grained Image Understanding</a> Ning Xie, Farley Lai, Derek Doran, Asim Kadav </li>
     
   
     
   
     
       <li> <a href="/publications/ive2019distilling/">Distilling Translations With Visual Awareness</a> Julia Ive, Pranava Madhyastha, Lucia Specia </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2020kvl/">KVL-BERT: Knowledge Enhanced Visual-and-linguistic BERT For Visual Commonsense Reasoning</a> Dandan Song, Siyi Ma, Zhanchen Sun, Sicheng Yang, Lejian Liao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shuster2020multi/">Multi-modal Open-domain Dialogue</a> Kurt Shuster, Eric Michael Smith, Da Ju, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020multilingual/">Multilingual Speech Translation With Efficient Finetuning Of Pretrained Models</a> Xian Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020bridging/">Bridging Text And Video: A Universal Multimodal Transformer For Video-audio Scene-aware Dialog</a> Zekang Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bugliarello2020multimodal/">Multimodal Pretraining Unmasked: A Meta-analysis And A Unified Framework Of Vision-and-language Berts</a> Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2020cross/">Imagebert: Cross-modal Pre-training With Large-scale Weak-supervised Image-text Data</a> Di Qi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020efficient/">Efficient Object-level Visual Context Modeling For Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding</a> Dexin Wang, Deyi Xiong </li>
     
   
     
       <li> <a href="/publications/cao2020behind/">Behind The Scene: Revealing The Secrets Of Pre-trained Vision-and-language Models</a> Jize Cao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mao2020exploring/">Dialoguetrm: Exploring The Intra- And Inter-modal Emotional Behaviors In The Conversation</a> Yuzhao Mao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2020text/">TAP: Text-aware Pre-training For Text-vqa And Text-caption</a> Zhengyuan Yang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2020vision/">Vision-dialog Navigation By Exploring Cross-modal Memory</a> Yi Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020confidence/">Confidence-aware Non-repetitive Multimodal Transformers For Textcaps</a> Zhaokai Wang, Renda Bao, Qi Wu, Si Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2020auto/">Auto-captions On GIF: A Large-scale Video-sentence Dataset For Vision-language Pre-training</a> Yingwei Pan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/long2020generative/">Generative Imagination Elevates Machine Translation</a> Quanyu Long, Mingxuan Wang, Lei Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meng2020large/">Openvidial: A Large-scale, Open-domain Dialogue Dataset With Visual Contexts</a> Yuxian Meng et al. </li>
     
   
     
       <li> <a href="/publications/chen2020learning/">Learning Modality Interaction For Temporal Sentence Localization And Event Captioning In Videos</a> Shaoxiang Chen, Wenhao Jiang, Wei Liu, Yu-gang Jiang </li>
     
   
     
       <li> <a href="/publications/zhu2020enhance/">Enhance Multimodal Transformer With External Label And In-domain Pretrain: Hateful Meme Challenge Winning Solution</a> Ron Zhu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parcalabescu2020seeing/">Seeing Past Words: Testing The Cross-modal Capabilities Of Pretrained V&L Models On Counting Tasks</a> Letitia Parcalabescu, Albert Gatt, Anette Frank, Iacer Calixto </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2020cross/">Cross-modality Relevance For Reasoning On Language And Vision</a> Chen Zheng, Quan Guo, Parisa Kordjamshidi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020object/">Oscar: Object-semantics Aligned Pre-training For Vision-language Tasks</a> Xiujun Li et al. </li>
     
   
     
       <li> <a href="/publications/rust2020how/">How Good Is Your Tokenizer? On The Monolingual Performance Of Multilingual Language Models</a> Phillip Rust, Jonas Pfeiffer, Ivan Vuliƒá, Sebastian Ruder, Iryna Gurevych </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chu2020multi/">Multi-step Joint-modality Attention Network For Scene-aware Dialogue System</a> Yun-wei Chu, Kuan-yen Lin, Chao-chun Hsu, Lun-wei Ku </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/seo2020look/">Look Before You Speak: Visually Contextualized Utterances</a> Paul Hongsuck Seo, Arsha Nagrani, Cordelia Schmid </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020text/">TIME: Text And Image Mutual-translation Adversarial Networks</a> Bingchen Liu, Kunpeng Song, Yizhe Zhu, Gerard De Melo, Ahmed Elgammal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2020multi/">Layoutlmv2: Multi-modal Pre-training For Visually-rich Document Understanding</a> Yang Xu et al. </li>
     
   
     
       <li> <a href="/publications/li2020widget/">Widget Captioning: Generating Natural Language Description For Mobile User Interface Elements</a> Yang Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020closer/">A Closer Look At The Robustness Of Vision-and-language Pre-trained Models</a> Linjie Li, Zhe Gan, Jingjing Liu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2020just/">Just Ask: Learning To Answer Questions From Millions Of Narrated Videos</a> Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2020unsupervised/">Unsupervised Multimodal Neural Machine Translation With Pseudo Visual Pivoting</a> Po-yao Huang, Junjie Hu, Xiaojun Chang, Alexander Hauptmann </li>
     
   
     
   
     
       <li> <a href="/publications/li2020unsupervised/">Unsupervised Vision-and-language Pre-training Without Parallel Images And Captions</a> Liunian Harold Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moon2020situated/">Situated And Interactive Multimodal Conversations</a> Seungwhan Moon et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singh2020are/">Are We Pretraining It Right? Digging Deeper Into Visio-linguistic Pretraining</a> Amanpreet Singh, Vedanuj Goswami, Devi Parikh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kant2020spatially/">Spatially Aware Multimodal Transformers For Textvqa</a> Yash Kant et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xia2020cross/">XGPT: Cross-modal Generative Pre-training For Image Captioning</a> Qiaolin Xia et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020vd/">VD-BERT: A Unified Vision And Dialog Transformer With BERT</a> Yue Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rahman2020improved/">An Improved Attention For Visual Question Answering</a> Tanzila Rahman, Shih-han Chou, Leonid Sigal, Giuseppe Carenini </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/do2020e/">E-snli-ve: Corrected Visual-textual Entailment With Natural Language Explanations</a> Virginie Do, Oana-maria Camburu, Zeynep Akata, Thomas Lukasiewicz </li>
     
   
     
       <li> <a href="/publications/cho2020x/">X-LXMERT: Paint, Caption And Answer Questions With Multi-modal Transformers</a> Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2020multimodal/">DSTC8-AVSD: Multimodal Semantic Transformer Network With Retrieval Style Word Generator</a> Hwanhee Lee et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/le2020bi/">Bist: Bi-directional Spatio-temporal Reasoning For Video-grounded Dialogues</a> Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020towards/">UNIMO: Towards Unified-modal Understanding And Generation Via Cross-modal Contrastive Learning</a> Wei Li et al. </li>
     
   
     
       <li> <a href="/publications/hong2020recurrent/">A Recurrent Vision-and-language BERT For Navigation</a> Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-opazo, Stephen Gould </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020comparison/">A Comparison Of Pre-trained Vision-and-language Models For Multimodal Representation Learning Across Medical Images And Reports</a> Yikuan Li, Hanyin Wang, Yuan Luo </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2020towards/">Towards Learning A Generic Agent For Vision-and-language Navigation Via Pre-training</a> Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao </li>
     
   
     
   
     
       <li> <a href="/publications/ni2020learning/">M3P: Learning Universal Representations Via Multitask Multilingual Multimodal Pre-training</a> Minheng Ni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2020improving/">Vokenization: Improving Language Understanding With Contextualized, Visual-grounded Supervision</a> Hao Tan, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2021unified/">Vlmo: Unified Vision-language Pre-training With Mixture-of-modality-experts</a> Hangbo Bao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021end/">E2E-VLP: End-to-end Vision-language Pre-training Enhanced By Visual Learning</a> Haiyang Xu et al. </li>
     
   
     
       <li> <a href="/publications/zhang2021ernie/">Ernie-vilg: Unified Generative Pre-training For Bidirectional Vision-language Generation</a> Han Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2021automatic/">Newsclippings: Automatic Generation Of Out-of-context Multimodal Media</a> Grace Luo, Trevor Darrell, Anna Rohrbach </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2021good/">A Good Prompt Is Worth Millions Of Parameters: Low-resource Prompt-based Learning For Vision-language Models</a> Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, Xiang Ren </li>
     
   
     
       <li> <a href="/publications/zhang2021tip/">Tip-adapter: Training-free Clip-adapter For Better Vision-language Modeling</a> Renrui Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mu2021self/">SLIP: Self-supervision Meets Language-image Pre-training</a> Norman Mu, Alexander Kirillov, David Wagner, Saining Xie </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021align/">Align And Prompt: Video-and-language Pre-training With Entity Prompts</a> Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021scene/">Scene-intuitive Agent For Remote Embodied Visual Grounding</a> Xiangru Lin, Guanbin Li, Yizhou Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021good/">Good For Misconceived Reasons: An Empirical Revisiting On The Need For Visual Context In Multimodal Machine Translation</a> Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2021causal/">Causal Attention For Vision-language Tasks</a> Xu Yang, Hanwang Zhang, Guojun Qi, Jianfei Cai </li>
     
   
     
   
     
       <li> <a href="/publications/eichenberg2021magma/">MAGMA -- Multimodal Augmentation Of Generative Models Through Adapter-based Finetuning</a> Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, Anette Frank </li>
     
   
     
       <li> <a href="/publications/liu2021omni/">OPT: Omni-perception Pre-trainer For Cross-modal Understanding And Generation</a> Jing Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2021automatic/">DOC2PPT: Automatic Presentation Slides Generation From Scientific Documents</a> Tsu-jui Fu, William Yang Wang, Daniel Mcduff, Yale Song </li>
     
   
     
   
     
       <li> <a href="/publications/ding2021mastering/">Cogview: Mastering Text-to-image Generation Via Transformers</a> Ming Ding et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021unified/">UFO: A Unified Transformer For Vision-language Representation Learning</a> Jianfeng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021simple/">Simvlm: Simple Visual Language Model Pretraining With Weak Supervision</a> Zirui Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2021universal/">UC2: Universal Cross-lingual Cross-modal Vision-and-language Pre-training</a> Mingyang Zhou et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cho2021unifying/">Unifying Vision-and-language Tasks Via Text Generation</a> Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kottur2021simmc/">SIMMC 2.0: A Task-oriented Dialog Dataset For Immersive Multimodal Conversations</a> Satwik Kottur, Seungwhan Moon, Alborz Geramifard, Babak Damavandi </li>
     
   
     
   
     
       <li> <a href="/publications/yang2021unifying/">Unitab: Unifying Text And Box Outputs For Grounded Vision-language Modeling</a> Zhengyuan Yang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021multimodal/">Layoutxlm: Multimodal Pre-training For Multilingual Visually-rich Document Understanding</a> Yiheng Xu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2021diagnosing/">Diagnosing Vision-and-language Navigation: What Really Matters</a> Wanrong Zhu et al. </li>
     
   
     
       <li> <a href="/publications/xu2021task/">VLM: Task-agnostic Video-language Model Pre-training For Video Understanding</a> Hu Xu et al. </li>
     
   
     
       <li> <a href="/publications/xue2021advancing/">Advancing High-resolution Video-language Representation With Large-scale Video Transcriptions</a> Hongwei Xue et al. </li>
     
   
     
   
     
       <li> <a href="/publications/rothe2021simple/">A Simple Recipe For Multilingual Grammatical Error Correction</a> Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause, Aliaksei Severyn </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2021colorful/">CPT: Colorful Prompt Tuning For Pre-trained Vision-language Models</a> Yuan Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021chinese/">M6: A Chinese Multimodal Pretrainer</a> Junyang Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huo2021bridging/">Wenlan: Bridging Vision And Language By Large-scale Multi-modal Pre-training</a> Yuqi Huo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moon2021multi/">Multi-modal Understanding And Generation For Medical Images And Text Via Vision-language Pre-training</a> Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-hak Kim, Edward Choi </li>
     
   
     
       <li> <a href="/publications/chang2021multihop/">Webqa: Multihop And Multimodal QA</a> Yingshan Chang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dou2021empirical/">An Empirical Study Of Training End-to-end Vision-and-language Transformers</a> Zi-yi Dou et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xing2021km/">KM-BART: Knowledge Enhanced Multimodal BART For Visual Commonsense Generation</a> Yiran Xing et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zellers2021multimodal/">MERLOT: Multimodal Neural Script Knowledge Models</a> Rowan Zellers et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2021vision/">Vision Guided Generative Pre-trained Language Models For Multimodal Abstractive Summarization</a> Tiezheng Yu, Wenliang Dai, Zihan Liu, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2021learning/">Learning To Prompt For Vision-language Models</a> Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frank2021vision/">Vision-and-language Or Vision-for-language? On Cross-modal Influence In Multimodal Transformers</a> Stella Frank, Emanuele Bugliarello, Desmond Elliott </li>
     
   
     
   
     
       <li> <a href="/publications/yang2021empirical/">An Empirical Study Of GPT-3 For Few-shot Knowledge-based VQA</a> Zhengyuan Yang et al. </li>
     
   
     
       <li> <a href="/publications/huang2021seeing/">Seeing Out Of The Box: End-to-end Pre-training For Vision-language Representation Learning</a> Zhicheng Huang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dancette2021beyond/">Beyond Question-based Biases: Assessing Multimodal Shortcut Learning In Visual Question Answering</a> Corentin Dancette, Remi Cadene, Damien Teney, Matthieu Cord </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021scene/">SGEITL: Scene Graph Enhanced Image-text Learning For Visual Commonsense Reasoning</a> Zhecan Wang et al. </li>
     
   
     
       <li> <a href="/publications/sun2021pre/">Lightningdot: Pre-training Visual-semantic Embeddings For Real-time Image-text Retrieval</a> Siqi Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021multimodal/">Multimodal Transformer With Variable-length Memory For Vision-and-language Navigation</a> Chuang Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021scheduled/">Scheduled Sampling In Vision-language Pretraining With Decoupled Encoder-decoder Network</a> Yehao Li, Yingwei Pan, Ting Yao, Jingwen Chen, Tao Mei </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2021new/">Iconqa: A New Benchmark For Abstract Diagram Understanding And Visual Language Reasoning</a> Pan Lu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021visual/">N\"UWA: Visual Synthesis Pre-training For Neural Visual World Creation</a> Chenfei Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021automatic/">Screen2words: Automatic Mobile UI Summarization With Multimodal Learning</a> Bryan Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2021clip/">Clip-adapter: Better Vision-language Models With Feature Adapters</a> Peng Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021history/">History Aware Multimodal Transformer For Vision-and-language Navigation</a> Shizhe Chen, Pierre-louis Guhur, Cordelia Schmid, Ivan Laptev </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2021contrastive/">Contrastive Vision-language Pre-training With Limited Resources</a> Quan Cui et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2021enhancing/">ROSITA: Enhancing Vision-and-language Semantic Alignments Via Cross- And Intra-modal Knowledge Integration</a> Yuhao Cui et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salaberria2021image/">Image Captioning For Effective Use Of Language Models In Knowledge-based Visual Question Answering</a> Ander Salaberria, Gorka Azkune, Oier Lopez De Lacalle, Aitor Soroa, Eneko Agirre </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/min2021following/">FILM: Following Instructions In Language With Modular Methods</a> So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, Ruslan Salakhutdinov </li>
     
   
     
   
     
       <li> <a href="/publications/singh2021foundational/">FLAVA: A Foundational Language And Vision Alignment Model</a> Amanpreet Singh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/talmor2021complex/">Multimodalqa: Complex Question Answering Over Text, Tables And Images</a> Alon Talmor et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/biten2021layout/">Latr: Layout-aware Transformer For Scene-text VQA</a> Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, R. Manmatha </li>
     
   
     
   
     
       <li> <a href="/publications/pashevich2021episodic/">Episodic Transformer For Vision-and-language Navigation</a> Alexander Pashevich, Cordelia Schmid, Chen Sun </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khare2021multimodal/">MMBERT: Multimodal BERT Pretraining For Improved Medical VQA</a> Yash Khare et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2021multimodal/">Multimodal Dialogue Response Generation</a> Qingfeng Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/atri2021leveraging/">See, Hear, Read: Leveraging Multimodality With Guided Attention For Abstractive Text Summarization</a> Yash Kumar Atri, Shraman Pramanick, Vikram Goyal, Tanmoy Chakraborty </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qiu2021vt/">VT-CLIP: Enhancing Vision-language Models With Visual-guided Texts</a> Longtian Qiu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gui2021knowledge/">KAT: A Knowledge Augmented Transformer For Vision-and-language</a> Liangke Gui et al. </li>
     
   
     
       <li> <a href="/publications/huang2021unifying/">Unifying Multimodal Transformer For Bi-directional Image And Text Generation</a> Yupan Huang, Hongwei Xue, Bei Liu, Yutong Lu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2021fine/">FILIP: Fine-grained Interactive Language-image Pre-training</a> Lewei Yao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/eslami2021does/">Does CLIP Benefit Visual Question Answering In The Medical Domain As Much As It Does In The General Domain?</a> Sedigheh Eslami, Gerard De Melo, Christoph Meinel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tsimpoukelli2021multimodal/">Multimodal Few-shot Learning With Frozen Language Models</a> Maria Tsimpoukelli et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kayser2021e/">E-vil: A Dataset And Benchmark For Natural Language Explanations In Vision-language Tasks</a> Maxime Kayser et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yao2022position/">PEVL: Position-enhanced Pre-training And Prompt Tuning For Vision-language Models</a> Yuan Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022think/">Think Global, Act Local: Dual-scale Graph Transformer For Vision-and-language Navigation</a> Shizhe Chen, Pierre-louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev </li>
     
   
     
       <li> <a href="/publications/lee2022screenshot/">Pix2struct: Screenshot Parsing As Pretraining For Visual Language Understanding</a> Kenton Lee et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhao2022vl/">Vl-checklist: Evaluating Pre-trained Vision-language Models With Objects, Attributes And Relations</a> Tiancheng Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xing2022dual/">Dual Modality Prompt Tuning For Vision-language Pre-trained Model</a> Yinghui Xing et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2022conditional/">Conditional Prompt Learning For Vision-language Models</a> Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu </li>
     
   
     
       <li> <a href="/publications/crowson2022vqgan/">VQGAN-CLIP: Open Domain Image Generation And Editing With Natural Language Guidance</a> Katherine Crowson et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022foundation/">Omnivl:one Foundation Model For Image-language And Video-language Tasks</a> Junke Wang et al. </li>
     
   
     
       <li> <a href="/publications/li2022bootstrapping/">BLIP: Bootstrapping Language-image Pre-training For Unified Vision-language Understanding And Generation</a> Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi </li>
     
   
     
   
     
       <li> <a href="/publications/su2022language/">Language Models Can See: Plugging Visual Controls In Text Generation</a> Yixuan Su et al. </li>
     
   
     
       <li> <a href="/publications/li2022fine/">Fine-grained Semantically Aligned Vision-language Pre-training</a> Juncheng Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2022multimodal/">Multimodal Knowledge Alignment With Reinforcement Learning</a> Youngjae Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2022vision/">Vision-language Pre-training With Triple Contrastive Learning</a> Jinyu Yang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thrush2022probing/">Winoground: Probing Vision And Language Models For Visio-linguistic Compositionality</a> Tristan Thrush et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022generative/">GIT: A Generative Image-to-text Transformer For Vision And Language</a> Jianfeng Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yu2022contrastive/">Coca: Contrastive Captioners Are Image-text Foundation Models</a> Jiahui Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2022shifting/">Shifting More Attention To Visual Backbone: Query-modulated Refinement Networks For End-to-end Visual Grounding</a> Jiabo Ye et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022end/">End-to-end Transformer Based Model For Image Captioning</a> Yiyu Wang, Jungang Xu, Yingfei Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alayrac2022visual/">Flamingo: A Visual Language Model For Few-shot Learning</a> Jean-baptiste Alayrac et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singer2022make/">Make-a-video: Text-to-video Generation Without Text-video Data</a> Uriel Singer et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cho2022dall/">Dall-eval: Probing The Reasoning Skills And Social Biases Of Text-to-image Generation Models</a> Jaemin Cho, Abhay Zala, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/merullo2022linearly/">Linearly Mapping From Image To Text Space</a> Jack Merullo, Louis Castricato, Carsten Eickhoff, Ellie Pavlick </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022vision/">VLT: Vision-language Transformer And Query Generation For Referring Segmentation</a> Henghui Ding, Chang Liu, Suchen Wang, Xudong Jiang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022collaborative/">COTS: Collaborative Two-stream Vision-language Pre-training Model For Cross-modal Retrieval</a> Haoyu Lu et al. </li>
     
   
     
       <li> <a href="/publications/song2022clip/">CLIP Models Are Few-shot Learners: Empirical Studies On VQA And Visual Entailment</a> Haoyu Song, Li Dong, Wei-nan Zhang, Ting Liu, Furu Wei </li>
     
   
     
       <li> <a href="/publications/du2022survey/">A Survey Of Vision-language Pre-trained Models</a> Yifan Du, Zikang Liu, Junyi Li, Wayne Xin Zhao </li>
     
   
     
       <li> <a href="/publications/jiang2022pseudo/">Pseudo-q: Generating Pseudo Language Queries For Visual Grounding</a> Haojun Jiang, Yuanze Lin, Dongchen Han, Shiji Song, Gao Huang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2022prompt/">Prompt Tuning For Generative Multimodal Pretrained Models</a> Hao Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2022vl/">Vl-beit: Generative Vision-language Pretraining</a> Hangbo Bao, Wenhui Wang, Li Dong, Furu Wei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022multimodal/">Murag: Multimodal Retrieval-augmented Generator For Open Question Answering Over Images And Text</a> Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2022enabling/">Enabling Multimodal Generation On CLIP Via Vision-language Knowledge Distillation</a> Wenliang Dai et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022uni/">Uni-perceiver V2: A Generalist Model For Large-scale Vision And Vision-language Tasks</a> Hao Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022vision/">Vision-language Intelligence: Tasks, Representation Learning, And Large Models</a> Feng Li et al. </li>
     
   
     
       <li> <a href="/publications/sammani2022nlx/">NLX-GPT: A Model For Natural Language Explanations In Vision And Vision-language Tasks</a> Fawaz Sammani, Tanmoy Mukherjee, Nikos Deligiannis </li>
     
   
     
   
     
       <li> <a href="/publications/cheng2022recipe/">Vindlu: A Recipe For Effective Video-and-language Pretraining</a> Feng Cheng et al. </li>
     
   
     
       <li> <a href="/publications/liu2022one/">Deplot: One-shot Visual Language Reasoning By Plot-to-table Translation</a> Fangyu Liu et al. </li>
     
   
     
       <li> <a href="/publications/peng2022sgva/">Sgva-clip: Semantic-guided Visual Adapting Of Vision-language Models For Few-shot Image Classification</a> Fang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang, Changsheng Xu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022hybrid/">Hybrid Transformer With Multi-level Fusion For Multimodal Knowledge Graph Completion</a> Xiang Chen et al. </li>
     
   
     
       <li> <a href="/publications/aflalo2022vl/">Vl-interpret: An Interactive Visualization Tool For Interpreting Vision-language Transformers</a> Estelle Aflalo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bugliarello2022benchmark/">IGLUE: A Benchmark For Transfer Learning Across Modalities, Tasks, And Languages</a> Emanuele Bugliarello et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022jointly/">Pali: A Jointly-scaled Multilingual Language-image Model</a> Xi Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/schwenk2022benchmark/">A-OKVQA: A Benchmark For Visual Question Answering Using World Knowledge</a> Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022library/">LAVIS: A Library For Language-vision Intelligence</a> Dongxu Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2022building/">Bridgetower: Building Bridges Between Encoders In Vision-language Representation Learning</a> Xiao Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dong2022masked/">Maskclip: Masked Self-distillation Advances Contrastive Language-image Pretraining</a> Xiaoyi Dong et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2022graph/">Greaselm: Graph Reasoning Enhanced Language Models For Question Answering</a> Xikun Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2022new/">Mixgen: A New Multi-modal Data Augmentation</a> Xiaoshuai Hao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022effective/">Mplug: Effective And Efficient Vision-language Learning By Cross-modal Skip-connections</a> Chenliang Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2022understanding/">Understanding And Mitigating Overfitting In Prompt Tuning For Vision-language Models</a> Chengcheng Ma et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liang2022visual/">Visual-language Navigation Pretraining Via Prompt-based Environmental Self-exploration</a> Xiwen Liang, Fengda Zhu, Lingling Li, Hang Xu, Xiaodan Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2022long/">Long-form Video-language Pre-training With Multimodal Temporal Contrastive Learning</a> Yuchong Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2022vision/">ADAPT: Vision-language Navigation With Modality-aligned Action Prompts</a> Bingqian Lin et al. </li>
     
   
     
       <li> <a href="/publications/chen2022altering/">Altclip: Altering The Language Encoder In CLIP For Extended Language Capabilities</a> Zhongzhi Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2022prompt/">Prompt-aligned Gradient For Prompt Tuning</a> Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, Hanwang Zhang </li>
     
   
     
   
     
       <li> <a href="/publications/yao2022end/">End-to-end Multimodal Fact-checking And Explanation Generation: A Challenging Dataset And Models</a> Barry Menglong Virginia Tech Yao, Aditya Virginia Tech Shah, Lichao Lehigh University Sun, Jin-hee Virginia Tech Cho, Lifu Virginia Tech Huang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022multimodal/">Mukea: Multimodal Knowledge Extraction And Accumulation For Knowledge-based Visual Question Answering</a> Yang Ding et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ling2022vision/">Vision-language Pre-training For Multimodal Aspect-based Sentiment Analysis</a> Yan Ling, Jianfei Yu, Rui Xia </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2022pre/">Layoutlmv3: Pre-training For Document AI With Unified Text And Image Masking</a> Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022prompt/">Prompt Distribution Learning</a> Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, Xinmei Tian </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2022unified/">A Unified End-to-end Retriever-reader Framework For Knowledge-based VQA</a> Yangyang Guo et al. </li>
     
   
     
       <li> <a href="/publications/tiong2022plug/">Plug-and-play VQA: Zero-shot VQA By Conjoining Large Pretrained Models With Zero Training</a> Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven C. H. Hoi </li>
     
   
     
       <li> <a href="/publications/bapna2022massively/">Mslam: Massively Multilingual Joint Pre-training For Speech And Text</a> Ankur Bapna et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2022socratic/">Socratic Models: Composing Zero-shot Multimodal Reasoning With Language</a> Andy Zeng et al. </li>
     
   
     
       <li> <a href="/publications/li2022scaling/">Scaling Language-image Pre-training Via Masking</a> Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, Kaiming He </li>
     
   
     
       <li> <a href="/publications/jiang2022general/">VIMA: General Robot Manipulation With Multimodal Prompts</a> Yunfan Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/burns2022dataset/">A Dataset For Interactive Vision-language Navigation With Unknown Command Feasibility</a> Andrea Burns et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dou2022coarse/">Coarse-to-fine Vision-language Pre-training With Fusion In The Backbone</a> Zi-yi Dou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2022language/">Language Models Are General-purpose Interfaces</a> Yaru Hao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zang2022unified/">Unified Vision And Language Prompt Learning</a> Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, Chen Change Loy </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022position/">Position-guided Text Prompt For Vision-language Pre-training</a> Alex Jinpeng Wang, Pan Zhou, Mike Zheng Shou, Shuicheng Yan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022image/">Image As A Foreign Language: Beit Pretraining For All Vision And Vision-language Tasks</a> Wenhui Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022unifying/">OFA: Unifying Architectures, Tasks, And Modalities Through A Simple Sequence-to-sequence Learning Framework</a> Peng Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/buch2022revisiting/">Revisiting The "video" In Video-language Understanding</a> Shyamal Buch et al. </li>
     
   
     
   
     
       <li> <a href="/publications/schramowski2022can/">Can Machines Help Us Answering Question 16 In Datasheets, And In Turn Reflecting On Inappropriate Content?</a> Patrick Schramowski, Christopher Tauchmann, Kristian Kersting </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022learn/">Learn To Explain: Multimodal Reasoning Via Thought Chains For Science Question Answering</a> Pan Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mees2022what/">What Matters In Language Conditioned Robotic Imitation Learning Over Unstructured Data</a> Oier Mees, Lukas Hermann, Wolfram Burgard </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cohen2022is/">"this Is My Unicorn, Fluffy": Personalizing Frozen Vision-language Representations</a> Niv Cohen, Rinon Gal, Eli A. Meirom, Gal Chechik, Yuval Atzmon </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nayak2022learning/">Learning To Compose Soft Prompts For Compositional Zero-shot Learning</a> Nihal V. Nayak, Peilin Yu, Stephen H. Bach </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khattak2022multi/">Maple: Multi-modal Prompt Learning</a> Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2022unsupervised/">Unsupervised Vision-and-language Pre-training Via Retrieval-based Multi-granular Alignment</a> Mingyang Zhou et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022faster/">Cogview2: Faster And Better Text-to-image Generation Via Hierarchical Transformers</a> Ming Ding, Wendi Zheng, Wenyi Hong, Jie Tang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yasunaga2022retrieval/">Retrieval-augmented Multimodal Language Modeling</a> Michihiro Yasunaga et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ma2022can/">CREPE: Can Vision-language Foundation Models Reason Compositionally?</a> Zixian Ma et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gan2022vision/">Vision-language Pre-training: Basics, Recent Advances, And Future Trends</a> Zhe Gan et al. </li>
     
   
     
       <li> <a href="/publications/yuksekgonul2022when/">When And Why Vision-language Models Behave Like Bags-of-words, And What To Do About It?</a> Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James Zou </li>
     
   
     
       <li> <a href="/publications/tschannen2022image/">CLIPPO: Image-and-language Understanding From Pixels Only</a> Michael Tschannen, Basil Mustafa, Neil Houlsby </li>
     
   
     
   
     
       <li> <a href="/publications/qu2022simple/">Siri: A Simple Selective Retraining Mechanism For Transformer-based Visual Grounding</a> Mengxue Qu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jia2022mner/">MNER-QG: An End-to-end MRC Framework For Multimodal Named Entity Recognition With Query Grounding</a> Meihuizi Jia et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gupta2022visual/">Visual Programming: Compositional Visual Reasoning Without Training</a> Tanmay Gupta, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shu2022test/">Test-time Prompt Tuning For Zero-shot Generalization In Vision-language Models</a> Manli Shu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/srinivasan2022continual/">Climb: A Continual Learning Benchmark For Vision-and-language Tasks</a> Tejas Srinivasan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wei2022multimodality/">MVP: Multimodality-guided Visual Pre-training</a> Longhui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, Qi Tian </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2022task/">Task Residual For Tuning Vision-language Models</a> Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, Xinchao Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2022retrieval/">REVEAL: Retrieval-augmented Visual-language Pre-training With Multi-source Multimodal Knowledge Memory</a> Ziniu Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2022robotic/">Robotic Skill Acquisition Via Instruction Augmentation With Vision-language Models</a> Ted Xiao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ravi2022vlc/">VLC-BERT: Visual Question Answering With Contextualized Commonsense Knowledge</a> Sahithya Ravi, Aditya Chinchure, Leonid Sigal, Renjie Liao, Vered Shwartz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zellers2022merlot/">MERLOT Reserve: Neural Script Knowledge Through Vision And Language And Sound</a> Rowan Zellers et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2022multitask/">Multitask Vision-language Prompt Tuning</a> Sheng Shen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2022neural/">Neural Machine Translation With Phrase-level Universal Visual Representations</a> Qingkai Fang, Yang Feng </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022clip/">CLIP-TD: CLIP Targeted Distillation For Vision-language Tasks</a> Zhecan Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/guo2022texts/">Texts As Images In Prompt Tuning For Multi-label Image Recognition</a> Zixian Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/goel2022cyclic/">Cyclip: Cyclic Contrastive Language-image Pretraining</a> Shashank Goel et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023retrieving/">Retrieving Multimodal Information For Augmented Generation: A Survey</a> Ruochen Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023teaching/">Gpt4tools: Teaching Large Language Model To Use Tools Via Self-instruction</a> Rui Yang et al. </li>
     
   
     
       <li> <a href="/publications/cao2023pro/">Pro-cap: Leveraging A Frozen Vision-language Model For Hateful Meme Detection</a> Rui Cao et al. </li>
     
   
     
       <li> <a href="/publications/cao2023prompting/">Prompting For Multimodal Hateful Meme Classification</a> Rui Cao, Roy Ka-wei Lee, Wen-haw Chong, Jing Jiang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023understanding/">Audiogpt: Understanding And Generating Speech, Music, Sound, And Talking Head</a> Rongjie Huang et al. </li>
     
   
     
       <li> <a href="/publications/huang2023make/">Make-an-audio: Text-to-audio Generation With Prompt-enhanced Diffusion Models</a> Rongjie Huang et al. </li>
     
   
     
       <li> <a href="/publications/paiss2023teaching/">Teaching CLIP To Count To Ten</a> Roni Paiss et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ramos2023retrieval/">Retrieval-augmented Image Captioning</a> Rita Ramos, Desmond Elliott, Bruno Martins </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schumann2023verbalization/">VELMA: Verbalization Embodiment Of LLM Agents For Vision And Language Navigation In Street View</a> Raphael Schumann et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/communication2023multilingual/">Seamless: Multilingual Expressive And Streaming Speech Translation</a> Seamless Communication et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023next/">Next-gpt: Any-to-any Multimodal LLM</a> Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-seng Chua </li>
     
   
     
   
     
       <li> <a href="/publications/shen2023scaling/">Scaling Vision-language Models With Sparse Mixture Of Experts</a> Sheng Shen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023language/">Language Is Not All You Need: Aligning Perception With Language Models</a> Shaohan Huang et al. </li>
     
   
     
       <li> <a href="/publications/geng2023towards/">VIP5: Towards Multimodal Foundation Models For Recommendation</a> Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, Yongfeng Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023joint/">SPHINX: The Joint Mixing Of Weights, Tasks, And Visual Embeddings For Multi-modal Large Language Models</a> Ziyi Lin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gozalobrizuela2023chatgpt/">Chatgpt Is Not All You Need. A State Of The Art Review Of Large Generative AI Models</a> Roberto Gozalo-brizuela, Eduardo C. Garrido-merchan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/marjieh2023large/">Large Language Models Predict Human Sensory Judgments Across Six Modalities</a> Raja Marjieh, Ilia Sucholutsky, Pol Van Rijn, Nori Jacoby, Thomas L. Griffiths </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023mplug/">Mplug-owl: Modularization Empowers Large Language Models With Multimodality</a> Qinghao Ye et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023pre/">3d-vista: Pre-trained Transformer For 3D Vision And Text Alignment</a> Ziyu Zhu et al. </li>
     
   
     
       <li> <a href="/publications/li2023masked/">Masked Vision And Language Pre-training With Unimodal And Multimodal Contrastive Losses For Medical Visual Question Answering</a> Pengfei Li, Gang Liu, Jinlong He, Zixu Zhao, Shenjun Zhong </li>
     
   
     
       <li> <a href="/publications/jin2023chat/">Chat-univi: Unified Visual Representation Empowers Large Language Models With Image And Video Understanding</a> Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, Li Yuan </li>
     
   
     
       <li> <a href="/publications/gao2023llama/">Llama-adapter V2: Parameter-efficient Visual Instruction Model</a> Peng Gao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2023git/">Git-mol: A Multi-modal Large Language Model For Molecular Science With Graph, Image, And Text</a> Pengfei Liu, Yiming Ren, Jun Tao, Zhixiang Ren </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rubenstein2023large/">Audiopalm: A Large Language Model That Can Speak And Listen</a> Paul K. Rubenstein et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023internlm/">Internlm-xcomposer: A Vision-language Large Model For Advanced Text-image Comprehension And Composition</a> Pan Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/openai2023gpt/">GPT-4 Technical Report</a> Openai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rotstein2023leveraging/">Fusecap: Leveraging Large Language Models For Enriched Fused Image Captions</a> Noam Rotstein, David Bensaid, Shaked Brody, Roy Ganz, Ron Kimmel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/carlini2023are/">Are Aligned Neural Networks Adversarially Aligned?</a> Nicholas Carlini et al. </li>
     
   
     
   
     
       <li> <a href="/publications/nguyen2023multimodal/">Openvivqa: Task, Dataset, And Multimodal Fusion Models For Visual Question Answering In Vietnamese</a> Nghia Hieu Nguyen, Duong T. D. Vo, Kiet Van Nguyen, Ngan Luu-thuy Nguyen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gruver2023large/">Large Language Models Are Zero-shot Time Series Forecasters</a> Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moor2023med/">Med-flamingo: A Multimodal Medical Few-shot Learner</a> Michael Moor et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023interpretable/">Drivegpt4: Interpretable End-to-end Autonomous Driving Via Large Language Model</a> Zhenhua Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maniparambil2023enhancing/">Enhancing CLIP With GPT-4: Harnessing Visual Descriptions As Prompts</a> Mayug Maniparambil et al. </li>
     
   
     
       <li> <a href="/publications/ishmam2023from/">From Image To Language: A Critical Analysis Of Visual Question Answering (VQA) Approaches, Challenges, And Opportunities</a> Md Farhan Ishmam, Md Sakib Hossain Shovon, M. F. Mridha, Nilanjan Dey </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parelli2023clip/">Clip-guided Vision-language Pre-training For Question Answering In 3D Scenes</a> Maria Parelli et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singha2023visual/">Applenet: Visual Attention Parameterized Prompt Learning For Few-shot Remote Sensing Image Generalization Using CLIP</a> Mainak Singha, Ankit Jha, Bhupendra Solanki, Shirsha Bose, Biplab Banerjee </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023driving/">Driving With Llms: Fusing Object-level Vector Modality For Explainable Autonomous Driving</a> Long Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yan2023generative/">Generative Artificial Intelligence In Learning Analytics: Contextualising Opportunities And Challenges Through The Learning Analytics Cycle</a> Lixiang Yan, Roberto Martinez-maldonado, Dragan Ga≈°eviƒá </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023scaling/">Scaling Autoregressive Multi-modal Models: Pretraining And Instruction Tuning</a> Lili Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xue2023ulip/">ULIP-2: Towards Scalable Multimodal Pre-training For 3D Understanding</a> Le Xue et al. </li>
     
   
     
       <li> <a href="/publications/seenivasan2023end/">Surgicalgpt: End-to-end Language-vision GPT For Visual Question Answering In Surgery</a> Lalithkumar Seenivasan, Mobarakol Islam, Gokul Kannan, Hongliang Ren </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maaz2023video/">Video-chatgpt: Towards Detailed Video Understanding Via Large Vision And Language Models</a> Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yin2023language/">LAMM: Language-assisted Multi-modal Instruction-tuning Dataset, Framework, And Benchmark</a> Zhenfei Yin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kuckreja2023grounded/">Geochat: Grounded Large Vision-language Model For Remote Sensing</a> Kartik Kuckreja et al. </li>
     
   
     
       <li> <a href="/publications/roth2023waffling/">Waffling Around For Performance: Visual Classification With Random Words And Broad Concepts</a> Karsten Roth et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023clip/">Tinyclip: CLIP Distillation Via Affinity Mimicking And Weight Inheritance</a> Kan Stephen Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023adaptive/">ALIP: Adaptive Language-image Pre-training With Synthetic Caption</a> Kaicheng Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kerr2023language/">LERF: Language Embedded Radiance Fields</a> Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, Matthew Tancik </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023evaluation/">Evaluation And Analysis Of Hallucination In Large Vision-language Models</a> Junyang Wang et al. </li>
     
   
     
       <li> <a href="/publications/li2023blip/">BLIP-2: Bootstrapping Language-image Pre-training With Frozen Image Encoders And Large Language Models</a> Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fei2023transferable/">Transferable Decoding With Visual Entities For Zero-shot Image Captioning</a> Junjie Fei et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xiao2023contrastive/">Contrastive Video Question Answering Via Video Graph Transformer</a> Junbin Xiao et al. </li>
     
   
     
       <li> <a href="/publications/cha2023locality/">Honeybee: Locality-enhanced Projector For Multimodal LLM</a> Junbum Cha, Wooyoung Kang, Jonghwan Mun, Byungseok Roh </li>
     
   
     
       <li> <a href="/publications/chen2023minigpt/">Minigpt-v2: Large Language Model As A Unified Interface For Vision-language Multi-task Learning</a> Jun Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/koh2023grounding/">Grounding Language Models To Images For Multimodal Inputs And Outputs</a> Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried </li>
     
   
     
       <li> <a href="/publications/koh2023generating/">Generating Images With Multimodal Language Models</a> Jing Yu Koh, Daniel Fried, Ruslan Salakhutdinov </li>
     
   
     
       <li> <a href="/publications/jiang2023redundancy/">Mixphm: Redundancy-aware Parameter-efficient Tuning For Low-resource Visual Question Answering</a> Jingjing Jiang, Nanning Zheng </li>
     
   
     
       <li> <a href="/publications/gu2023systematic/">A Systematic Survey Of Prompt Engineering On Vision-language Foundation Models</a> Jindong Gu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023set/">Set-of-mark Prompting Unleashes Extraordinary Visual Grounding In GPT-4V</a> Jianwei Yang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023llm/">Llm-grounder: Open-vocabulary 3D Visual Grounding With Large Language Model As An Agent</a> Jianing Yang et al. </li>
     
   
     
       <li> <a href="/publications/li2023empowering/">Empowering Molecule Discovery For Molecule-caption Translation With Large Language Models: A Chatgpt Perspective</a> Jiatong Li et al. </li>
     
   
     
       <li> <a href="/publications/han2023imagebind/">Imagebind-llm: Multi-modality Instruction Tuning</a> Jiaming Han et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/han2023one/">Onellm: One Framework To Align All Modalities With Language</a> Jiaming Han et al. </li>
     
   
     
       <li> <a href="/publications/ye2023universal/">Ureader: Universal Ocr-free Visually-situated Language Understanding With Multimodal Large Language Model</a> Jiabo Ye et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023pre/">VILA: On Pre-training For Visual Language Models</a> Ji Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023physically/">Physically Grounded Vision-language Models For Robotic Manipulation</a> Jensen Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023ip/">Ip-adapter: Text Compatible Image Prompt Adapter For Text-to-image Diffusion Models</a> Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, Wei Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023improved/">Improved Baselines With Visual Instruction Tuning</a> Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee </li>
     
   
     
       <li> <a href="/publications/liu2023visual/">Visual Instruction Tuning</a> Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee </li>
     
   
     
       <li> <a href="/publications/you2023refer/">Ferret: Refer And Ground Anything Anywhere At Any Granularity</a> Haoxuan You et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023q/">Q-instruct: Improving Low-level Visual Abilities For Multi-modality Foundation Models</a> Haoning Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023chatgpt/">Chatgpt For Shaping The Future Of Dentistry: The Potential Of Multi-modal Large Language Model</a> Hanyao Huang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fei2023scene/">Scene Graph As Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation With Visual Scene Hallucination</a> Hao Fei, Qian Liu, Meishan Zhang, Min Zhang, Tat-seng Chua </li>
     
   
     
   
     
       <li> <a href="/publications/rasheed2023pixel/">Glamm: Pixel Grounding Large Multimodal Model</a> Hanoona Rasheed et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023video/">Video-llama: An Instruction-tuned Audio-visual Language Model For Video Understanding</a> Hang Zhang, Xin Li, Lidong Bing </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liao2023gpt/">GPT-4 Enhanced Multimodal Grounding For Autonomous Driving: Leveraging Cross-modal Attention With Large Language Models</a> Haicheng Liao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xu2023mplug/">Mplug-2: A Modularized Multi-modal Foundation Model Across Text, Image And Video</a> Haiyang Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023cheap/">Cheap And Quick: Efficient Vision-language Instruction Tuning For Large Language Models</a> Gen Luo et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023aligning/">Aligning Large Multimodal Models With Factually Augmented RLHF</a> Zhiqing Sun et al. </li>
     
   
     
       <li> <a href="/publications/geminiteam2023family/">Gemini: A Family Of Highly Capable Multimodal Models</a> Gemini Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yan2023multimodal/">Multimodal Chatgpt For Medical Applications: An Experimental Study Of GPT-4V</a> Zhiling Yan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2023read/">Read-only Prompt Optimization For Vision-language Few-shot Learning</a> Dongjun Lee et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023empowering/">Speechgpt: Empowering Large Language Models With Intrinsic Cross-modal Conversational Abilities</a> Dong Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mollo2023vector/">The Vector Grounding Problem</a> Dimitri Coelho Mollo, Rapha√´l Milli√®re </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023minigpt/">Minigpt-4: Enhancing Vision-language Understanding With Advanced Large Language Models</a> Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny </li>
     
   
     
       <li> <a href="/publications/zhu2023chatgpt/">Chatgpt Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions</a> Deyao Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023mm/">MM-REACT: Prompting Chatgpt For Multimodal Reasoning And Action</a> Zhengyuan Yang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/driess2023palm/">Palm-e: An Embodied Multimodal Language Model</a> Danny Driess et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023multimodal/">Multimodal Foundation Models: From Specialists To General-purpose Assistants</a> Chunyuan Li et al. </li>
     
   
     
       <li> <a href="/publications/li2023llava/">Llava-med: Training A Large Language-and-vision Assistant For Biomedicine In One Day</a> Chunyuan Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sima2023driving/">Drivelm: Driving With Graph Visual Question Answering</a> Chonghao Sima et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chuang2023debiasing/">Debiasing Vision-language Models Via Biased Prompts</a> Ching-yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, Stefanie Jegelka </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lyu2023macaw/">Macaw-llm: Multi-modal Language Modeling With Image, Audio, Video, And Text Integration</a> Chenyang Lyu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023visual/">Visual Chatgpt: Talking, Drawing And Editing With Visual Foundation Models</a> Chenfei Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023hallucination/">Hallucination Augmented Contrastive Learning For Multimodal Large Language Model</a> Chaoya Jiang et al. </li>
     
   
     
       <li> <a href="/publications/fu2023comprehensive/">MME: A Comprehensive Evaluation Benchmark For Multimodal Large Language Models</a> Chaoyou Fu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mitra2023compositional/">Compositional Chain-of-thought Prompting For Large Multimodal Models</a> Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023seed/">Seed-bench-2: Benchmarking Multimodal Large Language Models</a> Bohao Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2023mimic/">MIMIC-IT: Multi-modal In-context Instruction Tuning</a> Bo Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023video/">Video-llava: Learning United Visual Representation By Alignment Before Projection</a> Bin Lin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023empower/">Vtimellm: Empower LLM To Grasp Video Moments</a> Bin Huang, Xin Wang, Hong Chen, Zihan Song, Wenwu Zhu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aberdam2023looking/">CLIPTER: Looking At The Bigger Picture In Scene Text Recognition</a> Aviad Aberdam et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/brohan2023rt/">RT-2: Vision-language-action Models Transfer Web Knowledge To Robotic Control</a> Anthony Brohan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gunjal2023detecting/">Detecting And Preventing Hallucinations In Large Vision Language Models</a> Anisha Gunjal, Jihan Yin, Erhan Bas </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/awadalla2023open/">Openflamingo: An Open-source Framework For Training Large Autoregressive Vision-language Models</a> Anas Awadalla et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xu2023bridging/">Bridging Vision And Language Encoders: Parameter-efficient Tuning For Referring Image Segmentation</a> Zunnan Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shtedritski2023what/">What Does CLIP Know About A Red Circle? Visual Prompt Engineering For Vlms</a> Aleksandar Shtedritski, Christian Rupprecht, Andrea Vedaldi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ghosh2023clip/">Clipsyntel: CLIP And LLM Synergy For Multimodal Question Summarization In Healthcare</a> Akash Ghosh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023kosmos/">Kosmos-2: Grounding Multimodal Large Language Models To The World</a> Zhiliang Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023vision/">Vision Language Models In Autonomous Driving: A Survey And Outlook</a> Xingcheng Zhou et al. </li>
     
   
     
       <li> <a href="/publications/mei2023chatgpt/">Wavcaps: A Chatgpt-assisted Weakly-labelled Audio Captioning Dataset For Audio-language Multimodal Research</a> Xinhao Mei et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lai2023reasoning/">LISA: Reasoning Segmentation Via Large Language Model</a> Xin Lai et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023delving/">Delving Into Multimodal Prompting For Fine-grained Visual Classification</a> Xin Jiang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2023visual/">Visual Adversarial Examples Jailbreak Aligned Large Language Models</a> Xiangyu Qi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yue2023massive/">MMMU: A Massive Multi-discipline Multimodal Understanding And Reasoning Benchmark For Expert AGI</a> Xiang Yue et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023pali/">Pali-3 Vision Language Models: Smaller, Faster, Stronger</a> Xi Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023multilevel/">M3exam: A Multilingual, Multimodal, Multilevel Benchmark For Examining Large Language Models</a> Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, Lidong Bing </li>
     
   
     
       <li> <a href="/publications/hong2023visual/">Cogagent: A Visual Language Model For GUI Agents</a> Wenyi Hong et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2023towards/">Instructblip: Towards General-purpose Vision-language Models With Instruction Tuning</a> Wenliang Dai et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hu2023simple/">BLIVA: A Simple Multimodal LLM For Better Handling Of Text-rich Visual Questions</a> Wenbo Hu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lin2023pmc/">PMC-CLIP: Contrastive Language-image Pre-training Using Biomedical Documents</a> Weixiong Lin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2023compositional/">Layoutgpt: Compositional Visual Planning And Generation With Large Language Models</a> Weixi Feng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023mm/">Mm-vet: Evaluating Large Multimodal Models For Integrated Capabilities</a> Weihao Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023rlhf/">RLHF-V: Towards Trustworthy Mllms Via Behavior Alignment From Fine-grained Correctional Human Feedback</a> Tianyu Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023caption/">Caption Anything: Interactive Image Description With Diverse Multimodal Controls</a> Teng Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gong2023multimodal/">Multimodal-gpt: A Vision And Language Model For Dialogue With Humans</a> Tao Gong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2023one/">Pandagpt: One Model To Instruction-follow Them All</a> Yixuan Su et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hong2023injecting/">3D-LLM: Injecting The 3D World Into Large Language Models</a> Yining Hong et al. </li>
     
   
     
   
     
       <li> <a href="/publications/feng2023interactive/">Promptmagician: Interactive Prompt Engineering For Text-to-image Creation</a> Yingchaojie Feng et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2023analyzing/">Analyzing And Mitigating Object Hallucination In Large Vision-language Models</a> Yiyang Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tian2023graph/">Graph Neural Prompting With Large Language Models</a> Yijun Tian et al. </li>
     
   
     
   
     
       <li> <a href="/publications/cao2023comprehensive/">A Comprehensive Survey Of Ai-generated Content (AIGC): A History Of Generative AI From GAN To Chatgpt</a> Yihan Cao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2023evaluating/">Evaluating Object Hallucination In Large Vision-language Models</a> Yifan Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023making/">Making Large Language Models Perform Better In Knowledge Graph Completion</a> Yichi Zhang et al. </li>
     
   
     
       <li> <a href="/publications/xin2023mmap/">Mmap : Multi-modal Alignment Prompt For Cross-domain Multi-task Learning</a> Yi Xin, Junlong Du, Qiang Wang, Ke Yan, Shouhong Ding </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bang2023multimodal/">A Multitask, Multilingual, Multimodal Evaluation Of Chatgpt On Reasoning, Hallucination, And Interactivity</a> Yejin Bang et al. </li>
     
   
     
       <li> <a href="/publications/ma2023language/">LIV: Language-image Representations And Rewards For Robotic Control</a> Yecheng Jason Ma et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fathullah2023prompting/">Prompting Large Language Models With Speech Recognition Abilities</a> Yassir Fathullah et al. </li>
     
   
     
       <li> <a href="/publications/mu2023vision/">Embodiedgpt: Vision-language Pre-training Via Embodied Chain Of Thought</a> Yao Mu et al. </li>
     
   
     
       <li> <a href="/publications/yao2023beyond/">Beyond Chain-of-thought, Effective Graph-of-thought Reasoning In Language Models</a> Yao Yao, Zuchao Li, Hai Zhao </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023enhanced/">Llavar: Enhanced Visual Instruction Tuning For Text-rich Image Understanding</a> Yanzhe Zhang et al. </li>
     
   
     
       <li> <a href="/publications/li2023llama/">Llama-vid: An Image Is Worth 2 Tokens In Large Language Models</a> Yanwei Li, Chengyao Wang, Jiaya Jia </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023enabling/">Bubogpt: Enabling Visual Grounding In Multi-modal Llms</a> Yang Zhao et al. </li>
     
   
     
       <li> <a href="/publications/feng2023chatting/">Chatpose: Chatting About 3D Human Pose</a> Yao Feng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023chat/">Chat With The Environment: Interactive Multimodal Perception Using Large Language Models</a> Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zang2023contextual/">Contextual Object Detection With Multimodal Large Language Models</a> Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, Chen Change Loy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023open/">Biomedgpt: Open Multimodal Generative Pre-trained Transformer For Biomedicine</a> Yizhen Luo et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023evaluating/">On Evaluating Adversarial Robustness Of Large Vision-language Models</a> Yunqing Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023visual/">LL3DA: Visual Interactive Instruction Tuning For Omni-3d Understanding, Reasoning, And Planning</a> Sijin Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/leng2023mitigating/">Mitigating Object Hallucinations In Large Vision-language Models Through Visual Contrastive Decoding</a> Sicong Leng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yin2023survey/">A Survey On Multimodal Large Language Models</a> Shukang Yin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2023time/">Timechat: A Time-sensitive Multimodal Large Language Model For Long Video Understanding</a> Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pramanick2023egocentric/">Egovlpv2: Egocentric Video-language Pre-training With Fusion In The Backbone</a> Shraman Pramanick et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023alpha/">Alpha-clip: A CLIP Model Focusing On Wherever You Want</a> Zeyi Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023radiology/">R2gengpt: Radiology Report Generation With Frozen Llms</a> Zhanyu Wang, Lingqiao Liu, Lei Wang, Luping Zhou </li>
     
   
     
       <li> <a href="/publications/lin2023pushing/">Pushing Large Language Models To The 6G Edge: Vision, Challenges, And Opportunities</a> Zheng Lin et al. </li>
     
   
     
       <li> <a href="/publications/zheng2023preventing/">Preventing Zero-shot Transfer Degradation In Continual Learning Of Vision-language Models</a> Zangwei Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023scaling/">Internvl: Scaling Up Vision Foundation Models And Aligning For Generic Visual-linguistic Tasks</a> Zhe Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tong2024eyes/">Eyes Wide Shut? Exploring The Visual Shortcomings Of Multimodal Llms</a> Shengbang Tong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2024hidden/">Hidden Flaws Behind Expert-level Accuracy Of Multimodal GPT-4 Vision In Medicine</a> Qiao Jin et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2024multimodal/">SNIFFER: Multimodal Large Language Model For Explainable Out-of-context Misinformation Detection</a> Peng Qi, Zehong Yan, Wynne Hsu, Mong Li Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2024when/">When Large Language Model Agents Meet 6G Networks: Perception, Grounding, And Alignment</a> Minrui Xu et al. </li>
     
   
     
       <li> <a href="/publications/hu2024findings/">Findings Of The Second Babylm Challenge: Sample-efficient Pretraining On Developmentally Plausible Corpora</a> Michael Y. Hu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xu2024survey/">A Survey Of Resource-efficient LLM And Multimodal Foundation Models</a> Mengwei Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/saab2024capabilities/">Capabilities Of Gemini Models In Medicine</a> Khaled Saab et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2024boosting/">Boosting Continual Learning Of Vision-language Models Via Mixture-of-experts Adapters</a> Jiazuo Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hartsock2024vision/">Vision-language Models For Medical Report Generation And Visual Question Answering: A Review</a> Iryna Hartsock, Ghulam Rasool </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/geminiteam2024gemini/">Gemini 1.5: Unlocking Multimodal Understanding Across Millions Of Tokens Of Context</a> Gemini Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caffagni2024revolution/">The Revolution Of Multimodal Large Language Models: A Survey</a> Davide Caffagni et al. </li>
     
   
     
       <li> <a href="/publications/novelli2024generative/">Generative AI In EU Law: Liability, Privacy, Intellectual Property, And Cybersecurity</a> Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato, Luciano Floridi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2024moe/">Moe-llava: Mixture Of Experts For Large Vision-language Models</a> Bin Lin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bewersdorff2024taking/">Taking The Next Step With Generative Artificial Intelligence: The Transformative Role Of Multimodal Large Language Models In Science Education</a> Arne Bewersdorff et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ai2024open/">Yi: Open Foundation Models By 01.AI</a> 01. Ai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deldjoo2024review/">A Review Of Modern Recommender Systems Using Generative Models (gen-recsys)</a> Yashar Deldjoo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2024searching/">Searching For Best Practices In Retrieval-augmented Generation</a> Xiaohua Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2024llava/">Llava-mr: Large Language-and-vision Assistant For Video Moment Retrieval</a> Weiheng Lu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2024universal/">Earthgpt: A Universal Multi-modal Large Language Model For Multi-sensor Image Comprehension In Remote Sensing Domain</a> Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, Xuerui Mao </li>
     
   
     
       <li> <a href="/publications/parthasarathy2024ultimate/">The Ultimate Guide To Fine-tuning Llms From Basics To Breakthroughs: An Exhaustive Review Of Technologies, Research, Best Practices, Applied Research Challenges And Opportunities</a> Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2024unsupervised/">Promptkd: Unsupervised Prompt Distillation For Vision-language Models</a> Zheng Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chen2024how/">How Far Are We To GPT-4V? Closing The Gap To Commercial Multimodal Models With Open-source Suites</a> Zhe Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ NeurIPS <a id="NeurIPS"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dinan2019second/">The Second Conversational Intelligence Challenge (convai2)</a> Emily Dinan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2024monitoring/">Monitoring Ai-modified Content At Scale: A Case Study On The Impact Of Chatgpt On AI Conference Peer Reviews</a> Weixin Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Pre-Training <a id="Pre-Training"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/devlin2018pre/">BERT: Pre-training Of Deep Bidirectional Transformers For Language Understanding</a> Jacob Devlin, Ming-wei Chang, Kenton Lee, Kristina Toutanova </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2018can/">Can You Tell Me How To Get Past Sesame Street? Sentence-level Pretraining Beyond Language Modeling</a> Alex Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2018language/">Language Modeling Teaches You More Syntax Than Translation Does: Lessons Learned Through Auxiliary Task Analysis</a> Kelly W. Zhang, Samuel R. Bowman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2018can/">"bilingual Expert" Can Find Translation Errors</a> Kai Fan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kitaev2018multilingual/">Multilingual Constituency Parsing With Self-attention And Pre-training</a> Nikita Kitaev, Steven Cao, Dan Klein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2018improving/">Improving Machine Reading Comprehension With General Reading Strategies</a> Kai Sun, Dian Yu, Dong Yu, Claire Cardie </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2018improving/">Improving The Transformer Translation Model With Document-level Context</a> Jiacheng Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rothe2019leveraging/">Leveraging Pre-trained Checkpoints For Sequence Generation Tasks</a> Sascha Rothe, Shashi Narayan, Aliaksei Severyn </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/budzianowski2019gpt/">Hello, It's GPT-2 -- How Can I Help You? Towards The Use Of Pretrained Language Models For Task-oriented Dialogue Systems</a> Pawe≈Ç Budzianowski, Ivan Vuliƒá </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019pretrained/">Pretrained Language Models For Document-level Neural Machine Translation</a> Liangyou Li, Xin Jiang, Qun Liu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2019vl/">VL-BERT: Pre-training Of Generic Visual-linguistic Representations</a> Weijie Su et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019incorporating/">Structbert: Incorporating Language Structures Into Pre-training For Deep Language Understanding</a> Wei Wang et al. </li>
     
   
     
       <li> <a href="/publications/xu2019review/">Review Conversational Reading Comprehension</a> Hu Xu, Bing Liu, Lei Shu, Philip S. Yu </li>
     
   
     
   
     
       <li> <a href="/publications/chen2019universal/">UNITER: Universal Image-text Representation Learning</a> Yen-chun Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019simple/">Visualbert: A Simple And Performant Baseline For Vision And Language</a> Liunian Harold Li, Mark Yatskar, Da Yin, Cho-jui Hsieh, Kai-wei Chang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019sequence/">Sequence-to-sequence Pre-training With Data Augmentation For Sentence Rewriting</a> Yi Zhang, Tao Ge, Furu Wei, Ming Zhou, Xu Sun </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sanh2019distilled/">Distilbert, A Distilled Version Of BERT: Smaller, Faster, Cheaper And Lighter</a> Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2019masked/">MASS: Masked Sequence To Sequence Pre-training For Language Generation</a> Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-yan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2019cross/">Cross-lingual Training For Automatic Question Generation</a> Vishwajeet Kumar, Nitish Joshi, Arijit Mukherjee, Ganesh Ramakrishnan, Preethi Jyothi </li>
     
   
     
       <li> <a href="/publications/hoang2019efficient/">Efficient Adaptation Of Pretrained Transformers For Abstractive Summarization</a> Andrew Hoang, Antoine Bosselut, Asli Celikyilmaz, Yejin Choi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/turc2019well/">Well-read Students Learn Better: On The Importance Of Pre-training Compact Models</a> Iulia Turc, Ming-wei Chang, Kenton Lee, Kristina Toutanova </li>
     
   
     
   
     
       <li> <a href="/publications/yang2019towards/">Towards Making The Most Of BERT In Neural Machine Translation</a> Jiacheng Yang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019bert/">BERT For Joint Intent Classification And Slot Filling</a> Qian Chen, Zhu Zhuo, Wen Wang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/raunak2019compositionality/">On Compositionality In Neural Machine Translation</a> Vikas Raunak, Vaibhav Kumar, Florian Metze </li>
     
   
     
       <li> <a href="/publications/zhao2019open/">UER: An Open-source Toolkit For Pre-training Models</a> Zhe Zhao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/henderson2019training/">Training Neural Response Selection For Task-oriented Dialogue Systems</a> Matthew Henderson et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/talmor2019olmpics/">Olmpics -- On What Language Model Pre-training Captures</a> Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2019mask/">Align, Mask And Select: A Simple Method For Incorporating Commonsense Knowledge Into Language Representation Models</a> Zhi-xiu Ye, Qian Chen, Wen Wang, Zhen-hua Ling </li>
     
   
     
       <li> <a href="/publications/li2019robust/">Robust Navigation With Language Pretraining And Stochastic Sampling</a> Xiujun Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khandelwal2019sample/">Sample Efficient Text Summarization Using A Single Pre-trained Transformer</a> Urvashi Khandelwal, Kevin Clark, Dan Jurafsky, Lukasz Kaiser </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alberti2019synthetic/">Synthetic QA Corpora Generation With Roundtrip Consistency</a> Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, Michael Collins </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/donahue2019improving/">Lakhnes: Improving Multi-instrumental Music Generation With Cross-domain Pre-training</a> Chris Donahue, Huanru Henry Mao, Yiting Ethan Li, Garrison W. Cottrell, Julian Mcauley </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2019how/">How To Fine-tune BERT For Text Classification?</a> Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2019sequential/">Bert4rec: Sequential Recommendation With Bidirectional Encoder Representations From Transformer</a> Fei Sun et al. </li>
     
   
     
       <li> <a href="/publications/huang2019universal/">Unicoder: A Universal Language Encoder By Pre-training With Multiple Cross-lingual Tasks</a> Haoyang Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2019learning/">LXMERT: Learning Cross-modality Encoder Representations From Transformers</a> Hao Tan, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/glass2019span/">Span Selection Pre-training For Question Answering</a> Michael Glass et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/whang2019effective/">An Effective Domain Adaptive Post-training Method For BERT In Response Selection</a> Taesun Whang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2019pre/">A Pre-training Based Personalized Dialogue Generation Model With Persona-sparse Data</a> Yinhe Zheng, Rongsheng Zhang, Xiaoxi Mao, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chi2019cross/">Cross-lingual Natural Language Generation Via Pre-training</a> Zewen Chi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/raffel2019exploring/">Exploring The Limits Of Transfer Learning With A Unified Text-to-text Transformer</a> Colin Raffel et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiao2019distilling/">Tinybert: Distilling BERT For Natural Language Understanding</a> Xiaoqi Jiao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dong2019unified/">Unified Language Model Pre-training For Natural Language Understanding And Generation</a> Li Dong et al. </li>
     
   
     
       <li> <a href="/publications/weng2019acquiring/">Acquiring Knowledge From Pre-trained Model To Neural Machine Translation</a> Rongxiang Weng, Heng Yu, Shujian Huang, Shanbo Cheng, Weihua Luo </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019pre/">PEGASUS: Pre-training With Extracted Gap-sentences For Abstractive Summarization</a> Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J. Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2019denoising/">BART: Denoising Sequence-to-sequence Pre-training For Natural Language Generation, Translation, And Comprehension</a> Mike Lewis et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/murahari2019large/">Large-scale Pretraining For Visual Dialog: A Simple State-of-the-art Baseline</a> Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das </li>
     
   
     
   
     
       <li> <a href="/publications/hao2019visualizing/">Visualizing And Understanding The Effectiveness Of BERT</a> Yaru Hao, Li Dong, Furu Wei, Ke Xu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/baevski2019cloze/">Cloze-driven Pretraining Of Self-attention Networks</a> Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael Auli </li>
     
   
     
   
     
       <li> <a href="/publications/edunov2019pre/">Pre-trained Language Model Representations For Language Generation</a> Sergey Edunov, Alexei Baevski, Michael Auli </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019stickier/">Superglue: A Stickier Benchmark For General-purpose Language Understanding Systems</a> Alex Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019tree/">Tree Transformer: Integrating Tree Structures Into Self-attention</a> Yau-shian Wang, Hung-yi Lee, Yun-nung Chen </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2019unified/">Unified Vision-language Pre-training For Image Captioning And VQA</a> Luowei Zhou et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ohsugi2019simple/">A Simple But Effective Method To Incorporate Multi-turn Context With BERT For Conversational Machine Comprehension</a> Yasuhito Ohsugi, Itsumi Saito, Kyosuke Nishida, Hisako Asano, Junji Tomita </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kanade2019learning/">Learning And Evaluating Contextual Embedding Of Source Code</a> Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019large/">Dialogpt: Large-scale Generative Pre-training For Conversational Response Generation</a> Yizhe Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019robustly/">Roberta: A Robustly Optimized BERT Pretraining Approach</a> Yinhan Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019unicoder/">Unicoder-vl: A Universal Encoder For Vision And Language By Cross-modal Pre-training</a> Gen Li et al. </li>
     
   
     
       <li> <a href="/publications/bansal2019learning/">Learning To Few-shot Learn Across Diverse Natural Language Classification Tasks</a> Trapit Bansal, Rishikesh Jha, Andrew Mccallum </li>
     
   
     
       <li> <a href="/publications/qiu2019blockwise/">Blockwise Self-attention For Long Document Understanding</a> Jiezhong Qiu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lample2019cross/">Cross-lingual Language Model Pretraining</a> Guillaume Lample, Alexis Conneau </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019linguistic/">Linguistic Knowledge And Transferability Of Contextual Representations</a> Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, Noah A. Smith </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2019probing/">Probing What Different NLP Tasks Teach Machines About Function Word Comprehension</a> Najoung Kim et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bao2019pre/">PLATO: Pre-trained Dialogue Generation Model With Discrete Latent Variable</a> Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2019model/">Model Compression With Two-stage Multi-teacher Knowledge Distillation For Web Question Answering System</a> Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, Daxin Jiang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kassner2019negated/">Negated And Misprimed Probes For Pretrained Language Models: Birds Can Talk, But Cannot Fly</a> Nora Kassner, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2019reweighted/">Reweighted Proximal Pruning For Large-scale Language Representation</a> Fu-ming Guo, Sijia Liu, Finlay S. Mungall, Xue Lin, Yanzhi Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2020pre/">Phobert: Pre-trained Language Models For Vietnamese</a> Dat Quoc Nguyen, Anh Tuan Nguyen </li>
     
   
     
       <li> <a href="/publications/ma2020xlm/">XLM-T: Scaling Up Multilingual Machine Translation With Pretrained Cross-lingual Transformer Encoders</a> Shuming Ma et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/khashabi2020crossing/">Unifiedqa: Crossing Format Boundaries With A Single QA System</a> Daniel Khashabi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/guu2020retrieval/">REALM: Retrieval-augmented Language Model Pre-training</a> Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-wei Chang </li>
     
   
     
       <li> <a href="/publications/song2020kvl/">KVL-BERT: Knowledge Enhanced Visual-and-linguistic BERT For Visual Commonsense Reasoning</a> Dandan Song, Siyi Ma, Zhanchen Sun, Sicheng Yang, Lejian Liao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rosset2020knowledge/">Knowledge-aware Language Model Pretraining</a> Corby Rosset et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shuster2020multi/">Multi-modal Open-domain Dialogue</a> Kurt Shuster, Eric Michael Smith, Da Ju, Jason Weston </li>
     
   
     
       <li> <a href="/publications/li2020organizing/">Optimus: Organizing Sentences Via Pre-trained Modeling Of A Latent Space</a> Chunyuan Li et al. </li>
     
   
     
       <li> <a href="/publications/pfeiffer2020mad/">MAD-X: An Adapter-based Framework For Multi-task Cross-lingual Transfer</a> Jonas Pfeiffer, Ivan Vuliƒá, Iryna Gurevych, Sebastian Ruder </li>
     
   
     
       <li> <a href="/publications/zhang2020large/">CPM: A Large-scale Generative Chinese Pre-trained Language Model</a> Zhengyan Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mehri2020natural/">Dialoglue: A Natural Language Understanding Benchmark For Task-oriented Dialogue</a> Shikib Mehri, Mihail Eric, Dilek Hakkani-tur </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2020tod/">TOD-BERT: Pre-trained Natural Language Understanding For Task-oriented Dialogue</a> Chien-sheng Wu, Steven Hoi, Richard Socher, Caiming Xiong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020when/">When Do You Need Billions Of Words Of Pretraining Data?</a> Yian Zhang, Alex Warstadt, Haau-sing Li, Samuel R. Bowman </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ainslie2020encoding/">ETC: Encoding Long And Structured Inputs In Transformers</a> Joshua Ainslie et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kale2020text/">Text-to-text Pre-training For Data-to-text Tasks</a> Mihir Kale, Abhinav Rastogi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maynez2020faithfulness/">On Faithfulness And Factuality In Abstractive Summarization</a> Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020encoding/">Encoding Syntactic Knowledge In Transformer Encoder For Intent Detection And Slot Filling</a> Jixuan Wang, Kai Wei, Martin Radfar, Weiwei Zhang, Clement Chung </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2020ernie/">ERNIE-GEN: An Enhanced Multi-flow Pre-training And Fine-tuning Framework For Natural Language Generation</a> Dongling Xiao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2020cross/">Imagebert: Cross-modal Pre-training With Large-scale Weak-supervised Image-text Data</a> Di Qi et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clark2020pre/">ELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators</a> Kevin Clark, Minh-thang Luong, Quoc V. Le, Christopher D. Manning </li>
     
   
     
   
     
       <li> <a href="/publications/liu2020adversarial/">Adversarial Training For Large Neural Language Models</a> Xiaodong Liu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2020behind/">Behind The Scene: Revealing The Secrets Of Pre-trained Vision-and-language Models</a> Jize Cao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2020contextualized/">Colake: Contextualized Language And Knowledge Embedding</a> Tianxiang Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sun2020contrastive/">Contrastive Distillation On Intermediate Representations For Language Model Compression</a> Siqi Sun et al. </li>
     
   
     
       <li> <a href="/publications/yang2020text/">TAP: Text-aware Pre-training For Text-vqa And Text-caption</a> Zhengyuan Yang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zandie2020multi/">Emptransfo: A Multi-head Transformer Architecture For Creating Empathetic Dialog Systems</a> Rohola Zandie, Mohammad H. Mahoor </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2020character/">Charbert: Character-aware Pre-trained Language Model</a> Wentao Ma et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ke2020rethinking/">Rethinking Positional Encoding In Language Pre-training</a> Guolin Ke, Di He, Tie-yan Liu </li>
     
   
     
       <li> <a href="/publications/penha2020what/">What Does BERT Know About Books, Movies And Music? Probing BERT For Conversational Recommendation</a> Gustavo Penha, Claudia Hauff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020large/">A Large-scale Chinese Short-text Conversation Dataset</a> Yida Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2020pre/">Pre-training Via Paraphrasing</a> Mike Lewis et al. </li>
     
   
     
       <li> <a href="/publications/pan2020auto/">Auto-captions On GIF: A Large-scale Video-sentence Dataset For Vision-language Pre-training</a> Yingwei Pan et al. </li>
     
   
     
       <li> <a href="/publications/ding2020ernie/">Ernie-doc: A Retrospective Long-document Modeling Transformer</a> Siyu Ding et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2020incorporating/">Incorporating External Knowledge Through Pre-training For Natural Language To Code Generation</a> Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, Graham Neubig </li>
     
   
     
   
     
       <li> <a href="/publications/liu2020multilingual/">Multilingual Denoising Pre-training For Neural Machine Translation</a> Yinhan Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qi2020bridging/">BANG: Bridging Autoregressive And Non-autoregressive Generation With Large Scale Pretraining</a> Weizhen Qi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bao2020pseudo/">Unilmv2: Pseudo-masked Language Models For Unified Language Model Pre-training</a> Hangbo Bao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2020lrc/">LRC-BERT: Latent-representation Contrastive Knowledge Distillation For Natural Language Understanding</a> Hao Fu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020knowledge/">KGPT: Knowledge-grounded Pre-training For Data-to-text Generation</a> Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang </li>
     
   
     
       <li> <a href="/publications/bhojanapalli2020low/">Low-rank Bottleneck In Multi-head Attention Models</a> Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2020predicting/">Prophetnet: Predicting Future N-gram For Sequence-to-sequence Pre-training</a> Weizhen Qi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/longpre2020how/">How Effective Is Task-agnostic Data Augmentation For Pretrained Transformers?</a> Shayne Longpre, Yu Wang, Christopher Dubois </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020pretrain/">To Pretrain Or Not To Pretrain: Examining The Benefits Of Pretraining On Resource Rich Tasks</a> Sinong Wang, Madian Khabsa, Hao Ma </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rabe2020mathematical/">Mathematical Reasoning Via Self-supervised Skip-tree Training</a> Markus N. Rabe, Dennis Lee, Kshitij Bansal, Christian Szegedy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ribeiro2020investigating/">Investigating Pretrained Language Models For Graph-to-text Generation</a> Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Sch√ºtze, Iryna Gurevych </li>
     
   
     
       <li> <a href="/publications/li2020object/">Oscar: Object-semantics Aligned Pre-training For Vision-language Tasks</a> Xiujun Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2020pre/">Codebert: A Pre-trained Model For Programming And Natural Languages</a> Zhangyin Feng et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jain2020contrastive/">Contrastive Code Representation Learning</a> Paras Jain et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hegde2020unsupervised/">Unsupervised Paraphrase Generation Using Pre-trained Language Models</a> Chaitra Hegde, Shrikumar Patil </li>
     
   
     
       <li> <a href="/publications/liang2020new/">XGLUE: A New Benchmark Dataset For Cross-lingual Pre-training, Understanding And Generation</a> Yaobo Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2020pre/">Pre-training Multilingual Neural Machine Translation By Leveraging Alignment Information</a> Zehui Lin et al. </li>
     
   
     
       <li> <a href="/publications/bunk2020lightweight/">DIET: Lightweight Language Understanding For Dialogue Systems</a> Tanja Bunk, Daksh Varshneya, Vladimir Vlasov, Alan Nichol </li>
     
   
     
       <li> <a href="/publications/webster2020measuring/">Measuring And Reducing Gendered Correlations In Pre-trained Models</a> Kellie Webster et al. </li>
     
   
     
       <li> <a href="/publications/wu2020contrastive/">CLEAR: Contrastive Learning For Sentence Representation</a> Zhuofeng Wu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2020knowledge/">Knowledge-grounded Dialogue Generation With Pre-trained Language Models</a> Xueliang Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2020mixup/">Mixup-transformer: Dynamic Data Augmentation For NLP Tasks</a> Lichao Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2020text/">TIME: Text And Image Mutual-translation Adversarial Networks</a> Bingchen Liu, Kunpeng Song, Yizhe Zhu, Gerard De Melo, Ahmed Elgammal </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bi2020pre/">PALM: Pre-training An Autoencoding&autoregressive Language Model For Context-conditioned Generation</a> Bin Bi et al. </li>
     
   
     
       <li> <a href="/publications/sellam2020learning/">BLEURT: Learning Robust Metrics For Text Generation</a> Thibault Sellam, Dipanjan Das, Ankur P. Parikh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yin2020pretraining/">Tabert: Pretraining For Joint Understanding Of Textual And Tabular Data</a> Pengcheng Yin, Graham Neubig, Wen-tau Yih, Sebastian Riedel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020constrained/">POINTER: Constrained Progressive Text Generation Via Insertion-based Generative Pre-training</a> Yizhe Zhang et al. </li>
     
   
     
       <li> <a href="/publications/zhu2020reading/">DUMA: Reading Comprehension With Transposition Thinking</a> Pengfei Zhu, Hai Zhao, Xiaoguang Li </li>
     
   
     
   
     
       <li> <a href="/publications/xu2020multi/">Layoutlmv2: Multi-modal Pre-training For Visually-rich Document Understanding</a> Yang Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xue2020massively/">Mt5: A Massively Multilingual Pre-trained Text-to-text Transformer</a> Linting Xue et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020unsupervised/">Unsupervised Vision-and-language Pre-training Without Parallel Images And Captions</a> Liunian Harold Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020empirical/">An Empirical Investigation Of Pre-trained Transformer Language Models For Open-domain Dialogue Generation</a> Piji Li </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/singh2020are/">Are We Pretraining It Right? Digging Deeper Into Visio-linguistic Pretraining</a> Amanpreet Singh, Vedanuj Goswami, Devi Parikh </li>
     
   
     
   
     
       <li> <a href="/publications/liu2020survey/">A Survey On Contextual Embeddings</a> Qi Liu, Matt J. Kusner, Phil Blunsom </li>
     
   
     
   
     
       <li> <a href="/publications/talmor2020leap/">Leap-of-thought: Teaching Pre-trained Models To Systematically Reason Over Implicit Knowledge</a> Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant </li>
     
   
     
   
     
       <li> <a href="/publications/lin2020conversational/">Conversational Question Reformulation Via Sequence-to-sequence Architectures And Pretrained Language Models</a> Sheng-chieh Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2020cross/">XGPT: Cross-modal Generative Pre-training For Image Captioning</a> Qiaolin Xia et al. </li>
     
   
     
   
     
       <li> <a href="/publications/cao2020decomposing/">Deformer: Decomposing Pre-trained Transformers For Faster Question Answering</a> Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rastogi2020schema/">Schema-guided Dialogue State Tracking Task At DSTC8</a> Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, Pranav Khaitan </li>
     
   
     
   
     
       <li> <a href="/publications/chen2020efficient/">Earlybert: Efficient BERT Training Via Early-bird Lottery Tickets</a> Xiaohan Chen et al. </li>
     
   
     
       <li> <a href="/publications/wu2020are/">Are All Languages Created Equal In Multilingual BERT?</a> Shijie Wu, Mark Dredze </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/marino2020integrating/">KRISP: Integrating Implicit And Symbolic Knowledge For Open-domain Knowledge-based VQA</a> Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, Marcus Rohrbach </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cho2020x/">X-LXMERT: Paint, Caption And Answer Questions With Multi-modal Transformers</a> Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi </li>
     
   
     
       <li> <a href="/publications/kaplan2020scaling/">Scaling Laws For Neural Language Models</a> Jared Kaplan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/han2020effective/">ECONET: Effective Continual Pretraining Of Language Models For Event Temporal Reasoning</a> Rujun Han, Xiang Ren, Nanyun Peng </li>
     
   
     
   
     
       <li> <a href="/publications/geva2020injecting/">Injecting Numerical Reasoning Skills Into Language Models</a> Mor Geva, Ankit Gupta, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/b2020language/">Language Models Are Few-shot Learners</a> Tom B. Brown et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/beltagy2020long/">Longformer: The Long-document Transformer</a> Iz Beltagy, Matthew E. Peters, Arman Cohan </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2020pre/">Pre-training Text-to-text Transformers For Concept-centric Common Sense</a> Wangchunshu Zhou et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020accelerating/">Accelerating Training Of Transformer-based Language Models With Progressive Layer Dropping</a> Minjia Zhang, Yuxiong He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chung2020rethinking/">Rethinking Embedding Coupling In Pre-trained Language Models</a> Hyung Won Chung, Thibault F√©vry, Henry Tsai, Melvin Johnson, Sebastian Ruder </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020towards/">UNIMO: Towards Unified-modal Understanding And Generation Via Cross-modal Contrastive Learning</a> Wei Li et al. </li>
     
   
     
       <li> <a href="/publications/hong2020recurrent/">A Recurrent Vision-and-language BERT For Navigation</a> Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-opazo, Stephen Gould </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2020distillation/">Ternarybert: Distillation-aware Ultra-low Bit BERT</a> Wei Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2020multilingual/">Multilingual Translation With Extensible Multilingual Pretraining And Finetuning</a> Yuqing Tang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2020beyond/">Beyond I.I.D.: Three Levels Of Generalization For Question Answering On Knowledge Bases</a> Yu Gu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bostrom2020byte/">Byte Pair Encoding Is Suboptimal For Language Model Pretraining</a> Kaj Bostrom, Greg Durrett </li>
     
   
     
       <li> <a href="/publications/hao2020towards/">Towards Learning A Generic Agent For Vision-and-language Navigation Via Pre-training</a> Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao </li>
     
   
     
   
     
       <li> <a href="/publications/ni2020learning/">M3P: Learning Universal Representations Via Multitask Multilingual Multimodal Pre-training</a> Minheng Ni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2020improving/">Vokenization: Improving Language Understanding With Contextualized, Visual-grounded Supervision</a> Hao Tan, Mohit Bansal </li>
     
   
     
   
     
       <li> <a href="/publications/zhao2020masking/">Masking As An Efficient Alternative To Finetuning For Pretrained Language Models</a> Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2020knowledge/">Knowledge-driven Data Construction For Zero-shot Evaluation In Commonsense Question Answering</a> Kaixin Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/goyal2021larger/">Larger-scale Transformers For Multilingual Masked Language Modeling</a> Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau </li>
     
   
     
   
     
       <li> <a href="/publications/bao2021unified/">Vlmo: Unified Vision-language Pre-training With Mixture-of-modality-experts</a> Hangbo Bao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021end/">E2E-VLP: End-to-end Vision-language Pre-training Enhanced By Visual Learning</a> Haiyang Xu et al. </li>
     
   
     
       <li> <a href="/publications/zhang2021ernie/">Ernie-vilg: Unified Generative Pre-training For Bidirectional Vision-language Generation</a> Han Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2021g/">G-transformer For Document-level Machine Translation</a> Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, Weihua Luo </li>
     
   
     
       <li> <a href="/publications/li2021contrast/">Contrast And Generation Make BART A Good Dialogue Emotion Recognizer</a> Shimin Li, Hang Yan, Xipeng Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/webersinke2021pretrained/">Climatebert: A Pretrained Language Model For Climate-related Text</a> Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler, Markus Leippold </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qu2021asking/">Asking Questions Like Educational Experts: Automatically Generating Question-answer Pairs On Real-world Examination Data</a> Fanyi Qu, Xin Jia, Yunfang Wu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2021language/">Language Model As An Annotator: Exploring Dialogpt For Dialogue Summarization</a> Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu </li>
     
   
     
       <li> <a href="/publications/zhou2021data/">MELM: Data Augmentation With Masked Entity Language Modeling For Low-resource NER</a> Ran Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kharitonov2021text/">Text-free Prosody-aware Generative Spoken Language Modeling</a> Eugene Kharitonov et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mu2021self/">SLIP: Self-supervision Meets Language-image Pre-training</a> Norman Mu, Alexander Kirillov, David Wagner, Saining Xie </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2021low/">Lora: Low-rank Adaptation Of Large Language Models</a> Edward J. Hu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2021augmenting/">Augmenting Sequential Recommendation With Pseudo-prior Items Via Reversely Pre-training Transformer</a> Zhiwei Liu, Ziwei Fan, Yu Wang, Philip S. Yu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021align/">Align And Prompt: Video-and-language Pre-training With Entity Prompts</a> Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2021scale/">Scale Efficiently: Insights From Pre-training And Fine-tuning Transformers</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021can/">Can Generative Pre-trained Language Models Serve As Knowledge Bases For Closed-book QA?</a> Cunxiang Wang, Pai Liu, Yue Zhang </li>
     
   
     
       <li> <a href="/publications/yang2021causal/">Causal Attention For Vision-language Tasks</a> Xu Yang, Hanwang Zhang, Guojun Qi, Jianfei Cai </li>
     
   
     
       <li> <a href="/publications/li2021stability/">The Stability-efficiency Dilemma: Investigating Sequence Length Warmup For Training GPT Models</a> Conglong Li, Minjia Zhang, Yuxiong He </li>
     
   
     
   
     
       <li> <a href="/publications/liu2021omni/">OPT: Omni-perception Pre-trainer For Cross-modal Understanding And Generation</a> Jing Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2021mastering/">Cogview: Mastering Text-to-image Generation Via Transformers</a> Ming Ding et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2021pre/">Dialoglm: Pre-trained Model For Long Dialogue Understanding And Summarization</a> Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021unified/">UFO: A Unified Transformer For Vision-language Representation Learning</a> Jianfeng Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guan2021long/">Long Text Generation By Modeling Sentence-level And Discourse-level Coherence</a> Jian Guan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/aribandi2021towards/">Ext5: Towards Extreme Multi-task Scaling For Transfer Learning</a> Vamsi Aribandi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sanh2021multitask/">Multitask Prompted Training Enables Zero-shot Task Generalization</a> Victor Sanh et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meng2021coco/">COCO-LM: Correcting And Contrasting Text Sequences For Language Model Pretraining</a> Yu Meng et al. </li>
     
   
     
       <li> <a href="/publications/fu2021violet/">VIOLET : End-to-end Video-language Transformers With Masked Visual-token Modeling</a> Tsu-jui Fu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2021effective/">Effective Sequence-to-sequence Dialogue State Tracking</a> Jeffrey Zhao, Mahdis Mahdieh, Ye Zhang, Yuan Cao, Yonghui Wu </li>
     
   
     
       <li> <a href="/publications/zhou2021universal/">UC2: Universal Cross-lingual Cross-modal Vision-and-language Pre-training</a> Mingyang Zhou et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2021clip/">Clip4caption: CLIP For Video Caption</a> Mingkang Tang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chada2021simple/">Fewshotqa: A Simple Framework For Few-shot Learning Of Question Answering Tasks Using Pre-trained Text-to-text Models</a> Rakesh Chada, Pradeep Natarajan </li>
     
   
     
   
     
       <li> <a href="/publications/sun2021ernie/">ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training For Language Understanding And Generation</a> Yu Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gu2021pre/">PPT: Pre-trained Prompt Tuning For Few-shot Learning</a> Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021generative/">GALAXY: A Generative Pre-trained Model For Task-oriented Dialog With Semi-supervised Learning And Explicit Policy Injection</a> Wanwei He et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021multimodal/">Layoutxlm: Multimodal Pre-training For Multilingual Visually-rich Document Understanding</a> Yiheng Xu et al. </li>
     
   
     
       <li> <a href="/publications/ahmad2021unified/">Unified Pre-training For Program Understanding And Generation</a> Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-wei Chang </li>
     
   
     
   
     
       <li> <a href="/publications/xu2021task/">VLM: Task-agnostic Video-language Model Pre-training For Video Understanding</a> Hu Xu et al. </li>
     
   
     
       <li> <a href="/publications/xue2021advancing/">Advancing High-resolution Video-language Representation With Large-scale Video Transcriptions</a> Hongwei Xue et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2021pangu/">Pangu-\(Œ±\): Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation</a> Wei Zeng et al. </li>
     
   
     
       <li> <a href="/publications/chi2021xlm/">XLM-E: Cross-lingual Language Model Pre-training Via ELECTRA</a> Zewen Chi et al. </li>
     
   
     
       <li> <a href="/publications/yao2021colorful/">CPT: Colorful Prompt Tuning For Pre-trained Vision-language Models</a> Yuan Yao et al. </li>
     
   
     
       <li> <a href="/publications/wang2021math/">Math Word Problem Generation With Mathematical Consistency And Problem Context Constraints</a> Zichao Wang, Andrew S. Lan, Richard G. Baraniuk </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sung2021vl/">Vl-adapter: Parameter-efficient Transfer Learning For Vision-and-language Tasks</a> Yi-lin Sung, Jaemin Cho, Mohit Bansal </li>
     
   
     
       <li> <a href="/publications/zhou2021open/">EVA: An Open-domain Chinese Dialogue System With Large-scale Generative Pre-training</a> Hao Zhou et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2021improving/">Tacl: Improving BERT Pre-training With Token-aware Contrastive Learning</a> Yixuan Su et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liang2021mwp/">MWP-BERT: Numeracy-augmented Pre-training For Math Word Problem Solving</a> Zhenwen Liang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021chinese/">M6: A Chinese Multimodal Pretrainer</a> Junyang Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huo2021bridging/">Wenlan: Bridging Vision And Language By Large-scale Multi-modal Pre-training</a> Yuqi Huo et al. </li>
     
   
     
       <li> <a href="/publications/su2021multi/">Multi-task Pre-training For Plug-and-play Task-oriented Dialogue System</a> Yixuan Su et al. </li>
     
   
     
       <li> <a href="/publications/schick2021generating/">Generating Datasets With Pretrained Language Models</a> Timo Schick, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rao2021language/">Denseclip: Language-guided Dense Prediction With Context-aware Prompting</a> Yongming Rao et al. </li>
     
   
     
       <li> <a href="/publications/p%C3%A9rez2021pre/">Robertuito: A Pre-trained Language Model For Social Media Text In Spanish</a> Juan Manuel P√©rez, Dami√°n A. Furman, Laura Alonso Alemany, Franco Luque </li>
     
   
     
       <li> <a href="/publications/herzig2021open/">Open Domain Question Answering Over Tables Via Dense Retrieval</a> Jonathan Herzig, Thomas M√ºller, Syrine Krichene, Julian Martin Eisenschlos </li>
     
   
     
       <li> <a href="/publications/clark2021pre/">CANINE: Pre-training An Efficient Tokenization-free Encoder For Language Representation</a> Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting </li>
     
   
     
   
     
       <li> <a href="/publications/herzig2021unlocking/">Unlocking Compositional Generalization In Pre-trained Models Using Intermediate Representations</a> Jonathan Herzig et al. </li>
     
   
     
       <li> <a href="/publications/moon2021multi/">Multi-modal Understanding And Generation For Medical Images And Text Via Vision-language Pre-training</a> Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-hak Kim, Edward Choi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dou2021empirical/">An Empirical Study Of Training End-to-end Vision-and-language Transformers</a> Zi-yi Dou et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xing2021km/">KM-BART: Knowledge Enhanced Multimodal BART For Visual Commonsense Generation</a> Yiran Xing et al. </li>
     
   
     
       <li> <a href="/publications/kulkarni2021learning/">Learning Rich Representation Of Keyphrases From Text</a> Mayank Kulkarni, Debanjan Mahata, Ravneet Arora, Rajarshi Bhowmik </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zellers2021multimodal/">MERLOT: Multimodal Neural Script Knowledge Models</a> Rowan Zellers et al. </li>
     
   
     
       <li> <a href="/publications/rohde2021hierarchical/">Hierarchical Learning For Generation With Long Source Sequences</a> Tobias Rohde, Xiaoxia Wu, Yinhan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2021learning/">Learning To Prompt For Vision-language Models</a> Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2021seeing/">Seeing Out Of The Box: End-to-end Pre-training For Vision-language Representation Learning</a> Zhicheng Huang et al. </li>
     
   
     
       <li> <a href="/publications/mishra2021cross/">Cross-task Generalization Via Natural Language Crowdsourcing Instructions</a> Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi </li>
     
   
     
       <li> <a href="/publications/castellon2021codified/">Codified Audio Language Modeling Learns Useful Representations For Music Information Retrieval</a> Rodrigo Castellon, Chris Donahue, Percy Liang </li>
     
   
     
       <li> <a href="/publications/lowphansirikul2021pretraining/">Wangchanberta: Pretraining Transformer-based Thai Language Models</a> Lalita Lowphansirikul, Charin Polpanumas, Nawat Jantrakulchai, Sarana Nutanong </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2021why/">Why Do Pretrained Language Models Help In Downstream Tasks? An Analysis Of Head And Prompt Tuning</a> Colin Wei, Sang Michael Xie, Tengyu Ma </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021scene/">SGEITL: Scene Graph Enhanced Image-text Learning For Visual Commonsense Reasoning</a> Zhecan Wang et al. </li>
     
   
     
       <li> <a href="/publications/sun2021pre/">Lightningdot: Pre-training Visual-semantic Embeddings For Real-time Image-text Retrieval</a> Siqi Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2021are/">Are Pre-trained Convolutions Better Than Pre-trained Transformers?</a> Yi Tay et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wu2021empowering/">Empowering News Recommendation With Pre-trained Language Models</a> Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021structural/">Structurallm: Structural Pre-training For Form Understanding</a> Chenliang Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ram2021few/">Few-shot Question Answering By Pretraining Span Selection</a> Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021visual/">N\"UWA: Visual Synthesis Pre-training For Neural Visual World Creation</a> Chenfei Wu et al. </li>
     
   
     
       <li> <a href="/publications/xu2021stacked/">Stacked Acoustic-and-textual Encoding: Integrating The Pre-trained Models Into Speech Translation Encoders</a> Chen Xu et al. </li>
     
   
     
       <li> <a href="/publications/zhang2021cpm/">CPM-2: Large-scale Cost-effective Pre-trained Language Models</a> Zhengyan Zhang et al. </li>
     
   
     
       <li> <a href="/publications/ju2021prompting/">Prompting Visual-language Models For Efficient Video Understanding</a> Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie </li>
     
   
     
   
     
       <li> <a href="/publications/xiao2021pre/">Lawformer: A Pre-trained Language Model For Chinese Legal Long Documents</a> Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, Maosong Sun </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2021compressing/">Compressing Visual-linguistic Model Via Knowledge Distillation</a> Zhiyuan Fang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sun2021nsp/">NSP-BERT: A Prompt-based Few-shot Learner Through An Original Pre-training Task--next Sentence Prediction</a> Yi Sun, Yu Zheng, Chao Hao, Hangping Qiu </li>
     
   
     
   
     
       <li> <a href="/publications/chen2021self/">Self-supervised Dialogue Learning For Spoken Conversational Question Answering</a> Nuo Chen, Chenyu You, Yuexian Zou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/min2021recent/">Recent Advances In Natural Language Processing Via Large Pre-trained Language Models: A Survey</a> Bonan Min et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021linear/">Luna: Linear Unified Nested Attention</a> Xuezhe Ma et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2021clip/">Clip-adapter: Better Vision-language Models With Feature Adapters</a> Peng Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021improving/">Debertav3: Improving Deberta Using Electra-style Pre-training With Gradient-disentangled Embedding Sharing</a> Pengcheng He, Jianfeng Gao, Weizhu Chen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/aghajanyan2021hyper/">HTLM: Hyper-text Pre-training And Prompting Of Language Models</a> Armen Aghajanyan et al. </li>
     
   
     
       <li> <a href="/publications/aghajanyan2021massive/">Muppet: Massive Multi-task Representations With Pre-finetuning</a> Armen Aghajanyan et al. </li>
     
   
     
       <li> <a href="/publications/guti%C3%A9rrezfandi%C3%B1o2021spanish/">Maria: Spanish Language Models</a> Asier Guti√©rrez-fandi√±o et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021supervision/">Supervision Exists Everywhere: A Data Efficient Contrastive Language-image Pre-training Paradigm</a> Yangguang Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dabre2021pre/">Indicbart: A Pre-trained Model For Indic Natural Language Generation</a> Raj Dabre et al. </li>
     
   
     
       <li> <a href="/publications/cui2021contrastive/">Contrastive Vision-language Pre-training With Limited Resources</a> Quan Cui et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/narayan2021planning/">Planning With Learned Entity Prompts For Abstractive Summarization</a> Shashi Narayan et al. </li>
     
   
     
       <li> <a href="/publications/maneriker2021improving/">Urltran: Improving Phishing URL Detection Using Transformers</a> Pranav Maneriker et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2021less/">Less Is More: Pre-train A Strong Text Encoder For Dense Retrieval Using A Weak Decoder</a> Shuqi Lu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/cahyawijaya2021benchmark/">Indonlg: Benchmark And Resources For Evaluating Indonesian Natural Language Generation</a> Samuel Cahyawijaya et al. </li>
     
   
     
       <li> <a href="/publications/askell2021general/">A General Language Assistant As A Laboratory For Alignment</a> Amanda Askell et al. </li>
     
   
     
       <li> <a href="/publications/wang2021ernie/">ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training For Language Understanding And Generation</a> Shuohuan Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021identifier/">Codet5: Identifier-aware Unified Pre-trained Encoder-decoder Models For Code Understanding And Generation</a> Yue Wang, Weishi Wang, Shafiq Joty, Steven C. H. Hoi </li>
     
   
     
   
     
       <li> <a href="/publications/biten2021layout/">Latr: Layout-aware Transformer For Scene-text VQA</a> Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, R. Manmatha </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021encoder/">Deltalm: Encoder-decoder Pre-training For Language Generation And Translation By Augmenting Pretrained Multilingual Encoders</a> Shuming Ma et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chi2021multilingual/">MT6: Multilingual Pretrained Text-to-text Transformer With Translation Pairs</a> Zewen Chi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kaliamoorthi2021distilling/">Distilling Large Language Models Into Tiny And Effective Students Using Pqrnn</a> Prabhu Kaliamoorthi, Aditya Siddhant, Edward Li, Melvin Johnson </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qiu2021vt/">VT-CLIP: Enhancing Vision-language Models With Visual-guided Texts</a> Longtian Qiu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/phan2021multi/">Cotext: Multi-task Learning With Code-text Transformer</a> Long Phan et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021grounded/">Grounded Language-image Pre-training</a> Liunian Harold Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xue2021towards/">Byt5: Towards A Token-free Future With Pre-trained Byte-to-byte Models</a> Linting Xue et al. </li>
     
   
     
       <li> <a href="/publications/li2021adversarial/">Adversarial VQA: A New Benchmark For Evaluating The Robustness Of VQA Models</a> Linjie Li, Jie Lei, Zhe Gan, Jingjing Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shleifer2021improved/">Normformer: Improved Transformer Pretraining With Extra Normalization</a> Sam Shleifer, Jason Weston, Myle Ott </li>
     
   
     
       <li> <a href="/publications/yao2021fine/">FILIP: Fine-grained Interactive Language-image Pre-training</a> Lewei Yao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/eslami2021does/">Does CLIP Benefit Visual Question Answering In The Medical Domain As Much As It Does In The General Domain?</a> Sedigheh Eslami, Gerard De Melo, Christoph Meinel </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2021transformer/">Transformer-based Conditional Variational Autoencoder For Controllable Story Generation</a> Le Fang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2021efficient/">Longt5: Efficient Text-to-text Transformer For Long Sequences</a> Mandy Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2021pre/">Condenser: A Pre-training Architecture For Dense Retrieval</a> Luyu Gao, Jamie Callan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2021unsupervised/">Unsupervised Corpus Aware Language Model Pre-training For Dense Passage Retrieval</a> Luyu Gao, Jamie Callan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022position/">PEVL: Position-enhanced Pre-training And Prompt Tuning For Vision-language Models</a> Yuan Yao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sejnowski2022large/">Large Language Models And The Reverse Turing Test</a> Terrence Sejnowski </li>
     
   
     
   
     
       <li> <a href="/publications/lee2022screenshot/">Pix2struct: Screenshot Parsing As Pretraining For Visual Language Understanding</a> Kenton Lee et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022foundation/">Omnivl:one Foundation Model For Image-language And Video-language Tasks</a> Junke Wang et al. </li>
     
   
     
       <li> <a href="/publications/li2022bootstrapping/">BLIP: Bootstrapping Language-image Pre-training For Unified Vision-language Understanding And Generation</a> Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022fine/">Fine-grained Semantically Aligned Vision-language Pre-training</a> Juncheng Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geiping2022training/">Cramming: Training A Language Model On A Single GPU In One Day</a> Jonas Geiping, Tom Goldstein </li>
     
   
     
   
     
       <li> <a href="/publications/lehman2022evolution/">Evolution Through Large Models</a> Joel Lehman et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022pretraining/">Coditt5: Pretraining For Source Code And Natural Language Editing</a> Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, Milos Gligoric </li>
     
   
     
   
     
       <li> <a href="/publications/yang2022vision/">Vision-language Pre-training With Triple Contrastive Learning</a> Jinyu Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2022empirical/">An Empirical Study Of End-to-end Video-language Transformers With Masked Visual Modeling</a> Tsu-jui Fu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022personalized/">Personalized Prompt For Sequential Recommendation</a> Yiqing Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022simple/">Lilt: A Simple Yet Effective Language-independent Layout Transformer For Structured Document Understanding</a> Jiapeng Wang, Lianwen Jin, Kai Ding </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022knowledge/">Knowledge Prompting In Pre-trained Language Model For Natural Language Understanding</a> Jianing Wang et al. </li>
     
   
     
       <li> <a href="/publications/wang2022generative/">GIT: A Generative Image-to-text Transformer For Vision And Language</a> Jianfeng Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yu2022contrastive/">Coca: Contrastive Captioners Are Image-text Foundation Models</a> Jiahui Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022end/">End-to-end Transformer Based Model For Image Captioning</a> Yiyu Wang, Jungang Xu, Yingfei Sun </li>
     
   
     
       <li> <a href="/publications/alabi2022adapting/">Adapting Pre-trained Language Models To African Languages Via Multilingual Adaptive Fine-tuning</a> Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, Dietrich Klakow </li>
     
   
     
       <li> <a href="/publications/li2022clinical/">Clinical-longformer And Clinical-bigbird: Transformers For Long Clinical Sequences</a> Yikuan Li, Ramsey M. Wehbe, Faraz S. Ahmad, Hanyin Wang, Yuan Luo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/delarosa2022efficient/">BERTIN: Efficient Pre-training Of A Spanish Language Model Using Perplexity Sampling</a> Javier De La Rosa et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022linguistically/">LERT: A Linguistically-motivated Pre-trained Language Model</a> Yiming Cui, Wanxiang Che, Shijin Wang, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alabdulmohsin2022revisiting/">Revisiting Neural Scaling Laws In Language And Vision</a> Ibrahim Alabdulmohsin, Behnam Neyshabur, Xiaohua Zhai </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yuan2022pretraining/">Biobart: Pretraining And Evaluation Of A Biomedical Generative Language Model</a> Hongyi Yuan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022collaborative/">COTS: Collaborative Two-stream Vision-language Pre-training Model For Cross-modal Retrieval</a> Haoyu Lu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/du2022survey/">A Survey Of Vision-language Pre-trained Models</a> Yifan Du, Zikang Liu, Junyi Li, Wayne Xin Zhao </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2022prompt/">Prompt Tuning For Generative Multimodal Pretrained Models</a> Hao Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022revisiting/">Revisiting Parameter-efficient Tuning: Are We Really There Yet?</a> Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, Shangsong Liang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2022enabling/">Enabling Multimodal Generation On CLIP Via Vision-language Knowledge Distillation</a> Wenliang Dai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022transferability/">On The Transferability Of Pre-trained Language Models For Low-resource Programming Languages</a> Fuxiang Chen, Fatemeh Fard, David Lo, Timofey Bryksin </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/christopoulou2022pangu/">Pangu-coder: Program Synthesis With Function-level Language Modeling</a> Fenia Christopoulou et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2022vision/">Vision-language Intelligence: Tasks, Representation Learning, And Large Models</a> Feng Li et al. </li>
     
   
     
       <li> <a href="/publications/sammani2022nlx/">NLX-GPT: A Model For Natural Language Explanations In Vision And Vision-language Tasks</a> Fawaz Sammani, Tanmoy Mukherjee, Nikos Deligiannis </li>
     
   
     
       <li> <a href="/publications/moiseev2022structured/">SKILL: Structured Knowledge Infusion For Large Language Models</a> Fedor Moiseev, Zhe Dong, Enrique Alfonseca, Martin Jaggi </li>
     
   
     
       <li> <a href="/publications/cheng2022recipe/">Vindlu: A Recipe For Effective Video-and-language Pretraining</a> Feng Cheng et al. </li>
     
   
     
   
     
       <li> <a href="/publications/peng2022sgva/">Sgva-clip: Semantic-guided Visual Adapting Of Vision-language Models For Few-shot Image Classification</a> Fang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang, Changsheng Xu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nijkamp2022open/">Codegen: An Open Large Language Model For Code With Multi-turn Program Synthesis</a> Erik Nijkamp et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kurtic2022optimal/">The Optimal BERT Surgeon: Scalable And Accurate Second-order Pruning For Large Language Models</a> Eldar Kurtic et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/he2022space/">SPACE-3: Unified Dialog Model Pre-training For Task-oriented Dialog Understanding And Generation</a> Wanwei He et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022library/">LAVIS: A Library For Language-vision Intelligence</a> Dongxu Li et al. </li>
     
   
     
       <li> <a href="/publications/gao2022multi/">MIST: Multi-modal Iterative Spatial-temporal Transformer For Long-form Video Question Answering</a> Difei Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wan2022factuality/">Factpegasus: Factuality-aware Pre-training And Fine-tuning For Abstractive Summarization</a> David Wan, Mohit Bansal </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2022building/">Bridgetower: Building Bridges Between Encoders In Vision-language Representation Learning</a> Xiao Xu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hernandez2022scaling/">Scaling Laws And Interpretability Of Learning From Repeated Data</a> Danny Hernandez et al. </li>
     
   
     
       <li> <a href="/publications/zan2022large/">Large Language Models Meet Nl2code: A Survey</a> Daoguang Zan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fried2022generative/">Incoder: A Generative Model For Code Infilling And Synthesis</a> Daniel Fried et al. </li>
     
   
     
       <li> <a href="/publications/zan2022continual/">CERT: Continual Pre-training On Sketches For Library-oriented Code Generation</a> Daoguang Zan et al. </li>
     
   
     
       <li> <a href="/publications/dong2022masked/">Maskclip: Masked Self-distillation Advances Contrastive Language-image Pretraining</a> Xiaoyi Dong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2022new/">Mixgen: A New Multi-modal Data Augmentation</a> Xiaoshuai Hao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022effective/">Mplug: Effective And Efficient Vision-language Learning By Cross-modal Skip-connections</a> Chenliang Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hsu2022language/">Language Model Compression With Weighted Low-rank Factorization</a> Yen-chang Hsu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dai2022dialog/">Dialog Inpainting: Turning Documents Into Dialogs</a> Zhuyun Dai et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2022unifying/">UL2: Unifying Language Learning Paradigms</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2022long/">Long-form Video-language Pre-training With Multimodal Temporal Contrastive Learning</a> Yuchong Sun et al. </li>
     
   
     
       <li> <a href="/publications/wang2022exploring/">Exploring The Limits Of Domain-adaptive Training For Detoxifying Large-scale Language Models</a> Boxin Wang et al. </li>
     
   
     
       <li> <a href="/publications/wang2022iteratively/">Iteratively Prompt Pre-trained Language Models For Chain Of Thought</a> Boshi Wang, Xiang Deng, Huan Sun </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/workshop2022open/">BLOOM: A 176b-parameter Open-access Multilingual Language Model</a> Bigscience Workshop et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022multimodal/">Mukea: Multimodal Knowledge Extraction And Accumulation For Knowledge-based Visual Question Answering</a> Yang Ding et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ling2022vision/">Vision-language Pre-training For Multimodal Aspect-based Sentiment Analysis</a> Yan Ling, Jianfei Yu, Rui Xia </li>
     
   
     
   
     
       <li> <a href="/publications/peng2022large/">GODEL: Large-scale Pre-training For Goal-directed Dialog</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2022pre/">Layoutlmv3: Pre-training For Document AI With Unified Text And Image Masking</a> Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2022unified/">A Unified End-to-end Retriever-reader Framework For Knowledge-based VQA</a> Yangyang Guo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bapna2022massively/">Mslam: Massively Multilingual Joint Pre-training For Speech And Text</a> Ankur Bapna et al. </li>
     
   
     
       <li> <a href="/publications/bapna2022building/">Building Machine Translation Systems For The Next Thousand Languages</a> Ankur Bapna et al. </li>
     
   
     
       <li> <a href="/publications/zeng2022glm/">GLM-130B: An Open Bilingual Pre-trained Model</a> Aohan Zeng et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022scaling/">Scaling Language-image Pre-training Via Masking</a> Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, Kaiming He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qiao2022history/">HOP: History-and-order Aware Pre-training For Vision-and-language Navigation</a> Yanyuan Qiao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022democratizing/">Democratizing Contrastive Language-image Pre-training: A CLIP Benchmark Of Data, Model, And Supervision</a> Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, Jing Shao </li>
     
   
     
       <li> <a href="/publications/wang2022prompt/">Promda: Prompt-based Data Augmentation For Low-resource NLU Tasks</a> Yufei Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhao2022learning/">Learning Video Representations From Large Language Models</a> Yue Zhao, Ishan Misra, Philipp Kr√§henb√ºhl, Rohit Girdhar </li>
     
   
     
       <li> <a href="/publications/wan2022what/">What Do They Capture? -- A Structural Analysis Of Pre-trained Language Models For Source Code</a> Yao Wan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dou2022coarse/">Coarse-to-fine Vision-language Pre-training With Fusion In The Backbone</a> Zi-yi Dou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wettig2022should/">Should You Mask 15% In Masked Language Modeling?</a> Alexander Wettig, Tianyu Gao, Zexuan Zhong, Danqi Chen </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022position/">Position-guided Text Prompt For Vision-language Pre-training</a> Alex Jinpeng Wang, Pan Zhou, Mike Zheng Shou, Shuicheng Yan </li>
     
   
     
       <li> <a href="/publications/gu2022investigating/">EVA2.0: Investigating Open-domain Chinese Dialogue Systems With Large-scale Pre-training</a> Yuxian Gu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewkowycz2022solving/">Solving Quantitative Reasoning Problems With Language Models</a> Aitor Lewkowycz et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/siddhant2022towards/">Towards The Next 1000 Languages In Multilingual Machine Translation: Exploring The Synergy Between Supervised And Self-supervised Learning</a> Aditya Siddhant et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kandpal2022large/">Large Language Models Struggle To Learn Long-tail Knowledge</a> Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/taylor2022clinical/">Clinical Prompt Learning With Frozen Language Models</a> Niall Taylor, Yi Zhang, Dan Joyce, Alejo Nevado-holgado, Andrey Kormilitzin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/eddine2022pretrained/">Arabart: A Pretrained Arabic Sequence-to-sequence Model For Abstractive Summarization</a> Moussa Kamal Eddine, Nadi Tomeh, Nizar Habash, Joseph Le Roux, Michalis Vazirgiannis </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kang2022knowledge/">KALA: Knowledge-augmented Language Model Adaptation</a> Minki Kang, Jinheon Baek, Sung Ju Hwang </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2022unsupervised/">Unsupervised Vision-and-language Pre-training Via Retrieval-based Multi-granular Alignment</a> Mingyang Zhou et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022faster/">Cogview2: Faster And Better Text-to-image Generation Via Hierarchical Transformers</a> Ming Ding, Wendi Zheng, Wenyi Hong, Jie Tang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gan2022vision/">Vision-language Pre-training: Basics, Recent Advances, And Future Trends</a> Zhe Gan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022efficient/">An Efficient Memory-augmented Transformer For Knowledge-intensive NLP Tasks</a> Yuxiang Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cherti2022reproducible/">Reproducible Scaling Laws For Contrastive Language-image Learning</a> Mehdi Cherti et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2022pre/">Retromae: Pre-training Retrieval-oriented Language Models Via Masked Auto-encoder</a> Shitao Xiao, Zheng Liu, Yingxia Shao, Zhao Cao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hou2022learning/">Learning Vector-quantized Item Representation For Transferable Sequential Recommenders</a> Yupeng Hou, Zhankui He, Julian Mcauley, Wayne Xin Zhao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kojima2022large/">Large Language Models Are Zero-shot Reasoners</a> Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa </li>
     
   
     
   
     
       <li> <a href="/publications/phan2022pretrained/">Vit5: Pretrained Text-to-text Transformer For Vietnamese Language Generation</a> Long Phan, Hieu Tran, Hieu Nguyen, Trieu H. Trinh </li>
     
   
     
       <li> <a href="/publications/wei2022multimodality/">MVP: Multimodality-guided Visual Pre-training</a> Longhui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, Qi Tian </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022multi/">Instructionner: A Multi-task Instruction-based Generative Framework For Few-shot NER</a> Liwen Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2022retrieval/">REVEAL: Retrieval-augmented Visual-language Pre-training With Multi-source Multimodal Knowledge Memory</a> Ziniu Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2022exploring/">Exploring The Universal Vulnerability Of Prompt-based Learning Paradigm</a> Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/scialom2022fine/">Fine-tuned Language Models Are Continual Learners</a> Thomas Scialom, Tuhin Chakrabarty, Smaranda Muresan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ramos2022lightweight/">Smallcap: Lightweight Image Captioning Prompted With Retrieval Augmentation</a> Rita Ramos, Bruno Martins, Desmond Elliott, Yova Kementchedjhieva </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/malladi2023fine/">Fine-tuning Language Models With Just Forward Passes</a> Sadhika Malladi et al. </li>
     
   
     
       <li> <a href="/publications/bulathwela2023scalable/">Scalable Educational Question Generation With Pre-trained Language Models</a> Sahan Bulathwela, Hamze Muse, Emine Yilmaz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023then/">Prompt, Generate, Then Cache: Cascade Of Foundation Models Makes Strong Few-shot Learners</a> Renrui Zhang et al. </li>
     
   
     
       <li> <a href="/publications/antonello2023scaling/">Scaling Laws For Language Encoding Models In Fmri</a> Richard Antonello, Aditya Vaidya, Alexander G. Huth </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pires2023portuguese/">Sabi\'a: Portuguese Large Language Models</a> Ramon Pires, Hugo Abonizio, Thales Sales Almeida, Rodrigo Nogueira </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023joint/">SPHINX: The Joint Mixing Of Weights, Tasks, And Visual Embeddings For Multi-modal Large Language Models</a> Ziyi Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023pre/">Codegeex: A Pre-trained Model For Code Generation With Multilingual Benchmarking On Humaneval-x</a> Qinkai Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2023unifying/">Unifying Large Language Models And Knowledge Graphs: A Roadmap</a> Shirui Pan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2023pre/">3d-vista: Pre-trained Transformer For 3D Vision And Text Alignment</a> Ziyu Zhu et al. </li>
     
   
     
       <li> <a href="/publications/li2023masked/">Masked Vision And Language Pre-training With Unimodal And Multimodal Contrastive Losses For Medical Visual Question Answering</a> Pengfei Li, Gang Liu, Jinlong He, Zixu Zhao, Shenjun Zhong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rotstein2023leveraging/">Fusecap: Leveraging Large Language Models For Enriched Fused Image Captions</a> Noam Rotstein, David Bensaid, Shaked Brody, Roy Ganz, Ron Kimmel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sengupta2023jais/">Jais And Jais-chat: Arabic-centric Foundation And Instruction-tuned Open Generative Large Language Models</a> Neha Sengupta et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moor2023med/">Med-flamingo: A Multimodal Medical Few-shot Learner</a> Michael Moor et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ishmam2023from/">From Image To Language: A Critical Analysis Of Visual Question Answering (VQA) Approaches, Challenges, And Opportunities</a> Md Farhan Ishmam, Md Sakib Hossain Shovon, M. F. Mridha, Nilanjan Dey </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parelli2023clip/">Clip-guided Vision-language Pre-training For Question Answering In 3D Scenes</a> Maria Parelli et al. </li>
     
   
     
   
     
       <li> <a href="/publications/binz2023turning/">Turning Large Language Models Into Cognitive Models</a> Marcel Binz, Eric Schulz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2023improving/">Improving CLIP Training With Language Rewrites</a> Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, Yonglong Tian </li>
     
   
     
   
     
       <li> <a href="/publications/yu2023scaling/">Scaling Autoregressive Multi-modal Models: Pretraining And Instruction Tuning</a> Lili Yu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023improving/">Improving Text Embeddings With Large Language Models</a> Liang Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xue2023ulip/">ULIP-2: Towards Scalable Multimodal Pre-training For 3D Understanding</a> Le Xue et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2023effective/">Tallrec: An Effective And Efficient Tuning Framework To Align Large Language Model With Recommendation</a> Keqin Bao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tian2023just/">Just Ask For Calibration: Strategies For Eliciting Calibrated Confidence Scores From Language Models Fine-tuned With Human Feedback</a> Katherine Tian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/soman2023biomedical/">Biomedical Knowledge Graph-optimized Prompt Generation For Large Language Models</a> Karthik Soman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023adaptive/">ALIP: Adaptive Language-image Pre-training With Synthetic Caption</a> Kaicheng Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023blip/">BLIP-2: Bootstrapping Language-image Pre-training With Frozen Image Encoders And Large Language Models</a> Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi </li>
     
   
     
   
     
       <li> <a href="/publications/lu2023llama/">Llama-reviewer: Advancing Code Review Automation With Large Language Models Through Parameter-efficient Fine-tuning</a> Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, Chun Zuo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ainslie2023training/">GQA: Training Generalized Multi-query Transformer Models From Multi-head Checkpoints</a> Joshua Ainslie et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023prompt/">Prompt-and-align: Prompt-based Social Alignment For Few-shot Fake News Detection</a> Jiaying Wu, Shen Li, Ailin Deng, Miao Xiong, Bryan Hooi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023empowering/">Empowering Molecule Discovery For Molecule-caption Translation With Large Language Models: A Chatgpt Perspective</a> Jiatong Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023unlearn/">Unlearn What You Want To Forget: Efficient Unlearning For Llms</a> Jiaao Chen, Diyi Yang </li>
     
   
     
   
     
       <li> <a href="/publications/lin2023pre/">VILA: On Pre-training For Visual Language Models</a> Ji Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/m%C3%B6kander2023auditing/">Auditing Large Language Models: A Three-layered Approach</a> Jakob M√∂kander, Jonas Schuett, Hannah Rose Kirk, Luciano Floridi </li>
     
   
     
       <li> <a href="/publications/jahan2023evaluation/">Evaluation Of Chatgpt On Biomedical Tasks: A Zero-shot Comparison With Fine-tuned Generative Transformers</a> Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jahan2023comprehensive/">A Comprehensive Evaluation Of Large Language Models On Benchmark Biomedical Text Processing Tasks</a> Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023pre/">Missrec: Pre-training And Transferring Multi-modal Interest-aware Sequence Representation For Recommendation</a> Jinpeng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zuccon2023dr/">Dr Chatgpt, Tell Me What I Want To Hear: How Prompt Knowledge Impacts Health Answer Correctness</a> Guido Zuccon, Bevan Koopman </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/penedo2023refinedweb/">The Refinedweb Dataset For Falcon LLM: Outperforming Curated Corpora With Web Data, And Web Data Only</a> Guilherme Penedo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xiao2023efficient/">Efficient Streaming Language Models With Attention Sinks</a> Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kortemeyer2023performance/">Performance Of The Pre-trained Large Language Model GPT-4 On Automated Short Answer Grading</a> Gerd Kortemeyer </li>
     
   
     
       <li> <a href="/publications/luo2023cheap/">Cheap And Quick: Efficient Vision-language Instruction Tuning For Large Language Models</a> Gen Luo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nijkamp2023lessons/">Codegen2: Lessons For Training Llms On Programming And Natural Languages</a> Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou </li>
     
   
     
       <li> <a href="/publications/lehman2023do/">Do We Still Need Clinical Language Models?</a> Eric Lehman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/almazrouei2023falcon/">The Falcon Series Of Open Language Models</a> Ebtesam Almazrouei et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023empowering/">Speechgpt: Empowering Large Language Models With Intrinsic Cross-modal Conversational Abilities</a> Dong Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mollo2023vector/">The Vector Grounding Problem</a> Dimitri Coelho Mollo, Rapha√´l Milli√®re </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023solar/">SOLAR 10.7B: Scaling Large Language Models With Simple Yet Effective Depth Up-scaling</a> Dahyun Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023iterative/">An Iterative Optimizing Framework For Radiology Report Summarization With Chatgpt</a> Chong Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2023foundation/">K2: A Foundation Language Model For Geoscience Knowledge Understanding And Utilization</a> Cheng Deng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023human/">Motiongpt: Human Motion As A Foreign Language</a> Biao Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023baichuan/">Baichuan 2: Open Large-scale Language Models</a> Aiyuan Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2023towards/">Instructblip: Towards General-purpose Vision-language Models With Instruction Tuning</a> Wenliang Dai et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023pmc/">PMC-CLIP: Contrastive Language-image Pre-training Using Biomedical Documents</a> Weixiong Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023is/">Is Chatgpt Good At Search? Investigating Large Language Models As Re-ranking Agents</a> Weiwei Sun et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xin2023survey/">A Survey Of Large Language Models</a> Wayne Xin Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kocmi2023large/">Large Language Models Are State-of-the-art Evaluators Of Translation Quality</a> Tom Kocmi, Christian Federmann </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023recommender/">Recommender Systems In The Era Of Large Language Models (llms)</a> Zihuai Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/naous2023having/">Having Beer After Prayer? Measuring Cultural Bias In Large Language Models</a> Tarek Naous, Michael J. Ryan, Alan Ritter, Wei Xu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2023efficient/">Efficient And Effective Text Encoding For Chinese Llama And Alpaca</a> Yiming Cui, Ziqing Yang, Xin Yao </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023comparative/">A Comparative Study Of Pretrained Language Models For Long Clinical Text</a> Yikuan Li, Ramsey M. Wehbe, Faraz S. Ahmad, Hanyin Wang, Yuan Luo </li>
     
   
     
   
     
       <li> <a href="/publications/fang2023mol/">Mol-instructions: A Large-scale Biomolecular Instruction Dataset For Large Language Models</a> Yin Fang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2023summary/">Summary Of Chatgpt-related Research And Perspective Towards The Future Of Large Language Models</a> Yiheng Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023making/">Making Large Language Models Perform Better In Knowledge Graph Completion</a> Yichi Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mu2023vision/">Embodiedgpt: Vision-language Pre-training Via Embodied Chain Of Thought</a> Yao Mu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023xuanyuan/">Xuanyuan 2.0: A Large Chinese Financial Chat Model With Hundreds Of Billions Parameters</a> Xuanyu Zhang, Qing Yang, Dongliang Xu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zang2023contextual/">Contextual Object Detection With Multimodal Large Language Models</a> Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, Chen Change Loy </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dan2023large/">Educhat: A Large-scale Language Model-based Chatbot System For Intelligent Education</a> Yuhao Dan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhuang2023dataset/">Toolqa: A Dataset For LLM Question Answering With External Tools</a> Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, Chao Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/biderman2023emergent/">Emergent And Predictable Memorization In Large Language Models</a> Stella Biderman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023enhancing/">Zhongjing: Enhancing The Chinese Medical Capabilities Of Large Language Model Through Expert Feedback And Real-world Multi-turn Dialogue</a> Songhua Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023self/">Self-chained Image-language Model For Video Localization And Question Answering</a> Shoubin Yu, Jaemin Cho, Prateek Yadav, Mohit Bansal </li>
     
   
     
   
     
       <li> <a href="/publications/pramanick2023egocentric/">Egovlpv2: Egocentric Video-language Pre-training With Fusion In The Backbone</a> Shraman Pramanick et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023alpha/">Alpha-clip: A CLIP Model Focusing On Wherever You Want</a> Zeyi Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chen2023meditron/">MEDITRON-70B: Scaling Medical Pretraining For Large Language Models</a> Zeming Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023preventing/">Preventing Zero-shot Transfer Degradation In Continual Learning Of Vision-language Models</a> Zangwei Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2023eva/">EVA-02: A Visual Representation For Neon Genesis</a> Yuxin Fang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tong2024eyes/">Eyes Wide Shut? Exploring The Visual Shortcomings Of Multimodal Llms</a> Shengbang Tong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2024findings/">Findings Of The Second Babylm Challenge: Sample-efficient Pretraining On Developmentally Plausible Corpora</a> Michael Y. Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gekhman2024does/">Does Fine-tuning Llms On New Knowledge Encourage Hallucinations?</a> Zorik Gekhman et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2024pixart/">Pixart-\sigma: Weak-to-strong Training Of Diffusion Transformer For 4K Text-to-image Generation</a> Junsong Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hartsock2024vision/">Vision-language Models For Medical Report Generation And Visual Question Answering: A Review</a> Iryna Hartsock, Ghulam Rasool </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gemmateam2024open/">Gemma: Open Models Based On Gemini Research And Technology</a> Gemma Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2024deepseek/">Deepseek-coder: When The Large Language Model Meets Programming -- The Rise Of Code Intelligence</a> Daya Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gholami2024ai/">AI And Memory Wall</a> Amir Gholami et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ai2024open/">Yi: Open Foundation Models By 01.AI</a> 01. Ai et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yuan2024prompt/">Unist: A Prompt-empowered Universal Model For Urban Spatio-temporal Prediction</a> Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2024understanding/">Understanding Llms: A Comprehensive Overview From Training To Inference</a> Yiheng Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/labrak2024collection/">Biomistral: A Collection Of Open-source Pretrained Large Language Models For Medical Domains</a> Yanis Labrak et al. </li>
     
   
     
       <li> <a href="/publications/liu2024datasets/">Datasets For Large Language Models: A Comprehensive Survey</a> Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2024unsupervised/">Promptkd: Unsupervised Prompt Distillation For Vision-language Models</a> Zheng Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2024exploratory/">Llmparser: An Exploratory Study On Using Large Language Models For Log Parsing</a> Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-hsun Chen, Shaowei Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/warstadt2025findings/">Findings Of The Babylm Challenge: Sample-efficient Pretraining On Developmentally Plausible Corpora</a> Alex Warstadt et al. </li>
     
   
   </ul>

   <h3>üè∑ Prompting <a id="Prompting"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2018hierarchical/">Hierarchical Neural Story Generation</a> Angela Fan, Mike Lewis, Yann Dauphin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2018response/">Response Generation By Context-aware Prototype Editing</a> Yu Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/weston2018retrieve/">Retrieve And Refine: Improved Sequence Generation Models For Dialogue</a> Jason Weston, Emily Dinan, Alexander H. Miller </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2018fine/">Lingke: A Fine-grained Multi-turn Chatbot For Customer Service</a> Pengfei Zhu, Zhuosheng Zhang, Jiangtong Li, Yafang Huang, Hai Zhao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/holtzman2019curious/">The Curious Case Of Neural Text Degeneration</a> Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clark2019exploring/">Boolq: Exploring The Surprising Difficulty Of Natural Yes/no Questions</a> Christopher Clark et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hsu2019knowledge/">Knowledge-enriched Visual Storytelling</a> Chao-chun Hsu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2019how/">How Can We Know What Language Models Know?</a> Zhengbao Jiang, Frank F. Xu, Jun Araki, Graham Neubig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kassner2019negated/">Negated And Misprimed Probes For Pretrained Language Models: Birds Can Talk, But Cannot Fly</a> Nora Kassner, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/adiwardana2020towards/">Towards A Human-like Open-domain Chatbot</a> Daniel Adiwardana et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gehman2020evaluating/">Realtoxicityprompts: Evaluating Neural Toxic Degeneration In Language Models</a> Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, Noah A. Smith </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/donahue2020enabling/">Enabling Language Models To Fill In The Blanks</a> Chris Donahue, Mina Lee, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hill2020grounded/">Grounded Language Learning Fast And Slow</a> Felix Hill et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nichols2020collaborative/">Collaborative Storytelling With Large-scale Neural Language Models</a> Eric Nichols, Leo Gao, Randy Gomez </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/penha2020what/">What Does BERT Know About Books, Movies And Music? Probing BERT For Conversational Recommendation</a> Gustavo Penha, Claudia Hauff </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020narrative/">Narrative Interpolation For Generating And Understanding Stories</a> Su Wang, Greg Durrett, Katrin Erk </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2020progressive/">Progressive Generation Of Long Text With Pretrained Language Models</a> Bowen Tan, Zichao Yang, Maruan Ai-shedivat, Eric P. Xing, Zhiting Hu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shin2020eliciting/">Autoprompt: Eliciting Knowledge From Language Models With Automatically Generated Prompts</a> Taylor Shin, Yasaman Razeghi, Robert L. Iv Logan, Eric Wallace, Sameer Singh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chan2020self/">Cocon: A Self-supervised Approach For Controlled Text Generation</a> Alvin Chan, Yew-soon Ong, Bill Pung, Aston Zhang, Jie Fu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fabbri2020template/">Template-based Question Generation From Retrieved Sentences For Improved Unsupervised Question Answering</a> Alexander R. Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, Bing Xiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/goldfarbtarrant2020content/">Content Planning For Neural Story Generation With Aristotelian Rescoring</a> Seraphina Goldfarb-tarrant, Tuhin Chakrabarty, Ralph Weischedel, Nanyun Peng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020making/">Making Pre-trained Language Models Better Few-shot Learners</a> Tianyu Gao, Adam Fisch, Danqi Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mcguffie2020radicalization/">The Radicalization Risks Of GPT-3 And Advanced Neural Language Models</a> Kris Mcguffie, Alex Newhouse </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021contrast/">Contrast And Generation Make BART A Good Dialogue Emotion Recognizer</a> Shimin Li, Hang Yan, Xipeng Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2021learning/">Learning How To Ask: Querying Lms With Mixtures Of Soft Prompts</a> Guanghui Qin, Jason Eisner </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2021good/">A Good Prompt Is Worth Millions Of Parameters: Low-resource Prompt-based Learning For Vision-language Models</a> Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, Xiang Ren </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021few/">Few-shot Learning With Multilingual Language Models</a> Xi Victoria Lin et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2021open/">Openprompt: An Open-source Framework For Prompt-learning</a> Ning Ding et al. </li>
     
   
     
       <li> <a href="/publications/perez2021true/">True Few-shot Learning With Language Models</a> Ethan Perez, Douwe Kiela, Kyunghyun Cho </li>
     
   
     
   
     
       <li> <a href="/publications/ding2021prompt/">Prompt-learning For Fine-grained Entity Typing</a> Ning Ding et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kharitonov2021text/">Text-free Prosody-aware Generative Spoken Language Modeling</a> Eugene Kharitonov et al. </li>
     
   
     
       <li> <a href="/publications/chen2021knowledge/">Knowprompt: Knowledge-aware Prompt-tuning With Synergistic Optimization For Relation Extraction</a> Xiang Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2021differentiable/">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners</a> Ningyu Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/reif2021recipe/">A Recipe For Arbitrary Text Style Transfer With Large Language Models</a> Emily Reif et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021lightweight/">Lightner: A Lightweight Tuning Paradigm For Low-resource NER Via Pluggable Prompting</a> Xiang Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021align/">Align And Prompt: Video-and-language Pre-training With Entity Prompts</a> Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, Steven C. H. Hoi </li>
     
   
     
   
     
       <li> <a href="/publications/dziri2021neural/">Neural Path Hunter: Reducing Hallucination In Dialogue Systems Via Path Grounding</a> Nouha Dziri, Andrea Madotto, Osmar Zaiane, Avishek Joey Bose </li>
     
   
     
       <li> <a href="/publications/liu2021gpt/">GPT Understands, Too</a> Xiao Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021p/">P-tuning V2: Prompt Tuning Can Be Comparable To Fine-tuning Universally Across Scales And Tasks</a> Xiao Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/rubin2021learning/">Learning To Retrieve Prompts For In-context Learning</a> Ohad Rubin, Jonathan Herzig, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pascual2021plug/">A Plug-and-play Method For Controlled Text Generation</a> Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell, Roger Wattenhofer </li>
     
   
     
       <li> <a href="/publications/sileo2021zero/">Zero-shot Recommendation As Language Modeling</a> Damien Sileo, Wout Vossen, Robbe Raymaekers </li>
     
   
     
   
     
       <li> <a href="/publications/wang2021can/">Can Generative Pre-trained Language Models Serve As Knowledge Bases For Closed-book QA?</a> Cunxiang Wang, Pai Liu, Yue Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vu2021better/">Spot: Better Frozen Model Adaptation Through Soft Prompt Transfer</a> Tu Vu, Brian Lester, Noah Constant, Rami Al-rfou, Daniel Cer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021revisiting/">Revisiting Self-training For Few-shot Learning Of Language Model</a> Yiming Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021what/">What Makes Good In-context Examples For GPT-\(3\)?</a> Jiachang Liu et al. </li>
     
   
     
       <li> <a href="/publications/sanh2021multitask/">Multitask Prompted Training Enables Zero-shot Task Generalization</a> Victor Sanh et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021generated/">Generated Knowledge Prompting For Commonsense Reasoning</a> Jiacheng Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2021finetuned/">Finetuned Language Models Are Zero-shot Learners</a> Jason Wei et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/austin2021program/">Program Synthesis With Large Language Models</a> Jacob Austin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2021pre/">PPT: Pre-trained Prompt Tuning For Few-shot Learning</a> Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/logan2021cutting/">Cutting Down On Prompts And Parameters: Simple Few-shot Learning With Language Models</a> Robert L. Iv Logan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021prefix/">Prefix-tuning: Optimizing Continuous Prompts For Generation</a> Xiang Lisa Li, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2021colorful/">CPT: Colorful Prompt Tuning For Pre-trained Vision-language Models</a> Yuan Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zou2021controllable/">Controllable Generation From Pre-trained Language Models Via Inverse Prompting</a> Xu Zou et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021study/">BERT, Mbert, Or Bibert? A Study On Contextualized Embeddings For Neural Machine Translation</a> Haoran Xu, Benjamin Van Durme, Kenton Murray </li>
     
   
     
       <li> <a href="/publications/sung2021vl/">Vl-adapter: Parameter-efficient Transfer Learning For Vision-and-language Tasks</a> Yi-lin Sung, Jaemin Cho, Mohit Bansal </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mishra2021reframing/">Reframing Instructional Prompts To Gptk's Language</a> Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/schick2021true/">True Few-shot Learning With Prompts -- A Real-world Perspective</a> Timo Schick, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021ai/">AI Chains: Transparent And Controllable Human-ai Interaction By Chaining Large Language Model Prompts</a> Tongshuang Wu, Michael Terry, Carrie J. Cai </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rao2021language/">Denseclip: Language-guided Dense Prediction With Context-aware Prompting</a> Yongming Rao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bragg2021unifying/">FLEX: Unifying Evaluation For Few-shot NLP</a> Jonathan Bragg, Arman Cohan, Kyle Lo, Iz Beltagy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/welbl2021challenges/">Challenges In Detoxifying Language Models</a> Johannes Welbl et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhao2021calibrate/">Calibrate Before Use: Improving Few-shot Performance Of Language Models</a> Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hambardzumyan2021word/">WARP: Word-level Adversarial Reprogramming</a> Karen Hambardzumyan, Hrant Khachatrian, Jonathan May </li>
     
   
     
       <li> <a href="/publications/yoo2021leveraging/">Gpt3mix: Leveraging Large-scale Language Models For Text Augmentation</a> Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-woo Lee, Woomyeong Park </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2021learning/">Learning To Prompt For Vision-language Models</a> Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2021empirical/">An Empirical Study Of GPT-3 For Few-shot Knowledge-based VQA</a> Zhengyuan Yang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mishra2021cross/">Cross-task Generalization Via Natural Language Crowdsourcing Instructions</a> Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schmitt2021supporting/">Characterchat: Supporting The Creation Of Fictional Characters Through Conversation And Progressive Manifestation With A Chatbot</a> Oliver Schmitt, Daniel Buschek </li>
     
   
     
       <li> <a href="/publications/wei2021why/">Why Do Pretrained Language Models Help In Downstream Tasks? An Analysis Of Head And Prompt Tuning</a> Colin Wei, Sang Michael Xie, Tengyu Ma </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2021exploring/">Exploring Prompt-based Few-shot Learning For Grounded Dialog Generation</a> Chujie Zheng, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2021dialogue/">Dialogue State Tracking With A Language Model Using Schema-driven Prompting</a> Chia-hsuan Lee, Hao Cheng, Mari Ostendorf </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2021unified/">LFPT5: A Unified Framework For Lifelong Few-shot Language Learning Based On Prompt Tuning Of T5</a> Chengwei Qin, Shafiq Joty </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2021cpm/">CPM-2: Large-scale Cost-effective Pre-trained Language Models</a> Zhengyan Zhang et al. </li>
     
   
     
       <li> <a href="/publications/ju2021prompting/">Prompting Visual-language Models For Efficient Video Understanding</a> Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tafjord2021general/">General-purpose Question-answering With Macaw</a> Oyvind Tafjord, Peter Clark </li>
     
   
     
   
     
       <li> <a href="/publications/han2021prompt/">PTR: Prompt Tuning With Rules For Text Classification</a> Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, Maosong Sun </li>
     
   
     
       <li> <a href="/publications/sun2021nsp/">NSP-BERT: A Prompt-based Few-shot Learner Through An Original Pre-training Task--next Sentence Prediction</a> Yi Sun, Yu Zheng, Chao Hao, Hangping Qiu </li>
     
   
     
   
     
       <li> <a href="/publications/chen2021self/">Self-supervised Dialogue Learning For Spoken Conversational Question Answering</a> Nuo Chen, Chenyu You, Yuexian Zou </li>
     
   
     
       <li> <a href="/publications/lester2021power/">The Power Of Scale For Parameter-efficient Prompt Tuning</a> Brian Lester, Rami Al-rfou, Noah Constant </li>
     
   
     
   
     
       <li> <a href="/publications/schramowski2021large/">Large Pre-trained Language Models Contain Human-like Biases Of What Is Right And Wrong To Do</a> Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A. Rothkopf, Kristian Kersting </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/min2021recent/">Recent Advances In Natural Language Processing Via Large Pre-trained Language Models: A Survey</a> Bonan Min et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021nlp/">Generate, Annotate, And Learn: NLP With Synthetic Text</a> Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, Mohammad Norouzi </li>
     
   
     
   
     
       <li> <a href="/publications/kim2021what/">What Changes Can Large-scale Language Models Bring? Intensive Study On Hyperclova: Billions-scale Korean Generative Pretrained Transformers</a> Boseop Kim et al. </li>
     
   
     
       <li> <a href="/publications/su2021transferability/">On Transferability Of Prompt Tuning For Natural Language Processing</a> Yusheng Su et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2021clip/">Clip-adapter: Better Vision-language Models With Feature Adapters</a> Peng Gao et al. </li>
     
   
     
       <li> <a href="/publications/chen2021meta/">Meta-learning Via Language Model In-context Tuning</a> Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, He He </li>
     
   
     
       <li> <a href="/publications/liu2021pre/">Pre-train, Prompt, And Predict: A Systematic Survey Of Prompting Methods In Natural Language Processing</a> Pengfei Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aghajanyan2021hyper/">HTLM: Hyper-text Pre-training And Prompting Of Language Models</a> Armen Aghajanyan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/narayan2021planning/">Planning With Learned Entity Prompts For Abstractive Summarization</a> Shashi Narayan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/madotto2021few/">Few-shot Bot: Prompt-based Learning For Dialogue Systems</a> Andrea Madotto, Zhaojiang Lin, Genta Indra Winata, Pascale Fung </li>
     
   
     
       <li> <a href="/publications/lu2021fantastically/">Fantastically Ordered Prompts And Where To Find Them: Overcoming Few-shot Prompt Order Sensitivity</a> Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/askell2021general/">A General Language Assistant As A Laboratory For Alignment</a> Amanda Askell et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/webson2021do/">Do Prompt-based Models Really Understand The Meaning Of Their Prompts?</a> Albert Webson, Ellie Pavlick </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/west2021symbolic/">Symbolic Knowledge Distillation: From General Language Models To Commonsense Models</a> Peter West et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhong2021adapting/">Adapting Language Models For Zero-shot Learning By Meta-tuning On Dataset And Prompt Collections</a> Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wiegreffe2021reframing/">Reframing Human-ai Collaboration For Generating Free-text Explanations</a> Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/scao2021how/">How Many Data Points Is A Prompt Worth?</a> Teven Le Scao, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2021explanation/">An Explanation Of In-context Learning As Implicit Bayesian Inference</a> Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/reynolds2021prompt/">Prompt Programming For Large Language Models: Beyond The Few-shot Paradigm</a> Laria Reynolds, Kyle Mcdonell </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tsimpoukelli2021multimodal/">Multimodal Few-shot Learning With Frozen Language Models</a> Maria Tsimpoukelli et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021evaluating/">Evaluating Large Language Models Trained On Code</a> Mark Chen et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sohn2022visual/">Visual Prompt Tuning For Generative Transfer Learning</a> Kihyuk Sohn et al. </li>
     
   
     
       <li> <a href="/publications/yao2022position/">PEVL: Position-enhanced Pre-training And Prompt Tuning For Vision-language Models</a> Yuan Yao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yang2022generating/">Re3: Generating Longer Stories With Recursive Reprompting And Revision</a> Kevin Yang, Yuandong Tian, Nanyun Peng, Dan Klein </li>
     
   
     
       <li> <a href="/publications/levine2022standing/">Standing On The Shoulders Of Giant Frozen Language Models</a> Yoav Levine et al. </li>
     
   
     
       <li> <a href="/publications/sejnowski2022large/">Large Language Models And The Reverse Turing Test</a> Terrence Sejnowski </li>
     
   
     
   
     
       <li> <a href="/publications/lee2022screenshot/">Pix2struct: Screenshot Parsing As Pretraining For Visual Language Understanding</a> Kenton Lee et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xing2022dual/">Dual Modality Prompt Tuning For Vision-language Pre-trained Model</a> Yinghui Xing et al. </li>
     
   
     
       <li> <a href="/publications/jeblick2022chatgpt/">Chatgpt Makes Medicine Easy To Swallow: An Exploratory Case Study On Simplified Radiology Reports</a> Katharina Jeblick et al. </li>
     
   
     
   
     
       <li> <a href="/publications/singhal2022large/">Large Language Models Encode Clinical Knowledge</a> Karan Singhal et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2022conditional/">Conditional Prompt Learning For Vision-language Models</a> Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu </li>
     
   
     
       <li> <a href="/publications/crowson2022vqgan/">VQGAN-CLIP: Open Domain Image Generation And Editing With Natural Language Guidance</a> Katherine Crowson et al. </li>
     
   
     
       <li> <a href="/publications/wang2022what/">What Language Model Architecture And Pretraining Objective Work Best For Zero-shot Generalization?</a> Thomas Wang et al. </li>
     
   
     
       <li> <a href="/publications/chang2022exploration/">Speechprompt: An Exploration Of Prompt Tuning On Generative Spoken Language Model For Speech Processing Tasks</a> Kai-wei Chang, Wei-cheng Tseng, Shang-wen Li, Hung-yi Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2022learning/">In-context Learning For Few-shot Dialogue State Tracking</a> Yushi Hu et al. </li>
     
   
     
       <li> <a href="/publications/su2022language/">Language Models Can See: Plugging Visual Controls In Text Generation</a> Yixuan Su et al. </li>
     
   
     
   
     
       <li> <a href="/publications/robinson2022leveraging/">Leveraging Large Language Models For Multiple Choice Question Answering</a> Joshua Robinson, Christopher Michael Rytting, David Wingate </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/oppenlaender2022taxonomy/">A Taxonomy Of Prompt Modifiers For Text-to-image Generation</a> Jonas Oppenlaender </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jang2022can/">Can Large Language Models Truly Understand Prompts? A Case Study With Negated Prompts</a> Joel Jang, Seonghyeon Ye, Minjoon Seo </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2022when/">When To Make Exceptions: Exploring Language Models As Accounts Of Human Moral Judgment</a> Zhijing Jin et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khot2022decomposed/">Decomposed Prompting: A Modular Approach For Solving Complex Tasks</a> Tushar Khot et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2022are/">Are Large Pre-trained Language Models Leaking Your Personal Information?</a> Jie Huang, Hanyin Shao, Kevin Chen-chuan Chang </li>
     
   
     
   
     
       <li> <a href="/publications/wu2022personalized/">Personalized Prompt For Sequential Recommendation</a> Yiqing Wu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/huang2022large/">Large Language Models Can Self-improve</a> Jiaxin Huang et al. </li>
     
   
     
       <li> <a href="/publications/dinh2022language/">LIFT: Language-interfaced Fine-tuning For Non-language Machine Learning Tasks</a> Tuan Dinh et al. </li>
     
   
     
       <li> <a href="/publications/chakrabarty2022help/">Help Me Write A Poem: Instruction Tuning As A Vehicle For Collaborative Poetry Writing</a> Tuhin Chakrabarty, Vishakh Padmakumar, He He </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022knowledge/">Knowledge Prompting In Pre-trained Language Model For Natural Language Understanding</a> Jianing Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2022scaling/">Scaling Autoregressive Models For Content-rich Text-to-image Generation</a> Jiahui Yu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li%C3%A9vin2022can/">Can Large Language Models Reason About Medical Questions?</a> Valentin Li√©vin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, Ole Winther </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2022black/">Black-box Tuning For Language-model-as-a-service</a> Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu </li>
     
   
     
       <li> <a href="/publications/hu2022fine/">A Fine-grained Comparison Of Pragmatic Language Understanding In Humans And Language Models</a> Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko, Edward Gibson </li>
     
   
     
   
     
       <li> <a href="/publications/alayrac2022visual/">Flamingo: A Visual Language Model For Few-shot Learning</a> Jean-baptiste Alayrac et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wei2022chain/">Chain-of-thought Prompting Elicits Reasoning In Large Language Models</a> Jason Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/smith2022coda/">Coda-prompt: Continual Decomposed Attention-based Prompting For Rehearsal-free Continual Learning</a> James Seale Smith et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jung2022maieutic/">Maieutic Prompting: Logically Consistent Reasoning With Recursive Explanations</a> Jaehun Jung et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022prompt/">Prompt Tuning For Discriminative Pre-trained Language Models</a> Yuan Yao et al. </li>
     
   
     
       <li> <a href="/publications/liu2022integrating/">3DALL-E: Integrating Text-to-image AI In 3D Design Workflows</a> Vivian Liu, Jo Vermeulen, George Fitzmaurice, Justin Matejka </li>
     
   
     
   
     
       <li> <a href="/publications/merullo2022linearly/">Linearly Mapping From Image To Text Space</a> Jack Merullo, Louis Castricato, Carsten Eickhoff, Ellie Pavlick </li>
     
   
     
       <li> <a href="/publications/sekuli%C4%872022evaluating/">Evaluating Mixed-initiative Conversational Search Systems Via User Simulation</a> Ivan Sekuliƒá, Mohammad Aliannejadi, Fabio Crestani </li>
     
   
     
       <li> <a href="/publications/levy2022diverse/">Diverse Demonstrations Improve In-context Compositional Generalization</a> Itay Levy, Ben Bogin, Jonathan Berant </li>
     
   
     
   
     
       <li> <a href="/publications/singh2022generating/">Progprompt: Generating Situated Robot Task Plans Using Large Language Models</a> Ishika Singh et al. </li>
     
   
     
       <li> <a href="/publications/cui2022generative/">M6-rec: Generative Pretrained Language Models Are Open-ended Recommender Systems</a> Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang </li>
     
   
     
   
     
       <li> <a href="/publications/weng2022large/">Large Language Models Are Better Reasoners With Self-verification</a> Yixuan Weng et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chung2022scaling/">Scaling Instruction-finetuned Language Models</a> Hyung Won Chung et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bahng2022exploring/">Exploring Visual Prompts For Adapting Large-scale Models</a> Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, Phillip Isola </li>
     
   
     
   
     
       <li> <a href="/publications/yuan2022pretraining/">Biobart: Pretraining And Evaluation Of A Biomedical Generative Language Model</a> Hongyi Yuan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gonen2022demystifying/">Demystifying Prompts In Language Models Via Perplexity Estimation</a> Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, Luke Zettlemoyer </li>
     
   
     
       <li> <a href="/publications/strobelt2022interactive/">Interactive And Visual Prompt Engineering For Ad-hoc Task Adaptation With Large Language Models</a> Hendrik Strobelt et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2022teaching/">Teaching Algorithmic Reasoning Via In-context Learning</a> Hattie Zhou et al. </li>
     
   
     
       <li> <a href="/publications/joshi2022repair/">Repair Is Nearly Generation: Multilingual Program Repair With Llms</a> Harshit Joshi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2022pseudo/">Pseudo-q: Generating Pseudo Language Queries For Visual Grounding</a> Haojun Jiang, Yuanze Lin, Dongchen Han, Shiji Song, Gao Huang </li>
     
   
     
       <li> <a href="/publications/liu2022few/">Few-shot Parameter-efficient Fine-tuning Is Better And Cheaper Than In-context Learning</a> Haokun Liu et al. </li>
     
   
     
       <li> <a href="/publications/su2022selective/">Selective Annotation Makes Language Models Better Few-shot Learners</a> Hongjin Su et al. </li>
     
   
     
       <li> <a href="/publications/yang2022prompt/">Prompt Tuning For Generative Multimodal Pretrained Models</a> Hao Yang et al. </li>
     
   
     
       <li> <a href="/publications/trivedi2022interleaving/">Interleaving Retrieval With Chain-of-thought Reasoning For Knowledge-intensive Multi-step Questions</a> Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2022rethinking/">Rethinking With Retrieval: Faithful Large Language Model Inference</a> Hangfeng He, Hongming Zhang, Dan Roth </li>
     
   
     
   
     
       <li> <a href="/publications/meng2022generating/">Generating Training Data With Language Models: Towards Zero-shot Language Understanding</a> Yu Meng, Jiaxin Huang, Yu Zhang, Jiawei Han </li>
     
   
     
       <li> <a href="/publications/chen2022large/">Large Language Models Are Few(1)-shot Table Reasoners</a> Wenhu Chen </li>
     
   
     
       <li> <a href="/publications/dang2022how/">How To Prompt? Opportunities And Challenges Of Zero- And Few-shot Learning For Human-ai Interaction In Creative Applications Of Generative Models</a> Hai Dang, Lukas Mecke, Florian Lehmann, Sven Goller, Daniel Buschek </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2022generate/">Generate Rather Than Retrieve: Large Language Models Are Strong Context Generators</a> Wenhao Yu et al. </li>
     
   
     
       <li> <a href="/publications/wang2022complementary/">Dualprompt: Complementary Prompting For Rehearsal-free Continual Learning</a> Zifeng Wang et al. </li>
     
   
     
       <li> <a href="/publications/li2022making/">Making Large Language Models Better Reasoners With Step-aware Verifier</a> Yifei Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jiang2022evaluating/">Evaluating And Inducing Personality In Pre-trained Language Models</a> Guangyuan Jiang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022program/">Program Of Thoughts Prompting: Disentangling Computation From Reasoning For Numerical Reasoning Tasks</a> Wenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sahu2022data/">Data Augmentation For Intent Classification With Off-the-shelf Large Language Models</a> Gaurav Sahu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022prototypical/">Prototypical Verbalizer For Prompt-based Few-shot Tuning</a> Ganqu Cui, Shengding Hu, Ning Ding, Longtao Huang, Zhiyuan Liu </li>
     
   
     
       <li> <a href="/publications/perez2022ignore/">Ignore Previous Prompt: Attack Techniques For Language Models</a> F√°bio Perez, Ian Ribeiro </li>
     
   
     
   
     
       <li> <a href="/publications/shi2022language/">Language Models Are Multilingual Chain-of-thought Reasoners</a> Freda Shi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2022language/">Language Models As Zero-shot Planners: Extracting Actionable Knowledge For Embodied Agents</a> Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2022one/">Deplot: One-shot Visual Language Reasoning By Plot-to-table Translation</a> Fangyu Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yu2022legal/">Legal Prompting: Teaching A Language Model To Think Like A Lawyer</a> Fangyi Yu, Lee Quartey, Frank Schilder </li>
     
   
     
       <li> <a href="/publications/perez2022red/">Red Teaming Language Models With Language Models</a> Ethan Perez et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jones2022capturing/">Capturing Failures Of Large Language Models Via Human Cognitive Biases</a> Erik Jones, Jacob Steinhardt </li>
     
   
     
       <li> <a href="/publications/nijkamp2022open/">Codegen: An Open Large Language Model For Code With Multi-turn Program Synthesis</a> Erik Nijkamp et al. </li>
     
   
     
       <li> <a href="/publications/zelikman2022bootstrapping/">Star: Bootstrapping Reasoning With Reasoning</a> Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D. Goodman </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022decoupling/">Decoupling Knowledge From Memorization: Retrieval-augmented Prompt Learning</a> Xiang Chen et al. </li>
     
   
     
       <li> <a href="/publications/deng2022what/">What Do Llms Know About Financial Markets? A Case Study On Reddit Market Sentiment Analysis</a> Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, Michael Bendersky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2022unreliability/">The Unreliability Of Explanations In Few-shot Prompting For Textual Reasoning</a> Xi Ye, Greg Durrett </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/trautmann2022legal/">Legal Prompt Engineering For Multilingual Legal Judgement Prediction</a> Dietrich Trautmann, Alina Petrova, Frank Schilder </li>
     
   
     
       <li> <a href="/publications/chen2022exploring/">Convfinqa: Exploring The Chain Of Numerical Reasoning In Conversational Finance Question Answering</a> Zhiyu Chen et al. </li>
     
   
     
       <li> <a href="/publications/sachan2022improving/">Improving Passage Retrieval With Zero-shot Question Generation</a> Devendra Singh Sachan et al. </li>
     
   
     
       <li> <a href="/publications/dua2022successive/">Successive Prompting For Decomposing Complex Questions</a> Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2022least/">Least-to-most Prompting Enables Complex Reasoning In Large Language Models</a> Denny Zhou et al. </li>
     
   
     
   
     
       <li> <a href="/publications/vilar2022prompting/">Prompting Palm For Translation: Assessing Strategies And Performance</a> David Vilar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dohan2022language/">Language Model Cascades</a> David Dohan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fried2022generative/">Incoder: A Generative Model For Code Infilling And Synthesis</a> Daniel Fried et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2022binding/">Binding Language Models In Symbolic Languages</a> Zhoujun Cheng et al. </li>
     
   
     
       <li> <a href="/publications/burns2022discovering/">Discovering Latent Knowledge In Language Models Without Supervision</a> Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022towards/">Towards Unified Conversational Recommender Systems Via Knowledge-enhanced Prompt Learning</a> Xiaolei Wang, Kun Zhou, Ji-rong Wen, Wayne Xin Zhao </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2022few/">Promptagator: Few-shot Dense Retrieval From 8 Examples</a> Zhuyun Dai et al. </li>
     
   
     
   
     
       <li> <a href="/publications/si2022prompting/">Prompting GPT-3 To Be Reliable</a> Chenglei Si et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2022understanding/">Understanding And Mitigating Overfitting In Prompt Tuning For Vision-language Models</a> Chengcheng Ma et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liang2022visual/">Visual-language Navigation Pretraining Via Prompt-based Environmental Self-exploration</a> Xiwen Liang, Fengda Zhu, Lingling Li, Hang Xu, Xiaodan Liang </li>
     
   
     
       <li> <a href="/publications/zhang2022automatic/">Automatic Chain Of Thought Prompting In Large Language Models</a> Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola </li>
     
   
     
       <li> <a href="/publications/tay2022unifying/">UL2: Unifying Language Learning Paradigms</a> Yi Tay et al. </li>
     
   
     
       <li> <a href="/publications/wang2022code/">Code4struct: Code Generation For Few-shot Event Structure Prediction</a> Xingyao Wang, Sha Li, Heng Ji </li>
     
   
     
       <li> <a href="/publications/wang2022no/">No More Fine-tuning? An Experimental Evaluation Of Prompt Tuning In Code Intelligence</a> Chaozheng Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/du2022retrieval/">Retrieval-augmented Generative Question Answering For Event Argument Extraction</a> Xinya Du, Heng Ji </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/anil2022exploring/">Exploring Length Generalization In Large Language Models</a> Cem Anil et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022iteratively/">Iteratively Prompt Pre-trained Language Models For Chain Of Thought</a> Boshi Wang, Xiang Deng, Huan Sun </li>
     
   
     
   
     
       <li> <a href="/publications/ni2022expanding/">Expanding Language-image Pretrained Models For General Video Recognition</a> Bolin Ni et al. </li>
     
   
     
       <li> <a href="/publications/lin2022vision/">ADAPT: Vision-language Navigation With Modality-aligned Action Prompts</a> Bingqian Lin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/workshop2022open/">BLOOM: A 176b-parameter Open-access Multilingual Language Model</a> Bigscience Workshop et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022enabling/">Enabling Conversational Interaction With Mobile UI Using Large Language Models</a> Bryan Wang, Gang Li, Yang Li </li>
     
   
     
       <li> <a href="/publications/bhavya2022analogy/">Analogy Generation By Prompting Large Language Models: A Case Study Of Instructgpt</a> Bhavya Bhavya, Jinjun Xiong, Chengxiang Zhai </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022rationale/">Rationale-augmented Ensembles In Language Models</a> Xuezhi Wang et al. </li>
     
   
     
       <li> <a href="/publications/zhu2022prompt/">Prompt-aligned Gradient For Prompt Tuning</a> Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, Hanwang Zhang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/athiwaratkun2022multi/">Multi-lingual Evaluation Of Code Generation Models</a> Ben Athiwaratkun et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022adaptive/">Adaprompt: Adaptive Model Training For Prompt-based NLP</a> Yulong Chen et al. </li>
     
   
     
       <li> <a href="/publications/he2022prompt/">Hyperprompt: Prompt-based Task-conditioning Of Transformers</a> Yun He et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2022unified/">A Unified Multi-task Learning Framework For Multi-goal Conversational Recommender Systems</a> Yang Deng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022prompt/">Prompt Distribution Learning</a> Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, Xinmei Tian </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/creswell2022faithful/">Faithful Reasoning Using Large Language Models</a> Antonia Creswell, Murray Shanahan </li>
     
   
     
   
     
       <li> <a href="/publications/tiong2022plug/">Plug-and-play VQA: Zero-shot VQA By Conjoining Large Pretrained Models With Zero Training</a> Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lazaridou2022internet/">Internet-augmented Language Models Through Few-shot Prompting For Open-domain Question Answering</a> Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev </li>
     
   
     
       <li> <a href="/publications/liu2022prompting/">Qaner: Prompting Question Answering Models For Few-shot Named Entity Recognition</a> Andy T. Liu et al. </li>
     
   
     
       <li> <a href="/publications/zeng2022socratic/">Socratic Models: Composing Zero-shot Multimodal Reasoning With Language</a> Andy Zeng et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jiang2022general/">VIMA: General Robot Manipulation With Multimodal Prompts</a> Yunfan Jiang et al. </li>
     
   
     
       <li> <a href="/publications/drozdov2022compositional/">Compositional Semantic Parsing With Large Language Models</a> Andrew Drozdov et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fu2022complexity/">Complexity-based Prompting For Multi-step Reasoning</a> Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022prompt/">Promda: Prompt-based Data Augmentation For Low-resource NLU Tasks</a> Yufei Wang et al. </li>
     
   
     
       <li> <a href="/publications/k2022can/">Can Language Models Learn From Explanations In Context?</a> Andrew K. Lampinen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2022prompt/">Promptcap: Prompt-guided Task-aware Image Captioning</a> Yushi Hu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022self/">Self-consistency Improves Chain Of Thought Reasoning In Language Models</a> Xuezhi Wang et al. </li>
     
   
     
       <li> <a href="/publications/glaese2022improving/">Improving Alignment Of Dialogue Agents Via Targeted Human Judgements</a> Amelia Glaese et al. </li>
     
   
     
   
     
       <li> <a href="/publications/madaan2022memory/">Memory-assisted Prompt Editing To Improve GPT-3 After Deployment</a> Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang </li>
     
   
     
       <li> <a href="/publications/madaan2022text/">Text And Patterns: For Effective Chain Of Thought, It Takes Two To Tango</a> Aman Madaan, Amir Yazdanbakhsh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2022optimizing/">Optimizing Prompts For Text-to-image Generation</a> Yaru Hao, Zewen Chi, Li Dong, Furu Wei </li>
     
   
     
       <li> <a href="/publications/zang2022unified/">Unified Vision And Language Prompt Learning</a> Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, Chen Change Loy </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022position/">Position-guided Text Prompt For Vision-language Pre-training</a> Alex Jinpeng Wang, Pan Zhou, Mike Zheng Shou, Shuicheng Yan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022instance/">IDPG: An Instance-dependent Prompt Generation Method</a> Zhuofeng Wu et al. </li>
     
   
     
       <li> <a href="/publications/prasad2022gradient/">Grips: Gradient-free, Edit-based Instruction Search For Prompting Large Language Models</a> Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/maharana2022storydall/">Storydall-e: Adapting Pretrained Text-to-image Transformers For Story Continuation</a> Adyasha Maharana, Darryl Hannan, Mohit Bansal </li>
     
   
     
       <li> <a href="/publications/bulat2022text/">LASP: Text-to-text Optimization For Language-aware Soft Prompting Of Vision & Language Models</a> Adrian Bulat, Georgios Tzimiropoulos </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/srivastava2022beyond/">Beyond The Imitation Game: Quantifying And Extrapolating The Capabilities Of Language Models</a> Aarohi Shammie Srivastava et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/saparov2022language/">Language Models Are Greedy Reasoners: A Systematic Formal Analysis Of Chain-of-thought</a> Abulhair Saparov, He He </li>
     
   
     
       <li> <a href="/publications/asai2022parameter/">ATTEMPT: Parameter-efficient Multi-task Tuning Via Attentional Mixtures Of Soft Prompts</a> Akari Asai, Mohammadreza Salehi, Matthew E. Peters, Hannaneh Hajishirzi </li>
     
   
     
       <li> <a href="/publications/hertz2022prompt/">Prompt-to-prompt Image Editing With Cross Attention Control</a> Amir Hertz et al. </li>
     
   
     
       <li> <a href="/publications/ma2022prompt/">Prompt For Extraction? PAIE: Prompting Argument Interaction For Event Argument Extraction</a> Yubo Ma et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2022holistic/">Holistic Evaluation Of Language Models</a> Percy Liang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qiao2022reasoning/">Reasoning With Language Model Prompting: A Survey</a> Shuofei Qiao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/diao2022black/">Black-box Prompt Learning For Pre-trained Language Models</a> Shizhe Diao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022faithful/">PINTO: Faithful Language Reasoning Using Prompt-generated Rationales</a> Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, Xiang Ren </li>
     
   
     
       <li> <a href="/publications/schramowski2022can/">Can Machines Help Us Answering Question 16 In Datasheets, And In Turn Reflecting On Inappropriate Content?</a> Patrick Schramowski, Christopher Tauchmann, Kristian Kersting </li>
     
   
     
       <li> <a href="/publications/denny2022conversing/">Conversing With Copilot: Exploring Prompt Engineering For Solving CS1 Problems Using Natural Language</a> Paul Denny, Viraj Kumar, Nasser Giacaman </li>
     
   
     
       <li> <a href="/publications/barei%C3%9F2022code/">Code Generation Tools (almost) For Free? A Study Of Few-shot, Pre-trained Language Models On Code</a> Patrick Barei√ü, Beatriz Souza, Marcelo D'amorim, Michael Pradel </li>
     
   
     
   
     
       <li> <a href="/publications/lu2022dynamic/">Dynamic Prompt Learning Via Policy Gradient For Semi-structured Mathematical Reasoning</a> Pan Lu et al. </li>
     
   
     
       <li> <a href="/publications/honovich2022unnatural/">Unnatural Instructions: Tuning Language Models With (almost) No Human Labor</a> Or Honovich, Thomas Scialom, Omer Levy, Timo Schick </li>
     
   
     
       <li> <a href="/publications/shaikh2022second/">On Second Thought, Let's Not Think Step By Step! Bias And Toxicity In Zero-shot Reasoning</a> Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, Diyi Yang </li>
     
   
     
       <li> <a href="/publications/gafni2022make/">Make-a-scene: Scene-based Text-to-image Generation With Human Priors</a> Oran Gafni et al. </li>
     
   
     
   
     
       <li> <a href="/publications/khattab2022demonstrate/">Demonstrate-search-predict: Composing Retrieval And Language Models For Knowledge-intensive NLP</a> Omar Khattab et al. </li>
     
   
     
       <li> <a href="/publications/golovneva2022suite/">ROSCOE: A Suite Of Metrics For Scoring Step-by-step Reasoning</a> Olga Golovneva et al. </li>
     
   
     
       <li> <a href="/publications/arora2022ask/">Ask Me Anything: A Simple Strategy For Prompting Language Models</a> Simran Arora et al. </li>
     
   
     
       <li> <a href="/publications/mees2022grounding/">Grounding Language With Visual Affordances Over Unstructured Data</a> Oier Mees, Jessica Borja-diaz, Wolfram Burgard </li>
     
   
     
       <li> <a href="/publications/press2022measuring/">Measuring And Narrowing The Compositionality Gap In Language Models</a> Ofir Press et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/muennighoff2022crosslingual/">Crosslingual Generalization Through Multitask Finetuning</a> Niklas Muennighoff et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nayak2022learning/">Learning To Compose Soft Prompts For Compositional Zero-shot Learning</a> Nihal V. Nayak, Peilin Yu, Stephen H. Bach </li>
     
   
     
       <li> <a href="/publications/muennighoff2022gpt/">SGPT: GPT Sentence Embeddings For Semantic Search</a> Niklas Muennighoff </li>
     
   
     
       <li> <a href="/publications/borsos2022language/">Audiolm: A Language Modeling Approach To Audio Generation</a> Zal√°n Borsos et al. </li>
     
   
     
       <li> <a href="/publications/li2022explanations/">Explanations From Large Language Models Make Small Reasoners Better</a> Shiyang Li et al. </li>
     
   
     
       <li> <a href="/publications/taylor2022clinical/">Clinical Prompt Learning With Frozen Language Models</a> Niall Taylor, Yi Zhang, Dan Joyce, Alejo Nevado-holgado, Andrey Kormilitzin </li>
     
   
     
       <li> <a href="/publications/carlini2022quantifying/">Quantifying Memorization Across Neural Language Models</a> Nicholas Carlini et al. </li>
     
   
     
       <li> <a href="/publications/zheng2022prompt/">Prompt Vision Transformer For Domain Generalization</a> Zangwei Zheng, Xiangyu Yue, Kai Wang, Yang You </li>
     
   
     
       <li> <a href="/publications/iyer2022opt/">OPT-IML: Scaling Language Model Instruction Meta Learning Through The Lens Of Generalization</a> Srinivasan Iyer et al. </li>
     
   
     
       <li> <a href="/publications/ho2022large/">Large Language Models Are Reasoning Teachers</a> Namgyu Ho, Laura Schmid, Se-young Yun </li>
     
   
     
       <li> <a href="/publications/khattak2022multi/">Maple: Multi-modal Prompt Learning</a> Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/varia2022instruction/">Instruction Tuning For Few-shot Aspect-based Sentiment Analysis</a> Siddharth Varia et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022language/">Language Models With Image Descriptors Are Strong Few-shot Video-language Learners</a> Zhenhailong Wang et al. </li>
     
   
     
       <li> <a href="/publications/lin2022teaching/">Teaching Models To Express Their Uncertainty In Words</a> Stephanie Lin, Jacob Hilton, Owain Evans </li>
     
   
     
       <li> <a href="/publications/bach2022integrated/">Promptsource: An Integrated Development Environment And Repository For Natural Language Prompts</a> Stephen H. Bach et al. </li>
     
   
     
   
     
       <li> <a href="/publications/suzgun2022challenging/">Challenging Big-bench Tasks And Whether Chain-of-thought Can Solve Them</a> Mirac Suzgun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sivarajkumar2022zero/">Healthprompt: A Zero-shot Learning Paradigm For Clinical Natural Language Processing</a> Sonish Sivarajkumar, Yanshan Wang </li>
     
   
     
   
     
       <li> <a href="/publications/deng2022optimizing/">Rlprompt: Optimizing Discrete Text Prompts With Reinforcement Learning</a> Mingkai Deng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bommarito2022gpt/">GPT Takes The Bar Exam</a> Michael Ii Bommarito, Daniel Martin Katz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jia2022visual/">Visual Prompt Tuning</a> Menglin Jia et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chaaben2022towards/">Towards Using Few-shot Prompt Learning For Automating Model Completion</a> Meriem Ben Chaaben, Lola Burgue√±o, Houari Sahraoui </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gupta2022visual/">Visual Programming: Compositional Visual Reasoning Without Training</a> Tanmay Gupta, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shu2022test/">Test-time Prompt Tuning For Zero-shot Generalization In Vision-language Models</a> Manli Shu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2022program/">PAL: Program-aided Language Models</a> Luyu Gao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022super/">Super-naturalinstructions: Generalization Via Declarative Instructions On 1600+ NLP Tasks</a> Yizhong Wang et al. </li>
     
   
     
       <li> <a href="/publications/goyal2022news/">News Summarization And Evaluation In The Era Of GPT-3</a> Tanya Goyal, Junyi Jessy Li, Greg Durrett </li>
     
   
     
       <li> <a href="/publications/kojima2022large/">Large Language Models Are Zero-shot Reasoners</a> Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ouyang2022training/">Training Language Models To Follow Instructions With Human Feedback</a> Long Ouyang et al. </li>
     
   
     
       <li> <a href="/publications/beurerkellner2022prompting/">Prompting Is Programming: A Query Language For Large Language Models</a> Luca Beurer-kellner, Marc Fischer, Martin Vechev </li>
     
   
     
       <li> <a href="/publications/yu2022task/">Task Residual For Tuning Vision-language Models</a> Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, Xinchao Wang </li>
     
   
     
       <li> <a href="/publications/wang2022multi/">Instructionner: A Multi-task Instruction-based Generative Framework For Few-shot NER</a> Liwen Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dugan2022real/">Real Or Fake Text?: Investigating Human Ability To Detect Boundaries Between Human-written And Machine-generated Text</a> Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Sherry Shi, Chris Callison-burch </li>
     
   
     
       <li> <a href="/publications/magee2022structured/">Structured Like A Language Model: Analysing AI As An Automated Subject</a> Liam Magee, Vanicka Arora, Luke Munn </li>
     
   
     
       <li> <a href="/publications/zhou2022large/">Large Language Models Are Human-level Prompt Engineers</a> Yongchao Zhou et al. </li>
     
   
     
       <li> <a href="/publications/hartvigsen2022large/">Toxigen: A Large-scale Machine-generated Dataset For Adversarial And Implicit Hate Speech Detection</a> Thomas Hartvigsen et al. </li>
     
   
     
       <li> <a href="/publications/xu2022exploring/">Exploring The Universal Vulnerability Of Prompt-based Learning Paradigm</a> Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu </li>
     
   
     
       <li> <a href="/publications/li2022personalized/">Personalized Prompt Learning For Explainable Recommendation</a> Lei Li, Yongfeng Zhang, Li Chen </li>
     
   
     
       <li> <a href="/publications/tunstall2022efficient/">Efficient Few-shot Learning Without Prompts</a> Lewis Tunstall et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/magister2022teaching/">Teaching Small Language Models To Reason</a> Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shuster2022language/">Language Models That Seek For Knowledge: Modular Search & Generation For Dialogue And Prompt Completion</a> Kurt Shuster et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/agrawal2022examples/">In-context Examples Selection For Machine Translation</a> Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, Marjan Ghazvininejad </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2022language/">Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey</a> Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos, Yulia Tsvetkov </li>
     
   
     
   
     
       <li> <a href="/publications/kalakonda2022action/">Action-gpt: Leveraging Large-scale Language Models For Improved And Generalized Action Generation</a> Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/villegas2022variable/">Phenaki: Variable Length Video Generation From Open Domain Textual Description</a> Ruben Villegas et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ramos2022lightweight/">Smallcap: Lightweight Image Captioning Prompted With Retrieval Augmentation</a> Rita Ramos, Bruno Martins, Desmond Elliott, Yova Kementchedjhieva </li>
     
   
     
       <li> <a href="/publications/shen2022multitask/">Multitask Vision-language Prompt Tuning</a> Sheng Shen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/abdelghani2022gpt/">Gpt-3-driven Pedagogical Agents For Training Children's Curious Question-asking Skills</a> Rania Abdelghani et al. </li>
     
   
     
       <li> <a href="/publications/geng2022recommendation/">Recommendation As Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)</a> Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2022texts/">Texts As Images In Prompt Tuning For Multi-label Image Recognition</a> Zixian Guo et al. </li>
     
   
     
       <li> <a href="/publications/yao2022synergizing/">React: Synergizing Reasoning And Acting In Language Models</a> Shunyu Yao et al. </li>
     
   
     
       <li> <a href="/publications/mirowski2022co/">Co-writing Screenplays And Theatre Scripts With Language Models: An Evaluation By Industry Professionals</a> Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, Richard Evans </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kadavath2022language/">Language Models (mostly) Know What They Know</a> Saurav Kadavath et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/welleck2022generating/">Generating Sequences By Learning To Self-correct</a> Sean Welleck et al. </li>
     
   
     
   
     
       <li> <a href="/publications/malladi2023fine/">Fine-tuning Language Models With Just Forward Passes</a> Sadhika Malladi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023verify/">Verify-and-edit: A Knowledge-enhanced Chain-of-thought Framework</a> Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, Lidong Bing </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023teaching/">Gpt4tools: Teaching Large Language Model To Use Tools Via Self-instruction</a> Rui Yang et al. </li>
     
   
     
       <li> <a href="/publications/cao2023pro/">Pro-cap: Leveraging A Frozen Vision-language Model For Hateful Meme Detection</a> Rui Cao et al. </li>
     
   
     
       <li> <a href="/publications/cao2023prompting/">Prompting For Multimodal Hateful Meme Classification</a> Rui Cao, Roy Ka-wei Lee, Wen-haw Chong, Jing Jiang </li>
     
   
     
   
     
       <li> <a href="/publications/xu2023chatgpt/">Chatgpt Vs. Google: A Comparative Study Of Search Performance And User Experience</a> Ruiyun Rayna Xu, Yue Katherine Feng, Hailiang Chen </li>
     
   
     
   
     
       <li> <a href="/publications/huang2023make/">Make-an-audio: Text-to-audio Generation With Prompt-enhanced Diffusion Models</a> Rongjie Huang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/girdhar2023emu/">Emu Video: Factorizing Text-to-video Generation By Explicit Image Conditioning</a> Rohit Girdhar et al. </li>
     
   
     
       <li> <a href="/publications/mandi2023dialectic/">Roco: Dialectic Multi-robot Collaboration With Large Language Models</a> Zhao Mandi, Shreeya Jain, Shuran Song </li>
     
   
     
       <li> <a href="/publications/chew2023llm/">Llm-assisted Content Analysis: Using Large Language Models To Support Deductive Coding</a> Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, Annice Kim </li>
     
   
     
   
     
       <li> <a href="/publications/arakawa2023domain/">Catalyst: Domain-extensible Intervention For Preventing Task Procrastination Using Large Generative Models</a> Riku Arakawa, Hiromu Yakura, Masataka Goto </li>
     
   
     
       <li> <a href="/publications/zhang2023then/">Prompt, Generate, Then Cache: Cascade Of Foundation Models Makes Strong Few-shot Learners</a> Renrui Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/pryzant2023automatic/">Automatic Prompt Optimization With "gradient Descent" And Beam Search</a> Reid Pryzant et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/khoury2023how/">How Secure Is Code Generated By Chatgpt?</a> Rapha√´l Khoury, Anderson R. Avila, Jacob Brunelle, Baba Mamadou Camara </li>
     
   
     
       <li> <a href="/publications/li2023may/">Starcoder: May The Source Be With You!</a> Raymond Li et al. </li>
     
   
     
       <li> <a href="/publications/schumann2023verbalization/">VELMA: Verbalization Embodiment Of LLM Agents For Vision And Language Navigation In Street View</a> Raphael Schumann et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/abukhalaf2023codex/">On Codex Prompt Engineering For OCL Generation: An Empirical Study</a> Seif Abukhalaf, Mohammad Hamdaqa, Foutse Khomh </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sanner2023large/">Large Language Models Are Competitive Near Cold-start Recommenders For Language- And Item-based Preferences</a> Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, Lucas Dixon </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/amani2023generative/">Generative AI Perceptions: A Survey To Measure The Perceptions Of Faculty, Staff, And Students On Generative AI Tools In Academia</a> Sara Amani et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/roy2023generating/">Generating Phishing Attacks Using Chatgpt</a> Sayak Saha Roy, Krishna Vamsi Naragam, Shirin Nilizadeh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mysore2023large/">Large Language Model Augmented Narrative Driven Recommendations</a> Sheshera Mysore, Andrew Mccallum, Hamed Zamani </li>
     
   
     
   
     
       <li> <a href="/publications/longpre2023flan/">The Flan Collection: Designing Data And Methods For Effective Instruction Tuning</a> Shayne Longpre et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhong2023sur/">Sur-adapter: Enhancing Text-to-image Pre-trained Diffusion Models With Large Language Models</a> Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Lin </li>
     
   
     
       <li> <a href="/publications/chen2023evaluation/">Evaluation Of Chatgpt Family Of Models For Biomedical Reasoning And Classification</a> Shan Chen et al. </li>
     
   
     
       <li> <a href="/publications/huang2023language/">Language Is Not All You Need: Aligning Perception With Language Models</a> Shaohan Huang et al. </li>
     
   
     
       <li> <a href="/publications/geng2023towards/">VIP5: Towards Multimodal Foundation Models For Recommendation</a> Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, Yongfeng Zhang </li>
     
   
     
       <li> <a href="/publications/hao2023augmenting/">Toolkengpt: Augmenting Frozen Language Models With Massive Tools Via Tool Embeddings</a> Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu </li>
     
   
     
       <li> <a href="/publications/hao2023reasoning/">Reasoning With Language Model Is Planning With World Model</a> Shibo Hao et al. </li>
     
   
     
       <li> <a href="/publications/dai2023llm/">Llm-in-the-loop: Leveraging Large Language Model For Thematic Analysis</a> Shih-chieh Dai, Aiping Xiong, Lun-wei Ku </li>
     
   
     
       <li> <a href="/publications/imani2023mathematical/">Mathprompter: Mathematical Reasoning Using Large Language Models</a> Shima Imani, Liang Du, Harsh Shrivastava </li>
     
   
     
       <li> <a href="/publications/moghaddam2023boosting/">Boosting Theory-of-mind Performance In Large Language Models Via Prompting</a> Shima Rahimi Moghaddam, Christopher J. Honey </li>
     
   
     
       <li> <a href="/publications/li2023label/">Label Supervised Llama Finetuning</a> Zongxi Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lapid2023open/">Open Sesame! Universal Black Box Jailbreaking Of Large Language Models</a> Raz Lapid, Ron Langberg, Moshe Sipper </li>
     
   
     
       <li> <a href="/publications/zhang2023llama/">Llama-adapter: Efficient Fine-tuning Of Language Models With Zero-init Attention</a> Renrui Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phung2023grounded/">Grounded Text-to-image Synthesis With Attention Refocusing</a> Quynh Phung, Songwei Ge, Jia-bin Huang </li>
     
   
     
       <li> <a href="/publications/khraisha2023can/">Can Large Language Models Replace Humans In The Systematic Review Process? Evaluating Gpt-4's Efficacy In Screening And Extracting Data From Peer-reviewed And Grey Literature In Multiple Languages</a> Qusai Khraisha, Sophie Put, Johanna Kappenberg, Azza Warraitch, Kristin Hadfield </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023prompt/">Prompt Engineering A Prompt Engineer</a> Qinyuan Ye, Maxamed Axmed, Reid Pryzant, Fereshte Khani </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023boosting/">ONCE: Boosting Content-based Recommendation With Both Open- And Closed-source Large Language Models</a> Qijiong Liu, Nuo Chen, Tetsuya Sakai, Xiao-ming Wu </li>
     
   
     
       <li> <a href="/publications/huang2023diversity/">Diversity-aware Meta Visual Prompting</a> Qidong Huang et al. </li>
     
   
     
       <li> <a href="/publications/lyu2023translating/">Translating Radiology Reports Into Plain Language Using Chatgpt And GPT-4 With Prompt Learning: Promising Results, Limitations, And Potential</a> Qing Lyu et al. </li>
     
   
     
       <li> <a href="/publications/jin2023augmenting/">Genegpt: Augmenting Large Language Models With Domain Tools For Improved Access To Biomedical Information</a> Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lyu2023faithful/">Faithful Chain-of-thought Reasoning</a> Qing Lyu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023prompting/">Prompting The Hidden Talent Of Web-scale Speech Models For Zero-shot Task Generalization</a> Puyuan Peng, Brian Yan, Shinji Watanabe, David Harwath </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sridhar2023harnessing/">Harnessing Llms In Curricular Design: Using GPT-4 To Support Authoring Of Learning Objectives</a> Pragnya Sridhar et al. </li>
     
   
     
       <li> <a href="/publications/bhandari2023are/">Are Large Language Models Geospatially Knowledgeable?</a> Prabin Bhandari, Antonios Anastasopoulos, Dieter Pfoser </li>
     
   
     
       <li> <a href="/publications/manakul2023zero/">Selfcheckgpt: Zero-resource Black-box Hallucination Detection For Generative Large Language Models</a> Potsawee Manakul, Adian Liusie, Mark J. F. Gales </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023speak/">Speak Foreign Languages With Your Own Voice: Cross-lingual Neural Codec Language Modeling</a> Ziqiang Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023pre/">Pre-train, Prompt And Recommendation: A Comprehensive Survey Of Language Modelling Paradigm Adaptations In Recommender Systems</a> Peng Liu, Lemei Zhang, Jon Atle Gulla </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rubenstein2023large/">Audiopalm: A Large Language Model That Can Speak And Listen</a> Paul K. Rubenstein et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jiang2023exploring/">Graphologue: Exploring Large Language Model Responses With Interactive Diagrams</a> Peiling Jiang, Jude Rayan, Steven P. Dow, Haijun Xia </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cascantebonilla2023going/">Going Beyond Nouns With Vision & Language Models Using Synthetic Data</a> Paola Cascante-bonilla et al. </li>
     
   
     
   
     
       <li> <a href="/publications/khattab2023compiling/">Dspy: Compiling Declarative Language Model Calls Into Self-improving Pipelines</a> Omar Khattab et al. </li>
     
   
     
   
     
       <li> <a href="/publications/guerreiro2023hallucinations/">Hallucinations In Large Multilingual Translation Models</a> Nuno M. Guerreiro et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/palagin2023ontochatgpt/">Ontochatgpt Information System: Ontology-driven Structured Prompts For Chatgpt Meta-learning</a> Oleksandr Palagin, Vladislav Kaverinskiy, Anna Litvin, Kyrylo Malakhov </li>
     
   
     
       <li> <a href="/publications/bian2023chatgpt/">Chatgpt Is A Knowledgeable But Inexperienced Solver: An Investigation Of Commonsense Problem In Large Language Models</a> Ning Bian et al. </li>
     
   
     
   
     
       <li> <a href="/publications/m%C3%BCndler2023self/">Self-contradictory Hallucinations Of Large Language Models: Evaluation, Detection And Mitigation</a> Niels M√ºndler, Jingxuan He, Slobodan Jenko, Martin Vechev </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pangakis2023automated/">Automated Annotation With Generative AI Requires Validation</a> Nicholas Pangakis, Samuel Wolken, Neil Fasching </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023lost/">Lost In The Middle: How Language Models Use Long Contexts</a> Nelson F. Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/inie2023designing/">Designing Participatory AI: Creative Professionals' Worries And Expectations About Generative AI</a> Nanna Inie, Jeanette Falk, Steven Tanimoto </li>
     
   
     
       <li> <a href="/publications/erik2023consistency/">Consistency Analysis Of Chatgpt</a> Myeongjun Erik Jang, Thomas Lukasiewicz </li>
     
   
     
   
     
       <li> <a href="/publications/khattak2023self/">Self-regulating Prompts: Foundational Model Adaptation Without Forgetting</a> Muhammad Uzair Khattak et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mizrahi2023state/">State Of What Art? A Call For Multi-prompt LLM Evaluation</a> Moran Mizrahi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/siddiq2023using/">Using Large Language Models To Generate Junit Tests: An Empirical Study</a> Mohammed Latif Siddiq et al. </li>
     
   
     
       <li> <a href="/publications/pourreza2023din/">DIN-SQL: Decomposed In-context Learning Of Text-to-sql With Self-correction</a> Mohammadreza Pourreza, Davood Rafiei </li>
     
   
     
       <li> <a href="/publications/reza2023rapid/">Abscribe: Rapid Exploration & Organization Of Multiple Writing Variations In Human-ai Co-writing Tasks Using Large Language Models</a> Mohi Reza et al. </li>
     
   
     
       <li> <a href="/publications/fraiwan2023review/">A Review Of Chatgpt Applications In Education, Marketing, Software Engineering, And Healthcare: Benefits, Drawbacks, And Research Directions</a> Mohammad Fraiwan, Natheer Khasawneh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khan2023introducing/">Introducing Language Guidance In Prompt-based Continual Learning</a> Muhammad Gul Zain Ali Khan et al. </li>
     
   
     
       <li> <a href="/publications/jin2023time/">Time-llm: Time Series Forecasting By Reprogramming Large Language Models</a> Ming Jin et al. </li>
     
   
     
       <li> <a href="/publications/kwon2023reward/">Reward Design With Language Models</a> Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh </li>
     
   
     
       <li> <a href="/publications/kosinski2023evaluating/">Evaluating Large Language Models In Theory Of Mind Tasks</a> Michal Kosinski </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/parker2023large/">A Large Language Model Approach To Educational Survey Feedback Analysis</a> Michael J. Parker, Caitlin Anderson, Claire Stone, Yearim Oh </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023chatgpt/">Chatgpt For Vulnerability Detection, Classification, And Repair: How Far Are We?</a> Michael Fu, Chakkrit Tantithamthavorn, Van Nguyen, Trung Le </li>
     
   
     
       <li> <a href="/publications/xiong2023can/">Can Llms Express Their Uncertainty? An Empirical Evaluation Of Confidence Elicitation In Llms</a> Miao Xiong et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sclar2023quantifying/">Quantifying Language Models' Sensitivity To Spurious Features In Prompt Design Or: How I Learned To Start Worrying About Prompt Formatting</a> Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr </li>
     
   
     
   
     
       <li> <a href="/publications/hasan2023zero/">Zero- And Few-shot Prompting With Llms: A Comparative Study With Fine-tuned Models For Bangla Sentiment Analysis</a> Md. Arid Hasan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maniparambil2023enhancing/">Enhancing CLIP With GPT-4: Harnessing Visual Descriptions As Prompts</a> Mayug Maniparambil et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023unleashing/">Unleashing The Emergent Cognitive Synergy In Large Language Models: A Task-solving Agent Through Multi-persona Self-collaboration</a> Zhenhailong Wang et al. </li>
     
   
     
       <li> <a href="/publications/sch%C3%A4fer2023empirical/">An Empirical Evaluation Of Using Large Language Models For Automated Unit Test Generation</a> Max Sch√§fer, Sarah Nadi, Aryaz Eghbali, Frank Tip </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/skreta2023errors/">Errors Are Useful Prompts: Instruction Guided Task Programming With Verifier-assisted Iterative Prompting</a> Marta Skreta et al. </li>
     
   
     
       <li> <a href="/publications/patil2023large/">Gorilla: Large Language Model Connected With Massive Apis</a> Shishir G. Patil, Tianjun Zhang, Xin Wang, Joseph E. Gonzalez </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phute2023llm/">LLM Self Defense: By Self Examination, Llms Know They Are Being Tricked</a> Mansi Phute et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/singha2023visual/">Applenet: Visual Attention Parameterized Prompt Learning For Few-shot Remote Sensing Image Generalization Using CLIP</a> Mainak Singha, Ankit Jha, Bhupendra Solanki, Shirsha Bose, Biplab Banerjee </li>
     
   
     
       <li> <a href="/publications/ghazvininejad2023dictionary/">Dictionary-based Phrase-level Prompting Of Large Language Models For Machine Translation</a> Marjan Ghazvininejad, Hila Gonen, Luke Zettlemoyer </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023document/">Document-level Machine Translation With Large Language Models</a> Longyue Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lian2023llm/">Llm-grounded Diffusion: Enhancing Prompt Understanding Of Text-to-image Diffusion Models With Large Language Models</a> Long Lian, Boyi Li, Adam Yala, Trevor Darrell </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023survey/">A Survey On Large Language Models For Recommendation</a> Likang Wu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/pan2023logic/">Logic-lm: Empowering Large Language Models With Symbolic Solvers For Faithful Logical Reasoning</a> Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023query/">Query2doc: Query Expansion With Large Language Models</a> Liang Wang, Nan Yang, Furu Wei </li>
     
   
     
       <li> <a href="/publications/pan2023automatically/">Automatically Correcting Large Language Models: Surveying The Landscape Of Diverse Self-correction Strategies</a> Liangming Pan et al. </li>
     
   
     
       <li> <a href="/publications/roest2023next/">Next-step Hint Generation For Introductory Programming Using Large Language Models</a> Lianne Roest, Hieke Keuning, Johan Jeuring </li>
     
   
     
       <li> <a href="/publications/guan2023leveraging/">Leveraging Pre-trained Large Language Models To Construct And Utilize World Models For Model-based Task Planning</a> Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati </li>
     
   
     
       <li> <a href="/publications/tunstall2023direct/">Zephyr: Direct Distillation Of LM Alignment</a> Lewis Tunstall et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023zero/">Zero-shot Next-item Recommendation Using Large Pretrained Language Models</a> Lei Wang, Ee-peng Lim </li>
     
   
     
       <li> <a href="/publications/qu2023layoutllm/">Layoutllm-t2i: Eliciting Layout Guidance From LLM For Text-to-image Generation</a> Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, Tat-seng Chua </li>
     
   
     
       <li> <a href="/publications/salewski2023impersonation/">In-context Impersonation Reveals Large Language Models' Strengths And Biases</a> Leonard Salewski, Stephan Alaniz, Isabel Rio-torto, Eric Schulz, Zeynep Akata </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kheiri2023exploiting/">Sentimentgpt: Exploiting GPT For Advanced Sentiment Analysis And Its Departure From Current Machine Learning</a> Kiana Kheiri, Hamid Karimi </li>
     
   
     
   
     
       <li> <a href="/publications/busch2023just/">Just Tell Me: Prompt Engineering In Business Process Management</a> Kiran Busch, Alexander Rochlitzer, Diana Sola, Henrik Leopold </li>
     
   
     
   
     
       <li> <a href="/publications/jesse2023large/">Large Language Models And Simple, Stupid Bugs</a> Kevin Jesse, Toufique Ahmed, Premkumar T. Devanbu, Emily Morgan </li>
     
   
     
   
     
       <li> <a href="/publications/bao2023effective/">Tallrec: An Effective And Efficient Tuning Framework To Align Large Language Model With Recommendation</a> Keqin Bao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/soman2023biomedical/">Biomedical Knowledge Graph-optimized Prompt Generation For Large Language Models</a> Karthik Soman et al. </li>
     
   
     
       <li> <a href="/publications/singhal2023towards/">Towards Expert-level Medical Question Answering With Large Language Models</a> Karan Singhal et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chang2023how/">Chipgpt: How Far Are We From Natural Language Hardware Design</a> Kaiyan Chang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gou2023multi/">Mvp: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction</a> Zhibin Gou, Qingyan Guo, Yujiu Yang </li>
     
   
     
   
     
       <li> <a href="/publications/chang2023speechprompt/">Speechprompt V2: Prompt Tuning For Speech Classification Tasks</a> Kai-wei Chang et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023aligning/">Aligning Instruction Tasks Unlocks Large Language Models As Zero-shot Relation Extractors</a> Kai Zhang, Bernal Jim√©nez Guti√©rrez, Yu Su </li>
     
   
     
   
     
       <li> <a href="/publications/greshake2023not/">Not What You've Signed Up For: Compromising Real-world Llm-integrated Applications With Indirect Prompt Injection</a> Kai Greshake et al. </li>
     
   
     
       <li> <a href="/publications/kerr2023language/">LERF: Language Embedded Radiance Fields</a> Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, Matthew Tancik </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023is/">Is Chatgpt A Good Recommender? A Preliminary Study</a> Junling Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023collaborative/">Agentcf: Collaborative Learning With Autonomous Language Agents For Recommender Systems</a> Junjie Zhang et al. </li>
     
   
     
       <li> <a href="/publications/fei2023transferable/">Transferable Decoding With Visual Entities For Zero-shot Image Captioning</a> Junjie Fei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hazell2023spear/">Spear Phishing With Large Language Models</a> Julian Hazell </li>
     
   
     
   
     
       <li> <a href="/publications/liu2023large/">Chatcounselor: A Large Language Models For Mental Health Support</a> June M. Liu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hartmann2023political/">The Political Ideology Of Conversational AI: Converging Evidence On Chatgpt's Pro-environmental, Left-libertarian Orientation</a> Jochen Hartmann, Jasper Schwenzow, Maximilian Witte </li>
     
   
     
       <li> <a href="/publications/jang2023exploring/">Exploring The Benefits Of Training Expert Language Models Over Instruction Tuning</a> Joel Jang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023potential/">The Potential And Pitfalls Of Using A Large Language Model Such As Chatgpt Or GPT-4 As A Clinical Assistant</a> Jingqing Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2023systematic/">A Systematic Survey Of Prompt Engineering On Vision-language Foundation Models</a> Jindong Gu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023prompt/">Prompt-and-align: Prompt-based Social Alignment For Few-shot Fake News Detection</a> Jiaying Wu, Shen Li, Ailin Deng, Miao Xiong, Bryan Hooi </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023think/">Think-on-graph: Deep And Responsible Reasoning Of Large Language Model On Knowledge Graph</a> Jiashuo Sun et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023set/">Set-of-mark Prompting Unleashes Extraordinary Visual Grounding In GPT-4V</a> Jianwei Yang et al. </li>
     
   
     
       <li> <a href="/publications/chen2023unified/">A Unified Generative Retriever For Knowledge-intensive Language Tasks Via Prompt Learning</a> Jiangui Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023compositional/">Compositional Exemplars For In-context Learning</a> Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/espejel2023gpt/">GPT-3.5, GPT-4, Or BARD? Evaluating Llms Reasoning Ability In Zero-shot Setting And Performance Boosting Through Prompts</a> Jessica L√≥pez Espejel, El Hassane Ettifouri, Mahaman Sanoussi Yahaya Alassan, El Mehdi Chouham, Walid Dahhane </li>
     
   
     
       <li> <a href="/publications/mu2023learning/">Learning To Compress Prompts With Gist Tokens</a> Jesse Mu, Xiang Lisa Li, Noah Goodman </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2023llm/">LLM Lies: Hallucinations Are Not Bugs, But Features As Adversarial Examples</a> Jia-yu Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2023prompting/">Prompting Is Not A Substitute For Probability Measurements In Large Language Models</a> Jennifer Hu, Roger Levy </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/holmes2023evaluating/">Evaluating Large Language Models On A Highly-specialized Topic, Radiation Oncology Physics</a> Jason Holmes et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/koco%C5%842023jack/">Chatgpt: Jack Of All Trades, Master Of None</a> Jan Koco≈Ñ et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/arawjo2023visual/">Chainforge: A Visual Toolkit For Prompt Engineering And LLM Hypothesis Testing</a> Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, Elena Glassman </li>
     
   
     
       <li> <a href="/publications/wei2023symbol/">Symbol Tuning Improves In-context Learning In Language Models</a> Jerry Wei et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jiang2023compressing/">Llmlingua: Compressing Prompts For Accelerated Inference Of Large Language Models</a> Huiqiang Jiang, Qianhui Wu, Chin-yew Lin, Yuqing Yang, Lili Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023ip/">Ip-adapter: Text Compatible Image Prompt Adapter For Text-to-image Diffusion Models</a> Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, Wei Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tran2023instruction/">Bioinstruct: Instruction Tuning Of Large Language Models For Biomedical Natural Language Processing</a> Hieu Tran, Zhichao Yang, Zonghai Yao, Hong Yu </li>
     
   
     
       <li> <a href="/publications/gilbert2023semantic/">Semantic Compression With Large Language Models</a> Henry Gilbert, Michael Sandborn, Douglas C. Schmidt, Jesse Spencer-smith, Jules White </li>
     
   
     
       <li> <a href="/publications/koziolek2023chatgpt/">Chatgpt For PLC/DCS Control Logic Generation</a> Heiko Koziolek, Sten Gruener, Virendra Ashiwal </li>
     
   
     
       <li> <a href="/publications/nori2023capabilities/">Capabilities Of GPT-4 On Medical Challenge Problems</a> Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz </li>
     
   
     
       <li> <a href="/publications/nori2023can/">Can Generalist Foundation Models Outcompete Special-purpose Tuning? Case Study In Medicine</a> Harsha Nori et al. </li>
     
   
     
   
     
       <li> <a href="/publications/tian2023is/">Is Chatgpt The Ultimate Programming Assistant -- How Far Is It?</a> Haoye Tian et al. </li>
     
   
     
       <li> <a href="/publications/huang2023not/">Not All Languages Are Created Equal In Llms: Improving Multilingual Capability By Cross-lingual-thought Prompting</a> Haoyang Huang et al. </li>
     
   
     
       <li> <a href="/publications/liu2023improved/">Improved Baselines With Visual Instruction Tuning</a> Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023measuring/">CMMLU: Measuring Massive Multitask Language Understanding In Chinese</a> Haonan Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/rao2023can/">Can Chatgpt Assess Human Personalities? A General Evaluation Framework</a> Haocong Rao, Cyril Leung, Chunyan Miao </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023safety/">Safety Assessment Of Chinese Large Language Models</a> Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang </li>
     
   
     
       <li> <a href="/publications/kumar2023geotechnical/">Geotechnical Parrot Tales (GPT): Harnessing Large Language Models In Geotechnical Engineering</a> Krishna Kumar </li>
     
   
     
   
     
       <li> <a href="/publications/fei2023reasoning/">Reasoning Implicit Sentiment With Chain-of-thought Prompting</a> Hao Fei et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2023visual/">Visual-language Prompt Tuning With Knowledge-guided Context Optimization</a> Hantao Yao, Rui Zhang, Changsheng Xu </li>
     
   
     
       <li> <a href="/publications/rasheed2023pixel/">Glamm: Pixel Grounding Large Multimodal Model</a> Hanoona Rasheed et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lyu2023llm/">Llm-rec: Personalized Recommendation Via Prompting Large Language Models</a> Hanjia Lyu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023prompting/">Prompting Large Language Models For Topic Modeling</a> Han Wang et al. </li>
     
   
     
       <li> <a href="/publications/inan2023llama/">Llama Guard: Llm-based Input-output Safeguard For Human-ai Conversations</a> Hakan Inan et al. </li>
     
   
     
       <li> <a href="/publications/zhao2023explainability/">Explainability For Large Language Models: A Survey</a> Haiyan Zhao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dang2023choice/">Choice Over Control: How Users Write With Large Language Models Using Diegetic And Non-diegetic Prompting</a> Hai Dang, Sven Goller, Florian Lehmann, Daniel Buschek </li>
     
   
     
   
     
       <li> <a href="/publications/lee2023applying/">Applying Large Language Models And Chain-of-thought For Automatic Scoring</a> Gyeong-geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu, Xiaoming Zhai </li>
     
   
     
       <li> <a href="/publications/li2023revisiting/">Revisiting Large Language Models As Zero-shot Relation Extractors</a> Guozheng Li, Peng Wang, Wenjun Ke </li>
     
   
     
   
     
       <li> <a href="/publications/zuccon2023dr/">Dr Chatgpt, Tell Me What I Want To Hear: How Prompt Knowledge Impacts Health Answer Correctness</a> Guido Zuccon, Bevan Koopman </li>
     
   
     
   
     
       <li> <a href="/publications/zuccon2023chatgpt/">Chatgpt Hallucinates When Attributing Answers</a> Guido Zuccon, Bevan Koopman, Razia Shaik </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023open/">Voyager: An Open-ended Embodied Agent With Large Language Models</a> Guanzhi Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kotek2023gender/">Gender Bias And Stereotypes In Large Language Models</a> Hadas Kotek, Rikker Dockum, David Q. Sun </li>
     
   
     
   
     
       <li> <a href="/publications/serapiogarc%C3%ADa2023personality/">Personality Traits In Large Language Models</a> Greg Serapio-garc√≠a et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023language/">Language Models Can Solve Computer Tasks</a> Geunwoo Kim, Pierre Baldi, Stephen Mcaleer </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/suri2023do/">Do Large Language Models Show Decision Heuristics Similar To Humans? A Case Study Using GPT-3.5</a> Gaurav Suri, Lily R. Slater, Ali Ziaee, Morgan Nguyen </li>
     
   
     
   
     
       <li> <a href="/publications/shi2023large/">Large Language Models Can Be Easily Distracted By Irrelevant Context</a> Freda Shi et al. </li>
     
   
     
       <li> <a href="/publications/zheng2023chatgpt/">Chatgpt Chemistry Assistant For Text Mining And Prediction Of MOF Synthesis</a> Zhiling Zheng, Oufan Zhang, Christian Borgs, Jennifer T. Chayes, Omar M. Yaghi </li>
     
   
     
       <li> <a href="/publications/yan2023multimodal/">Multimodal Chatgpt For Medical Applications: An Experimental Study Of GPT-4V</a> Zhiling Yan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/delatorre2023real/">LLMR: Real-time Prompting Of Interactive Worlds Using Large Language Models</a> Fernanda De La Torre et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023is/">Is Chatgpt Better Than Human Annotators? Potential And Limitations Of Chatgpt In Explaining Implicit Hate Speech</a> Fan Huang, Haewoon Kwak, Jisun An </li>
     
   
     
       <li> <a href="/publications/hu2023llm/">Llm-adapters: An Adapter Family For Parameter-efficient Fine-tuning Of Large Language Models</a> Zhiqiang Hu et al. </li>
     
   
     
       <li> <a href="/publications/mollick2023assigning/">Assigning AI: Seven Approaches For Students, With Prompts</a> Ethan Mollick, Lilach Mollick </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meyerson2023language/">Language Model Crossover: Variation Through Few-shot Prompting</a> Elliot Meyerson et al. </li>
     
   
     
       <li> <a href="/publications/theophilou2023learning/">Learning To Prompt In The Classroom To Understand AI Limits: A Pilot Study</a> Emily Theophilou et al. </li>
     
   
     
   
     
       <li> <a href="/publications/garridomerch%C3%A1n2023simulating/">Simulating H.P. Lovecraft Horror Literature With The Chatgpt Large Language Model</a> Eduardo C. Garrido-merch√°n, Jos√© Luis Arroyo-barrig√ºete, Roberto Gozalo-brizuela </li>
     
   
     
   
     
       <li> <a href="/publications/chen2023chatgpt/">Gptutor: A Chatgpt-powered Programming Tool For Code Explanation</a> Eason Chen, Ray Huang, Han-shin Chen, Yuen-hsien Tseng, Liang-yi Li </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2023read/">Read-only Prompt Optimization For Vision-language Few-shot Learning</a> Dongjun Lee et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jang2023gpt/">GPT-4 Can Pass The Korean National Licensing Examination For Korean Medicine Doctors</a> Dongyeop Jang, Tae-rim Yun, Choong-yeol Lee, Young-kyu Kwon, Chang-eop Kim </li>
     
   
     
   
     
       <li> <a href="/publications/sun2023principle/">Principle-driven Self-alignment Of Language Models From Scratch With Minimal Human Supervision</a> Zhiqing Sun et al. </li>
     
   
     
       <li> <a href="/publications/ashok2023prompting/">Promptner: Prompting For Named Entity Recognition</a> Dhananjay Ashok, Zachary C. Lipton </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2023chatgpt/">Chatgpt Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions</a> Deyao Zhu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/nunes2023evaluating/">Evaluating GPT-3.5 And GPT-4 Models On Brazilian University Admission Exams</a> Desnes Nunes, Ricardo Primi, Ramon Pires, Roberto Lotufo, Rodrigo Nogueira </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/paul2023reasoning/">REFINER: Reasoning Feedback On Intermediate Representations</a> Debjit Paul et al. </li>
     
   
     
       <li> <a href="/publications/nam2023using/">Using An LLM To Help With Code Understanding</a> Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, Brad Myers </li>
     
   
     
       <li> <a href="/publications/gao2023text/">Text-to-sql Empowered By Large Language Models: A Benchmark Evaluation</a> Dawei Gao et al. </li>
     
   
     
       <li> <a href="/publications/yang2023mm/">MM-REACT: Prompting Chatgpt For Multimodal Reasoning And Action</a> Zhengyuan Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/arora2023have/">Have Llms Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models</a> Daman Arora, Himanshu Gaurav Singh, Mausam </li>
     
   
     
   
     
       <li> <a href="/publications/west2023ai/">AI And The FCI: Can Chatgpt Project An Understanding Of Introductory Physics?</a> Colin G. West </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chan2023chatgpt/">Chatgpt Evaluation On Sentence Level Relations: A Focus On Temporal, Causal, And Discourse Relations</a> Chunkit Chan et al. </li>
     
   
     
       <li> <a href="/publications/zheng2023progressive/">Progressive-hint Prompting Improves Reasoning In Large Language Models</a> Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li </li>
     
   
     
   
     
       <li> <a href="/publications/xia2023conversational/">Conversational Automated Program Repair</a> Chunqiu Steven Xia, Lingming Zhang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chuang2023debiasing/">Debiasing Vision-language Models Via Biased Prompts</a> Ching-yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, Stefanie Jegelka </li>
     
   
     
       <li> <a href="/publications/chan2023towards/">Chateval: Towards Better Llm-based Evaluators Through Multi-agent Debate</a> Chi-min Chan et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023iterative/">An Iterative Optimizing Framework For Radiology Report Summarization With Chatgpt</a> Chong Ma et al. </li>
     
   
     
       <li> <a href="/publications/zhou2023less/">LIMA: Less Is More For Alignment</a> Chunting Zhou et al. </li>
     
   
     
       <li> <a href="/publications/qin2023is/">Is Chatgpt A General-purpose Natural Language Processing Task Solver?</a> Chengwei Qin et al. </li>
     
   
     
       <li> <a href="/publications/peng2023model/">Model Tuning Or Prompt Tuning? A Study Of Large Language Models For Clinical Concept And Relation Extraction</a> Cheng Peng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hsieh2023distilling/">Distilling Step-by-step! Outperforming Larger Language Models With Less Training Data And Smaller Model Sizes</a> Cheng-yu Hsieh et al. </li>
     
   
     
       <li> <a href="/publications/wu2023visual/">Visual Chatgpt: Talking, Drawing And Editing With Visual Foundation Models</a> Chenfei Wu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/han2023effective/">E^2VPT: An Effective And Efficient Approach For Visual Prompt Tuning</a> Cheng Han et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023dipping/">Dipping Plms Sauce: Bridging Structure And Text For Effective Knowledge Graph Completion Via Conditional Soft Prompting</a> Chen Chen, Yufei Wang, Aixin Sun, Bing Li, Kwok-yan Lam </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023comprehensive/">MME: A Comprehensive Evaluation Benchmark For Multimodal Large Language Models</a> Chaoyou Fu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yang2023generative/">Generative Speech Recognition Error Correction With Large Language Models And Task-activating Prompting</a> Chao-han Huck Yang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mitra2023compositional/">Compositional Chain-of-thought Prompting For Large Multimodal Models</a> Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig </li>
     
   
     
       <li> <a href="/publications/oh2023black/">Blackvip: Black-box Visual Prompting For Robust Transfer Learning</a> Changdae Oh et al. </li>
     
   
     
   
     
       <li> <a href="/publications/tony2023dataset/">Llmseceval: A Dataset Of Natural Language Prompts For Security Evaluations</a> Catherine Tony, Markus Mutas, Nicol√°s E. D√≠az Ferreyra, Riccardo Scandariato </li>
     
   
     
   
     
       <li> <a href="/publications/cui2023drive/">Receive, Reason, And React: Drive As You Say With Large Language Models In Autonomous Vehicles</a> Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Ziran Wang </li>
     
   
     
       <li> <a href="/publications/jones2023does/">Does GPT-4 Pass The Turing Test?</a> Cameron R. Jones, Benjamin K. Bergen </li>
     
   
     
   
     
       <li> <a href="/publications/ziems2023can/">Can Large Language Models Transform Computational Social Science?</a> Caleb Ziems et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023prompting/">Prompting Or Fine-tuning? A Comparative Study Of Large Language Models For Taxonomy Construction</a> Boqi Chen, Fandi Yi, D√°niel Varr√≥ </li>
     
   
     
   
     
       <li> <a href="/publications/lamichhane2023evaluation/">Evaluation Of Chatgpt For Nlp-based Mental Health Applications</a> Bishal Lamichhane </li>
     
   
     
       <li> <a href="/publications/lin2023generative/">Swiftsage: A Generative Agent With Fast And Slow Thinking For Complex Interactive Tasks</a> Bill Yuchen Lin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/paranjape2023automatic/">ART: Automatic Multi-step Reasoning And Tool-use For Large Language Models</a> Bhargavi Paranjape et al. </li>
     
   
     
       <li> <a href="/publications/clavi%C3%A92023large/">Large Language Models In The Workplace: A Case Study On Prompt Engineering For Job Type Classification</a> Benjamin Clavi√©, Alexandru Ciceu, Frederick Naylor, Guillaume Souli√©, Thomas Brightwell </li>
     
   
     
   
     
       <li> <a href="/publications/xu2023instructing/">Expertprompting: Instructing Large Language Models To Be Distinguished Experts</a> Benfeng Xu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023human/">Motiongpt: Human Motion As A Foreign Language</a> Biao Jiang et al. </li>
     
   
     
       <li> <a href="/publications/cheng2023batch/">Batch Prompting: Efficient Inference With Large Language Model Apis</a> Zhoujun Cheng, Jungo Kasai, Tao Yu </li>
     
   
     
       <li> <a href="/publications/peng2023check/">Check Your Facts And Try Again: Improving Large Language Models With External Knowledge And Automated Feedback</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023prompting/">Prompting Large Language Model For Machine Translation: A Case Study</a> Biao Zhang, Barry Haddow, Alexandra Birch </li>
     
   
     
   
     
       <li> <a href="/publications/hellas2023exploring/">Exploring The Responses Of Large Language Models To Beginner Programmers' Help Requests</a> Arto Hellas et al. </li>
     
   
     
       <li> <a href="/publications/mitra2023orca/">Orca 2: Teaching Small Language Models How To Reason</a> Arindam Mitra et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kong2023better/">Better Zero-shot Reasoning With Role-play Prompting</a> Aobo Kong et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zou2023universal/">Universal And Transferable Adversarial Attacks On Aligned Language Models</a> Andy Zou et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/caines2023application/">On The Application Of Large Language Models For Language Teaching And Assessment Technology</a> Andrew Caines et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/olga2023generative/">Generative AI: Implications And Applications For Education</a> Anastasia Olnancy Olga et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hendy2023how/">How Good Are GPT Models At Machine Translation? A Comprehensive Evaluation</a> Amr Hendy et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2023robots/">Robots That Ask For Help: Uncertainty Alignment For Large Language Model Planners</a> Allen Z. Ren et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2023how/">Jailbroken: How Does LLM Safety Training Fail?</a> Alexander Wei, Nika Haghtalab, Jacob Steinhardt </li>
     
   
     
       <li> <a href="/publications/robey2023defending/">Smoothllm: Defending Large Language Models Against Jailbreaking Attacks</a> Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shtedritski2023what/">What Does CLIP Know About A Red Circle? Visual Prompt Engineering For Vlms</a> Aleksandar Shtedritski, Christian Rupprecht, Andrea Vedaldi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khademi2023can/">Can Chatgpt And Bard Generate Aligned Assessment Items? A Reliability Analysis Against Human Performance</a> Abdolvahab Khademi </li>
     
   
     
       <li> <a href="/publications/kocaballi2023conversational/">Conversational Ai-powered Design: Chatgpt As Designer, User, And Product</a> A. Baki Kocaballi </li>
     
   
     
       <li> <a href="/publications/he2023exploring/">Exploring Human-like Translation Strategy With Large Language Models</a> Zhiwei He et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023multitask/">Multitask Prompt Tuning Enables Parameter-efficient Transfer Learning</a> Zhen Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023anything/">"do Anything Now": Characterizing And Evaluating In-the-wild Jailbreak Prompts On Large Language Models</a> Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023query/">Query Rewriting For Retrieval-augmented Large Language Models</a> Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan </li>
     
   
     
   
     
       <li> <a href="/publications/xu2023how/">How To Unleash The Power Of Large Language Models For Few-shot Relation Extraction?</a> Xin Xu, Yuqi Zhu, Xiaohan Wang, Ningyu Zhang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023delving/">Delving Into Multimodal Prompting For Fine-grained Visual Classification</a> Xin Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023trust/">Don't Trust Chatgpt When Your Question Is Not In English: A Study Of Multilingual Abilities And Types Of Llms</a> Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, Grzegorz Kondrak </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2023bias/">Bias Of Ai-generated Content: An Examination Of News Produced By Large Language Models</a> Xiao Fang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/seo2023leveraging/">Chacha: Leveraging Large Language Models To Prompt Children To Share Their Emotions About Personal Events</a> Woosuk Seo, Chanmo Yang, Young-ho Kim </li>
     
   
     
   
     
       <li> <a href="/publications/gurnee2023language/">Language Models Represent Space And Time</a> Wes Gurnee, Max Tegmark </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jiao2023is/">Is Chatgpt A Good Translator? Yes With GPT-4 As The Engine</a> Wenxiang Jiao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2023simple/">BLIVA: A Simple Multimodal LLM For Better Handling Of Text-rich Visual Questions</a> Wenbo Hu et al. </li>
     
   
     
       <li> <a href="/publications/pan2023preliminary/">A Preliminary Evaluation Of Chatgpt For Zero-shot Dialogue Understanding</a> Wenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che, Libo Qin </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2023gpt/">GPT Detectors Are Biased Against Non-native English Writers</a> Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2023supporting/">Supporting Qualitative Analysis With Large Language Models: Combining Codebook With GPT-3 For Deductive Coding</a> Ziang Xiao, Xingdi Yuan, Q. Vera Liao, Rania Abdelghani, Pierre-yves Oudeyer </li>
     
   
     
       <li> <a href="/publications/jeronymo2023inpars/">Inpars-v2: Large Language Models As Efficient Dataset Generators For Information Retrieval</a> Vitor Jeronymo et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hackl2023is/">Is GPT-4 A Reliable Rater? Evaluating Consistency In GPT-4 Text Ratings</a> Veronika Hackl, Alexandra Elena M√ºller, Michael Granitzer, Maximilian Sailer </li>
     
   
     
   
     
       <li> <a href="/publications/liventsev2023fully/">Fully Autonomous Programming With Large Language Models</a> Vadim Liventsev, Anastasiia Grishina, Aki H√§rm√§, Leon Moonen </li>
     
   
     
   
     
       <li> <a href="/publications/bezirhan2023automated/">Automated Reading Passage Generation With Openai's Large Language Model</a> Ummugul Bezirhan, Matthias Von Davier </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phung2023automating/">Automating Human Tutor-style Programming Feedback: Leveraging GPT-4 Tutor Model For Hint Generation And GPT-3.5 Student Model For Hint Validation</a> Tung Phung et al. </li>
     
   
     
       <li> <a href="/publications/vu2023refreshing/">Freshllms: Refreshing Large Language Models With Search Engine Augmentation</a> Tu Vu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ahmed2023better/">Better Patching Using LLM Prompting, Via Self-consistency</a> Toufique Ahmed, Premkumar Devanbu </li>
     
   
     
       <li> <a href="/publications/ahmed2023automatic/">Automatic Semantic Augmentation Of Language Model Prompts (for Code Summarization)</a> Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, Earl T. Barr </li>
     
   
     
   
     
       <li> <a href="/publications/hariri2023unlocking/">Unlocking The Potential Of Chatgpt: A Comprehensive Exploration Of Its Applications, Advantages, Limitations, And Future Directions In Natural Language Processing</a> Walid Hariri </li>
     
   
     
   
     
       <li> <a href="/publications/korbak2023pretraining/">Pretraining Language Models With Human Preferences</a> Tomasz Korbak et al. </li>
     
   
     
       <li> <a href="/publications/silver2023generalized/">Generalized Planning In PDDL Domains With Pretrained Large Language Models</a> Tom Silver et al. </li>
     
   
     
       <li> <a href="/publications/kocmi2023large/">Large Language Models Are State-of-the-art Evaluators Of Translation Quality</a> Tom Kocmi, Christian Federmann </li>
     
   
     
       <li> <a href="/publications/vansonsbeek2023open/">Open-ended Medical Visual Question Answering Through Prefix Tuning Of Language Models</a> Tom Van Sonsbeek, Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees G. M. Snoek, Marcel Worring </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2023empirical/">Empirical Study Of Zero-shot NER With Chatgpt</a> Tingyu Xie et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023enabling/">Enabling Large Language Models To Generate Text With Citations</a> Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023recommender/">Recommender Systems In The Era Of Large Language Models (llms)</a> Zihuai Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/savage2023diagnostic/">Diagnostic Reasoning Prompts Reveal The Potential For Large Language Model Interpretability In Medicine</a> Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, Jonathan H Chen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sumers2023cognitive/">Cognitive Architectures For Language Agents</a> Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L. Griffiths </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023caption/">Caption Anything: Interactive Image Description With Diverse Multimodal Controls</a> Teng Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/naous2023having/">Having Beer After Prayer? Measuring Cultural Bias In Large Language Models</a> Tarek Naous, Michael J. Ryan, Alan Ritter, Wei Xu </li>
     
   
     
   
     
       <li> <a href="/publications/guo2023what/">What Can Large Language Models Do In Chemistry? A Comprehensive Benchmark On Eight Tasks</a> Taicheng Guo et al. </li>
     
   
     
       <li> <a href="/publications/kuzman2023beginning/">Chatgpt: Beginning Of An End Of Manual Linguistic Data Annotation? Use Case Of Automatic Genre Identification</a> Taja Kuzman, Igor Mozetiƒç, Nikola Ljube≈°iƒá </li>
     
   
     
       <li> <a href="/publications/kim2023interactive/">Evallm: Interactive Evaluation Of Large Language Model Prompts On User-defined Criteria</a> Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-ho Kim, Juho Kim </li>
     
   
     
       <li> <a href="/publications/bubeck2023sparks/">Sparks Of Artificial General Intelligence: Early Experiments With GPT-4</a> S√©bastien Bubeck et al. </li>
     
   
     
       <li> <a href="/publications/mirchandani2023large/">Large Language Models As General Pattern Machines</a> Suvir Mirchandani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/coyne2023analyzing/">Analyzing The Performance Of GPT-3.5 And GPT-4 In Grammatical Error Correction</a> Steven Coyne, Keisuke Sakaguchi, Diana Galvan-sosa, Michael Zock, Kentaro Inui </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2023injecting/">3D-LLM: Injecting The 3D World Into Large Language Models</a> Yining Hong et al. </li>
     
   
     
   
     
       <li> <a href="/publications/feng2023interactive/">Promptmagician: Interactive Prompt Engineering For Text-to-image Creation</a> Yingchaojie Feng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023element/">Element-aware Summarization With Large Language Models: Expert-aligned Evaluation And Chain-of-thought Method</a> Yiming Wang, Zhuosheng Zhang, Rui Wang </li>
     
   
     
   
     
       <li> <a href="/publications/du2023improving/">Improving Factuality And Reasoning In Language Models Through Multiagent Debate</a> Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch </li>
     
   
     
       <li> <a href="/publications/wen2023knowledge/">Mindmap: Knowledge Graph Prompting Sparks Graph Of Thoughts In Large Language Models</a> Yilin Wen, Zifeng Wang, Jimeng Sun </li>
     
   
     
   
     
       <li> <a href="/publications/tian2023graph/">Graph Neural Prompting With Large Language Models</a> Yijun Tian et al. </li>
     
   
     
       <li> <a href="/publications/fang2023mol/">Mol-instructions: A Large-scale Biomolecular Instruction Dataset For Large Language Models</a> Yin Fang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023making/">Making Large Language Models Perform Better In Knowledge Graph Completion</a> Yichi Zhang et al. </li>
     
   
     
       <li> <a href="/publications/xin2023mmap/">Mmap : Multi-modal Alignment Prompt For Cross-domain Multi-task Learning</a> Yi Xin, Junlong Du, Qiang Wang, Ke Yan, Shouhong Ding </li>
     
   
     
       <li> <a href="/publications/lin2023llm/">Llm-eval: Unified Multi-dimensional Automatic Evaluation For Open-domain Conversations With Large Language Models</a> Yen-ting Lin, Yun-nung Chen </li>
     
   
     
   
     
       <li> <a href="/publications/liu2023jailbreaking/">Jailbreaking Chatgpt Via Prompt Engineering: An Empirical Study</a> Yi Liu et al. </li>
     
   
     
       <li> <a href="/publications/yang2023human/">Human-centric Autonomous Systems With Llms For User Command Reasoning</a> Yi Yang et al. </li>
     
   
     
       <li> <a href="/publications/bang2023multimodal/">A Multitask, Multilingual, Multimodal Evaluation Of Chatgpt On Reasoning, Hallucination, And Interactivity</a> Yejin Bang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/moslem2023adaptive/">Adaptive Machine Translation With Large Language Models</a> Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way </li>
     
   
     
       <li> <a href="/publications/xie2023translating/">Translating Natural Language To Planning Goals With Large-language Models</a> Yaqi Xie et al. </li>
     
   
     
       <li> <a href="/publications/zhu2023collaborative/">Collaborative Large Language Model For Recommender Systems</a> Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li </li>
     
   
     
       <li> <a href="/publications/fathullah2023prompting/">Prompting Large Language Models With Speech Recognition Abilities</a> Yassir Fathullah et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023specializing/">Specializing Smaller Language Models Towards Multi-step Reasoning</a> Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot </li>
     
   
     
   
     
       <li> <a href="/publications/lu2023open/">RTLLM: An Open-source Benchmark For Design RTL Generation With Large Language Model</a> Yao Lu, Shang Liu, Qijun Zhang, Zhiyao Xie </li>
     
   
     
       <li> <a href="/publications/zhang2023enhanced/">Llavar: Enhanced Visual Instruction Tuning For Text-rich Image Understanding</a> Yanzhe Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qiao2023march/">March In Chat: Interactive Prompting For Remote Embodied Referring Expression</a> Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, Qi Wu </li>
     
   
     
       <li> <a href="/publications/dubois2023simulation/">Alpacafarm: A Simulation Framework For Methods That Learn From Human Feedback</a> Yann Dubois et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2023improving/">Improving Large Language Models For Clinical Named Entity Recognition Via Prompt Engineering</a> Yan Hu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023training/">Controlvideo: Training-free Controllable Text-to-video Generation</a> Yabo Zhang et al. </li>
     
   
     
       <li> <a href="/publications/xu2023mental/">Mental-llm: Leveraging Large Language Models For Mental Health Prediction Via Online Text Data</a> Xuhai Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023how/">How Robust Is GPT-3.5 To Predecessors? A Comprehensive Study On Language Understanding Tasks</a> Xuanting Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yu2023large/">Large Language Model As Attributed Training Data Generator: A Tale Of Diversity And Bias</a> Yue Yu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dan2023large/">Educhat: A Large-scale Language Model-based Chatbot System For Intelligent Education</a> Yuhao Dan et al. </li>
     
   
     
       <li> <a href="/publications/xie2023prompt/">A Prompt Log Analysis Of Text-to-image Generation Systems</a> Yutong Xie, Zhaoying Pan, Jinge Ma, Luo Jie, Qiaozhu Mei </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2023when/">When Prompt-based Incremental Learning Does Not Meet Strong Pretraining</a> Yu-ming Tang, Yi-xing Peng, Wei-shi Zheng </li>
     
   
     
   
     
       <li> <a href="/publications/chuang2023soft/">Spec: A Soft Prompt-based Calibration On Performance Variability Of Large Language Model In Clinical Notes Summarization</a> Yu-neng Chuang, Ruixiang Tang, Xiaoqian Jiang, Xia Hu </li>
     
   
     
       <li> <a href="/publications/wolf2023fundamental/">Fundamental Limitations Of Alignment In Large Language Models</a> Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua </li>
     
   
     
   
     
       <li> <a href="/publications/chen2023autoregressive/">Autotamp: Autoregressive Task And Motion Planning With Llms As Translators And Checkers</a> Yongchao Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fu2023towards/">Gpt4aigchip: Towards Next-generation AI Accelerator Design Automation Via Large Language Models</a> Yonggan Fu et al. </li>
     
   
     
       <li> <a href="/publications/cao2023assessing/">Assessing Cross-cultural Alignment Between Chatgpt And Human Societies: An Empirical Study</a> Yong Cao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xi2023towards/">Towards Open-world Recommendation With Knowledge Augmentation From Large Language Models</a> Yunjia Xi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shao2023character/">Character-llm: A Trainable Agent For Role-playing</a> Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu </li>
     
   
     
       <li> <a href="/publications/gao2023chat/">Chat-rec: Towards Interactive And Explainable Llms-augmented Recommender System</a> Yunfan Gao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qin2023facilitating/">Toolllm: Facilitating Large Language Models To Master 16000+ Real-world Apis</a> Yujia Qin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/du2023guiding/">Guiding Pretraining In Reinforcement Learning With Large Language Models</a> Yuqing Du et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hou2023large/">Large Language Models Are Zero-shot Rankers For Recommender Systems</a> Yupeng Hou et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2023representation/">Representation Learning With Large Language Models For Recommendation</a> Xubin Ren et al. </li>
     
   
     
       <li> <a href="/publications/dai2023uncovering/">Uncovering Chatgpt's Capabilities In Recommender Systems</a> Sunhao Dai et al. </li>
     
   
     
       <li> <a href="/publications/brade2023text/">Promptify: Text-to-image Generation Through Interactive Prompt Exploration With Large Language Models</a> Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore, Tovi Grossman </li>
     
   
     
   
     
       <li> <a href="/publications/ghosh2023chatgpt/">Chatgpt Perpetuates Gender Bias In Machine Translation And Ignores Non-gendered Pronouns: Findings Across Bengali And Five Other Low-resource Languages</a> Sourojit Ghosh, Aylin Caliskan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ge2023expressive/">Expressive Text-to-image Generation With Rich Text</a> Songwei Ge, Taesung Park, Jun-yan Zhu, Jia-bin Huang </li>
     
   
     
       <li> <a href="/publications/jentzsch2023chatgpt/">Chatgpt Is Fun, But It Is Not Funny! Humor Is Still Challenging Large Language Models</a> Sophie Jentzsch, Kristian Kersting </li>
     
   
     
       <li> <a href="/publications/wadhwa2023revisiting/">Revisiting Relation Extraction In The Era Of Large Language Models</a> Somin Wadhwa, Silvio Amir, Byron C. Wallace </li>
     
   
     
       <li> <a href="/publications/bsharat2023principled/">Principled Instructions Are All You Need For Questioning Llama-1/2, GPT-3.5/4</a> Sondos Mahmoud Bsharat, Aidar Myrzakhan, Zhiqiang Shen </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2023meta/">Metagpt: Meta Programming For A Multi-agent Collaborative Framework</a> Sirui Hong et al. </li>
     
   
     
       <li> <a href="/publications/huo2023retrieving/">Retrieving Supporting Evidence For Generative Question Answering</a> Siqing Huo, Negar Arabzadeh, Charles L. A. Clarke </li>
     
   
     
       <li> <a href="/publications/chen2023llm/">Llm-empowered Chatbots For Psychiatrist And Patient Simulation: Application And Evaluation</a> Siyuan Chen et al. </li>
     
   
     
       <li> <a href="/publications/ott2023central/">Thoughtsource: A Central Hub For Large Language Model Reasoning Data</a> Simon Ott et al. </li>
     
   
     
       <li> <a href="/publications/chen2023visual/">LL3DA: Visual Interactive Instruction Tuning For Omni-3d Understanding, Reasoning, And Planning</a> Sijin Chen et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sudhakaran2023open/">Mariogpt: Open-ended Text2level Generation Through Large Language Models</a> Shyam Sudhakaran et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yao2023tree/">Tree Of Thoughts: Deliberate Problem Solving With Large Language Models</a> Shunyu Yao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023automl/">Automl-gpt: Automatic Machine Learning With GPT</a> Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, Mingyuan Zhou </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bai2023prompt/">Prompt-based Distribution Alignment For Unsupervised Domain Adaptation</a> Shuanghao Bai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023is/">Is Chatgpt A Good Sentiment Analyzer? A Preliminary Study</a> Zengzhi Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/borsos2023efficient/">Soundstorm: Efficient Parallel Audio Generation</a> Zal√°n Borsos et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/abbasiantaeb2023let/">Let The Llms Talk: Simulating Human-to-human Conversational QA Via Zero-shot Llm-to-llm Interactions</a> Zahra Abbasiantaeb, Yifei Yuan, Evangelos Kanoulas, Mohammad Aliannejadi </li>
     
   
     
       <li> <a href="/publications/wen2023hard/">Hard Prompts Made Easy: Gradient-based Discrete Optimization For Prompt Tuning And Discovery</a> Yuxin Wen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023guiding/">Guiding Large Language Models Via Directional Stimulus Prompting</a> Zekun Li et al. </li>
     
   
     
       <li> <a href="/publications/qin2023large/">Large Language Models Are Effective Text Rankers With Pairwise Ranking Prompting</a> Zhen Qin et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bi2023code/">Codekgc: Code Language Model For Generative Knowledge Graph Construction</a> Zhen Bi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sahoo2024systematic/">A Systematic Survey Of Prompt Engineering In Large Language Models: Techniques And Applications</a> Pranab Sahoo et al. </li>
     
   
     
       <li> <a href="/publications/chung2024large/">Large Language Model Capabilities In Perioperative Risk Prediction And Prognostication</a> Philip Chung et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bassner2024ai/">Iris: An Ai-driven Virtual Tutor For Computer Science Education</a> Patrick Bassner, Eduard Frankford, Stephan Krusche </li>
     
   
     
       <li> <a href="/publications/dhillon2024shaping/">Shaping Human-ai Collaboration: Varied Scaffolding Levels In Co-writing With Language Models</a> Paramveer S. Dhillon et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wiratunga2024cbr/">CBR-RAG: Case-based Reasoning For Retrieval Augmented Generation In Llms For Legal Question Answering</a> Nirmalie Wiratunga et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gruver2024fine/">Fine-tuned Language Models Generate Stable Inorganic Materials As Text</a> Nate Gruver et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hedderich2024piece/">A Piece Of Theatre: Investigating How Teachers Design LLM Chatbots To Assist Adolescent Cyberbullying Education</a> Michael A. Hedderich et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/renze2024effect/">The Effect Of Sampling Temperature On Problem Solving In Large Language Models</a> Matthew Renze, Erhan Guven </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gero2024supporting/">Supporting Sensemaking Of Large Language Model Outputs At Scale</a> Katy Ilonka Gero, Chelse Swoopes, Ziwei Gu, Jonathan K. Kummerfeld, Elena L. Glassman </li>
     
   
     
       <li> <a href="/publications/chang2024data/">Data Is All You Need: Finetuning Llms For Chip Design Via An Automated Design-data Augmentation Framework</a> Kaiyan Chang et al. </li>
     
   
     
       <li> <a href="/publications/chen2024pixart/">Pixart-\sigma: Weak-to-strong Training Of Diffusion Transformer For 4K Text-to-image Generation</a> Junsong Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maharjan2024prompt/">Openmedlm: Prompt Engineering Can Out-perform Fine-tuning In Medical Question-answering With Open-source Large Language Models</a> Jenish Maharjan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhao2024revolutionizing/">Revolutionizing Finance With Llms: An Overview Of Applications And Insights</a> Huaqin Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xiong2024benchmarking/">Benchmarking Retrieval-augmented Generation For Medicine</a> Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cuconasu2024power/">The Power Of Noise: Redefining Retrieval For RAG Systems</a> Florin Cuconasu et al. </li>
     
   
     
       <li> <a href="/publications/bozkir2024embedding/">Embedding Large Language Models Into Extended Reality: Opportunities And Challenges For Inclusion, Engagement, And Privacy</a> Efe Bozkir et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khurana2024why/">Why And When Llm-based Assistants Can Go Wrong: Investigating The Effectiveness Of Prompt-based Interactions For Software Help-seeking</a> Anjali Khurana, Hari Subramonyam, Parmit K Chilana </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2024trustworthiness/">Trustllm: Trustworthiness In Large Language Models</a> Yue Huang et al. </li>
     
   
     
       <li> <a href="/publications/yuan2024prompt/">Unist: A Prompt-empowered Universal Model For Urban Spatio-temporal Prediction</a> Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deldjoo2024understanding/">Understanding Biases In Chatgpt-based Recommender Systems: Provider Fairness, Temporal Stability, And Recency</a> Yashar Deldjoo </li>
     
   
     
   
     
       <li> <a href="/publications/zeng2024how/">How Johnny Can Persuade Llms To Jailbreak Them: Rethinking Persuasion To Challenge AI Safety By Humanizing Llms</a> Yi Zeng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2024assessing/">Assessing AI Detectors In Identifying Ai-generated Code: Implications For Education</a> Wei Hung Pan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nepal2024contextual/">Contextual AI Journaling: Integrating LLM And Time Series Behavioral Sensing Technology To Promote Self-reflection And Well-being Using The Mindscape App</a> Subigya Nepal et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shankar2024who/">Who Validates The Validators? Aligning Llm-assisted Evaluation Of LLM Outputs With Human Preferences</a> Shreya Shankar, J. D. Zamfirescu-pereira, Bj√∂rn Hartmann, Aditya G. Parameswaran, Ian Arawjo </li>
     
   
     
   
     
       <li> <a href="/publications/zheng2024harnessing/">Harnessing Large Language Models For Text-rich Sequential Recommendation</a> Zhi Zheng, Wenshuo Chao, Zhaopeng Qiu, Hengshu Zhu, Hui Xiong </li>
     
   
     
   
     
       <li> <a href="/publications/li2024unsupervised/">Promptkd: Unsupervised Prompt Distillation For Vision-language Models</a> Zheng Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2024prompting/">Prompting Large Language Models With Rationale Heuristics For Knowledge-based Visual Question Answering</a> Zhongjian Hu, Peng Yang, Bing Li, Fengyuan Liu </li>
     
   
     
       <li> <a href="/publications/wang2024fostering/">Farsight: Fostering Responsible AI Awareness During AI Application Prototyping</a> Zijie J. Wang, Chinmay Kulkarni, Lauren Wilcox, Michael Terry, Michael Madaio </li>
     
   
     
       <li> <a href="/publications/ni2025measurement/">Measurement Of Llm's Philosophies Of Human Nature</a> Minheng Ni et al. </li>
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Pruning <a id="Pruning"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/see2016compression/">Compression Of Neural Machine Translation Models Via Pruning</a> Abigail See, Minh-thang Luong, Christopher D. Manning </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2016sequence/">Sequence-level Knowledge Distillation</a> Yoon Kim, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2018efficient/">Efficient Contextualized Representation: Language Model Pruning For Sequence Labeling</a> Liyuan Liu, Xiang Ren, Jingbo Shang, Jian Peng, Jiawei Han </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/michel2019are/">Are Sixteen Heads Really Better Than One?</a> Paul Michel, Omer Levy, Graham Neubig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2019reducing/">Reducing Transformer Depth On Demand With Structured Dropout</a> Angela Fan, Edouard Grave, Armand Joulin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019structured/">Structured Pruning Of Large Language Models</a> Ziheng Wang, Jeremy Wohlwend, Tao Lei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mccarley2019structured/">Structured Pruning Of A Bert-based Question Answering Model</a> J. S. Mccarley, Rishav Chakravarti, Avirup Sil </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/voita2019analyzing/">Analyzing Multi-head Self-attention: Specialized Heads Do The Heavy Lifting, The Rest Can Be Pruned</a> Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2019reweighted/">Reweighted Proximal Pruning For Large-scale Language Representation</a> Fu-ming Guo, Sijia Liu, Finlay S. Mungall, Xue Lin, Yanzhi Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tambe2020sentence/">Edgebert: Sentence-level Energy Optimizations For Latency-aware Multi-task NLP Inference</a> Thierry Tambe et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020train/">Train Large, Then Compress: Rethinking Model Size For Efficient Training And Inference Of Transformers</a> Zhuohan Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020efficient/">Efficient Transformer-based Large Scale Language Representations Using Hardware-friendly Block Structured Pruning</a> Bingbing Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/prasanna2020when/">When BERT Plays The Lottery, All Tickets Are Winning</a> Sai Prasanna, Anna Rogers, Anna Rumshisky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2020lite/">Lite Transformer With Long-short Range Attention</a> Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, Song Han </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sajjad2020effect/">On The Effect Of Dropping Layers Of Pre-trained Transformer Models</a> Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Preslav Nakov </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peer2021greedy/">Greedy-layer Pruning: Speeding Up Transformer Models For Natural Language Processing</a> David Peer, Sebastian Stabinger, Stefan Engl, Antonio Rodriguez-sanchez </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singh2021nlp/">The NLP Cookbook: Modern Recipes For Transformer Based Deep Learning Architectures</a> Sushant Singh, Ausif Mahmood </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zafrir2021prune/">Prune Once For All: Sparse Pre-trained Language Models</a> Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, Moshe Wasserblat </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021learned/">Learned Token Pruning For Transformers</a> Sehoon Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022generative/">M6-rec: Generative Pretrained Language Models Are Open-ended Recommender Systems</a> Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kurtic2022optimal/">The Optimal BERT Surgeon: Scalable And Accurate Second-order Pruning For Large Language Models</a> Eldar Kurtic et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lahiri2022interactive/">Interactive Code Generation Via Test-driven User-intent Formalization</a> Shuvendu K. Lahiri et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022accelerating/">Accelerating Attention Through Gradient-based Learned Runtime Pruning</a> Zheng Li, Soroush Ghodrati, Amir Yazdanbakhsh, Hadi Esmaeilzadeh, Mingu Kang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2022structured/">Structured Pruning Learns Compact And Accurate Models</a> Mengzhou Xia, Zexuan Zhong, Danqi Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023simple/">A Simple And Effective Pruning Approach For Large Language Models</a> Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frantar2023massive/">Sparsegpt: Massive Language Models Can Be Accurately Pruned In One-shot</a> Elias Frantar, Dan Alistarh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023llm/">Llm-pruner: On The Structural Pruning Of Large Language Models</a> Xinyin Ma, Gongfan Fang, Xinchao Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023survey/">A Survey On Model Compression For Large Language Models</a> Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2024data/">Data-efficient Fine-tuning For Llm-based Recommendation</a> Xinyu Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Quantization <a id="Quantization"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nie2018operations/">Operations Guided Neural Networks For High Fidelity Data-to-text Generation</a> Feng Nie, Jinpeng Wang, Jin-ge Yao, Rong Pan, Chin-yew Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/henderson2019efficient/">Convert: Efficient And Accurate Conversational Representations From Transformers</a> Matthew Henderson et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zafrir2019quantized/">Q8BERT: Quantized 8bit BERT</a> Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/prato2019fully/">Fully Quantized Transformer For Machine Translation</a> Gabriele Prato, Ella Charlaix, Mehdi Rezagholizadeh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tambe2020sentence/">Edgebert: Sentence-level Energy Optimizations For Latency-aware Multi-task NLP Inference</a> Thierry Tambe et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020train/">Train Large, Then Compress: Rethinking Model Size For Efficient Training And Inference Of Transformers</a> Zhuohan Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020survey/">A Survey On Contextual Embeddings</a> Qi Liu, Matt J. Kusner, Phil Blunsom </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zadeh2020quantizing/">GOBO: Quantizing Attention-based NLP Models For Low Latency And Energy Efficient Inference</a> Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, Andreas Moshovos </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2020lite/">Lite Transformer With Long-short Range Attention</a> Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, Song Han </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020distillation/">Ternarybert: Distillation-aware Ultra-low Bit BERT</a> Wei Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singh2021nlp/">The NLP Cookbook: Modern Recipes For Transformer Based Deep Learning Architectures</a> Sushant Singh, Ausif Mahmood </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bondarenko2021understanding/">Understanding And Overcoming The Challenges Of Efficient Transformer Quantization</a> Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zafrir2021prune/">Prune Once For All: Sparse Pre-trained Language Models</a> Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, Moshe Wasserblat </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021i/">I-BERT: Integer-only BERT Quantization</a> Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dettmers2022matrix/">Llm.int8(): 8-bit Matrix Multiplication For Transformers At Scale</a> Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022efficient/">Zeroquant: Efficient And Affordable Post-training Quantization For Large-scale Transformers</a> Zhewei Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/park2022lut/">LUT-GEMM: Quantized Matrix Multiplication Based On Luts For Efficient Inference In Large-scale Generative Language Models</a> Gunho Park et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2022accurate/">Smoothquant: Accurate And Efficient Post-training Quantization For Large Language Models</a> Guangxuan Xiao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frantar2022accurate/">GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers</a> Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hsu2022language/">Language Model Compression With Weighted Low-rank Factorization</a> Yen-chang Hsu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2022survey/">A Survey On Model Compression And Acceleration For Pretrained Language Models</a> Canwen Xu, Julian Mcauley </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2022glm/">GLM-130B: An Open Bilingual Pre-trained Model</a> Aohan Zeng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023memory/">Memory-efficient Fine-tuning Of Compressed Large Language Models Via Sub-4-bit Integer Quantization</a> Jeonghoon Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frantar2023massive/">Sparsegpt: Massive Language Models Can Be Accurately Pruned In One-shot</a> Elias Frantar, Dan Alistarh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023human/">Motiongpt: Human Motion As A Foreign Language</a> Biao Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dettmers2023efficient/">Qlora: Efficient Finetuning Of Quantized Llms</a> Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer </li>
     
   
     
   
     
       <li> <a href="/publications/dettmers2023sparse/">Spqr: A Sparse-quantized Representation For Near-lossless LLM Weight Compression</a> Tim Dettmers et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023survey/">A Survey On Model Compression For Large Language Models</a> Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023pushing/">Pushing Large Language Models To The 6G Edge: Vision, Challenges, And Opportunities</a> Zheng Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/labrak2024collection/">Biomistral: A Collection Of Open-source Pretrained Large Language Models For Medical Domains</a> Yanis Labrak et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2024pushing/">Billm: Pushing The Limit Of Post-training Quantization For Llms</a> Wei Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ RAG <a id="RAG"></a></h3>
   <ul>
   
     
   
     
       <li> <a href="/publications/weissenborn2016separating/">Separating Answers From Queries For Neural Reading Comprehension</a> Dirk Weissenborn </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2016latent/">Latent Attention For If-then Program Synthesis</a> Xinyun Chen, Chang Liu, Richard Shin, Dawn Song, Mingcheng Chen </li>
     
   
     
       <li> <a href="/publications/serban2016generative/">Generative Deep Neural Networks For Dialogue: A Short Review</a> Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, Joelle Pineau </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bahdanau2016actor/">An Actor-critic Algorithm For Sequence Prediction</a> Dzmitry Bahdanau et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mostafazadeh2016generating/">Generating Natural Questions About An Image</a> Nasrin Mostafazadeh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/andreas2016reasoning/">Reasoning About Pragmatics With Neural Listeners And Speakers</a> Jacob Andreas, Dan Klein </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2016response/">Response Selection With Topic Clues For Retrieval-based Chatbots</a> Yu Wu, Wei Wu, Zhoujun Li, Ming Zhou </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bordes2016learning/">Learning End-to-end Goal-oriented Dialog</a> Antoine Bordes, Y-lan Boureau, Jason Weston </li>
     
   
     
       <li> <a href="/publications/wang2016neural/">Neural Machine Translation Advised By Statistical Machine Translation</a> Xing Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xing2016topic/">Topic Aware Neural Response Generation</a> Chen Xing et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2016user/">A User Simulator For Task-completion Dialogues</a> Xiujun Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/welbl2017constructing/">Constructing Datasets For Multi-hop Reading Comprehension Across Documents</a> Johannes Welbl, Pontus Stenetorp, Sebastian Riedel </li>
     
   
     
       <li> <a href="/publications/joshi2017large/">Triviaqa: A Large Scale Distantly Supervised Challenge Dataset For Reading Comprehension</a> Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer </li>
     
   
     
   
     
       <li> <a href="/publications/anderson2017vision/">Vision-and-language Navigation: Interpreting Visually-grounded Navigation Instructions In Real Environments</a> Peter Anderson et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2017question/">Question Answering And Question Generation As Dual Tasks</a> Duyu Tang, Nan Duan, Tao Qin, Zhao Yan, Ming Zhou </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2017reinforced/">Reinforced Mnemonic Reader For Machine Reading Comprehension</a> Minghao Hu et al. </li>
     
   
     
       <li> <a href="/publications/strub2017end/">End-to-end Optimization Of Goal-driven And Visually Grounded Dialogue Systems</a> Florian Strub et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2017chinese/">Dureader: A Chinese Machine Reading Comprehension Dataset From Real-world Applications</a> Wei He et al. </li>
     
   
     
       <li> <a href="/publications/ko%C4%8Disk%C3%BD2017narrativeqa/">The Narrativeqa Reading Comprehension Challenge</a> Tom√°≈° Koƒçisk√Ω et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dunn2017new/">Searchqa: A New Q&A Dataset Augmented With Context From A Search Engine</a> Matthew Dunn et al. </li>
     
   
     
       <li> <a href="/publications/song2017unified/">A Unified Query-based Generative Model For Question Generation And Question Answering</a> Linfeng Song, Zhiguo Wang, Wael Hamza </li>
     
   
     
       <li> <a href="/publications/zhang2017asking/">Asking The Difficult Questions: Goal-oriented Visual Question Generation Via Intermediate Rewards</a> Junjie Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kenter2017attentive/">Attentive Memory Networks: Efficient Machine Reading For Conversational Search</a> Tom Kenter, Maarten De Rijke </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/subramanian2017neural/">Neural Models For Key Phrase Detection And Question Generation</a> Sandeep Subramanian et al. </li>
     
   
     
       <li> <a href="/publications/zhou2017generating/">Mojitalk: Generating Emotional Responses At Scale</a> Xianda Zhou, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clark2017simple/">Simple And Effective Multi-paragraph Reading Comprehension</a> Christopher Clark, Matt Gardner </li>
     
   
     
   
     
       <li> <a href="/publications/lu2017best/">Best Of Both Worlds: Transferring Knowledge From Discriminative Learning To A Generative Visual Dialog Model</a> Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/asri2017corpus/">Frames: A Corpus For Adding Memory To Goal-oriented Dialogue Systems</a> Layla El Asri et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2017learning/">Learning To Ask: Neural Question Generation For Reading Comprehension</a> Xinya Du, Junru Shao, Claire Cardie </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2018generating/">Livebot: Generating Live Video Comments Based On Visual And Textual Contexts</a> Shuming Ma, Lei Cui, Damai Dai, Furu Wei, Xu Sun </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2018topic/">Topic-based Evaluation For Conversational Bots</a> Fenfei Guo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/rashkin2018towards/">Towards Empathetic Open-domain Conversation Models: A New Benchmark And Dataset</a> Hannah Rashkin, Eric Michael Smith, Margaret Li, Y-lan Boureau </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/elsahar2018zero/">Zero-shot Question Generation From Knowledge Graphs For Unseen Predicates And Entity Types</a> Hady Elsahar, Christophe Gravier, Frederique Laforest </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cai2018skeleton/">Skeleton-to-response: Dialogue Generation Guided By Retrieval Memory</a> Deng Cai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2018fast/">Fast Abstractive Summarization With Reinforce-selected Sentence Rewriting</a> Yen-chun Chen, Mohit Bansal </li>
     
   
     
   
     
       <li> <a href="/publications/asai2018multilingual/">Multilingual Extractive Reading Comprehension By Runtime Machine Translation</a> Akari Asai, Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mazar%C3%A92018training/">Training Millions Of Personalized Dialogue Agents</a> Pierre-emmanuel Mazar√©, Samuel Humeau, Martin Raison, Antoine Bordes </li>
     
   
     
       <li> <a href="/publications/kumar2018putting/">Putting The Horse Before The Cart:a Generator-evaluator Framework For Question Generation From Text</a> Vishwajeet Kumar, Ganesh Ramakrishnan, Yuan-fang Li </li>
     
   
     
       <li> <a href="/publications/gehrmann2018end/">End-to-end Content And Plan Selection For Data-to-text Generation</a> Sebastian Gehrmann, Falcon Z. Dai, Henry Elder, Alexander M. Rush </li>
     
   
     
       <li> <a href="/publications/niu2018polite/">Polite Dialogue Generation Without Parallel Data</a> Tong Niu, Mohit Bansal </li>
     
   
     
       <li> <a href="/publications/parthasarathi2018extending/">Extending Neural Generative Conversational Model Using External Knowledge Sources</a> Prasanna Parthasarathi, Joelle Pineau </li>
     
   
     
       <li> <a href="/publications/li2018hybrid/">Hybrid Retrieval-generation Reinforced Agent For Medical Image Report Generation</a> Christy Y. Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hayati2018retrieval/">Retrieval-based Neural Code Generation</a> Shirley Anugrah Hayati et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2018contextualized/">Sdnet: Contextualized Attention-based Deep Network For Conversational Question Answering</a> Chenguang Zhu, Michael Zeng, Xuedong Huang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/saha2018complex/">Complex Sequential Question Answering: Towards Learning To Converse Over Linked Question Answer Pairs With A Knowledge Graph</a> Amrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan, Sarath Chandar </li>
     
   
     
   
     
       <li> <a href="/publications/caccia2018language/">Language Gans Falling Short</a> Massimo Caccia et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2018response/">Response Generation By Context-aware Prototype Editing</a> Yu Wu et al. </li>
     
   
     
       <li> <a href="/publications/wang2018robust/">Robust Text-to-sql Generation With Execution-guided Decoding</a> Chenglong Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2018reinforced/">Reinforced Cross-modal Matching And Self-supervised Imitation Learning For Vision-language Navigation</a> Xin Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hashimoto2018retrieve/">A Retrieve-and-edit Framework For Predicting Structured Outputs</a> Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pampari2018large/">Emrqa: A Large Corpus For Question Answering On Electronic Medical Records</a> Anusri Pampari, Preethi Raghavan, Jennifer Liang, Jian Peng </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nishida2018retrieve/">Retrieve-and-read: Multi-task Learning Of Information Retrieval And Reading Comprehension</a> Kyosuke Nishida, Itsumi Saito, Atsushi Otsuka, Hisako Asano, Junji Tomita </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tandon2018reasoning/">Reasoning About Actions And State Changes By Injecting Commonsense Knowledge</a> Niket Tandon et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/weston2018retrieve/">Retrieve And Refine: Improved Sequence Generation Models For Dialogue</a> Jason Weston, Emily Dinan, Alexander H. Miller </li>
     
   
     
   
     
       <li> <a href="/publications/hou2018sequence/">Sequence-to-sequence Data Augmentation For Dialogue Language Understanding</a> Yutai Hou, Yijia Liu, Wanxiang Che, Ting Liu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2018retrieval/">Retrieval-enhanced Adversarial Training For Neural Response Generation</a> Qingfu Zhu, Lei Cui, Weinan Zhang, Furu Wei, Ting Liu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2018attention/">Attention-guided Answer Distillation For Machine Reading Comprehension</a> Minghao Hu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2018ranking/">Ranking Paragraphs For Improving Answer Recall In Open-domain Question Answering</a> Jinhyuk Lee, Seongjun Yun, Hyunjae Kim, Miyoung Ko, Jaewoo Kang </li>
     
   
     
   
     
       <li> <a href="/publications/xiao2018dual/">Dual Ask-answer Network For Machine Reading Comprehension</a> Han Xiao, Feng Wang, Jianfeng Yan, Jingyao Zheng </li>
     
   
     
       <li> <a href="/publications/stahlberg2018simple/">Simple Fusion: Return Of The Language Model</a> Felix Stahlberg, James Cross, Veselin Stoyanov </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2018multi/">Multi-granularity Hierarchical Attention Fusion Networks For Reading Comprehension And Question Answering</a> Wei Wang, Ming Yan, Chen Wu </li>
     
   
     
       <li> <a href="/publications/zhu2018fine/">Lingke: A Fine-grained Multi-turn Chatbot For Customer Service</a> Pengfei Zhu, Zhuosheng Zhang, Jiangtong Li, Yafang Huang, Hai Zhao </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clark2018think/">Think You Have Solved Question Answering? Try ARC, The AI2 Reasoning Challenge</a> Peter Clark et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fried2018speaker/">Speaker-follower Models For Vision-and-language Navigation</a> Daniel Fried et al. </li>
     
   
     
       <li> <a href="/publications/zhang2018generating/">Generating Informative And Diverse Conversational Responses Via Adversarial Information Maximization</a> Yizhe Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khatri2018advancing/">Advancing The State Of The Art In Open Domain Dialog Systems Through The Alexa Prize</a> Chandra Khatri et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sun2018improving/">Improving Machine Reading Comprehension With General Reading Strategies</a> Kai Sun, Dian Yu, Dong Yu, Claire Cardie </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2018learning/">Learning Personalized End-to-end Goal-oriented Dialog</a> Liangchen Luo, Wenhao Huang, Qi Zeng, Zaiqing Nie, Xu Sun </li>
     
   
     
   
     
       <li> <a href="/publications/shah2018building/">Building A Conversational Agent Overnight With Dialogue Self-play</a> Pararth Shah et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wen2018sequence/">Sequence-to-sequence Learning For Task-oriented Dialogue With Dialogue State Representation</a> Haoyang Wen, Yijia Liu, Wanxiang Che, Libo Qin, Ting Liu </li>
     
   
     
       <li> <a href="/publications/reddy2018conversational/">Coqa: A Conversational Question Answering Challenge</a> Siva Reddy, Danqi Chen, Christopher D. Manning </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2018dataset/">A Dataset For Document Grounded Conversations</a> Kangyan Zhou, Shrimai Prabhumoye, Alan W Black </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kratzwald2019neural/">Rankqa: Neural Question Answering With Answer Re-ranking</a> Bernhard Kratzwald, Anna Eigenmann, Stefan Feuerriegel </li>
     
   
     
   
     
       <li> <a href="/publications/rothe2019leveraging/">Leveraging Pre-trained Checkpoints For Sequence Generation Tasks</a> Sascha Rothe, Shashi Narayan, Aliaksei Severyn </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bi2019incorporating/">Incorporating External Knowledge Into Machine Reading For Generative Question Answering</a> Bin Bi et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2019towards/">Retrieve, Read, Rerank: Towards End-to-end Multi-document Reading Comprehension</a> Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2019cosmos/">Cosmos QA: Machine Reading Comprehension With Contextual Commonsense Reasoning</a> Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2019reinforced/">Reinforced Dynamic Reasoning For Conversational Question Generation</a> Boyuan Pan, Hao Li, Ziyu Yao, Deng Cai, Huan Sun </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019knowledge/">Knowledge Aware Conversation Generation With Explainable Reasoning Over Augmented Graphs</a> Zhibin Liu, Zheng-yu Niu, Hua Wu, Haifeng Wang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2019conversing/">Conversing By Reading: Contentful Neural Conversation With On-demand Machine Reading</a> Lianhui Qin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2019answering/">Answering Complex Open-domain Questions Through Iterative Query Generation</a> Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, Christopher D. Manning </li>
     
   
     
       <li> <a href="/publications/chen2019enabling/">Enabling Robots To Understand Incomplete Natural Language Instructions Using Commonsense Reasoning</a> Haonan Chen, Hao Tan, Alan Kuntz, Mohit Bansal, Ron Alterovitz </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alamri2019audio/">Audio-visual Scene-aware Dialog</a> Huda Alamri et al. </li>
     
   
     
       <li> <a href="/publications/lu2019multi/">12-in-1: Multi-task Vision And Language Representation Learning</a> Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2019review/">Review Conversational Reading Comprehension</a> Hu Xu, Bing Liu, Lei Shu, Philip S. Yu </li>
     
   
     
       <li> <a href="/publications/chen2019gmail/">Gmail Smart Compose: Real-time Assisted Writing</a> Mia Xu Chen et al. </li>
     
   
     
       <li> <a href="/publications/chen2019universal/">UNITER: Universal Image-text Representation Learning</a> Yen-chun Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/neelakantan2019neural/">Neural Assistant: Joint Action Prediction, Response Generation, And Latent Knowledge Reasoning</a> Arvind Neelakantan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2019text/">Text Infilling</a> Wanrong Zhu, Zhiting Hu, Eric Xing </li>
     
   
     
       <li> <a href="/publications/wang2019multi/">Multi-passage BERT: A Globally Normalized BERT Model For Open-domain Question Answering</a> Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, Bing Xiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yavuz2019grounded/">Deepcopy: Grounded Response Generation With Hierarchical Pointer Networks</a> Semih Yavuz, Abhinav Rastogi, Guan-lin Chao, Dilek Hakkani-tur </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahmad2019evaluation/">Reqa: An Evaluation For End-to-end Answer Retrieval Models</a> Amin Ahmad, Noah Constant, Yinfei Yang, Daniel Cer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hua2019sentence/">Sentence-level Content Planning And Style Specification For Neural Text Generation</a> Xinyu Hua, Lu Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2019visual/">Help, Anna! Visual Navigation With Natural Multimodal Assistance Via Retrospective Curiosity-encouraging Imitation Learning</a> Khanh Nguyen, Hal Iii Daum√© </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alberti2019fusion/">Fusion Of Detected Objects In Text For Visual Question Answering</a> Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/caglayan2019probing/">Probing The Need For Visual Context In Multimodal Machine Translation</a> Ozan Caglayan, Pranava Madhyastha, Lucia Specia, Lo√Øc Barrault </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rajani2019explain/">Explain Yourself! Leveraging Language Models For Commonsense Reasoning</a> Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2019improving/">Improving Question Generation With Sentence-level Semantic Matching And Answer Position Inferring</a> Xiyao Ma, Qile Zhu, Yanlin Zhou, Xiaolin Li, Dapeng Wu </li>
     
   
     
       <li> <a href="/publications/hsu2019knowledge/">Knowledge-enriched Visual Storytelling</a> Chao-chun Hsu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/min2019multi/">Multi-hop Reading Comprehension Through Question Decomposition And Rescoring</a> Sewon Min, Victor Zhong, Luke Zettlemoyer, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qu2019attentive/">Attentive History Selection For Conversational Question Answering</a> Chen Qu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2019curiosity/">Curiosity-driven Reinforcement Learning For Diverse Visual Paragraph Generation</a> Yadan Luo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lewis2019evaluating/">MLQA: Evaluating Cross-lingual Extractive Question Answering</a> Patrick Lewis, Barlas Oƒüuz, Ruty Rinott, Sebastian Riedel, Holger Schwenk </li>
     
   
     
   
     
       <li> <a href="/publications/lewis2019unsupervised/">Unsupervised Question Answering By Cloze Translation</a> Patrick Lewis, Ludovic Denoyer, Sebastian Riedel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yogatama2019learning/">Learning And Evaluating General Linguistic Intelligence</a> Dani Yogatama et al. </li>
     
   
     
       <li> <a href="/publications/lee2019contextualized/">Contextualized Sparse Representations For Real-time Open-domain Question Answering</a> Jinhyuk Lee, Minjoon Seo, Hannaneh Hajishirzi, Jaewoo Kang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chi2019cross/">Cross-lingual Natural Language Generation Via Pre-training</a> Zewen Chi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/richardson2019probing/">Probing Natural Language Inference Models Through Semantic Fragments</a> Kyle Richardson, Hai Hu, Lawrence S. Moss, Ashish Sabharwal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cadene2019multimodal/">MUREL: Multimodal Relational Reasoning For Visual Question Answering</a> Remi Cadene, Hedi Ben-younes, Matthieu Cord, Nicolas Thome </li>
     
   
     
   
     
       <li> <a href="/publications/talmor2019empirical/">Multiqa: An Empirical Investigation Of Generalization And Transfer In Reading Comprehension</a> Alon Talmor, Jonathan Berant </li>
     
   
     
       <li> <a href="/publications/cho2019mixture/">Mixture Content Selection For Diverse Sequence Generation</a> Jaemin Cho, Minjoon Seo, Hannaneh Hajishirzi </li>
     
   
     
       <li> <a href="/publications/murahari2019large/">Large-scale Pretraining For Visual Dialog: A Simple State-of-the-art Baseline</a> Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chi2019just/">Just Ask:an Interactive Learning Framework For Vision And Language Navigation</a> Ta-chung Chi, Mihail Eric, Seokhwan Kim, Minmin Shen, Dilek Hakkani-tur </li>
     
   
     
       <li> <a href="/publications/jain2019stay/">Stay On The Path: Instruction Fidelity In Vision-and-language Navigation</a> Vihan Jain et al. </li>
     
   
     
       <li> <a href="/publications/asai2019learning/">Learning To Retrieve Reasoning Paths Over Wikipedia Graph For Question Answering</a> Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, Caiming Xiong </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/das2019multi/">Multi-step Retriever-reader Interaction For Scalable Open-domain Question Answering</a> Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Andrew Mccallum </li>
     
   
     
   
     
       <li> <a href="/publications/ran2019option/">Option Comparison Network For Multiple-choice Reading Comprehension</a> Qiu Ran, Peng Li, Weiwei Hu, Jie Zhou </li>
     
   
     
   
     
       <li> <a href="/publications/kim2019eighth/">The Eighth Dialog System Technology Challenge</a> Seokhwan Kim et al. </li>
     
   
     
       <li> <a href="/publications/fisch2019mrqa/">MRQA 2019 Shared Task: Evaluating Generalization In Reading Comprehension</a> Adam Fisch et al. </li>
     
   
     
       <li> <a href="/publications/henderson2019efficient/">Convert: Efficient And Accurate Conversational Representations From Transformers</a> Matthew Henderson et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/seo2019real/">Real-time Open-domain Question Answering With Dense-sparse Phrase Index</a> Minjoon Seo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/logan2019wife/">Barack's Wife Hillary: Using Knowledge-graphs For Fact-aware Language Modeling</a> Robert L. Iv Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, Sameer Singh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qiao2019understanding/">Understanding The Behaviors Of BERT In Ranking</a> Yifan Qiao, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lample2019cross/">Cross-lingual Language Model Pretraining</a> Guillaume Lample, Alexis Conneau </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiong2019pretrained/">Pretrained Encyclopedia: Weakly Supervised Knowledge-pretrained Language Model</a> Wenhan Xiong, Jingfei Du, William Yang Wang, Veselin Stoyanov </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2019pre/">PLATO: Pre-trained Dialogue Generation Model With Discrete Latent Variable</a> Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jean2019context/">Context-aware Learning For Neural Machine Translation</a> S√©bastien Jean, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lian2019learning/">Learning To Select Knowledge For Response Generation In Dialog Systems</a> Rongzhong Lian, Min Xie, Fan Wang, Jinhua Peng, Hua Wu </li>
     
   
     
   
     
       <li> <a href="/publications/gao2019jointly/">Jointly Optimizing Diversity And Relevance In Neural Response Generation</a> Xiang Gao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/vilares2019head/">HEAD-QA: A Healthcare Dataset For Complex Reasoning</a> David Vilares, Carlos G√≥mez-rodr√≠guez </li>
     
   
     
       <li> <a href="/publications/liu2019attention/">Attention-informed Mixed-language Training For Zero-shot Cross-lingual Task-oriented Dialogue Systems</a> Zihan Liu, Genta Indra Winata, Zhaojiang Lin, Peng Xu, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guu2020retrieval/">REALM: Retrieval-augmented Language Model Pre-training</a> Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-wei Chang </li>
     
   
     
   
     
       <li> <a href="/publications/adiwardana2020towards/">Towards A Human-like Open-domain Chatbot</a> Daniel Adiwardana et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rosset2020knowledge/">Knowledge-aware Language Model Pretraining</a> Corby Rosset et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2020generative/">Generative Data Augmentation For Commonsense Reasoning</a> Yiben Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pradeep2020scientific/">Scientific Claim Verification With VERT5ERINI</a> Ronak Pradeep, Xueguang Ma, Rodrigo Nogueira, Jimmy Lin </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/aky%C3%BCrek2020learning/">Learning To Recombine And Resample Data For Compositional Generalization</a> Ekin Aky√ºrek, Afra Feyza Aky√ºrek, Jacob Andreas </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hosseiniasl2020simple/">A Simple Language Model For Task-oriented Dialogue</a> Ehsan Hosseini-asl, Bryan Mccann, Chien-sheng Wu, Semih Yavuz, Richard Socher </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020kg/">KG-BART: Knowledge Graph-augmented BART For Generative Commonsense Reasoning</a> Ye Liu, Yao Wan, Lifang He, Hao Peng, Philip S. Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2020sequence/">Sequence-level Mixed Sample Data Augmentation</a> Demi Guo, Yoon Kim, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020reading/">Rikinet: Reading Wikipedia Pages For Natural Question Answering</a> Dayiheng Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2020goal/">Doc2dial: A Goal-oriented Document-grounded Dialogue Dataset</a> Song Feng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/heyman2020neural/">Neural Code Search Revisited: Enhancing Code Snippet Retrieval Through Natural Language Intent</a> Geert Heyman, Tom Van Cutsem </li>
     
   
     
       <li> <a href="/publications/devries2020as/">As Good As New. How To Successfully Recycle English GPT-2 To Make Models For Other Languages</a> Wietse De Vries, Malvina Nissim </li>
     
   
     
       <li> <a href="/publications/izacard2020leveraging/">Leveraging Passage Retrieval With Generative Models For Open Domain Question Answering</a> Gautier Izacard, Edouard Grave </li>
     
   
     
   
     
       <li> <a href="/publications/jiang2020x/">X-FACTR: Multilingual Factual Knowledge Retrieval From Pretrained Language Models</a> Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, Graham Neubig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020high/">Logic2text: High-fidelity Natural Language Generation From Logical Forms</a> Zhiyu Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ho2020constructing/">Constructing A Multi-hop QA Dataset For Comprehensive Evaluation Of Reasoning Steps</a> Xanh Ho, Anh-khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pearce2020deriving/">DAVE: Deriving Automatically Verilog From English</a> Hammond Pearce, Benjamin Tan, Ramesh Karri </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020knowledge/">KGPT: Knowledge-grounded Pre-training For Data-to-text Generation</a> Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang </li>
     
   
     
   
     
       <li> <a href="/publications/lei2020memory/">MART: Memory-augmented Recurrent Transformer For Coherent Video Paragraph Captioning</a> Jie Lei et al. </li>
     
   
     
   
     
       <li> <a href="/publications/deng2020residual/">Residual Energy-based Models For Text Generation</a> Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, Marc'aurelio Ranzato </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/meng2020large/">Openvidial: A Large-scale, Open-domain Dialogue Dataset With Visual Contexts</a> Yuxian Meng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2020retrieval/">Retrieval-augmented Generation For Knowledge-intensive NLP Tasks</a> Patrick Lewis et al. </li>
     
   
     
       <li> <a href="/publications/qu2020open/">Open-retrieval Conversational Question Answering</a> Chen Qu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hegde2020unsupervised/">Unsupervised Paraphrase Generation Using Pre-trained Language Models</a> Chaitra Hegde, Shrikumar Patil </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2020pre/">Pre-training Multilingual Neural Machine Translation By Leveraging Alignment Information</a> Zehui Lin et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thompson2020paraphrase/">Paraphrase Generation As Zero-shot Multilingual Translation: Disentangling Semantic Similarity From Lexical And Syntactic Diversity</a> Brian Thompson, Matt Post </li>
     
   
     
       <li> <a href="/publications/seo2020look/">Look Before You Speak: Visually Contextualized Utterances</a> Paul Hongsuck Seo, Arsha Nagrani, Cordelia Schmid </li>
     
   
     
   
     
       <li> <a href="/publications/zhao2020knowledge/">Knowledge-grounded Dialogue Generation With Pre-trained Language Models</a> Xueliang Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/majumder2020like/">Like Hiking? You Probably Enjoy Nature: Persona-grounded Dialog With Commonsense Expansions</a> Bodhisattwa Prasad Majumder, Harsh Jhamtani, Taylor Berg-kirkpatrick, Julian Mcauley </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kapanipathi2020leveraging/">Leveraging Abstract Meaning Representation For Knowledge Base Question Answering</a> Pavan Kapanipathi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020asking/">Asking Questions The Human Way: Scalable Question-answer Generation From Text Corpus</a> Bang Liu, Haojie Wei, Di Niu, Haolan Chen, Yancheng He </li>
     
   
     
   
     
       <li> <a href="/publications/paranjape2020neural/">Neural Generation Meets Real People: Towards Emotionally Engaging Mixed-initiative Conversations</a> Ashwin Paranjape et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/majumdar2020improving/">Improving Vision-and-language Navigation With Image-text Pairs From The Web</a> Arjun Majumdar et al. </li>
     
   
     
       <li> <a href="/publications/cohan2020document/">SPECTER: Document-level Representation Learning Using Citation-informed Transformers</a> Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S. Weld </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2020just/">Just Ask: Learning To Answer Questions From Millions Of Narrated Videos</a> Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/macavaney2020analyzing/">ABNIRML: Analyzing The Behavior Of Neural IR Models</a> Sean Macavaney, Sergey Feldman, Nazli Goharian, Doug Downey, Arman Cohan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/marasovi%C4%872020natural/">Natural Language Rationales With Full-stack Visual Reasoning: From Pixels To Semantic Frames To Commonsense Graphs</a> Ana Marasoviƒá et al. </li>
     
   
     
       <li> <a href="/publications/mao2020generation/">Generation-augmented Retrieval For Open-domain Question Answering</a> Yuning Mao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2020conversational/">Conversational Question Reformulation Via Sequence-to-sequence Architectures And Pretrained Language Models</a> Sheng-chieh Lin et al. </li>
     
   
     
       <li> <a href="/publications/agarwal2020history/">History For Visual Dialog: Do We Really Need It?</a> Shubham Agarwal, Trung Bui, Joon-young Lee, Ioannis Konstas, Verena Rieser </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lopez2020simplifying/">Simplifying Paragraph-level Question Generation Via Transformer Language Models</a> Luis Enrico Lopez, Diane Kathryn Cruz, Jan Christian Blaise Cruz, Charibeth Cheng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/asai2020logic/">Logic-guided Data Augmentation And Regularization For Consistent Question Answering</a> Akari Asai, Hannaneh Hajishirzi </li>
     
   
     
       <li> <a href="/publications/akakzia2020grounding/">Grounding Language To Autonomously-acquired Skills Via Goal Generation</a> Ahmed Akakzia, C√©dric Colas, Pierre-yves Oudeyer, Mohamed Chetouani, Olivier Sigaud </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2020sequential/">Sequential Latent Knowledge Selection For Knowledge-grounded Dialogue</a> Byeongchang Kim, Jaewoo Ahn, Gunhee Kim </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/karpukhin2020dense/">Dense Passage Retrieval For Open-domain Question Answering</a> Vladimir Karpukhin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020data/">Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation</a> Ruibo Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kasai2020non/">Non-autoregressive Machine Translation With Disentangled Context Transformer</a> Jungo Kasai, James Cross, Marjan Ghazvininejad, Jiatao Gu </li>
     
   
     
       <li> <a href="/publications/kim2020will/">Will I Sound Like Me? Improving Persona Consistency In Dialogues Through Pragmatic Self-consciousness</a> Hyunwoo Kim, Byeongchang Kim, Gunhee Kim </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2020multilingual/">Multilingual Translation With Extensible Multilingual Pretraining And Finetuning</a> Yuqing Tang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gao2020from/">From Machine Reading Comprehension To Dialogue State Tracking: Bridging The Gap</a> Shuyang Gao, Sanchit Agarwal, Tagyoung Chung, Di Jin, Dilek Hakkani-tur </li>
     
   
     
   
     
       <li> <a href="/publications/zhao2020inducing/">Inducing Language-agnostic Multilingual Representations</a> Wei Zhao, Steffen Eger, Johannes Bjerva, Isabelle Augenstein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rei2020neural/">COMET: A Neural Framework For MT Evaluation</a> Ricardo Rei, Craig Stewart, Ana C Farinha, Alon Lavie </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ni2020learning/">M3P: Learning Universal Representations Via Multitask Multilingual Multimodal Pre-training</a> Minheng Ni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lan2020novel/">PONE: A Novel Automatic Evaluation Metric For Open-domain Generative Dialogue Systems</a> Tian Lan, Xian-ling Mao, Wei Wei, Xiaoyan Gao, Heyan Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/latcinnik2020explaining/">Explaining Question Answering Models Through Text Generation</a> Veronica Latcinnik, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khandelwal2020nearest/">Nearest Neighbor Machine Translation</a> Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rashkin2021increasing/">Increasing Faithfulness In Knowledge-grounded Dialogue With Controllable Features</a> Hannah Rashkin, David Reitter, Gaurav Singh Tomar, Dipanjan Das </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2021unified/">Vlmo: Unified Vision-language Pre-training With Mixture-of-modality-experts</a> Hangbo Bao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/penha2021evaluating/">Evaluating The Robustness Of Retrieval Pipelines With Query Variation Generators</a> Gustavo Penha, Arthur C√¢mara, Claudia Hauff </li>
     
   
     
       <li> <a href="/publications/ma2021one/">One Chatbot Per Person: Creating Personalized Chatbots Based On Implicit User Profiles</a> Zhengyi Ma, Zhicheng Dou, Yutao Zhu, Hanxun Zhong, Ji-rong Wen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zaib2021short/">A Short Survey Of Pre-trained Language Models For Conversational AI-A Newage In NLP</a> Munazza Zaib, Quan Z. Sheng, Wei Emma Zhang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021bilingual/">Bitod: A Bilingual Multi-domain Dataset For Task-oriented Dialogue Modeling</a> Zhaojiang Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/razumovskaia2021crossing/">Crossing The Conversational Chasm: A Primer On Natural Language Processing For Multilingual Task-oriented Dialogue Systems</a> Evgeniia Razumovskaia et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kassner2021multilingual/">Multilingual LAMA: Investigating Knowledge In Multilingual Pretrained Language Models</a> Nora Kassner, Philipp Dufter, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dziri2021neural/">Neural Path Hunter: Reducing Hallucination In Dialogue Systems Via Path Grounding</a> Nouha Dziri, Andrea Madotto, Osmar Zaiane, Avishek Joey Bose </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2021contrastive/">Contrastive Learning For Many-to-many Multilingual Neural Machine Translation</a> Xiao Pan, Mingxuan Wang, Liwei Wu, Lei Li </li>
     
   
     
       <li> <a href="/publications/sachan2021end/">End-to-end Training Of Multi-document Reader And Retriever For Open-domain Question Answering</a> Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, Dani Yogatama </li>
     
   
     
   
     
       <li> <a href="/publications/tam2021improving/">Improving And Simplifying Pattern Exploiting Training</a> Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, Colin Raffel </li>
     
   
     
       <li> <a href="/publications/cai2021neural/">Neural Machine Translation With Monolingual Translation Memory</a> Deng Cai, Yan Wang, Huayang Li, Wai Lam, Lemao Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kacupaj2021conversational/">Conversational Question Answering Over Knowledge Graphs With Transformer And Graph Attention Networks</a> Endri Kacupaj et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/santhanam2021rome/">Rome Was Built In 1776: A Case Study On Factual Correctness In Knowledge-grounded Response Generation</a> Sashank Santhanam et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021beyond/">Beyond Goldfish Memory: Long-term Open-domain Conversation</a> Jing Xu, Arthur Szlam, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mao2021dynamic/">DYLE: Dynamic Latent Extraction For Abstractive Long-input Summarization</a> Ziming Mao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021what/">What Makes Good In-context Examples For GPT-\(3\)?</a> Jiachang Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2021universal/">UC2: Universal Cross-lingual Cross-modal Vision-and-language Pre-training</a> Mingyang Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yamada2021efficient/">Efficient Passage Retrieval With Hashing For Open-domain Question Answering</a> Ikuya Yamada, Akari Asai, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021math/">Math Word Problem Generation With Mathematical Consistency And Problem Context Constraints</a> Zichao Wang, Andrew S. Lan, Richard G. Baraniuk </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/borgeaud2021improving/">Improving Language Models By Retrieving From Trillions Of Tokens</a> Sebastian Borgeaud et al. </li>
     
   
     
       <li> <a href="/publications/li2021few/">Few-shot Knowledge Graph-to-text Generation With Pretrained Language Models</a> Junyi Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wu2021ai/">AI Chains: Transparent And Controllable Human-ai Interaction By Chaining Large Language Model Prompts</a> Tongshuang Wu, Michael Terry, Carrie J. Cai </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021scalable/">Scalable And Efficient Moe Training For Multitask Multilingual Models</a> Young Jin Kim et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/schick2021generating/">Generating Datasets With Pretrained Language Models</a> Timo Schick, Hinrich Sch√ºtze </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rao2021language/">Denseclip: Language-guided Dense Prediction With Context-aware Prompting</a> Yongming Rao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xing2021km/">KM-BART: Knowledge Enhanced Multimodal BART For Visual Commonsense Generation</a> Yiran Xing et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2021context/">Compm: Context Modeling With Speaker's Pre-trained Memory Tracking For Emotion Recognition In Conversation</a> Joosung Lee, Wooin Lee </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/rohde2021hierarchical/">Hierarchical Learning For Generation With Long Source Sequences</a> Tobias Rohde, Xiaoxia Wu, Yinhan Liu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2021vision/">Vision Guided Generative Pre-trained Language Models For Multimodal Abstractive Summarization</a> Tiezheng Yu, Wenliang Dai, Zihan Liu, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/krishna2021hurdles/">Hurdles To Progress In Long-form Question Answering</a> Kalpesh Krishna, Aurko Roy, Mohit Iyyer </li>
     
   
     
       <li> <a href="/publications/zhou2021learning/">Learning To Prompt For Vision-language Models</a> Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu </li>
     
   
     
   
     
       <li> <a href="/publications/parvez2021retrieval/">Retrieval Augmented Code Generation And Summarization</a> Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-wei Chang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/castellon2021codified/">Codified Audio Language Modeling Learns Useful Representations For Music Information Retrieval</a> Rodrigo Castellon, Chris Donahue, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khattab2021robust/">Baleen: Robust Multi-hop Reasoning At Scale Via Condensed Retrieval</a> Omar Khattab, Christopher Potts, Matei Zaharia </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021parallel/">Parallel Refinements For Lexically Constrained Text Generation With BART</a> Xingwei He </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021structural/">Structurallm: Structural Pre-training For Form Understanding</a> Chenliang Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zuo2021taming/">Taming Sparsely Activated Transformer With Stochastic Experts</a> Simiao Zuo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2021million/">PAQ: 65 Million Probably-asked Questions And What You Can Do With Them</a> Patrick Lewis et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bai2021semantic/">Semantic Representation For Dialogue Modeling</a> Xuefeng Bai, Yulong Chen, Linfeng Song, Yue Zhang </li>
     
   
     
       <li> <a href="/publications/chintagunta2021medically/">Medically Aware GPT-3 As A Data Generator For Medical Dialogue Summarization</a> Bharath Chintagunta, Namit Katariya, Xavier Amatriain, Anitha Kannan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caciularu2021cross/">CDLM: Cross-document Language Modeling</a> Avi Caciularu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/paranjape2021posterior/">Hindsight: Posterior-guided Training Of Retrievers For Improved Open-ended Generation</a> Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, Christopher D. Manning </li>
     
   
     
       <li> <a href="/publications/sharma2021towards/">Towards Facilitating Empathic Conversations In Online Mental Health Support: A Reinforcement Learning Approach</a> Ashish Sharma, Inna W. Lin, Adam S. Miner, David C. Atkins, Tim Althoff </li>
     
   
     
   
     
       <li> <a href="/publications/aghajanyan2021massive/">Muppet: Massive Multi-task Representations With Pre-finetuning</a> Armen Aghajanyan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pang2021question/">Quality: Question Answering With Long Input Texts, Yes!</a> Richard Yuanzhe Pang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dou2021is/">Is GPT-3 Text Indistinguishable From Human Text? Scarecrow: A Framework For Scrutinizing Machine Text</a> Yao Dou, Maxwell Forbes, Rik Koncel-kedziorski, Noah A. Smith, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/min2021following/">FILM: Following Instructions In Language With Modular Methods</a> So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, Ruslan Salakhutdinov </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/talmor2021complex/">Multimodalqa: Complex Question Answering Over Text, Tables And Images</a> Alon Talmor et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thulke2021efficient/">Efficient Retrieval Augmented Generation From Unstructured Knowledge For Task-oriented Dialog</a> David Thulke, Nico Daheim, Christian Dugast, Hermann Ney </li>
     
   
     
       <li> <a href="/publications/pashevich2021episodic/">Episodic Transformer For Vision-and-language Navigation</a> Alexander Pashevich, Cordelia Schmid, Chen Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/asai2021one/">One Question Answering Model For Many Languages With Cross-lingual Dense Passage Retrieval</a> Akari Asai, Xinyan Yu, Jungo Kasai, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/manzoor2021towards/">Towards Retrieval-based Conversational Recommendation</a> Ahtsham Manzoor, Dietmar Jannach </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/haviv2021learning/">Bertese: Learning To Speak To BERT</a> Adi Haviv, Jonathan Berant, Amir Globerson </li>
     
   
     
       <li> <a href="/publications/atri2021leveraging/">See, Hear, Read: Leveraging Multimodality With Guided Attention For Abstractive Text Summarization</a> Yash Kumar Atri, Shraman Pramanick, Vikram Goyal, Tanmoy Chakraborty </li>
     
   
     
       <li> <a href="/publications/xiao2021training/">Training Large-scale News Recommenders With Pretrained Language Models In The Loop</a> Shitao Xiao, Zheng Liu, Yingxia Shao, Tao Di, Xing Xie </li>
     
   
     
       <li> <a href="/publications/ma2021replication/">A Replication Study Of Dense Passage Retriever</a> Xueguang Ma, Kai Sun, Ronak Pradeep, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021grounded/">Grounded Language-image Pre-training</a> Liunian Harold Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021adversarial/">Adversarial VQA: A New Benchmark For Evaluating The Robustness Of VQA Models</a> Linjie Li, Jie Lei, Zhe Gan, Jingjing Liu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gui2021knowledge/">KAT: A Knowledge Augmented Transformer For Vision-and-language</a> Liangke Gui et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2021fine/">FILIP: Fine-grained Interactive Language-image Pre-training</a> Lewei Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shuster2021retrieval/">Retrieval Augmentation Reduces Hallucination In Conversation</a> Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2021rethink/">Rethink Training Of BERT Rerankers In Multi-stage Retrieval Pipeline</a> Luyu Gao, Zhuyun Dai, Jamie Callan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2021unsupervised/">Unsupervised Corpus Aware Language Model Pre-training For Dense Passage Retrieval</a> Luyu Gao, Jamie Callan </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bartolo2021improving/">Improving Question Answering Model Robustness With Synthetic Adversarial Data Generation</a> Max Bartolo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/levine2022standing/">Standing On The Shoulders Of Giant Frozen Language Models</a> Yoav Levine et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singhal2022large/">Large Language Models Encode Clinical Knowledge</a> Karan Singhal et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022bootstrapping/">BLIP: Bootstrapping Language-image Pre-training For Unified Vision-language Understanding And Generation</a> Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/robinson2022leveraging/">Leveraging Large Language Models For Multiple Choice Question Answering</a> Joshua Robinson, Christopher Michael Rytting, David Wingate </li>
     
   
     
       <li> <a href="/publications/hoffmann2022training/">Training Compute-optimal Large Language Models</a> Jordan Hoffmann et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2022unifying/">Unifiedskg: Unifying And Multi-tasking Structured Knowledge Grounding With Text-to-text Language Models</a> Tianbao Xie et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2022integrating/">RASAT: Integrating Relational Structures Into Pretrained Seq2seq Model For Text-to-sql</a> Jiexing Qi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022knowledge/">Knowledge Prompting In Pre-trained Language Model For Natural Language Understanding</a> Jianing Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li%C3%A9vin2022can/">Can Large Language Models Reason About Medical Questions?</a> Valentin Li√©vin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, Ole Winther </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022generative/">M6-rec: Generative Pretrained Language Models Are Open-ended Recommender Systems</a> Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022survey/">A Survey On Retrieval-augmented Text Generation</a> Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2022one/">One Embedder, Any Task: Instruction-finetuned Text Embeddings</a> Hongjin Su et al. </li>
     
   
     
       <li> <a href="/publications/lu2022collaborative/">COTS: Collaborative Two-stream Vision-language Pre-training Model For Cross-modal Retrieval</a> Haoyu Lu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2022pseudo/">Pseudo-q: Generating Pseudo Language Queries For Visual Grounding</a> Haojun Jiang, Yuanze Lin, Dongchen Han, Shiji Song, Gao Huang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2022less/">Less Is More: Learning To Refine Dialogue History For Personalized Dialogue Generation</a> Hanxun Zhong, Zhicheng Dou, Yutao Zhu, Hongjin Qian, Ji-rong Wen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022multimodal/">Murag: Multimodal Retrieval-augmented Generator For Open Question Answering Over Images And Text</a> Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2022evaluating/">Evaluating And Inducing Personality In Pre-trained Language Models</a> Guangyuan Jiang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lin2022retrieval/">Retrieval Augmented Visual Question Answering With Outside Knowledge</a> Weizhe Lin, Bill Byrne </li>
     
   
     
       <li> <a href="/publications/chen2022program/">Program Of Thoughts Prompting: Disentangling Computation From Reasoning For Numerical Reasoning Tasks</a> Wenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/izacard2022few/">Atlas: Few-shot Learning With Retrieval Augmented Language Models</a> Gautier Izacard et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ke2022hierarchical/">Hitskt: A Hierarchical Transformer Model For Session-aware Knowledge Tracing</a> Fucai Ke et al. </li>
     
   
     
       <li> <a href="/publications/poesia2022reliable/">Synchromesh: Reliable Code Generation From Pre-trained Language Models</a> Gabriel Poesia et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2022inner/">Inner Monologue: Embodied Reasoning Through Planning With Language Models</a> Wenlong Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022hybrid/">Hybrid Transformer With Multi-level Fusion For Multimodal Knowledge Graph Completion</a> Xiang Chen et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zelikman2022bootstrapping/">Star: Bootstrapping Reasoning With Reasoning</a> Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D. Goodman </li>
     
   
     
       <li> <a href="/publications/mitchell2022memory/">Memory-based Model Editing At Scale</a> Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, Chelsea Finn </li>
     
   
     
       <li> <a href="/publications/sun2022recitation/">Recitation-augmented Language Models</a> Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, Denny Zhou </li>
     
   
     
       <li> <a href="/publications/chen2022decoupling/">Decoupling Knowledge From Memorization: Retrieval-augmented Prompt Learning</a> Xiang Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hosseiniasl2022generative/">A Generative Language Model For Few-shot Aspect-based Sentiment Analysis</a> Ehsan Hosseini-asl, Wenhao Liu, Caiming Xiong </li>
     
   
     
   
     
       <li> <a href="/publications/chen2022jointly/">Pali: A Jointly-scaled Multilingual Language-image Model</a> Xi Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sachan2022improving/">Improving Passage Retrieval With Zero-shot Question Generation</a> Devendra Singh Sachan et al. </li>
     
   
     
       <li> <a href="/publications/dua2022successive/">Successive Prompting For Decomposing Complex Questions</a> Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2022dialogue/">Dialfred: Dialogue-enabled Agents For Embodied Instruction Following</a> Xiaofeng Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zan2022continual/">CERT: Continual Pre-training On Sketches For Library-oriented Code Generation</a> Daoguang Zan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2022binding/">Binding Language Models In Symbolic Languages</a> Zhoujun Cheng et al. </li>
     
   
     
       <li> <a href="/publications/burns2022discovering/">Discovering Latent Knowledge In Language Models Without Supervision</a> Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt </li>
     
   
     
   
     
       <li> <a href="/publications/stevenson2022putting/">Putting Gpt-3's Creativity To The (alternative Uses) Test</a> Claire Stevenson, Iris Smal, Matthijs Baas, Raoul Grasman, Han Van Der Maas </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2022few/">Promptagator: Few-shot Dense Retrieval From 8 Examples</a> Zhuyun Dai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2022visual/">Visual-language Navigation Pretraining Via Prompt-based Environmental Self-exploration</a> Xiwen Liang, Fengda Zhu, Lingling Li, Hang Xu, Xiaodan Liang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022code/">Code4struct: Code Generation For Few-shot Event Structure Prediction</a> Xingyao Wang, Sha Li, Heng Ji </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/du2022retrieval/">Retrieval-augmented Generative Question Answering For Event Argument Extraction</a> Xinya Du, Heng Ji </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022rationale/">Rationale-augmented Ensembles In Language Models</a> Xuezhi Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chen2022code/">Codet: Code Generation With Generated Tests</a> Bei Chen et al. </li>
     
   
     
       <li> <a href="/publications/yao2022end/">End-to-end Multimodal Fact-checking And Explanation Generation: A Challenging Dataset And Models</a> Barry Menglong Virginia Tech Yao, Aditya Virginia Tech Shah, Lichao Lehigh University Sun, Jin-hee Virginia Tech Cho, Lifu Virginia Tech Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2022large/">GODEL: Large-scale Pre-training For Goal-directed Dialog</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ushio2022generative/">Generative Language Models For Paragraph-level Question Generation</a> Asahi Ushio, Fernando Alva-manchego, Jose Camacho-collados </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2022unified/">A Unified End-to-end Retriever-reader Framework For Knowledge-based VQA</a> Yangyang Guo et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bapna2022building/">Building Machine Translation Systems For The Next Thousand Languages</a> Ankur Bapna et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2022socratic/">Socratic Models: Composing Zero-shot Multimodal Reasoning With Language</a> Andy Zeng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2022complexity/">Complexity-based Prompting For Multi-step Reasoning</a> Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2022learning/">Learning Video Representations From Large Language Models</a> Yue Zhao, Ishan Misra, Philipp Kr√§henb√ºhl, Rohit Girdhar </li>
     
   
     
   
     
       <li> <a href="/publications/hu2022prompt/">Promptcap: Prompt-guided Task-aware Image Captioning</a> Yushi Hu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022self/">Self-consistency Improves Chain Of Thought Reasoning In Language Models</a> Xuezhi Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2022retrieval/">Retrieval Augmentation Of Large Language Models For Lay Language Generation</a> Yue Guo, Wei Qiu, Gondy Leroy, Sheng Wang, Trevor Cohen </li>
     
   
     
   
     
       <li> <a href="/publications/liu2022worker/">WANLI: Worker And AI Collaboration For Natural Language Inference Dataset Creation</a> Alisa Liu, Swabha Swayamdipta, Noah A. Smith, Yejin Choi </li>
     
   
     
   
     
       <li> <a href="/publications/razeghi2022impact/">Impact Of Pretraining Term Frequencies On Few-shot Reasoning</a> Yasaman Razeghi, Robert L. Iv Logan, Matt Gardner, Sameer Singh </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mallen2022when/">When Not To Trust Language Models: Investigating Effectiveness Of Parametric And Non-parametric Memories</a> Alex Mallen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/prasad2022gradient/">Grips: Gradient-free, Edit-based Instruction Search For Prompting Large Language Models</a> Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal </li>
     
   
     
   
     
       <li> <a href="/publications/kamath2022new/">A New Path: Scaling Vision-and-language Navigation With Synthetic Instructions And Imitation Learning</a> Aishwarya Kamath et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/siddhant2022towards/">Towards The Next 1000 Languages In Multilingual Machine Translation: Exploring The Synergy Between Supervised And Self-supervised Learning</a> Aditya Siddhant et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2022prompt/">Prompt For Extraction? PAIE: Prompting Argument Interaction For Event Argument Extraction</a> Yubo Ma et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/christmann2022conversational/">Conversational Question Answering On Heterogeneous Sources</a> Philipp Christmann, Rishiraj Saha Roy, Gerhard Weikum </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lahiri2022interactive/">Interactive Code Generation Via Test-driven User-intent Formalization</a> Shuvendu K. Lahiri et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022learn/">Learn To Explain: Multimodal Reasoning Via Thought Chains For Science Question Answering</a> Pan Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khattab2022demonstrate/">Demonstrate-search-predict: Composing Retrieval And Language Models For Knowledge-intensive NLP</a> Omar Khattab et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ratner2022parallel/">Parallel Context Windows For Large Language Models</a> Nir Ratner et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kandpal2022large/">Large Language Models Struggle To Learn Long-tail Knowledge</a> Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/borsos2022language/">Audiolm: A Language Modeling Approach To Audio Generation</a> Zal√°n Borsos et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ho2022large/">Large Language Models Are Reasoning Teachers</a> Namgyu Ho, Laura Schmid, Se-young Yun </li>
     
   
     
       <li> <a href="/publications/khattak2022multi/">Maple: Multi-modal Prompt Learning</a> Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yasunaga2022deep/">Deep Bidirectional Language-knowledge Graph Pretraining</a> Michihiro Yasunaga et al. </li>
     
   
     
       <li> <a href="/publications/yasunaga2022retrieval/">Retrieval-augmented Multimodal Language Modeling</a> Michihiro Yasunaga et al. </li>
     
   
     
       <li> <a href="/publications/zhao2022educational/">Educational Question Generation Of Children Storybooks Via Question Type Distribution Learning And Event-centric Summarization</a> Zhenjie Zhao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/glass2022generate/">Re2g: Retrieve, Rerank, Generate</a> Michael Glass et al. </li>
     
   
     
       <li> <a href="/publications/ahn2022do/">Do As I Can, Not As I Say: Grounding Language In Robotic Affordances</a> Michael Ahn et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022efficient/">An Efficient Memory-augmented Transformer For Knowledge-intensive NLP Tasks</a> Yuxiang Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ivgi2022efficient/">Efficient Long-text Understanding With Short-text Models</a> Maor Ivgi, Uri Shaham, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sap2022neural/">Neural Theory-of-mind? On The Limits Of Social Intelligence In Large Lms</a> Maarten Sap, Ronan Lebras, Daniel Fried, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2022retrieval/">REVEAL: Retrieval-augmented Visual-language Pre-training With Multi-source Multimodal Knowledge Memory</a> Ziniu Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2022robotic/">Robotic Skill Acquisition Via Instruction Augmentation With Vision-language Models</a> Ted Xiao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kalakonda2022action/">Action-gpt: Leveraging Large-scale Language Models For Improved And Generalized Action Generation</a> Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022retrieval/">Reacc: A Retrieval-augmented Code Completion Framework</a> Shuai Lu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chandel2022training/">Training And Evaluating A Jupyter Notebook Data Science Assistant</a> Shubham Chandel, Colin B. Clement, Guillermo Serrato, Neel Sundaresan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/siriwardhana2022improving/">Improving The Domain Adaptation Of Retrieval Augmented Generation (RAG) Models For Open Domain Question Answering</a> Shamane Siriwardhana et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kadavath2022language/">Language Models (mostly) Know What They Know</a> Saurav Kadavath et al. </li>
     
   
     
       <li> <a href="/publications/bowman2022measuring/">Measuring Progress On Scalable Oversight For Large Language Models</a> Samuel R. Bowman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/poldrack2023ai/">Ai-assisted Coding: Experiments With GPT-4</a> Russell A Poldrack, Thomas Lu, Ga≈°per Begu≈° </li>
     
   
     
       <li> <a href="/publications/zhao2023retrieving/">Retrieving Multimodal Information For Augmented Generation: A Survey</a> Ruochen Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023make/">Make-an-audio: Text-to-audio Generation With Prompt-enhanced Diffusion Models</a> Rongjie Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ramos2023retrieval/">Retrieval-augmented Image Captioning</a> Rita Ramos, Desmond Elliott, Bruno Martins </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023then/">Prompt, Generate, Then Cache: Cascade Of Foundation Models Makes Strong Few-shot Learners</a> Renrui Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/omar2023universal/">A Universal Question-answering Platform For Knowledge Graphs</a> Reham Omar, Ishika Dhall, Panos Kalnis, Essam Mansour </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/min2023fine/">Factscore: Fine-grained Atomic Evaluation Of Factual Precision In Long Form Text Generation</a> Sewon Min et al. </li>
     
   
     
       <li> <a href="/publications/es2023automated/">Ragas: Automated Evaluation Of Retrieval Augmented Generation</a> Shahul Es, Jithin James, Luis Espinosa-anke, Steven Schockaert </li>
     
   
     
   
     
       <li> <a href="/publications/kim2023cot/">The Cot Collection: Improving Zero-shot And Few-shot Learning Of Language Models Via Chain-of-thought Fine-tuning</a> Seungone Kim et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sanner2023large/">Large Language Models Are Competitive Near Cold-start Recommenders For Language- And Item-based Preferences</a> Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, Lucas Dixon </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2023llm/">Llm-in-the-loop: Leveraging Large Language Model For Thematic Analysis</a> Shih-chieh Dai, Aiping Xiong, Lun-wei Ku </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023label/">Label Supervised Llama Finetuning</a> Zongxi Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023lawyer/">Lawyer Llama Technical Report</a> Quzhe Huang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023mplug/">Mplug-owl: Modularization Empowers Large Language Models With Multimodality</a> Qinghao Ye et al. </li>
     
   
     
       <li> <a href="/publications/liu2023boosting/">ONCE: Boosting Content-based Recommendation With Both Open- And Closed-source Large Language Models</a> Qijiong Liu, Nuo Chen, Tetsuya Sakai, Xiao-ming Wu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023augmenting/">Genegpt: Augmenting Large Language Models With Domain Tools For Improved Access To Biomedical Information</a> Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023prompting/">Prompting The Hidden Talent Of Web-scale Speech Models For Zero-shot Task Generalization</a> Puyuan Peng, Brian Yan, Shinji Watanabe, David Harwath </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/manakul2023zero/">Selfcheckgpt: Zero-resource Black-box Hallucination Detection For Generative Large Language Models</a> Potsawee Manakul, Adian Liusie, Mark J. F. Gales </li>
     
   
     
       <li> <a href="/publications/pan2023unifying/">Unifying Large Language Models And Knowledge Graphs: A Roadmap</a> Shirui Pan et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023chat/">Chat-univi: Unified Visual Representation Empowers Large Language Models With Image And Video Understanding</a> Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, Li Yuan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rubenstein2023large/">Audiopalm: A Large Language Model That Can Speak And Listen</a> Paul K. Rubenstein et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ram2023retrieval/">In-context Retrieval-augmented Language Models</a> Ori Ram et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ovadia2023fine/">Fine-tuning Or Retrieval? Comparing Knowledge Injection In Llms</a> Oded Ovadia, Menachem Brief, Moshik Mishaeli, Oren Elisha </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/rotstein2023leveraging/">Fusecap: Leveraging Large Language Models For Enriched Fused Image Captions</a> Noam Rotstein, David Bensaid, Shaked Brody, Roy Ganz, Ron Kimmel </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rao2023cat/">CAT-LM: Training Language Models On Aligned Code And Tests</a> Nikitha Rao, Kush Jain, Uri Alon, Claire Le Goues, Vincent J. Hellendoorn </li>
     
   
     
   
     
       <li> <a href="/publications/ding2023enhancing/">Enhancing Chat Language Models By Scaling High-quality Instructional Conversations</a> Ning Ding et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2023multimodal/">Openvivqa: Task, Dataset, And Multimodal Fusion Models For Visual Question Answering In Vietnamese</a> Nghia Hieu Nguyen, Duong T. D. Vo, Kiet Van Nguyen, Ngan Luu-thuy Nguyen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/varshney2023stitch/">A Stitch In Time Saves Nine: Detecting And Mitigating Hallucinations Of Llms By Validating Low-confidence Generation</a> Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sultanum2023investigating/">Datatales: Investigating The Use Of Large Language Models For Authoring Data-driven Articles</a> Nicole Sultanum, Arjun Srinivasan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023api/">Api-bank: A Comprehensive Benchmark For Tool-augmented Llms</a> Minghao Li et al. </li>
     
   
     
       <li> <a href="/publications/wu2023lamini/">Lamini-lm: A Diverse Herd Of Distilled Models From Large-scale Instructions</a> Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-mageed, Alham Fikri Aji </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kwon2023reward/">Reward Design With Language Models</a> Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023scaffolding/">Selenite: Scaffolding Online Sensemaking With Comprehensive Overviews Elicited From Large Language Models</a> Michael Xieyang Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cosler2023interactively/">Nl2spec: Interactively Translating Unstructured Natural Language To Temporal Logics With Large Language Models</a> Matthias Cosler, Christopher Hahn, Daniel Mendoza, Frederik Schmitt, Caroline Trippel </li>
     
   
     
   
     
       <li> <a href="/publications/karpinska2023large/">Large Language Models Effectively Leverage Document-level Context For Literary Translation, But Critical Errors Persist</a> Marzena Karpinska, Mohit Iyyer </li>
     
   
     
   
     
       <li> <a href="/publications/patil2023large/">Gorilla: Large Language Model Connected With Massive Apis</a> Shishir G. Patil, Tianjun Zhang, Xin Wang, Joseph E. Gonzalez </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023active/">Active Retrieval Augmented Generation</a> Zhengbao Jiang et al. </li>
     
   
     
       <li> <a href="/publications/wang2023document/">Document-level Machine Translation With Large Language Models</a> Longyue Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lian2023llm/">Llm-grounded Diffusion: Enhancing Prompt Understanding Of Text-to-image Diffusion Models With Large Language Models</a> Long Lian, Boyi Li, Adam Yala, Trevor Darrell </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wong2023from/">From Word Models To World Models: Translating From Natural Language To The Probabilistic Language Of Thought</a> Lionel Wong et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023bilingual/">Taiyi: A Bilingual Fine-tuned Large Language Model For Diverse Biomedical Tasks</a> Ling Luo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fan2023improving/">Improving CLIP Training With Language Rewrites</a> Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, Yonglong Tian </li>
     
   
     
       <li> <a href="/publications/wu2023survey/">A Survey On Large Language Models For Recommendation</a> Likang Wu et al. </li>
     
   
     
       <li> <a href="/publications/yu2023scaling/">Scaling Autoregressive Multi-modal Models: Pretraining And Instruction Tuning</a> Lili Yu et al. </li>
     
   
     
       <li> <a href="/publications/pan2023logic/">Logic-lm: Empowering Large Language Models With Symbolic Solvers For Faithful Logical Reasoning</a> Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023improving/">Improving Text Embeddings With Large Language Models</a> Liang Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/pan2023automatically/">Automatically Correcting Large Language Models: Surveying The Landscape Of Diverse Self-correction Strategies</a> Liangming Pan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/guan2023leveraging/">Leveraging Pre-trained Large Language Models To Construct And Utilize World Models For Model-based Task Planning</a> Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/seenivasan2023end/">Surgicalgpt: End-to-end Language-vision GPT For Visual Question Answering In Surgery</a> Lalithkumar Seenivasan, Mobarakol Islam, Gokul Kannan, Hongliang Ren </li>
     
   
     
       <li> <a href="/publications/huang2023survey/">A Survey On Hallucination In Large Language Models: Principles, Taxonomy, Challenges, And Open Questions</a> Lei Huang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023flexible/">Flexkbqa: A Flexible Llm-powered Framework For Few-shot Knowledge Base Question Answering</a> Zhenyu Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caramancion2023news/">News Verifiers Showdown: A Comparative Performance Evaluation Of Chatgpt 3.5, Chatgpt 4.0, Bing AI, And Bard In News Fact-checking</a> Kevin Matthe Caramancion </li>
     
   
     
   
     
       <li> <a href="/publications/yager2023domain/">Domain-specific Chatbots For Science Using Embeddings</a> Kevin G. Yager </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2023effective/">Tallrec: An Effective And Efficient Tuning Framework To Align Large Language Model With Recommendation</a> Keqin Bao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/pandya2023automating/">Automating Customer Service Using Langchain: Building Custom Open-source GPT Chatbot For Organizations</a> Keivalya Pandya, Mehfuza Holia </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gopalakrishnan2023topical/">Topical-chat: Towards Knowledge-grounded Open-domain Conversations</a> Karthik Gopalakrishnan et al. </li>
     
   
     
       <li> <a href="/publications/soman2023biomedical/">Biomedical Knowledge Graph-optimized Prompt Generation For Large Language Models</a> Karthik Soman et al. </li>
     
   
     
       <li> <a href="/publications/singhal2023towards/">Towards Expert-level Medical Question Answering With Large Language Models</a> Karan Singhal et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gou2023multi/">Mvp: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction</a> Zhibin Gou, Qingyan Guo, Yujiu Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023evaluation/">Evaluation And Analysis Of Hallucination In Large Vision-language Models</a> Junyang Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kim2023ai/">Ai-augmented Surveys: Leveraging Large Language Models And Surveys For Opinion Prediction</a> Junsol Kim, Byungkyu Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fei2023transferable/">Transferable Decoding With Visual Entities For Zero-shot Image Captioning</a> Junjie Fei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023large/">Chatcounselor: A Large Language Models For Mental Health Support</a> June M. Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jang2023exploring/">Exploring The Benefits Of Training Expert Language Models Over Instruction Tuning</a> Joel Jang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/koh2023grounding/">Grounding Language Models To Images For Multimodal Inputs And Outputs</a> Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried </li>
     
   
     
       <li> <a href="/publications/koh2023generating/">Generating Images With Multimodal Language Models</a> Jing Yu Koh, Daniel Fried, Ruslan Salakhutdinov </li>
     
   
     
       <li> <a href="/publications/jiang2023redundancy/">Mixphm: Redundancy-aware Parameter-efficient Tuning For Low-resource Visual Question Answering</a> Jingjing Jiang, Nanning Zheng </li>
     
   
     
   
     
       <li> <a href="/publications/chen2023when/">When Large Language Models Meet Personalization: Perspectives Of Challenges And Opportunities</a> Jin Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023benchmarking/">Benchmarking Large Language Models In Retrieval-augmented Generation</a> Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun </li>
     
   
     
   
     
       <li> <a href="/publications/xi2023rise/">The Rise And Potential Of Large Language Model Based Agents: A Survey</a> Zhiheng Xi et al. </li>
     
   
     
       <li> <a href="/publications/sun2023think/">Think-on-graph: Deep And Responsible Reasoning Of Large Language Model On Knowledge Graph</a> Jiashuo Sun et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023unified/">A Unified Generative Retriever For Knowledge-intensive Language Tasks Via Prompt Learning</a> Jiangui Chen et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023empowering/">Empowering Molecule Discovery For Molecule-caption Translation With Large Language Models: A Chatgpt Perspective</a> Jiatong Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wu2023decoder/">On Decoder-only Architecture For Speech-to-text And Large Language Model Integration</a> Jian Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/harte2023leveraging/">Leveraging Large Language Models For Sequential Recommendation</a> Jesse Harte et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shao2023enhancing/">Enhancing Retrieval-augmented Large Language Models With Iterative Retrieval-generation Synergy</a> Zhihong Shao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/blocklove2023chip/">Chip-chat: Challenges And Opportunities In Conversational Hardware Design</a> Jason Blocklove, Siddharth Garg, Ramesh Karri, Hammond Pearce </li>
     
   
     
   
     
       <li> <a href="/publications/holmes2023evaluating/">Evaluating Large Language Models On A Highly-specialized Topic, Radiation Oncology Physics</a> Jason Holmes et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/l%C3%A1la2023retrieval/">Paperqa: Retrieval-augmented Generative Agent For Scientific Research</a> Jakub L√°la et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023cognitive/">Cognitive Mirage: A Review Of Hallucinations In Large Language Models</a> Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, Weiqiang Jia </li>
     
   
     
   
     
       <li> <a href="/publications/gilbert2023semantic/">Semantic Compression With Large Language Models</a> Henry Gilbert, Michael Sandborn, Douglas C. Schmidt, Jesse Spencer-smith, Jules White </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nori2023can/">Can Generalist Foundation Models Outcompete Special-purpose Tuning? Case Study In Medicine</a> Harsha Nori et al. </li>
     
   
     
   
     
       <li> <a href="/publications/tian2023is/">Is Chatgpt The Ultimate Programming Assistant -- How Far Is It?</a> Haoye Tian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023generate/">Chatkbqa: A Generate-then-retrieve Framework For Knowledge Base Question Answering With Fine-tuned Large Language Models</a> Haoran Luo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2023measuring/">CMMLU: Measuring Massive Multitask Language Understanding In Chinese</a> Haonan Li et al. </li>
     
   
     
       <li> <a href="/publications/wu2023q/">Q-instruct: Improving Low-level Visual Abilities For Multi-modality Foundation Models</a> Haoning Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shao2023closed/">Lmdrive: Closed-loop End-to-end Driving With Large Language Models</a> Hao Shao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2023leveraging/">Auggpt: Leveraging Chatgpt For Text Data Augmentation</a> Haixing Dai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zuccon2023dr/">Dr Chatgpt, Tell Me What I Want To Hear: How Prompt Knowledge Impacts Health Answer Correctness</a> Guido Zuccon, Bevan Koopman </li>
     
   
     
   
     
       <li> <a href="/publications/zuccon2023chatgpt/">Chatgpt Hallucinates When Attributing Answers</a> Guido Zuccon, Bevan Koopman, Razia Shaik </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/todd2023level/">Level Generation Through Large Language Models</a> Graham Todd, Sam Earle, Muhammad Umair Nasir, Michael Cerny Green, Julian Togelius </li>
     
   
     
       <li> <a href="/publications/mialon2023augmented/">Augmented Language Models: A Survey</a> Gr√©goire Mialon et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/delatorre2023real/">LLMR: Real-time Prompting Of Interactive Worlds Using Large Language Models</a> Fernanda De La Torre et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023repository/">Repocoder: Repository-level Code Completion Through Iterative Retrieval And Generation</a> Fengji Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sur%C3%ADs2023visual/">Vipergpt: Visual Inference Via Python Execution For Reasoning</a> D√≠dac Sur√≠s, Sachit Menon, Carl Vondrick </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023llm/">Llm-blender: Ensembling Large Language Models With Pairwise Ranking And Generative Fusion</a> Dongfu Jiang, Xiang Ren, Bill Yuchen Lin </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2023event/">Event Extraction As Question Generation And Answering</a> Di Lu, Shihao Ran, Joel Tetreault, Alejandro Jaimes </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023minigpt/">Minigpt-4: Enhancing Vision-language Understanding With Advanced Large Language Models</a> Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zakka2023retrieval/">Almanac: Retrieval-augmented Language Models For Clinical Medicine</a> Cyril Zakka et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023llava/">Llava-med: Training A Large Language-and-vision Assistant For Biomedicine In One Day</a> Chunyuan Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/small2023opportunities/">Opportunities And Risks Of Llms For Scalable Deliberation With Polis</a> Christopher T. Small et al. </li>
     
   
     
       <li> <a href="/publications/xia2023conversational/">Conversational Automated Program Repair</a> Chunqiu Steven Xia, Lingming Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jeong2023study/">A Study On The Implementation Of Generative AI Services Using An Enterprise Data-based LLM Application Architecture</a> Cheonsu Jeong </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023iterative/">An Iterative Optimizing Framework For Radiology Report Summarization With Chatgpt</a> Chong Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/whitehouse2023llm/">Llm-powered Data Augmentation For Enhanced Cross-lingual Performance</a> Chenxi Whitehouse, Monojit Choudhury, Alham Fikri Aji </li>
     
   
     
       <li> <a href="/publications/rastogi2023supporting/">Supporting Human-ai Collaboration In Auditing Llms With Llms</a> Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, Harsha Nori, Saleema Amershi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/qian2023communicative/">Chatdev: Communicative Agents For Software Development</a> Chen Qian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2023drive/">Receive, Reason, And React: Drive As You Say With Large Language Models In Autonomous Vehicles</a> Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Ziran Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/soong2023improving/">Improving Accuracy Of GPT-3/4 Results On Biomedical Data Using A Retrieval-augmented Language Model</a> David Soong et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023adapting/">Adapting Large Language Models By Integrating Collaborative Semantics For Recommendation</a> Bowen Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023empowering/">LLM+P: Empowering Large Language Models With Optimal Planning Proficiency</a> Bo Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/toma2023clinical/">Clinical Camel: An Open Expert-level Medical Language Model With Dialogue-based Knowledge Encoding</a> Augustin Toma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gudibande2023false/">The False Promise Of Imitating Proprietary Llms</a> Arnav Gudibande et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/louis2023interpretable/">Interpretable Long-form Legal Question Answering With Retrieval-augmented Large Language Models</a> Antoine Louis, Gijs Van Dijck, Gerasimos Spanakis </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023llm/">Expel: LLM Agents Are Experiential Learners</a> Andrew Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023generative/">On Generative Agents In Recommendation</a> An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, Tat-seng Chua </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/madaan2023self/">Self-refine: Iterative Refinement With Self-feedback</a> Aman Madaan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/salemi2023when/">Lamp: When Large Language Models Meet Personalization</a> Alireza Salemi, Sheshera Mysore, Michael Bendersky, Hamed Zamani </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sha2023large/">Languagempc: Large Language Models As Decision Makers For Autonomous Driving</a> Hao Sha et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023mistral/">Mistral 7B</a> Albert Q. Jiang et al. </li>
     
   
     
       <li> <a href="/publications/asai2023self/">Self-rag: Learning To Retrieve, Generate, And Critique Through Self-reflection</a> Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi </li>
     
   
     
       <li> <a href="/publications/ghosh2023clip/">Clipsyntel: CLIP And LLM Synergy For Multimodal Question Summarization In Healthcare</a> Akash Ghosh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023ghost/">Ghost In The Minecraft: Generally Capable Agents For Open-world Environments Via Large Language Models With Text-based Knowledge And Memory</a> Xizhou Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guan2023mitigating/">Mitigating Large Language Model Hallucinations Via Autonomous Knowledge Graph-based Retrofitting</a> Xinyan Guan et al. </li>
     
   
     
       <li> <a href="/publications/ma2023query/">Query Rewriting For Retrieval-augmented Large Language Models</a> Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023pmc/">PMC-VQA: Visual Instruction Tuning For Medical Visual Question Answering</a> Xiaoman Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2023large/">Medagents: Large Language Models As Collaborators For Zero-shot Medical Reasoning</a> Xiangru Tang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/seo2023leveraging/">Chacha: Leveraging Large Language Models To Prompt Children To Share Their Emotions About Personal Events</a> Woosuk Seo, Chanmo Yang, Young-ho Kim </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023generative/">Generative Recommendation: Towards Next-generation Recommender Paradigm</a> Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, Tat-seng Chua </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2023can/">Can Large Language Models Provide Useful Feedback On Research Papers? A Large-scale Empirical Analysis</a> Weixin Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023retrieval/">REPLUG: Retrieval-augmented Black-box Language Models</a> Weijia Shi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2023large/">Llmrec: Large Language Models With Graph Augmentation For Recommendation</a> Wei Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rawte2023troubling/">The Troubling Emergence Of Hallucination In Large Language Models -- An Extensive Definition, Quantification, And Prescriptive Remediations</a> Vipula Rawte et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/adlakha2023evaluating/">Evaluating Correctness And Faithfulness Of Instruction-following Models For Question Answering</a> Vaibhav Adlakha, Parishad Behnamghader, Xing Han Lu, Nicholas Meade, Siva Reddy </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phung2023automating/">Automating Human Tutor-style Programming Feedback: Leveraging GPT-4 Tutor Model For Hint Generation And GPT-3.5 Student Model For Hint Validation</a> Tung Phung et al. </li>
     
   
     
       <li> <a href="/publications/vu2023refreshing/">Freshllms: Refreshing Large Language Models With Search Engine Augmentation</a> Tu Vu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vansonsbeek2023open/">Open-ended Medical Visual Question Answering Through Prefix Tuning Of Language Models</a> Tom Van Sonsbeek, Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees G. M. Snoek, Marcel Worring </li>
     
   
     
   
     
       <li> <a href="/publications/lai2023psy/">Psy-llm: Scaling Up Global Mental Health Psychological Services With Ai-based Large Language Models</a> Tin Lai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023few/">Few-shot In-context Learning For Knowledge Base Question Answering</a> Tianle Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2023encouraging/">Encouraging Divergent Thinking In Large Language Models Through Multi-agent Debate</a> Tian Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023caption/">Caption Anything: Interactive Image Description With Diverse Multimodal Controls</a> Teng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2023enhancing/">Enhancing Job Recommendation Through Llm-based Generative Adversarial Networks</a> Yingpeng Du et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023can/">Can Chatgpt Reproduce Human-generated Labels? A Study Of Social Computing Tasks</a> Yiming Zhu, Peixian Zhang, Ehsan-ul Haq, Pan Hui, Gareth Tyson </li>
     
   
     
   
     
       <li> <a href="/publications/wen2023knowledge/">Mindmap: Knowledge Graph Prompting Sparks Graph Of Thoughts In Large Language Models</a> Yilin Wen, Zifeng Wang, Jimeng Sun </li>
     
   
     
   
     
       <li> <a href="/publications/tian2023graph/">Graph Neural Prompting With Large Language Models</a> Yijun Tian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023llm/">Llm-eval: Unified Multi-dimensional Automatic Evaluation For Open-domain Conversations With Large Language Models</a> Yen-ting Lin, Yun-nung Chen </li>
     
   
     
       <li> <a href="/publications/chia2023towards/">INSTRUCTEVAL: Towards Holistic Evaluation Of Instruction-tuned Large Language Models</a> Yew Ken Chia, Pengfei Hong, Lidong Bing, Soujanya Poria </li>
     
   
     
   
     
       <li> <a href="/publications/yang2023human/">Human-centric Autonomous Systems With Llms For User Command Reasoning</a> Yi Yang et al. </li>
     
   
     
       <li> <a href="/publications/bang2023multimodal/">A Multitask, Multilingual, Multimodal Evaluation Of Chatgpt On Reasoning, Hallucination, And Interactivity</a> Yejin Bang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2023translating/">Translating Natural Language To Planning Goals With Large-language Models</a> Yaqi Xie et al. </li>
     
   
     
       <li> <a href="/publications/zhu2023collaborative/">Collaborative Large Language Model For Recommender Systems</a> Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2023chatting/">Chatpose: Chatting About 3D Human Pose</a> Yao Feng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023large/">Recmind: Large Language Model Powered Agent For Recommendation</a> Yancheng Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ding2023integrating/">Integrating Action Knowledge And Llms For Task Planning And Situation Handling In Open Worlds</a> Yan Ding et al. </li>
     
   
     
       <li> <a href="/publications/miao2023accelerating/">Specinfer: Accelerating Generative Large Language Model Serving With Tree-based Speculative Inference And Verification</a> Xupeng Miao et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023training/">Controlvideo: Training-free Controllable Text-to-video Generation</a> Yabo Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023fine/">Fine-tuning Llama For Multi-stage Text Retrieval</a> Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023large/">Large Language Model As Attributed Training Data Generator: A Tale Of Diversity And Bias</a> Yue Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023solving/">Hugginggpt: Solving AI Tasks With Chatgpt And Its Friends In Hugging Face</a> Yongliang Shen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023towards/">Gpt4aigchip: Towards Next-generation AI Accelerator Design Automation Via Large Language Models</a> Yonggan Fu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023large/">Large Language Models Are Versatile Decomposers: Decompose Evidence And Questions For Table-based Reasoning</a> Yunhu Ye et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/du2023guiding/">Guiding Pretraining In Reinforcement Learning With Large Language Models</a> Yuqing Du et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huo2023retrieving/">Retrieving Supporting Evidence For Generative Question Answering</a> Siqing Huo, Negar Arabzadeh, Charles L. A. Clarke </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023automl/">Automl-gpt: Automatic Machine Learning With GPT</a> Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, Mingyuan Zhou </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023self/">Self-chained Image-language Model For Video Localization And Question Answering</a> Shoubin Yu, Jaemin Cho, Prateek Yadav, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023c/">C-eval: A Multi-level Multi-discipline Chinese Evaluation Suite For Foundation Models</a> Yuzhen Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2023copiloting/">Copiloting The Copilots: Fusing Large Language Models With Completion Engines For Automated Program Repair</a> Yuxiang Wei, Chunqiu Steven Xia, Lingming Zhang </li>
     
   
     
   
     
       <li> <a href="/publications/qin2023large/">Large Language Models Are Effective Text Rankers With Pairwise Ranking Prompting</a> Zhen Qin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2024large/">Large Language Models Meet Collaborative Filtering: An Efficient All-round Llm-based Recommender System</a> Sein Kim et al. </li>
     
   
     
       <li> <a href="/publications/tonmoy2024comprehensive/">A Comprehensive Survey Of Hallucination Mitigation Techniques In Large Language Models</a> S. M Towhidul Islam Tonmoy et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2024multimodal/">SNIFFER: Multimodal Large Language Model For Explainable Out-of-context Misinformation Detection</a> Peng Qi, Zehong Yan, Wynne Hsu, Mong Li Lee </li>
     
   
     
   
     
       <li> <a href="/publications/dhillon2024shaping/">Shaping Human-ai Collaboration: Varied Scaffolding Levels In Co-writing With Language Models</a> Paramveer S. Dhillon et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lieber2024hybrid/">Jamba: A Hybrid Transformer-mamba Language Model</a> Opher Lieber et al. </li>
     
   
     
       <li> <a href="/publications/wiratunga2024cbr/">CBR-RAG: Case-based Reasoning For Retrieval Augmented Generation In Llms For Legal Question Answering</a> Nirmalie Wiratunga et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2024when/">When Large Language Model Agents Meet 6G Networks: Perception, Grounding, And Alignment</a> Minrui Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/saab2024capabilities/">Capabilities Of Gemini Models In Medicine</a> Khaled Saab et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2024revolutionizing/">Revolutionizing Finance With Llms: An Overview Of Applications And Insights</a> Huaqin Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xiong2024benchmarking/">Benchmarking Retrieval-augmented Generation For Medicine</a> Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lei2024materials/">Materials Science In The Era Of Large Language Models: A Perspective</a> Ge Lei, Ronan Docherty, Samuel J. Cooper </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cuconasu2024power/">The Power Of Noise: Redefining Retrieval For RAG Systems</a> Florin Cuconasu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/balaguer2024rag/">RAG Vs Fine-tuning: Pipelines, Tradeoffs, And A Case Study On Agriculture</a> Angels Balaguer et al. </li>
     
   
     
       <li> <a href="/publications/salemi2024optimization/">Optimization Methods For Personalizing Large Language Models Through Retrieval Augmentation</a> Alireza Salemi, Surya Kallumadi, Hamed Zamani </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2024autonomous/">Autocoderover: Autonomous Program Improvement</a> Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, Abhik Roychoudhury </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lyu2024crud/">CRUD-RAG: A Comprehensive Chinese Benchmark For Retrieval-augmented Generation Of Large Language Models</a> Yuanjie Lyu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2024searching/">Searching For Best Practices In Retrieval-augmented Generation</a> Xiaohua Wang et al. </li>
     
   
     
       <li> <a href="/publications/fan2024survey/">A Survey On RAG Meeting Llms: Towards Retrieval-augmented Large Language Models</a> Wenqi Fan et al. </li>
     
   
     
       <li> <a href="/publications/liang2024monitoring/">Monitoring Ai-modified Content At Scale: A Case Study On The Impact Of Chatgpt On AI Conference Peer Reviews</a> Weixin Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parthasarathy2024ultimate/">The Ultimate Guide To Fine-tuning Llms From Basics To Breakthroughs: An Exhaustive Review Of Technologies, Research, Best Practices, Applied Research Challenges And Opportunities</a> Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid </li>
     
   
     
   
     
       <li> <a href="/publications/wu2024continual/">Continual Learning For Large Language Models: A Survey</a> Tongtong Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nepal2024contextual/">Contextual AI Journaling: Integrating LLM And Time Series Behavioral Sensing Technology To Promote Self-reflection And Well-being Using The Mindscape App</a> Subigya Nepal et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2024exploratory/">Llmparser: An Exploratory Study On Using Large Language Models For Log Parsing</a> Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-hsun Chen, Shaowei Wang </li>
     
   
     
       <li> <a href="/publications/hu2024prompting/">Prompting Large Language Models With Rationale Heuristics For Knowledge-based Visual Question Answering</a> Zhongjian Hu, Peng Yang, Bing Li, Fengyuan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Reinforcement Learning <a id="Reinforcement Learning"></a></h3>
   <ul>
   
     
       <li> <a href="/publications/li2016deep/">Deep Reinforcement Learning For Dialogue Generation</a> Jiwei Li et al. </li>
     
   
     
       <li> <a href="/publications/weissenborn2016separating/">Separating Answers From Queries For Neural Reading Comprehension</a> Dirk Weissenborn </li>
     
   
     
       <li> <a href="/publications/bhoopchand2016learning/">Learning Python Code Suggestion With A Sparse Pointer Network</a> Avishkar Bhoopchand, Tim Rockt√§schel, Earl Barr, Sebastian Riedel </li>
     
   
     
   
     
       <li> <a href="/publications/asghar2016deep/">Deep Active Learning For Dialogue Generation</a> Nabiha Asghar, Pascal Poupart, Xin Jiang, Hang Li </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lebret2016neural/">Neural Text Generation From Structured Data With Application To The Biography Domain</a> Remi Lebret, David Grangier, Michael Auli </li>
     
   
     
   
     
       <li> <a href="/publications/vijayakumar2016diverse/">Diverse Beam Search: Decoding Diverse Solutions From Neural Sequence Models</a> Ashwin K Vijayakumar et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bahdanau2016actor/">An Actor-critic Algorithm For Sequence Prediction</a> Dzmitry Bahdanau et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/trischler2016natural/">Natural Language Comprehension With The Epireader</a> Adam Trischler, Zheng Ye, Xingdi Yuan, Kaheer Suleman </li>
     
   
     
       <li> <a href="/publications/andreas2016reasoning/">Reasoning About Pragmatics With Neural Listeners And Speakers</a> Jacob Andreas, Dan Klein </li>
     
   
     
       <li> <a href="/publications/li2016persona/">A Persona-based Neural Conversation Model</a> Jiwei Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bordes2016learning/">Learning End-to-end Goal-oriented Dialog</a> Antoine Bordes, Y-lan Boureau, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2016fast/">A Simple, Fast Diverse Decoding Algorithm For Neural Generation</a> Jiwei Li, Will Monroe, Dan Jurafsky </li>
     
   
     
       <li> <a href="/publications/johnson2016multilingual/">Google's Multilingual Neural Machine Translation System: Enabling Zero-shot Translation</a> Melvin Johnson et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2016user/">A User Simulator For Task-completion Dialogues</a> Xiujun Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mo2017fine/">Fine Grained Knowledge Transfer For Personalized Task-oriented Dialogue Systems</a> Kaixiang Mo, Yu Zhang, Qiang Yang, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/anderson2017vision/">Vision-and-language Navigation: Interpreting Visually-grounded Navigation Instructions In Real Environments</a> Peter Anderson et al. </li>
     
   
     
       <li> <a href="/publications/narasimhan2017grounding/">Grounding Language For Transfer In Deep Reinforcement Learning</a> Karthik Narasimhan, Regina Barzilay, Tommi Jaakkola </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kandasamy2017batch/">Batch Policy Gradient Methods For Improving Neural Conversation Models</a> Kirthevasan Kandasamy, Yoram Bachrach, Ryota Tomioka, Daniel Tarlow, David Carter </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2017reinforced/">Reinforced Mnemonic Reader For Machine Reading Comprehension</a> Minghao Hu et al. </li>
     
   
     
       <li> <a href="/publications/strub2017end/">End-to-end Optimization Of Goal-driven And Visually Grounded Dialogue Systems</a> Florian Strub et al. </li>
     
   
     
       <li> <a href="/publications/guo2017long/">Long Text Generation Via Adversarial Training With Leaked Information</a> Jiaxian Guo et al. </li>
     
   
     
       <li> <a href="/publications/buck2017ask/">Ask The Right Questions: Active Question Reformulation With Reinforcement Learning</a> Christian Buck et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2017data/">Data Distillation For Controlling Specificity In Dialogue Generation</a> Jiwei Li, Will Monroe, Dan Jurafsky </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/miller2017dialog/">Parlai: A Dialog Research Software Platform</a> Alexander H. Miller et al. </li>
     
   
     
       <li> <a href="/publications/wen2017latent/">Latent Intention Dialogue Models</a> Tsung-hsien Wen, Yishu Miao, Phil Blunsom, Steve Young </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2017sample/">Sample-efficient Actor-critic Reinforcement Learning With Supervised Data For Dialogue Management</a> Pei-hao Su, Pawel Budzianowski, Stefan Ultes, Milica Gasic, Steve Young </li>
     
   
     
       <li> <a href="/publications/wu2017neural/">Neural Response Generation With Dynamic Vocabularies</a> Yu Wu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/song2017unified/">A Unified Query-based Generative Model For Question Generation And Question Answering</a> Linfeng Song, Zhiguo Wang, Wael Hamza </li>
     
   
     
       <li> <a href="/publications/zhang2017asking/">Asking The Difficult Questions: Goal-oriented Visual Question Generation Via Intermediate Rewards</a> Junjie Zhang et al. </li>
     
   
     
       <li> <a href="/publications/serban2017deep/">A Deep Reinforcement Learning Chatbot</a> Iulian V. Serban et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2017reinforced/">R\(^3\): Reinforced Reader-ranker For Open-domain Question Answering</a> Shuohang Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2017flexible/">Flexible And Creative Chinese Poetry Generation Using Neural Memory</a> Jiyuan Zhang et al. </li>
     
   
     
       <li> <a href="/publications/li2017adversarial/">Adversarial Learning For Neural Dialogue Generation</a> Jiwei Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2017generating/">Mojitalk: Generating Emotional Responses At Scale</a> Xianda Zhou, William Yang Wang </li>
     
   
     
       <li> <a href="/publications/guu2017from/">From Language To Programs: Bridging Reinforcement Learning And Maximum Marginal Likelihood</a> Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2017best/">Best Of Both Worlds: Transferring Knowledge From Discriminative Learning To A Generative Visual Dialog Model</a> Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra </li>
     
   
     
       <li> <a href="/publications/fast2017conversational/">Iris: A Conversational Agent For Complex Tasks</a> Ethan Fast, Binbin Chen, Julia Mendelsohn, Jonathan Bassen, Michael Bernstein </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/rothe2017question/">Question Asking As Program Generation</a> Anselm Rothe, Brenden M. Lake, Todd M. Gureckis </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2017are/">Are You Talking To Me? Reasoned Visual Dialog Generation Through Adversarial Learning</a> Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, Anton Van Den Hengel </li>
     
   
     
       <li> <a href="/publications/gu2017non/">Non-autoregressive Neural Machine Translation</a> Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, Richard Socher </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shao2017generating/">Generating High-quality And Informative Conversation Responses With Sequence-to-sequence Models</a> Louis Shao et al. </li>
     
   
     
       <li> <a href="/publications/xie2017neural/">Neural Text Generation: A Practical Guide</a> Ziang Xie </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2018automating/">Automating Reading Comprehension By Generating Question And Answer Pairs</a> Vishwajeet Kumar, Kireeti Boorla, Yogesh Meena, Ganesh Ramakrishnan, Yuan-fang Li </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2018dialogue/">Dialogue Generation: From Imitation Learning To Inverse Reinforcement Learning</a> Ziming Li, Julia Kiseleva, Maarten De Rijke </li>
     
   
     
       <li> <a href="/publications/moghe2018towards/">Towards Exploiting Background Knowledge For Building Conversation Systems</a> Nikita Moghe, Siddhartha Arora, Suman Banerjee, Mitesh M. Khapra </li>
     
   
     
   
     
       <li> <a href="/publications/xu2018dp/">DP-GAN: Diversity-promoting Generative Adversarial Network For Generating Informative And Diversified Text</a> Jingjing Xu, Xuancheng Ren, Junyang Lin, Xu Sun </li>
     
   
     
       <li> <a href="/publications/fedus2018better/">Maskgan: Better Text Generation Via Filling In The______</a> William Fedus, Ian Goodfellow, Andrew M. Dai </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2018regularizing/">Regularizing Neural Machine Translation By Target-bidirectional Agreement</a> Zhirui Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chevalierboisvert2018platform/">Babyai: A Platform To Study The Sample Efficiency Of Grounded Language Learning</a> Maxime Chevalier-boisvert et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ilievski2018goal/">Goal-oriented Chatbot Dialog Management Bootstrapping With Transfer Learning</a> Vladimir Ilievski, Claudiu Musat, Andreea Hossmann, Michael Baeriswyl </li>
     
   
     
       <li> <a href="/publications/chen2018fast/">Fast Abstractive Summarization With Reinforce-selected Sentence Rewriting</a> Yen-chun Chen, Mohit Bansal </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2018d/">D-PAGE: Diverse Paraphrase Generation</a> Qiongkai Xu, Juyan Zhang, Lizhen Qu, Lexing Xie, Richard Nock </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2018putting/">Putting The Horse Before The Cart:a Generator-evaluator Framework For Question Generation From Text</a> Vishwajeet Kumar, Ganesh Ramakrishnan, Yuan-fang Li </li>
     
   
     
       <li> <a href="/publications/gehrmann2018end/">End-to-end Content And Plan Selection For Data-to-text Generation</a> Sebastian Gehrmann, Falcon Z. Dai, Henry Elder, Alexander M. Rush </li>
     
   
     
       <li> <a href="/publications/niu2018polite/">Polite Dialogue Generation Without Parallel Data</a> Tong Niu, Mohit Bansal </li>
     
   
     
   
     
       <li> <a href="/publications/li2018hybrid/">Hybrid Retrieval-generation Reinforced Agent For Medical Image Report Generation</a> Christy Y. Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing </li>
     
   
     
       <li> <a href="/publications/bunel2018leveraging/">Leveraging Grammar And Reinforcement Learning For Neural Program Synthesis</a> Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, Pushmeet Kohli </li>
     
   
     
       <li> <a href="/publications/tambwekar2018controllable/">Controllable Neural Story Plot Generation Via Reward Shaping</a> Pradyumna Tambwekar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/saha2018complex/">Complex Sequential Question Answering: Towards Learning To Converse Over Linked Question Answer Pairs With A Knowledge Graph</a> Amrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan, Sarath Chandar </li>
     
   
     
       <li> <a href="/publications/ott2018scaling/">Scaling Neural Machine Translation</a> Myle Ott, Sergey Edunov, David Grangier, Michael Auli </li>
     
   
     
   
     
       <li> <a href="/publications/nie2018operations/">Operations Guided Neural Networks For High Fidelity Data-to-text Generation</a> Feng Nie, Jinpeng Wang, Jin-ge Yao, Rong Pan, Chin-yew Lin </li>
     
   
     
       <li> <a href="/publications/wang2018task/">A Task In A Suit And A Tie: Paraphrase Generation With Semantic Augmentation</a> Su Wang, Rahul Gupta, Nancy Chang, Jason Baldridge </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2018guided/">Guided Feature Transformation (GFT): A Neural Language Grounding Module For Embodied Agents</a> Haonan Yu, Xiaochen Lian, Haichao Zhang, Wei Xu </li>
     
   
     
   
     
       <li> <a href="/publications/wang2018reinforced/">Reinforced Cross-modal Matching And Self-supervised Imitation Learning For Vision-language Navigation</a> Xin Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/venkatesh2018evaluating/">On Evaluating And Comparing Open Domain Dialog Systems</a> Anu Venkatesh et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chen2018best/">The Best Of Both Worlds: Combining Recent Advances In Neural Machine Translation</a> Mia Xu Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/coreyes2018guiding/">Guiding Policies With Language Via Meta-learning</a> John D. Co-reyes et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2018sentiment/">Sentiment Adaptive End-to-end Dialog Systems</a> Weiyan Shi, Zhou Yu </li>
     
   
     
       <li> <a href="/publications/phang2018sentence/">Sentence Encoders On Stilts: Supplementary Training On Intermediate Labeled-data Tasks</a> Jason Phang, Thibault F√©vry, Samuel R. Bowman </li>
     
   
     
       <li> <a href="/publications/tandon2018reasoning/">Reasoning About Actions And State Changes By Injecting Commonsense Knowledge</a> Niket Tandon et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2018toward/">Toward Diverse Text Generation With Inverse Reinforcement Learning</a> Zhan Shi, Xinchi Chen, Xipeng Qiu, Xuanjing Huang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hou2018sequence/">Sequence-to-sequence Data Augmentation For Dialogue Language Understanding</a> Yutai Hou, Yijia Liu, Wanxiang Che, Ting Liu </li>
     
   
     
       <li> <a href="/publications/das2018neural/">Neural Modular Control For Embodied Question Answering</a> Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra </li>
     
   
     
       <li> <a href="/publications/raghu2018disentangling/">Disentangling Language And Knowledge In Task-oriented Dialogs</a> Dinesh Raghu, Nikhil Gupta, Mausam </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2018neural/">A Neural Interlingua For Multilingual Machine Translation</a> Yichao Lu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2018context/">Context-aware Visual Policy Network For Sequence-level Image Captioning</a> Daqing Liu, Zheng-jun Zha, Hanwang Zhang, Yongdong Zhang, Feng Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2018vision/">Vision-based Navigation With Language-based Assistance Via Imitation Learning With Indirect Intervention</a> Khanh Nguyen, Debadeepta Dey, Chris Brockett, Bill Dolan </li>
     
   
     
       <li> <a href="/publications/kreyssig2018neural/">Neural User Simulation For Corpus-based Policy Optimisation For Spoken Dialogue Systems</a> Florian Kreyssig, Inigo Casanueva, Pawel Budzianowski, Milica Gasic </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fried2018speaker/">Speaker-follower Models For Vision-and-language Navigation</a> Daniel Fried et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xu2018towards/">Towards Explainable And Controllable Open Domain Dialogue Generation With Dialogue Acts</a> Can Xu, Wei Wu, Yu Wu </li>
     
   
     
   
     
       <li> <a href="/publications/ram2018conversational/">Conversational AI: The Science Behind The Alexa Prize</a> Ashwin Ram et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kreutzer2018can/">Can Neural Machine Translation Be Improved With User Feedback?</a> Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, Stefan Riezler </li>
     
   
     
   
     
       <li> <a href="/publications/c%C3%B4t%C3%A92018learning/">Textworld: A Learning Environment For Text-based Games</a> Marc-alexandre C√¥t√© et al. </li>
     
   
     
       <li> <a href="/publications/liu2018reinforcement/">Reinforcement Learning On Web Interfaces Using Workflow-guided Exploration</a> Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, Percy Liang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2018study/">A Study Of Reinforcement Learning For Neural Machine Translation</a> Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, Tie-yan Liu </li>
     
   
     
   
     
       <li> <a href="/publications/gu2018universal/">Universal Neural Machine Translation For Extremely Low Resource Languages</a> Jiatao Gu, Hany Hassan, Jacob Devlin, Victor O. K. Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kratzwald2019neural/">Rankqa: Neural Question Answering With Answer Re-ranking</a> Bernhard Kratzwald, Anna Eigenmann, Stefan Feuerriegel </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019reinforcement/">Reinforcement Learning Based Graph-to-sequence Model For Natural Question Generation</a> Yu Chen, Lingfei Wu, Mohammed J. Zaki </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2019evaluating/">Evaluating Commonsense In Pre-trained Language Models</a> Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2019entity/">Entity-consistent End-to-end Task-oriented Dialogue System With KB Retriever</a> Libo Qin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2019counterfactual/">Counterfactual Story Reasoning And Generation</a> Lianhui Qin et al. </li>
     
   
     
       <li> <a href="/publications/pan2019reinforced/">Reinforced Dynamic Reasoning For Conversational Question Generation</a> Boyuan Pan, Hao Li, Ziyu Yao, Deng Cai, Huan Sun </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cuay%C3%A1huitl2019ensemble/">Ensemble-based Deep Reinforcement Learning For Chatbots</a> Heriberto Cuay√°huitl et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2019conversing/">Conversing By Reading: Contentful Neural Conversation With On-demand Machine Reading</a> Lianhui Qin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019learning/">Learning To Generate Questions By Learning What Not To Generate</a> Bang Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2019multi/">12-in-1: Multi-task Vision And Language Representation Learning</a> Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2019from/">From Language To Goals: Inverse Reinforcement Learning For Vision-based Instruction Following</a> Justin Fu, Anoop Korattikara, Sergey Levine, Sergio Guadarrama </li>
     
   
     
   
     
       <li> <a href="/publications/zha2019context/">Context-aware Visual Policy Network For Fine-grained Image Captioning</a> Zheng-jun Zha, Daqing Liu, Hanwang Zhang, Yongdong Zhang, Feng Wu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2019long/">ELI5: Long Form Question Answering</a> Angela Fan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/klein2019learning/">Learning To Answer By Learning To Ask: Getting The Best Of GPT-2 And BERT Worlds</a> Tassilo Klein, Moin Nabi </li>
     
   
     
   
     
       <li> <a href="/publications/li2019reinforcement/">Reinforcement Learning Based Emotional Editing Constraint Conversation Generation</a> Jia Li, Xiao Sun, Xing Wei, Changliang Li, Jianhua Tao </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/goyal2019using/">Using Natural Language For Reward Shaping In Reinforcement Learning</a> Prasoon Goyal, Scott Niekum, Raymond J. Mooney </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nema2019ask/">Let's Ask Again: Refine Network For Automatic Question Generation</a> Preksha Nema, Akash Kumar Mohankumar, Mitesh M. Khapra, Balaji Vasan Srinivasan, Balaraman Ravindran </li>
     
   
     
       <li> <a href="/publications/li2019say/">Don't Say That! Making Inconsistent Dialogue Unlikely With Unlikelihood Training</a> Margaret Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thomason2019improving/">Improving Grounded Natural Language Understanding Through Human-robot Dialog</a> Jesse Thomason et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2019say/">Say What I Want: Towards The Dark Side Of Neural Dialogue Models</a> Haochen Liu, Tyler Derr, Zitao Liu, Jiliang Tang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/paxton2019interpretable/">Prospection: Interpretable Plans From Language By Predicting The Future</a> Chris Paxton, Yonatan Bisk, Jesse Thomason, Arunkumar Byravan, Dieter Fox </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/naseem2019rewarding/">Rewarding Smatch: Transition-based AMR Parsing With Reinforcement Learning</a> Tahira Naseem et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rajani2019explain/">Explain Yourself! Leveraging Language Models For Commonsense Reasoning</a> Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher </li>
     
   
     
   
     
       <li> <a href="/publications/schmidt2019generalization/">Generalization In Generation: A Closer Look At Exposure Bias</a> Florian Schmidt </li>
     
   
     
   
     
       <li> <a href="/publications/forbes2019do/">Do Neural Language Representations Learn Physical Commonsense?</a> Maxwell Forbes, Ari Holtzman, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bhagavatula2019abductive/">Abductive Commonsense Reasoning</a> Chandra Bhagavatula et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ott2019extensible/">Fairseq: A Fast, Extensible Toolkit For Sequence Modeling</a> Myle Ott et al. </li>
     
   
     
   
     
       <li> <a href="/publications/luo2019curiosity/">Curiosity-driven Reinforcement Learning For Diverse Visual Paragraph Generation</a> Yadan Luo et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2019unsupervised/">Unsupervised Question Answering By Cloze Translation</a> Patrick Lewis, Ludovic Denoyer, Sebastian Riedel </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2019insertion/">Insertion-based Decoding With Automatically Inferred Generation Order</a> Jiatao Gu, Qi Liu, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tian2019sticking/">Sticking To The Facts: Confident Decoding For Faithful Data-to-text Generation</a> Ran Tian, Shashi Narayan, Thibault Sellam, Ankur P. Parikh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/landi2019multimodal/">Multimodal Attention Networks For Low-level Vision-and-language Navigation</a> Federico Landi, Lorenzo Baraldi, Marcella Cornia, Massimiliano Corsini, Rita Cucchiara </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rajbhandari2019memory/">Zero: Memory Optimizations Toward Training Trillion Parameter Models</a> Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He </li>
     
   
     
   
     
       <li> <a href="/publications/kuchaiev2019toolkit/">Nemo: A Toolkit For Building AI Applications Using Neural Modules</a> Oleksii Kuchaiev et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ziegler2019fine/">Fine-tuning Language Models From Human Preferences</a> Daniel M. Ziegler et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shridhar2019benchmark/">ALFRED: A Benchmark For Interpreting Grounded Instructions For Everyday Tasks</a> Mohit Shridhar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shin2019generating/">Generating Empathetic Responses By Looking Ahead The User's Sentiment</a> Jamin Shin, Peng Xu, Andrea Madotto, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bae2019summary/">Summary Level Training Of Sentence Rewriting For Abstractive Summarization</a> Sanghwan Bae, Taeuk Kim, Jihoon Kim, Sang-goo Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chi2019just/">Just Ask:an Interactive Learning Framework For Vision And Language Navigation</a> Ta-chung Chi, Mihail Eric, Seokhwan Kim, Minmin Shen, Dilek Hakkani-tur </li>
     
   
     
       <li> <a href="/publications/jain2019stay/">Stay On The Path: Instruction Fidelity In Vision-and-language Navigation</a> Vihan Jain et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lee2019countering/">Countering Language Drift Via Visual Grounding</a> Jason Lee, Kyunghyun Cho, Douwe Kiela </li>
     
   
     
   
     
       <li> <a href="/publications/li2019multiresolution/">Empdg: Multiresolution Interactive Empathetic Dialogue Generation</a> Qintong Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vlasov2019dialogue/">Dialogue Transformers</a> Vladimir Vlasov, Johannes E. M. Mosig, Alan Nichol </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rastogi2019towards/">Towards Scalable Multi-domain Conversational Agents: The Schema-guided Dialogue Dataset</a> Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, Pranav Khaitan </li>
     
   
     
   
     
       <li> <a href="/publications/khot2019dataset/">QASC: A Dataset For Question Answering Via Sentence Composition</a> Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, Ashish Sabharwal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2019generating/">Generating Persona Consistent Dialogues By Exploiting Natural Language Inference</a> Haoyu Song, Wei-nan Zhang, Jingwen Hu, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ellis2019program/">Write, Execute, Assess: Program Synthesis With A REPL</a> Kevin Ellis et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2019language/">Language As An Abstraction For Hierarchical Deep Reinforcement Learning</a> Yiding Jiang, Shixiang Gu, Kevin Murphy, Chelsea Finn </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019incremental/">Incremental Transformer With Deliberation Decoder For Document Grounded Conversations</a> Zekang Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tuan2019benchmarking/">Dykgchat: Benchmarking Dialogue Generation Grounding On Dynamic Knowledge Graphs</a> Yi-lin Tuan, Yun-nung Chen, Hung-yi Lee </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/santhanam2019survey/">A Survey Of Natural Language Generation Techniques With A Focus On Dialogue Systems - Past, Present And Future Directions</a> Sashank Santhanam, Samira Shaikh </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kang2019recommendation/">Recommendation As A Communication Game: Self-supervised Bot-play For Goal-oriented Dialogue</a> Dongyeop Kang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2019pre/">PLATO: Pre-trained Dialogue Generation Model With Discrete Latent Variable</a> Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2019making/">Making History Matter: History-advantage Sequence Training For Visual Dialog</a> Tianhao Yang, Zheng-jun Zha, Hanwang Zhang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parisotto2019stabilizing/">Stabilizing Transformers For Reinforcement Learning</a> Emilio Parisotto et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019addressing/">Addressing Semantic Drift In Question Generation For Semi-supervised Question Answering</a> Shiyue Zhang, Mohit Bansal </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2019latent/">Latent Retrieval For Weakly Supervised Open Domain Question Answering</a> Kenton Lee, Ming-wei Chang, Kristina Toutanova </li>
     
   
     
   
     
       <li> <a href="/publications/liu2019attention/">Attention-informed Mixed-language Training For Zero-shot Cross-lingual Task-oriented Dialogue Systems</a> Zihan Liu, Genta Indra Winata, Zhaojiang Lin, Peng Xu, Pascale Fung </li>
     
   
     
   
     
       <li> <a href="/publications/chen2019multi/">Multi-hop Question Answering Via Reasoning Chains</a> Jifan Chen, Shih-ting Lin, Greg Durrett </li>
     
   
     
   
     
       <li> <a href="/publications/ziegler2019encoder/">Encoder-agnostic Adaptation For Conditional Language Generation</a> Zachary M. Ziegler, Luke Melas-kyriazi, Sebastian Gehrmann, Alexander M. Rush </li>
     
   
     
       <li> <a href="/publications/kumar2019reinforcement/">Reinforcement Learning Based Curriculum Optimization For Neural Machine Translation</a> Gaurav Kumar, George Foster, Colin Cherry, Maxim Krikun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/keskar2019conditional/">CTRL: A Conditional Transformer Language Model For Controllable Generation</a> Nitish Shirish Keskar, Bryan Mccann, Lav R. Varshney, Caiming Xiong, Richard Socher </li>
     
   
     
   
     
       <li> <a href="/publications/noever2020chess/">The Chess Transformer: Mastering Play Using Generative Language Models</a> David Noever, Matt Ciolino, Josh Kalin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meister2020if/">If Beam Search Is The Answer, What Was The Question?</a> Clara Meister, Tim Vieira, Ryan Cotterell </li>
     
   
     
       <li> <a href="/publications/shuster2020multi/">Multi-modal Open-domain Dialogue</a> Kurt Shuster, Eric Michael Smith, Da Ju, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020meaningful/">Meaningful Answer Generation Of E-commerce Question-answering</a> Shen Gao, Xiuying Chen, Zhaochun Ren, Dongyan Zhao, Rui Yan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/tafjord2020generating/">Proofwriter: Generating Implications, Proofs, And Abductive Statements Over Natural Language</a> Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2020curriculum/">CDL: Curriculum Dual Learning For Emotion-controllable Response Generation</a> Lei Shen, Yang Feng </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hill2020human/">Human Instruction-following With Deep Reinforcement Learning Via Transfer-learning From Text</a> Felix Hill, Sona Mokra, Nathaniel Wong, Tim Harley </li>
     
   
     
       <li> <a href="/publications/hill2020grounded/">Grounded Language Learning Fast And Slow</a> Felix Hill et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/petroni2020how/">How Context Affects Language Models' Factual Predictions</a> Fabio Petroni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pradeep2020scientific/">Scientific Claim Verification With VERT5ERINI</a> Ronak Pradeep, Xueguang Ma, Rodrigo Nogueira, Jimmy Lin </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/aky%C3%BCrek2020learning/">Learning To Recombine And Resample Data For Compositional Generalization</a> Ekin Aky√ºrek, Afra Feyza Aky√ºrek, Jacob Andreas </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pang2020text/">Text Generation By Learning From Demonstrations</a> Richard Yuanzhe Pang, He He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gontier2020measuring/">Measuring Systematic Generalization In Neural Proof Generation With Transformers</a> Nicolas Gontier, Koustuv Sinha, Siva Reddy, Christopher Pal </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2020x/">X-FACTR: Multilingual Factual Knowledge Retrieval From Pretrained Language Models</a> Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, Graham Neubig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020logical/">Logical Natural Language Generation From Open-domain Tables</a> Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2020going/">Babywalk: Going Farther In Vision-and-language Navigation By Taking Baby Steps</a> Wang Zhu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jeretic2020are/">Are Natural Language Inference Models Imppressive? Learning Implicature And Presupposition</a> Paloma Jeretic, Alex Warstadt, Suvrat Bhooshan, Adina Williams </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2020retrieval/">Retrieval-augmented Generation For Knowledge-intensive NLP Tasks</a> Patrick Lewis et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/verga2020facts/">Facts As Experts: Adaptable And Interpretable Neural Memory Over Symbolic Knowledge</a> Pat Verga, Haitian Sun, Livio Baldini Soares, William W. Cohen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2020detecting/">Detecting Hallucinated Content In Conditional Neural Sequence Generation</a> Chunting Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020paraphrase/">Paraphrase Augmented Task-oriented Dialog Generation</a> Silin Gao, Yichi Zhang, Zhijian Ou, Zhou Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2020megatron/">MEGATRON-CNTRL: Controllable Story Generation With External Knowledge Using Large-scale Language Models</a> Peng Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2020building/">SOLOIST: Building Task Bots At Scale With Transfer Learning And Machine Teaching</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/efrat2020turking/">The Turking Test: Can Language Models Understand Instructions?</a> Avia Efrat, Omer Levy </li>
     
   
     
       <li> <a href="/publications/paranjape2020neural/">Neural Generation Meets Real People: Towards Emotionally Engaging Mixed-initiative Conversations</a> Ashwin Paranjape et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2020reading/">DUMA: Reading Comprehension With Transposition Thinking</a> Pengfei Zhu, Hai Zhao, Xiaoguang Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tu2020empirical/">An Empirical Study On Robustness To Spurious Correlations Using Pre-trained Language Models</a> Lifu Tu, Garima Lalwani, Spandana Gella, He He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2020addressing/">Addressing Some Limitations Of Transformers With Feedback Memory</a> Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar </li>
     
   
     
   
     
       <li> <a href="/publications/fan2020beyond/">Beyond English-centric Multilingual Machine Translation</a> Angela Fan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020you/">You Impress Me: Dialogue Generation Via Mutual Persona Perception</a> Qian Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/talmor2020leap/">Leap-of-thought: Teaching Pre-trained Models To Systematically Reason Over Implicit Knowledge</a> Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/akakzia2020grounding/">Grounding Language To Autonomously-acquired Skills Via Goal Generation</a> Ahmed Akakzia, C√©dric Colas, Pierre-yves Oudeyer, Mohamed Chetouani, Olivier Sigaud </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nie2020contextualized/">Coregen: Contextualized Code Representation Learning For Commit Message Generation</a> Lun Yiu Nie et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nogueira2020document/">Document Ranking With A Pretrained Sequence-to-sequence Model</a> Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shridhar2020aligning/">Alfworld: Aligning Text And Embodied Environments For Interactive Learning</a> Mohit Shridhar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020data/">Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation</a> Ruibo Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020trading/">Trading Off Diversity And Quality In Natural Language Generation</a> Hugh Zhang, Daniel Duckworth, Daphne Ippolito, Arvind Neelakantan </li>
     
   
     
   
     
       <li> <a href="/publications/xu2020user/">User Memory Reasoning For Conversational Recommendation</a> Hu Xu et al. </li>
     
   
     
       <li> <a href="/publications/filippova2020controlled/">Controlled Hallucinations: Learning To Generate Faithfully From Noisy Data</a> Katja Filippova </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ji2020language/">Language Generation With Multi-hop Reasoning On Commonsense Knowledge Graph</a> Haozhe Ji et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020mitigating/">Mitigating Gender Bias For Neural Dialogue Generation With Adversarial Learning</a> Haochen Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lan2020novel/">PONE: A Novel Automatic Evaluation Metric For Open-domain Generative Dialogue Systems</a> Tian Lan, Xian-ling Mao, Wei Wei, Xiaoyan Gao, Heyan Huang </li>
     
   
     
   
     
       <li> <a href="/publications/guan2020knowledge/">A Knowledge-enhanced Pretraining Model For Commonsense Story Generation</a> Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020modelling/">Modelling Hierarchical Structure Between Dialogue Policy And Natural Language Generator With Option Framework For Task-oriented Dialogue System</a> Jianhong Wang, Yuan Zhang, Tae-kyun Kim, Yunjie Gu </li>
     
   
     
       <li> <a href="/publications/gao2020robust/">Robust Conversational AI With Grounded Text Generation</a> Jianfeng Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021one/">One Chatbot Per Person: Creating Personalized Chatbots Based On Implicit User Profiles</a> Zhengyi Ma, Zhicheng Dou, Yutao Zhu, Hanxun Zhong, Ji-rong Wen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/flemotomos2021automated/">Automated Quality Assessment Of Cognitive Behavioral Therapy Sessions Through Highly Contextualized Language Representations</a> Nikolaos Flemotomos et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rahimi2021explaining/">Explaining Documents' Relevance To Search Queries</a> Razieh Rahimi, Youngwoo Kim, Hamed Zamani, James Allan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021controllable/">Controllable Neural Dialogue Summarization With Personal Named Entity Planning</a> Zhengyuan Liu, Nancy F. Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021augmenting/">Augmenting Sequential Recommendation With Pseudo-prior Items Via Reversely Pre-training Transformer</a> Zhiwei Liu, Ziwei Fan, Yu Wang, Philip S. Yu </li>
     
   
     
   
     
       <li> <a href="/publications/nakano2021browser/">Webgpt: Browser-assisted Question-answering With Human Feedback</a> Reiichiro Nakano et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2021exploratory/">An Exploratory Study On Long Dialogue Summarization: What Works And What's Next</a> Yusen Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2021tr/">TR-BERT: Dynamic Token Reduction For Accelerating BERT Inference</a> Deming Ye, Yankai Lin, Yufei Huang, Maosong Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mees2021benchmark/">CALVIN: A Benchmark For Language-conditioned Policy Learning For Long-horizon Robot Manipulation Tasks</a> Oier Mees, Lukas Hermann, Erick Rosete-beas, Wolfram Burgard </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pascual2021plug/">A Plug-and-play Method For Controlled Text Generation</a> Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell, Roger Wattenhofer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021beyond/">Beyond Goldfish Memory: Long-term Open-domain Conversation</a> Jing Xu, Arthur Szlam, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2021pre/">Dialoglm: Pre-trained Model For Long Dialogue Understanding And Summarization</a> Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021fast/">Fastmoe: A Fast Mixture-of-expert Training System</a> Jiaao He et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021recursively/">Recursively Summarizing Books With Human Feedback</a> Jeff Wu et al. </li>
     
   
     
       <li> <a href="/publications/zhao2021effective/">Effective Sequence-to-sequence Dialogue State Tracking</a> Jeffrey Zhao, Mahdis Mahdieh, Ye Zhang, Yuan Cao, Yonghui Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/austin2021program/">Program Synthesis With Large Language Models</a> Jacob Austin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/rae2021scaling/">Scaling Language Models: Methods, Analysis & Insights From Training Gopher</a> Jack W. Rae et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lai2021thank/">Thank You BART! Rewarding Pre-trained Models Improves Formality Style Transfer</a> Huiyuan Lai, Antonio Toral, Malvina Nissim </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2021diagnosing/">Diagnosing Vision-and-language Navigation: What Really Matters</a> Wanrong Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021math/">Math Word Problem Generation With Mathematical Consistency And Problem Context Constraints</a> Zichao Wang, Andrew S. Lan, Richard G. Baraniuk </li>
     
   
     
       <li> <a href="/publications/wei2021emotion/">Emotion-aware Chat Machine: Automatic Emotional Response Generation For Human-like Emotional Interaction</a> Wei Wei et al. </li>
     
   
     
       <li> <a href="/publications/min2021learning/">Metaicl: Learning To Learn In Context</a> Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2021chinese/">Psyqa: A Chinese Dataset For Generating Long Counseling Text For Mental Health Support</a> Hao Sun, Zhenru Lin, Chujie Zheng, Siyang Liu, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021dialogue/">Dialogue History Matters! Personalized Response Selectionin Multi-turn Retrieval-based Chatbots</a> Juntao Li et al. </li>
     
   
     
       <li> <a href="/publications/liu2021topic/">Topic-aware Contrastive Learning For Abstractive Dialogue Summarization</a> Junpeng Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021scalable/">Scalable And Efficient Moe Training For Multitask Multilingual Models</a> Young Jin Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jang2021towards/">Towards Continual Knowledge Learning Of Language Models</a> Joel Jang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cobbe2021training/">Training Verifiers To Solve Math Word Problems</a> Karl Cobbe et al. </li>
     
   
     
       <li> <a href="/publications/du2021towards/">Towards Interpreting And Mitigating Shortcut Learning Behavior Of NLU Models</a> Mengnan Du et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/krishna2021hurdles/">Hurdles To Progress In Long-form Question Answering</a> Kalpesh Krishna, Aurko Roy, Mohit Iyyer </li>
     
   
     
   
     
       <li> <a href="/publications/su2021plan/">Plan-then-generate: Controlled Data-to-text Generation Via Planning</a> Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, Nigel Collier </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schmitt2021supporting/">Characterchat: Supporting The Creation Of Fictional Characters Through Conversation And Progressive Manifestation With A Chatbot</a> Oliver Schmitt, Daniel Buschek </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/buschek2021impact/">The Impact Of Multiple Parallel Phrase Suggestions On Email Input And Composition Behaviour Of Native And Non-native English Writers</a> Daniel Buschek, Martin Z√ºrn, Malin Eiband </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2021counterfactual/">Counterfactual Memorization In Neural Language Models</a> Chiyuan Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ren2021learning/">Learning To Ask Appropriate Questions In Conversational Recommendation</a> Xuhui Ren et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sharma2021towards/">Towards Facilitating Empathic Conversations In Online Mental Health Support: A Reinforcement Learning Approach</a> Ashish Sharma, Inna W. Lin, Adam S. Miner, David C. Atkins, Tim Althoff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021history/">History Aware Multimodal Transformer For Vision-and-language Navigation</a> Shizhe Chen, Pierre-louis Guhur, Cordelia Schmid, Ivan Laptev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salaberria2021image/">Image Captioning For Effective Use Of Language Models In Knowledge-based Visual Question Answering</a> Ander Salaberria, Gorka Azkune, Oier Lopez De Lacalle, Aitor Soroa, Eneko Agirre </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/askell2021general/">A General Language Assistant As A Laboratory For Alignment</a> Amanda Askell et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/suglia2021embodied/">Embodied BERT: A Transformer Model For Embodied, Language-guided Visual Task Completion</a> Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/manzoor2021towards/">Towards Retrieval-based Conversational Recommendation</a> Ahtsham Manzoor, Dietmar Jannach </li>
     
   
     
       <li> <a href="/publications/lelkes2021quiz/">Quiz-style Question Generation For News Stories</a> Adam D. Lelkes, Vinh Q. Tran, Cong Yu </li>
     
   
     
       <li> <a href="/publications/uchendu2021benchmark/">TURINGBENCH: A Benchmark Environment For Turing Test In The Age Of Neural Text Generation</a> Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, Dongwon Lee </li>
     
   
     
       <li> <a href="/publications/moudgil2021scene/">SOAT: A Scene- And Object-aware Transformer For Vision-and-language Navigation</a> Abhinav Moudgil, Arjun Majumdar, Harsh Agrawal, Stefan Lee, Dhruv Batra </li>
     
   
     
       <li> <a href="/publications/haviv2021learning/">Bertese: Learning To Speak To BERT</a> Adi Haviv, Jonathan Berant, Amir Globerson </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wiegreffe2021reframing/">Reframing Human-ai Collaboration For Generating Free-text Explanations</a> Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/barikeri2021real/">Redditbias: A Real-world Resource For Bias Evaluation And Debiasing Of Conversational Language Models</a> Soumya Barikeri, Anne Lauscher, Ivan Vuliƒá, Goran Glava≈° </li>
     
   
     
   
     
       <li> <a href="/publications/liu2021mitigating/">Mitigating Political Bias In Language Models Through Reinforced Calibration</a> Ruibo Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gaur2021information/">ISEEQ: Information Seeking Question Generation Using Dynamic Meta-information Retrieval And Knowledge Graphs</a> Manas Gaur, Kalpa Gunaratna, Vijay Srinivasan, Hongxia Jin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/levine2022standing/">Standing On The Shoulders Of Giant Frozen Language Models</a> Yoav Levine et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/valmeekam2022extensible/">Planbench: An Extensible Benchmark For Evaluating Large Language Models On Planning And Reasoning About Change</a> Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2022language/">Language Models Can See: Plugging Visual Controls In Text Generation</a> Yixuan Su et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2022multimodal/">Multimodal Knowledge Alignment With Reinforcement Learning</a> Youngjae Yu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lehman2022evolution/">Evolution Through Large Models</a> Joel Lehman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022unified/">Unified-io: A Unified Model For Vision, Language, And Multi-modal Tasks</a> Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022active/">Active Example Selection For In-context Learning</a> Yiming Zhang, Shi Feng, Chenhao Tan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pereira2022multi/">Visconde: Multi-document QA With GPT-3 And Neural Reranking</a> Jayr Pereira, Robson Fidalgo, Roberto Lotufo, Rodrigo Nogueira </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/menick2022teaching/">Teaching Language Models To Support Answers With Verified Quotes</a> Jacob Menick et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/andreas2022language/">Language Models As Agent Models</a> Jacob Andreas </li>
     
   
     
   
     
       <li> <a href="/publications/sekuli%C4%872022evaluating/">Evaluating Mixed-initiative Conversational Search Systems Via User Simulation</a> Ivan Sekuliƒá, Mohammad Aliannejadi, Fabio Crestani </li>
     
   
     
   
     
       <li> <a href="/publications/dasgupta2022language/">Language Models Show Human-like Content Effects On Reasoning Tasks</a> Ishita Dasgupta et al. </li>
     
   
     
       <li> <a href="/publications/singh2022generating/">Progprompt: Generating Situated Robot Task Plans Using Large Language Models</a> Ishika Singh et al. </li>
     
   
     
       <li> <a href="/publications/cui2022generative/">M6-rec: Generative Pretrained Language Models Are Open-ended Recommender Systems</a> Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/le2022mastering/">Coderl: Mastering Code Generation Through Pretrained Models And Deep Reinforcement Learning</a> Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/joshi2022repair/">Repair Is Nearly Generation: Multilingual Program Repair With Llms</a> Harshit Joshi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/trivedi2022interleaving/">Interleaving Retrieval With Chain-of-thought Reasoning For Knowledge-intensive Multi-step Questions</a> Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal </li>
     
   
     
       <li> <a href="/publications/zhong2022less/">Less Is More: Learning To Refine Dialogue History For Personalized Dialogue Generation</a> Hanxun Zhong, Zhicheng Dou, Yutao Zhu, Hongjin Qian, Ji-rong Wen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2022generate/">Generate Rather Than Retrieve: Large Language Models Are Strong Context Generators</a> Wenhao Yu et al. </li>
     
   
     
       <li> <a href="/publications/wang2022complementary/">Dualprompt: Complementary Prompting For Rehearsal-free Continual Learning</a> Zifeng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2022language/">Language Models Are Multilingual Chain-of-thought Reasoners</a> Freda Shi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/poesia2022reliable/">Synchromesh: Reliable Code Generation From Pre-trained Language Models</a> Gabriel Poesia et al. </li>
     
   
     
       <li> <a href="/publications/huang2022language/">Language Models As Zero-shot Planners: Extracting Actionable Knowledge For Embodied Agents</a> Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch </li>
     
   
     
       <li> <a href="/publications/christopoulou2022pangu/">Pangu-coder: Program Synthesis With Function-level Language Modeling</a> Fenia Christopoulou et al. </li>
     
   
     
       <li> <a href="/publications/huang2022inner/">Inner Monologue: Embodied Reasoning Through Planning With Language Models</a> Wenlong Huang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/moiseev2022structured/">SKILL: Structured Knowledge Infusion For Large Language Models</a> Fedor Moiseev, Zhe Dong, Enrique Alfonseca, Martin Jaggi </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/perez2022red/">Red Teaming Language Models With Language Models</a> Ethan Perez et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mitchell2022memory/">Memory-based Model Editing At Scale</a> Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, Chelsea Finn </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022exploring/">Convfinqa: Exploring The Chain Of Numerical Reasoning In Conversational Finance Question Answering</a> Zhiyu Chen et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022prototypical/">Protoclip: Prototypical Contrastive Language Image Pretraining</a> Delong Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2022least/">Least-to-most Prompting Enables Complex Reasoning In Large Language Models</a> Denny Zhou et al. </li>
     
   
     
       <li> <a href="/publications/gong2022future/">Future Transformer For Long-term Action Anticipation</a> Dayoung Gong, Joonseok Lee, Manjin Kim, Seong Jong Ha, Minsu Cho </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shah2022lm/">Lm-nav: Robotic Navigation With Large Pre-trained Models Of Language, Vision, And Action</a> Dhruv Shah, Blazej Osinski, Brian Ichter, Sergey Levine </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2022dialogue/">Dialfred: Dialogue-enabled Agents For Embodied Instruction Following</a> Xiaofeng Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2022why/">Why Can GPT Learn In-context? Language Models Implicitly Perform Gradient Descent As Meta-optimizers</a> Damai Dai et al. </li>
     
   
     
       <li> <a href="/publications/zhang2022graph/">Greaselm: Graph Reasoning Enhanced Language Models For Question Answering</a> Xikun Zhang et al. </li>
     
   
     
       <li> <a href="/publications/lynch2022interactive/">Interactive Language: Talking To Robots In Real Time</a> Corey Lynch et al. </li>
     
   
     
       <li> <a href="/publications/cheng2022binding/">Binding Language Models In Symbolic Languages</a> Zhoujun Cheng et al. </li>
     
   
     
       <li> <a href="/publications/burns2022discovering/">Discovering Latent Knowledge In Language Models Without Supervision</a> Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/colas2022language/">Language And Culture Internalisation For Human-like Autotelic AI</a> C√©dric Colas, Tristan Karch, Cl√©ment Moulin-frier, Pierre-yves Oudeyer </li>
     
   
     
       <li> <a href="/publications/lu2022controllable/">Quark: Controllable Text Generation With Reinforced Unlearning</a> Ximing Lu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xu2022long/">Long Time No See! Open-domain Conversation With Long-term Persona Memory</a> Xinchao Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2022proposal/">Don't Generate, Discriminate: A Proposal For Grounding Language Models To Real-world Environments</a> Yu Gu, Xiang Deng, Yu Su </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/oh2022why/">Why Does Surprisal From Larger Transformer-based Language Models Provide A Poorer Fit To Human Reading Times?</a> Byung-doh Oh, William Schuler </li>
     
   
     
   
     
       <li> <a href="/publications/an2022contrastive/">Cont: Contrastive Neural Text Generation</a> Chenxin An et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022competition/">Competition-level Code Generation With Alphacode</a> Yujia Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2022large/">GODEL: Large-scale Pre-training For Goal-directed Dialog</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bucker2022reshaping/">Reshaping Robot Trajectories Using Natural Language Commands: A Study Of Multi-modal Data Alignment Using Transformers</a> Arthur Bucker et al. </li>
     
   
     
   
     
       <li> <a href="/publications/creswell2022selection/">Selection-inference: Exploiting Large Language Models For Interpretable Logical Reasoning</a> Antonia Creswell, Murray Shanahan, Irina Higgins </li>
     
   
     
   
     
       <li> <a href="/publications/creswell2022faithful/">Faithful Reasoning Using Large Language Models</a> Antonia Creswell, Murray Shanahan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2022general/">VIMA: General Robot Manipulation With Multimodal Prompts</a> Yunfan Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qiao2022history/">HOP: History-and-order Aware Pre-training For Vision-and-language Navigation</a> Yanyuan Qiao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/tack2022ai/">The AI Teacher Test: Measuring The Pedagogical Ability Of Blender And GPT-3 In Educational Dialogues</a> Ana√Øs Tack, Chris Piech </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/glaese2022improving/">Improving Alignment Of Dialogue Agents Via Targeted Human Judgements</a> Amelia Glaese et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022mixture/">Adamix: Mixture-of-adaptations For Parameter-efficient Model Tuning</a> Yaqing Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2022language/">Language Models Are General-purpose Interfaces</a> Yaru Hao et al. </li>
     
   
     
       <li> <a href="/publications/hao2022optimizing/">Optimizing Prompts For Text-to-image Generation</a> Yaru Hao, Zewen Chi, Li Dong, Furu Wei </li>
     
   
     
   
     
       <li> <a href="/publications/mallen2022when/">When Not To Trust Language Models: Investigating Effectiveness Of Parametric And Non-parametric Memories</a> Alex Mallen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewkowycz2022solving/">Solving Quantitative Reasoning Problems With Language Models</a> Aitor Lewkowycz et al. </li>
     
   
     
       <li> <a href="/publications/kamath2022new/">A New Path: Scaling Vision-and-language Navigation With Synthetic Instructions And Imitation Learning</a> Aishwarya Kamath et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sarkar2022what/">What Is It Like To Program With Artificial Intelligence?</a> Advait Sarkar et al. </li>
     
   
     
   
     
       <li> <a href="/publications/srivastava2022beyond/">Beyond The Imitation Game: Quantifying And Extrapolating The Capabilities Of Language Models</a> Aarohi Shammie Srivastava et al. </li>
     
   
     
       <li> <a href="/publications/parisi2022tool/">TALM: Tool Augmented Language Models</a> Aaron Parisi, Yao Zhao, Noah Fiedel </li>
     
   
     
   
     
       <li> <a href="/publications/saparov2022language/">Language Models Are Greedy Reasoners: A Systematic Formal Analysis Of Chain-of-thought</a> Abulhair Saparov, He He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qiao2022reasoning/">Reasoning With Language Model Prompting: A Survey</a> Shuofei Qiao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/diao2022black/">Black-box Prompt Learning For Pre-trained Language Models</a> Shizhe Diao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/denny2022conversing/">Conversing With Copilot: Exploring Prompt Engineering For Solving CS1 Problems Using Natural Language</a> Paul Denny, Viraj Kumar, Nasser Giacaman </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022dynamic/">Dynamic Prompt Learning Via Policy Gradient For Semi-structured Mathematical Reasoning</a> Pan Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mees2022grounding/">Grounding Language With Visual Affordances Over Unstructured Data</a> Oier Mees, Jessica Borja-diaz, Wolfram Burgard </li>
     
   
     
   
     
       <li> <a href="/publications/mees2022what/">What Matters In Language Conditioned Robotic Imitation Learning Over Unstructured Data</a> Oier Mees, Lukas Hermann, Wolfram Burgard </li>
     
   
     
   
     
       <li> <a href="/publications/dziri2022origin/">On The Origin Of Hallucinations In Conversational Models: Is It The Datasets Or The Models?</a> Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, Siva Reddy </li>
     
   
     
       <li> <a href="/publications/dziri2022faithful/">Faithdial: A Faithful Benchmark For Information-seeking Dialogue</a> Nouha Dziri et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nllb2022no/">No Language Left Behind: Scaling Human-centered Machine Translation</a> Nllb Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2022optimizing/">Rlprompt: Optimizing Discrete Text Prompts With Reinforcement Learning</a> Mingkai Deng et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2022evaluating/">Evaluating Human-language Model Interaction</a> Mina Lee et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bommarito2022gpt/">GPT Takes The Bar Exam</a> Michael Ii Bommarito, Daniel Martin Katz </li>
     
   
     
   
     
       <li> <a href="/publications/ahn2022do/">Do As I Can, Not As I Say: Grounding Language In Robotic Affordances</a> Michael Ahn et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/tschannen2022image/">CLIPPO: Image-and-language Understanding From Pixels Only</a> Michael Tschannen, Basil Mustafa, Neil Houlsby </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2022empowering/">Empowering Language Models With Knowledge Graph Reasoning For Question Answering</a> Ziniu Hu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ivgi2022efficient/">Efficient Long-text Understanding With Short-text Models</a> Maor Ivgi, Uri Shaham, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schuster2022confident/">Confident Adaptive Language Modeling</a> Tal Schuster et al. </li>
     
   
     
       <li> <a href="/publications/sap2022neural/">Neural Theory-of-mind? On The Limits Of Social Intelligence In Large Lms</a> Maarten Sap, Ronan Lebras, Daniel Fried, Yejin Choi </li>
     
   
     
       <li> <a href="/publications/gao2022researching/">RARR: Researching And Revising What Language Models Say, Using Language Models</a> Luyu Gao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bonifacio2022data/">Inpars: Data Augmentation For Information Retrieval Using Large Language Models</a> Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Rodrigo Nogueira </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/srinivasan2022continual/">Climb: A Continual Learning Benchmark For Vision-and-language Tasks</a> Tejas Srinivasan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ouyang2022training/">Training Language Models To Follow Instructions With Human Feedback</a> Long Ouyang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2022fine/">FUM: Fine-grained And Fast User Modeling For News Recommendation</a> Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ruis2022goldilocks/">The Goldilocks Of Pragmatic Understanding: Fine-tuning Strategy Matters For Implicature Resolution By Llms</a> Laura Ruis et al. </li>
     
   
     
   
     
       <li> <a href="/publications/scialom2022fine/">Fine-tuned Language Models Are Continual Learners</a> Thomas Scialom, Tuhin Chakrabarty, Smaranda Muresan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2022robotic/">Robotic Skill Acquisition Via Instruction Augmentation With Vision-language Models</a> Ted Xiao et al. </li>
     
   
     
       <li> <a href="/publications/agrawal2022examples/">In-context Examples Selection For Machine Translation</a> Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, Marjan Ghazvininejad </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/taylor2022large/">Galactica: A Large Language Model For Science</a> Ross Taylor et al. </li>
     
   
     
       <li> <a href="/publications/zellers2022merlot/">MERLOT Reserve: Neural Script Knowledge Through Vision And Language And Sound</a> Rowan Zellers et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geng2022recommendation/">Recommendation As Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)</a> Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chu2022meta/">Meta Policy Learning For Cold-start Conversational Recommendation</a> Zhendong Chu, Hongning Wang, Yun Xiao, Bo Long, Lingfei Wu </li>
     
   
     
   
     
       <li> <a href="/publications/yao2022towards/">Webshop: Towards Scalable Real-world Web Interaction With Grounded Language Agents</a> Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan </li>
     
   
     
   
     
       <li> <a href="/publications/yao2022synergizing/">React: Synergizing Reasoning And Acting In Language Models</a> Shunyu Yao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ramamurthy2022is/">Is Reinforcement Learning (not) For Natural Language Processing: Benchmarks, Baselines, And Building Blocks For Natural Language Policy Optimization</a> Rajkumar Ramamurthy et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/trott2022do/">Do Large Language Models Know What Humans Know?</a> Sean Trott, Cameron Jones, Tyler Chang, James Michaelov, Benjamin Bergen </li>
     
   
     
   
     
       <li> <a href="/publications/shahriar2023have/">Let's Have A Chat! A Conversation With Chatgpt: Technology, Applications, And Limitations</a> Sakib Shahriar, Kadhim Hayawi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023secrets/">Secrets Of RLHF In Large Language Models Part I: PPO</a> Rui Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/anil2023palm/">Palm 2 Technical Report</a> Rohan Anil et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mandi2023dialectic/">Roco: Dialectic Multi-robot Collaboration With Large Language Models</a> Zhao Mandi, Shreeya Jain, Shuran Song </li>
     
   
     
       <li> <a href="/publications/chew2023llm/">Llm-assisted Content Analysis: Using Large Language Models To Support Deductive Coding</a> Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, Annice Kim </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pryzant2023automatic/">Automatic Prompt Optimization With "gradient Descent" And Beam Search</a> Reid Pryzant et al. </li>
     
   
     
   
     
       <li> <a href="/publications/omar2023chatgpt/">Chatgpt Versus Traditional Question Answering For Knowledge Graphs: Current Status And Future Directions Towards Knowledge Graph Chatbots</a> Reham Omar, Omij Mangukiya, Panos Kalnis, Essam Mansour </li>
     
   
     
       <li> <a href="/publications/khoury2023how/">How Secure Is Code Generated By Chatgpt?</a> Rapha√´l Khoury, Anderson R. Avila, Jacob Brunelle, Baba Mamadou Camara </li>
     
   
     
       <li> <a href="/publications/li2023may/">Starcoder: May The Source Be With You!</a> Raymond Li et al. </li>
     
   
     
       <li> <a href="/publications/schumann2023verbalization/">VELMA: Verbalization Embodiment Of LLM Agents For Vision And Language Navigation In Street View</a> Raphael Schumann et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023comparative/">A Comparative Study Of Open-source Large Language Models, GPT-4 And Claude 2: Multiple-choice Test Taking In Nephrology</a> Sean Wu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fleming2023clinician/">Medalign: A Clinician-generated Dataset For Instruction Following With Electronic Medical Records</a> Scott L. Fleming et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023next/">Next-gpt: Any-to-any Multimodal LLM</a> Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-seng Chua </li>
     
   
     
   
     
       <li> <a href="/publications/shen2023scaling/">Scaling Vision-language Models With Sparse Mixture Of Experts</a> Sheng Shen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mysore2023large/">Large Language Model Augmented Narrative Driven Recommendations</a> Sheshera Mysore, Andrew Mccallum, Hamed Zamani </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2023augmenting/">Toolkengpt: Augmenting Frozen Language Models With Massive Tools Via Tool Embeddings</a> Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu </li>
     
   
     
       <li> <a href="/publications/hao2023reasoning/">Reasoning With Language Model Is Planning With World Model</a> Shibo Hao et al. </li>
     
   
     
       <li> <a href="/publications/dai2023llm/">Llm-in-the-loop: Leveraging Large Language Model For Thematic Analysis</a> Shih-chieh Dai, Aiping Xiong, Lun-wei Ku </li>
     
   
     
   
     
       <li> <a href="/publications/moghaddam2023boosting/">Boosting Theory-of-mind Performance In Large Language Models Via Prompting</a> Shima Rahimi Moghaddam, Christopher J. Honey </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rafailov2023direct/">Direct Preference Optimization: Your Language Model Is Secretly A Reward Model</a> Rafael Rafailov et al. </li>
     
   
     
       <li> <a href="/publications/aiyappa2023can/">Can We Trust The Evaluation On Chatgpt?</a> Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-yeol Ahn </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023lawyer/">Lawyer Llama Technical Report</a> Quzhe Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023diversity/">Diversity-aware Meta Visual Prompting</a> Qidong Huang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023contrastive/">Medcpt: Contrastive Pre-trained Transformers With Large-scale Pubmed Search Logs For Zero-shot Biomedical Information Retrieval</a> Qiao Jin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023llama/">Llama-adapter V2: Parameter-efficient Visual Instruction Model</a> Peng Gao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cai2023do/">Do Large Language Models Resemble Humans In Language Use?</a> Zhenguang G. Cai, Xufeng Duan, David A. Haslett, Shuqi Wang, Martin J. Pickering </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/owoicho2023exploiting/">Exploiting Simulated User Feedback For Conversational Search: Ranking, Rewriting, And Beyond</a> Paul Owoicho, Ivan Sekuliƒá, Mohammad Aliannejadi, Jeffrey Dalton, Fabio Crestani </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/openai2023gpt/">GPT-4 Technical Report</a> Openai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ziems2023large/">Large Language Models Are Built-in Autoregressive Search Engines</a> Noah Ziems, Wenhao Yu, Zhihan Zhang, Meng Jiang </li>
     
   
     
       <li> <a href="/publications/palagin2023ontochatgpt/">Ontochatgpt Information System: Ontology-driven Structured Prompts For Chatgpt Meta-learning</a> Oleksandr Palagin, Vladislav Kaverinskiy, Anna Litvin, Kyrylo Malakhov </li>
     
   
     
       <li> <a href="/publications/bian2023chatgpt/">Chatgpt Is A Knowledgeable But Inexperienced Solver: An Investigation Of Commonsense Problem In Large Language Models</a> Ning Bian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shinn2023language/">Reflexion: Language Agents With Verbal Reinforcement Learning</a> Noah Shinn et al. </li>
     
   
     
       <li> <a href="/publications/robinson2023chatgpt/">Chatgpt MT: Competitive For High- (but Not Low-) Resource Languages</a> Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig </li>
     
   
     
       <li> <a href="/publications/gruver2023large/">Large Language Models Are Zero-shot Time Series Forecasters</a> Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/varshney2023stitch/">A Stitch In Time Saves Nine: Detecting And Mitigating Hallucinations Of Llms By Validating Low-confidence Generation</a> Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sharma2023towards/">Towards Understanding Sycophancy In Language Models</a> Mrinank Sharma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khan2023introducing/">Introducing Language Guidance In Prompt-based Continual Learning</a> Muhammad Gul Zain Ali Khan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kwon2023reward/">Reward Design With Language Models</a> Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh </li>
     
   
     
   
     
       <li> <a href="/publications/nasr2023scalable/">Scalable Extraction Of Training Data From (production) Language Models</a> Milad Nasr et al. </li>
     
   
     
       <li> <a href="/publications/poli2023hyena/">Hyena Hierarchy: Towards Larger Convolutional Language Models</a> Michael Poli et al. </li>
     
   
     
       <li> <a href="/publications/parker2023large/">A Large Language Model Approach To Educational Survey Feedback Analysis</a> Michael J. Parker, Caitlin Anderson, Claire Stone, Yearim Oh </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023chatgpt/">Chatgpt For Vulnerability Detection, Classification, And Repair: How Far Are We?</a> Michael Fu, Chakkrit Tantithamthavorn, Van Nguyen, Trung Le </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/laskar2023systematic/">A Systematic Study And Comprehensive Evaluation Of Chatgpt On Benchmark Datasets</a> Md Tahmid Rahman Laskar et al. </li>
     
   
     
       <li> <a href="/publications/khondaker2023comprehensive/">Gptaraeval: A Comprehensive Evaluation Of Chatgpt On Arabic NLP</a> Md Tawkat Islam Khondaker, Abdul Waheed, El Moatez Billah Nagoudi, Muhammad Abdul-mageed </li>
     
   
     
       <li> <a href="/publications/arefeen2023cost/">Leancontext: Cost-efficient Domain-specific Question Answering Using Llms</a> Md Adnan Arefeen, Biplob Debnath, Srimat Chakradhar </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phute2023llm/">LLM Self Defense: By Self Examination, Llms Know They Are Being Tricked</a> Mansi Phute et al. </li>
     
   
     
   
     
       <li> <a href="/publications/berglund2023reversal/">The Reversal Curse: Llms Trained On "A Is B" Fail To Learn "B Is A"</a> Lukas Berglund et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023document/">Document-level Machine Translation With Large Language Models</a> Longyue Wang et al. </li>
     
   
     
       <li> <a href="/publications/chen2023driving/">Driving With Llms: Fusing Object-level Vector Modality For Explainable Autonomous Driving</a> Long Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023comparing/">Comparing Sentence-level Suggestions To Message-level Suggestions In Ai-mediated Communication</a> Liye Fu, Benjamin Newman, Maurice Jakesch, Sarah Kreps </li>
     
   
     
   
     
       <li> <a href="/publications/wong2023from/">From Word Models To World Models: Translating From Natural Language To The Probabilistic Language Of Thought</a> Lionel Wong et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023reasoning/">Reasoning On Graphs: Faithful And Interpretable Large Language Model Reasoning</a> Linhao Luo, Yuan-fang Li, Gholamreza Haffari, Shirui Pan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/roest2023next/">Next-step Hint Generation For Introductory Programming Using Large Language Models</a> Lianne Roest, Hieke Keuning, Johan Jeuring </li>
     
   
     
       <li> <a href="/publications/guan2023leveraging/">Leveraging Pre-trained Large Language Models To Construct And Utilize World Models For Model-based Task Planning</a> Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati </li>
     
   
     
       <li> <a href="/publications/tunstall2023direct/">Zephyr: Direct Distillation Of LM Alignment</a> Lewis Tunstall et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023zero/">Zero-shot Next-item Recommendation Using Large Pretrained Language Models</a> Lei Wang, Ee-peng Lim </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mahowald2023dissociating/">Dissociating Language And Thought In Large Language Models</a> Kyle Mahowald et al. </li>
     
   
     
       <li> <a href="/publications/li2023comprehensive/">Mvbench: A Comprehensive Multi-modal Video Understanding Benchmark</a> Kunchang Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2023flexible/">Flexkbqa: A Flexible Llm-powered Framework For Few-shot Knowledge Base Question Answering</a> Zhenyu Li et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023human/">VISAR: A Human-ai Argumentative Writing Assistant With Visual Programming And Rapid Draft Prototyping</a> Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, Toby Jia-jun Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chang2023archaeology/">Speak, Memory: An Archaeology Of Books Known To Chatgpt/gpt-4</a> Kent K. Chang, Mackenzie Cramer, Sandeep Soni, David Bamman </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tian2023just/">Just Ask For Calibration: Strategies For Eliciting Calibrated Confidence Scores From Language Models Fine-tuned With Human Feedback</a> Katherine Tian et al. </li>
     
   
     
       <li> <a href="/publications/collins2023evaluating/">Evaluating Language Models For Mathematics Through Interactions</a> Katherine M. Collins et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kuckreja2023grounded/">Geochat: Grounded Large Vision-language Model For Remote Sensing</a> Kartik Kuckreja et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gopalakrishnan2023topical/">Topical-chat: Towards Knowledge-grounded Open-domain Conversations</a> Karthik Gopalakrishnan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/benharrak2023writer/">Writer-defined AI Personas For On-demand Feedback Generation</a> Karim Benharrak, Tim Zindulka, Florian Lehmann, Hendrik Heuer, Daniel Buschek </li>
     
   
     
       <li> <a href="/publications/li2023inference/">Inference-time Intervention: Eliciting Truthful Answers From A Language Model</a> Kenneth Li, Oam Patel, Fernanda Vi√©gas, Hanspeter Pfister, Martin Wattenberg </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hayawi2023imitation/">The Imitation Game: Detecting Human And Ai-generated Texts In The Era Of Chatgpt And BARD</a> Kadhim Hayawi, Sakib Shahriar, Sujith Samuel Mathew </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023ai/">Ai-augmented Surveys: Leveraging Large Language Models And Surveys For Opinion Prediction</a> Junsol Kim, Byungkyu Lee </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023recommendation/">Recommendation As Instruction Following: A Large Language Model Empowered Recommendation Approach</a> Junjie Zhang et al. </li>
     
   
     
       <li> <a href="/publications/ye2023comprehensive/">A Comprehensive Capability Analysis Of GPT-3 And GPT-3.5 Series Models</a> Junjie Ye et al. </li>
     
   
     
       <li> <a href="/publications/zhang2023collaborative/">Agentcf: Collaborative Learning With Autonomous Language Agents For Recommender Systems</a> Junjie Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hazell2023spear/">Spear Phishing With Large Language Models</a> Julian Hazell </li>
     
   
     
   
     
       <li> <a href="/publications/liu2023large/">Chatcounselor: A Large Language Models For Mental Health Support</a> June M. Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/schneider2023towards/">Towards Llm-based Autograding For Short Textual Answers</a> Johannes Schneider, Bernd Schenk, Christina Niklaus </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bai2023qwen/">Qwen Technical Report</a> Jinze Bai et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023general/">Structgpt: A General Framework For Large Language Model To Reason Over Structured Data</a> Jinhao Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023exploring/">Badgpt: Exploring Security Vulnerabilities Of Chatgpt Via Backdoor Attacks To Instructgpt</a> Jiawen Shi, Yixin Liu, Pan Zhou, Lichao Sun </li>
     
   
     
   
     
       <li> <a href="/publications/wu2023fake/">Fake News In Sheep's Clothing: Robust Fake News Detection Against Llm-empowered Style Attacks</a> Jiaying Wu, Jiafeng Guo, Bryan Hooi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xiang2023language/">Language Models Meet World Models: Embodied Experiences Enhance Language Models</a> Jiannan Xiang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023retrieval/">Rella: Retrieval-enhanced Large Language Models For Lifelong Sequential Behavior Comprehension In Recommendation</a> Jianghao Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shao2023enhancing/">Enhancing Retrieval-augmented Large Language Models With Iterative Retrieval-generation Synergy</a> Zhihong Shao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023physically/">Physically Grounded Vision-language Models For Robotic Manipulation</a> Jensen Gao et al. </li>
     
   
     
       <li> <a href="/publications/blocklove2023chip/">Chip-chat: Challenges And Opportunities In Conversational Hardware Design</a> Jason Blocklove, Siddharth Garg, Ramesh Karri, Hammond Pearce </li>
     
   
     
       <li> <a href="/publications/savelka2023thrilled/">Thrilled By Your Progress! Large Language Models (GPT-4) No Longer Struggle To Pass Assessments In Higher Education Programming Courses</a> Jaromir Savelka, Arav Agarwal, Marshall An, Chris Bogart, Majd Sakr </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/copet2023simple/">Simple And Controllable Music Generation</a> Jade Copet et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gou2023large/">CRITIC: Large Language Models Can Self-correct With Tool-interactive Critiquing</a> Zhibin Gou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023theory/">Theory Of Mind For Multi-agent Collaboration Via Large Language Models</a> Huao Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023building/">Building Cooperative Embodied Agents Modularly With Large Language Models</a> Hongxin Zhang et al. </li>
     
   
     
       <li> <a href="/publications/xiong2023fine/">Doctorglm: Fine-tuning Your Chinese Doctor Is Not A Herculean Task</a> Honglin Xiong et al. </li>
     
   
     
       <li> <a href="/publications/yang2023open/">Fingpt: Open-source Financial Large Language Models</a> Hongyang Yang, Xiao-yang Liu, Christina Dan Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nori2023capabilities/">Capabilities Of GPT-4 On Medical Challenge Problems</a> Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz </li>
     
   
     
   
     
       <li> <a href="/publications/peters2023large/">Large Language Models Can Infer Psychological Dispositions Of Social Media Users</a> Heinrich Peters, Sandra Matz </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023visual/">Visual Instruction Tuning</a> Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee </li>
     
   
     
   
     
       <li> <a href="/publications/wu2023chatgpt/">Chatgpt Or Grammarly? Evaluating Chatgpt On Grammatical Error Correction Benchmark</a> Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, Michael Lyu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wen2023llm/">Autodroid: Llm-powered Task Automation In Android</a> Hao Wen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kirk2023personalisation/">Personalisation Within Bounds: A Risk Taxonomy And Policy Framework For The Alignment Of Large Language Models With Personalised Feedback</a> Hannah Rose Kirk, Bertie Vidgen, Paul R√∂ttger, Scott A. Hale </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/faggioli2023perspectives/">Perspectives On Large Language Models For Relevance Judgment</a> Guglielmo Faggioli et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023open/">Voyager: An Open-ended Embodied Agent With Large Language Models</a> Guanzhi Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023fair/">"it's A Fair Game", Or Is It? Examining How Users Navigate Disclosure Risks And Benefits When Using Llm-based Conversational Agents</a> Zhiping Zhang et al. </li>
     
   
     
       <li> <a href="/publications/serapiogarc%C3%ADa2023personality/">Personality Traits In Large Language Models</a> Greg Serapio-garc√≠a et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023language/">Language Models Can Solve Computer Tasks</a> Geunwoo Kim, Pierre Baldi, Stephen Mcaleer </li>
     
   
     
       <li> <a href="/publications/sun2023aligning/">Aligning Large Multimodal Models With Factually Augmented RLHF</a> Zhiqing Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/delatorre2023real/">LLMR: Real-time Prompting Of Interactive Worlds Using Large Language Models</a> Fernanda De La Torre et al. </li>
     
   
     
   
     
       <li> <a href="/publications/song2023preference/">Preference Ranking Optimization For Human Alignment</a> Feifan Song et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shayegani2023survey/">Survey Of Vulnerabilities In Large Language Models Revealed By Adversarial Attacks</a> Erfan Shayegani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/almazrouei2023falcon/">The Falcon Series Of Open Language Models</a> Ebtesam Almazrouei et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kamalloo2023evaluating/">Evaluating Open-domain Question Answering In The Era Of Large Language Models</a> Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, Davood Rafiei </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mollo2023vector/">The Vector Grounding Problem</a> Dimitri Coelho Mollo, Rapha√´l Milli√®re </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023principle/">Principle-driven Self-alignment Of Language Models From Scratch With Minimal Human Supervision</a> Zhiqing Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhu2023minigpt/">Minigpt-4: Enhancing Vision-language Understanding With Advanced Large Language Models</a> Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023chain/">Chain Of Hindsight Aligns Language Models With Feedback</a> Hao Liu, Carmelo Sferrazza, Pieter Abbeel </li>
     
   
     
       <li> <a href="/publications/ganguli2023capacity/">The Capacity For Moral Self-correction In Large Language Models</a> Deep Ganguli et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/driess2023palm/">Palm-e: An Embodied Multimodal Language Model</a> Danny Driess et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zakka2023retrieval/">Almanac: Retrieval-augmented Language Models For Clinical Medicine</a> Cyril Zakka et al. </li>
     
   
     
   
     
       <li> <a href="/publications/burns2023weak/">Weak-to-strong Generalization: Eliciting Strong Capabilities With Weak Supervision</a> Collin Burns et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/small2023opportunities/">Opportunities And Risks Of Llms For Scalable Deliberation With Polis</a> Christopher T. Small et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sima2023driving/">Drivelm: Driving With Graph Visual Question Answering</a> Chonghao Sima et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023less/">LIMA: Less Is More For Alignment</a> Chunting Zhou et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023study/">A Study Of Generative Large Language Model For Medical Research And Healthcare</a> Cheng Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/packer2023towards/">Memgpt: Towards Llms As Operating Systems</a> Charles Packer et al. </li>
     
   
     
       <li> <a href="/publications/chen2023dipping/">Dipping Plms Sauce: Bridging Structure And Text For Effective Knowledge Graph Completion Via Conditional Soft Prompting</a> Chen Chen, Yufei Wang, Aixin Sun, Bing Li, Kwok-yan Lam </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2023drive/">Receive, Reason, And React: Drive As You Say With Large Language Models In Autonomous Vehicles</a> Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Ziran Wang </li>
     
   
     
       <li> <a href="/publications/jones2023does/">Does GPT-4 Pass The Turing Test?</a> Cameron R. Jones, Benjamin K. Bergen </li>
     
   
     
       <li> <a href="/publications/gulcehre2023reinforced/">Reinforced Self-training (rest) For Language Modeling</a> Caglar Gulcehre et al. </li>
     
   
     
       <li> <a href="/publications/ziems2023can/">Can Large Language Models Transform Computational Social Science?</a> Caleb Ziems et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023empowering/">LLM+P: Empowering Large Language Models With Optimal Planning Proficiency</a> Bo Liu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023generative/">Swiftsage: A Generative Agent With Fast And Slow Thinking For Complex Interactive Tasks</a> Bill Yuchen Lin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/paranjape2023automatic/">ART: Automatic Multi-step Reasoning And Tool-use For Large Language Models</a> Bhargavi Paranjape et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023instructing/">Expertprompting: Instructing Large Language Models To Be Distinguished Experts</a> Benfeng Xu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/rozi%C3%A8re2023code/">Code Llama: Open Foundation Models For Code</a> Baptiste Rozi√®re et al. </li>
     
   
     
       <li> <a href="/publications/peng2023instruction/">Instruction Tuning With GPT-4</a> Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023check/">Check Your Facts And Try Again: Improving Large Language Models With External Knowledge And Automated Feedback</a> Baolin Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sharma2023facilitating/">Facilitating Self-guided Mental Health Interventions Through Human-language Model Interaction: A Case Study Of Cognitive Restructuring</a> Ashish Sharma, Kevin Rushton, Inna Wanyin Lin, Theresa Nguyen, Tim Althoff </li>
     
   
     
       <li> <a href="/publications/hellas2023exploring/">Exploring The Responses Of Large Language Models To Beginner Programmers' Help Requests</a> Arto Hellas et al. </li>
     
   
     
       <li> <a href="/publications/mitra2023orca/">Orca 2: Teaching Small Language Models How To Reason</a> Arindam Mitra et al. </li>
     
   
     
       <li> <a href="/publications/gudibande2023false/">The False Promise Of Imitating Proprietary Llms</a> Arnav Gudibande et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023llm/">Expel: LLM Agents Are Experiential Learners</a> Andrew Zhao et al. </li>
     
   
     
       <li> <a href="/publications/gunjal2023detecting/">Detecting And Preventing Hallucinations In Large Vision Language Models</a> Anisha Gunjal, Jihan Yin, Erhan Bas </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/k%C3%B6pf2023openassistant/">Openassistant Conversations -- Democratizing Large Language Model Alignment</a> Andreas K√∂pf et al. </li>
     
   
     
       <li> <a href="/publications/liesenfeld2023opening/">Opening Up Chatgpt: Tracking Openness, Transparency, And Accountability In Instruction-tuned Text Generators</a> Andreas Liesenfeld, Alianda Lopez, Mark Dingemanse </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023generative/">On Generative Agents In Recommendation</a> An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, Tat-seng Chua </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bahrini2023threats/">Chatgpt: Applications, Opportunities, And Threats</a> Aram Bahrini et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/madaan2023self/">Self-refine: Iterative Refinement With Self-feedback</a> Aman Madaan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2023how/">Jailbroken: How Does LLM Safety Training Fail?</a> Alexander Wei, Nika Haghtalab, Jacob Steinhardt </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kalai2023calibrated/">Calibrated Language Models Must Hallucinate</a> Adam Tauman Kalai, Santosh S. Vempala </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023kosmos/">Kosmos-2: Grounding Multimodal Large Language Models To The World</a> Zhiliang Peng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023ghost/">Ghost In The Minecraft: Generally Capable Agents For Open-world Environments Via Large Language Models With Text-based Knowledge And Memory</a> Xizhou Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2023query/">Query Rewriting For Retrieval-augmented Large Language Models</a> Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan </li>
     
   
     
       <li> <a href="/publications/lai2023reasoning/">LISA: Reasoning Segmentation Via Large Language Model</a> Xin Lai et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023pmc/">PMC-VQA: Visual Instruction Tuning For Medical Visual Question Answering</a> Xiaoman Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wu2023unveiling/">Unveiling Security, Privacy, And Ethical Concerns Of Chatgpt</a> Xiaodong Wu, Ran Duan, Jianbing Ni </li>
     
   
     
   
     
       <li> <a href="/publications/qi2023fine/">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</a> Xiangyu Qi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2023large/">Medagents: Large Language Models As Collaborators For Zero-shot Medical Reasoning</a> Xiangru Tang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2023can/">Can Large Language Models Provide Useful Feedback On Research Papers? A Large-scale Empirical Analysis</a> Weixin Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023trusting/">Trusting Your Evidence: Hallucinate Less With Context-aware Decoding</a> Weijia Shi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sun2023is/">Is Chatgpt Good At Search? Investigating Large Language Models As Re-ranking Agents</a> Weiwei Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wei2023large/">Llmrec: Large Language Models With Graph Augmentation For Recommendation</a> Wei Wei et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhong2023enhancing/">Memorybank: Enhancing Large Language Models With Long-term Memory</a> Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang </li>
     
   
     
       <li> <a href="/publications/kang2023do/">Do Llms Understand User Preferences? Evaluating Llms On User Rating Prediction</a> Wang-cheng Kang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/adlakha2023evaluating/">Evaluating Correctness And Faithfulness Of Instruction-following Models For Question Answering</a> Vaibhav Adlakha, Parishad Behnamghader, Xing Han Lu, Nicholas Meade, Siva Reddy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phung2023generative/">Generative AI For Programming Education: Benchmarking Chatgpt, GPT-4, And Human Tutors</a> Tung Phung et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rebedea2023nemo/">Nemo Guardrails: A Toolkit For Controllable And Safe LLM Applications With Programmable Rails</a> Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, Jonathan Cohen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/korbak2023pretraining/">Pretraining Language Models With Human Preferences</a> Tomasz Korbak et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lai2023psy/">Psy-llm: Scaling Up Global Mental Health Psychological Services With Ai-based Large Language Models</a> Tin Lai et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023rlhf/">RLHF-V: Towards Trustworthy Mllms Via Behavior Alignment From Fine-grained Correctional Human Feedback</a> Tianyu Yu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023enabling/">Enabling Large Language Models To Generate Text With Citations</a> Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen </li>
     
   
     
   
     
       <li> <a href="/publications/schick2023language/">Toolformer: Language Models Can Teach Themselves To Use Tools</a> Timo Schick et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liang2023encouraging/">Encouraging Divergent Thinking In Large Language Models Through Multi-agent Debate</a> Tian Liang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/hagendorff2023deception/">Deception Abilities Emerged In Large Language Models</a> Thilo Hagendorff </li>
     
   
     
       <li> <a href="/publications/carta2023grounding/">Grounding Large Language Models In Interactive Environments With Online Reinforcement Learning</a> Thomas Carta et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bubeck2023sparks/">Sparks Of Artificial General Intelligence: Early Experiments With GPT-4</a> S√©bastien Bubeck et al. </li>
     
   
     
       <li> <a href="/publications/mirchandani2023large/">Large Language Models As General Pattern Machines</a> Suvir Mirchandani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2023enhancing/">Enhancing Job Recommendation Through Llm-based Generative Adversarial Networks</a> Yingpeng Du et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2023can/">Can Chatgpt Replace Traditional KBQA Models? An In-depth Analysis Of The Question Answering Performance Of The GPT LLM Family</a> Yiming Tan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023summary/">Summary Of Chatgpt-related Research And Perspective Towards The Future Of Large Language Models</a> Yiheng Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chia2023towards/">INSTRUCTEVAL: Towards Holistic Evaluation Of Instruction-tuned Large Language Models</a> Yew Ken Chia, Pengfei Hong, Lidong Bing, Soujanya Poria </li>
     
   
     
   
     
       <li> <a href="/publications/yang2023human/">Human-centric Autonomous Systems With Llms For User Command Reasoning</a> Yi Yang et al. </li>
     
   
     
       <li> <a href="/publications/bang2023multimodal/">A Multitask, Multilingual, Multimodal Evaluation Of Chatgpt On Reasoning, Hallucination, And Interactivity</a> Yejin Bang et al. </li>
     
   
     
       <li> <a href="/publications/ma2023language/">LIV: Language-image Representations And Rewards For Robotic Control</a> Yecheng Jason Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023specializing/">Specializing Smaller Language Models Towards Multi-step Reasoning</a> Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023llama/">Llama-vid: An Image Is Worth 2 Tokens In Large Language Models</a> Yanwei Li, Chengyao Wang, Jiaya Jia </li>
     
   
     
   
     
       <li> <a href="/publications/dubois2023simulation/">Alpacafarm: A Simulation Framework For Methods That Learn From Human Feedback</a> Yann Dubois et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023trustworthy/">Trustworthy Llms: A Survey And Guideline For Evaluating Large Language Models' Alignment</a> Yang Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2023integrating/">Integrating Action Knowledge And Llms For Task Planning And Situation Handling In Open Worlds</a> Yan Ding et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023chat/">Chat With The Environment: Interactive Multimodal Perception Using Large Language Models</a> Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023emotional/">Emotional Intelligence Of Large Language Models</a> Xuena Wang, Xueting Li, Zi Yin, Yue Wu, Liu Jia </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dan2023large/">Educhat: A Large-scale Language Model-based Chatbot System For Intelligent Education</a> Yuhao Dan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhuang2023dataset/">Toolqa: A Dataset For LLM Question Answering With External Tools</a> Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, Chao Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wolf2023fundamental/">Fundamental Limitations Of Alignment In Large Language Models</a> Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023medical/">Chatdoctor: A Medical Chat Model Fine-tuned On A Large Language Model Meta-ai (llama) Using Medical Domain Knowledge</a> Yunxiang Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xi2023towards/">Towards Open-world Recommendation With Knowledge Augmentation From Large Language Models</a> Yunjia Xi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sun2023retentive/">Retentive Network: A Successor To Transformer For Large Language Models</a> Yutao Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gao2023chat/">Chat-rec: Towards Interactive And Explainable Llms-augmented Recommender System</a> Yunfan Gao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qin2023facilitating/">Toolllm: Facilitating Large Language Models To Master 16000+ Real-world Apis</a> Yujia Qin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/du2023guiding/">Guiding Pretraining In Reinforcement Learning With Large Language Models</a> Yuqing Du et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2023editing/">Editing Large Language Models: Problems, Methods, And Opportunities</a> Yunzhi Yao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ren2023representation/">Representation Learning With Large Language Models For Recommendation</a> Xubin Ren et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/herbold2023write/">AI, Write An Essay For Me: A Large-scale Comparison Of Human-written Versus Chatgpt-generated Essays</a> Steffen Herbold, Annette Hautli-janisz, Ute Heuer, Zlata Kikteva, Alexander Trautsch </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2023enhancing/">Zhongjing: Enhancing The Chinese Medical Capabilities Of Large Language Model Through Expert Feedback And Real-world Multi-turn Dialogue</a> Songhua Yang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huo2023retrieving/">Retrieving Supporting Evidence For Generative Question Answering</a> Siqing Huo, Negar Arabzadeh, Charles L. A. Clarke </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dhingra2023mind/">Mind Meets Machine: Unravelling Gpt-4's Cognitive Psychology</a> Sifatkaur Dhingra, Manmeet Singh, Vaisakh Sb, Neetiraj Malviya, Sukhpal Singh Gill </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yin2023survey/">A Survey On Multimodal Large Language Models</a> Shukang Yin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023plan/">Describe, Explain, Plan And Select: Interactive Planning With Large Language Models Enables Open-world Multi-task Agents</a> Zihao Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023is/">Is Chatgpt A Good Sentiment Analyzer? A Preliminary Study</a> Zengzhi Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/he2023large/">Large Language Models As Zero-shot Conversational Recommenders</a> Zhankui He et al. </li>
     
   
     
       <li> <a href="/publications/wu2023fine/">Fine-grained Human Feedback Gives Better Rewards For Language Model Training</a> Zeqiu Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2023copiloting/">Copiloting The Copilots: Fusing Large Language Models With Completion Engines For Automated Program Repair</a> Yuxiang Wei, Chunqiu Steven Xia, Lingming Zhang </li>
     
   
     
       <li> <a href="/publications/li2023guiding/">Guiding Large Language Models Via Directional Stimulus Prompting</a> Zekun Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023chatgpt/">Chatgpt As A Factual Inconsistency Evaluator For Text Summarization</a> Zheheng Luo, Qianqian Xie, Sophia Ananiadou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bassner2024ai/">Iris: An Ai-driven Virtual Tutor For Computer Science Education</a> Patrick Bassner, Eduard Frankford, Stephan Krusche </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lieber2024hybrid/">Jamba: A Hybrid Transformer-mamba Language Model</a> Opher Lieber et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ramos2024review/">A Review Of Large Language Models And Autonomous Agents In Chemistry</a> Mayk Caldas Ramos, Christopher J. Collison, Andrew D. White </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gekhman2024does/">Does Fine-tuning Llms On New Knowledge Encourage Hallucinations?</a> Zorik Gekhman et al. </li>
     
   
     
       <li> <a href="/publications/saab2024capabilities/">Capabilities Of Gemini Models In Medicine</a> Khaled Saab et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ha2024understanding/">Clochat: Understanding How People Customize, Interact, And Experience Personas In Large Language Models</a> Juhye Ha, Hyeon Jeon, Daeun Han, Jinwook Seo, Changhoon Oh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geminiteam2024gemini/">Gemini 1.5: Unlocking Multimodal Understanding Across Millions Of Tokens Of Context</a> Gemini Team et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gemmateam2024open/">Gemma: Open Models Based On Gemini Research And Technology</a> Gemma Team et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jo2024understanding/">Understanding The Impact Of Long-term Memory On Self-disclosure With Large Language Model-driven Chatbots For Public Health Intervention</a> Eunkyung Jo, Yuin Jeong, Sohyun Park, Daniel A. Epstein, Young-ho Kim </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deepseekai2024deepseek/">Deepseek-v2: A Strong, Economical, And Efficient Mixture-of-experts Language Model</a> Deepseek-ai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/anderson2024homogenization/">Homogenization Effects Of Large Language Models On Human Creative Ideation</a> Barrett R. Anderson, Jash Hemant Shah, Max Kreminski </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salemi2024optimization/">Optimization Methods For Personalizing Large Language Models Through Retrieval Augmentation</a> Alireza Salemi, Surya Kallumadi, Hamed Zamani </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2024autonomous/">Autocoderover: Autonomous Program Improvement</a> Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, Abhik Roychoudhury </li>
     
   
     
       <li> <a href="/publications/cao2024survey/">Survey On Large Language Model-enhanced Reinforcement Learning: Concept, Taxonomy, And Methods</a> Yuji Cao et al. </li>
     
   
     
       <li> <a href="/publications/huang2024trustworthiness/">Trustllm: Trustworthiness In Large Language Models</a> Yue Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deldjoo2024review/">A Review Of Modern Recommender Systems Using Generative Models (gen-recsys)</a> Yashar Deldjoo et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2024how/">How Johnny Can Persuade Llms To Jailbreak Them: Rethinking Persuasion To Challenge AI Safety By Humanizing Llms</a> Yi Zeng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2024assessing/">Assessing AI Detectors In Identifying Ai-generated Code: Implications For Education</a> Wei Hung Pan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/parthasarathy2024ultimate/">The Ultimate Guide To Fine-tuning Llms From Basics To Breakthroughs: An Exhaustive Review Of Technologies, Research, Best Practices, Applied Research Challenges And Opportunities</a> Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tu2024towards/">Towards Conversational Diagnostic AI</a> Tao Tu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shankar2024who/">Who Validates The Validators? Aligning Llm-assisted Evaluation Of LLM Outputs With Human Preferences</a> Shreya Shankar, J. D. Zamfirescu-pereira, Bj√∂rn Hartmann, Aditya G. Parameswaran, Ian Arawjo </li>
     
   
     
   
     
       <li> <a href="/publications/zheng2024harnessing/">Harnessing Large Language Models For Text-rich Sequential Recommendation</a> Zhi Zheng, Wenshuo Chao, Zhaopeng Qiu, Hengshu Zhu, Hui Xiong </li>
     
   
     
       <li> <a href="/publications/tan2024large/">Large Language Models For Data Annotation And Synthesis: A Survey</a> Zhen Tan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alaofi2025can/">Can Generative Llms Create Query Variants For Test Collections? An Exploratory Study</a> Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, Paul Thomas </li>
     
   
     
       <li> <a href="/publications/deepseekai2025deepseek/">Deepseek-r1: Incentivizing Reasoning Capability In Llms Via Reinforcement Learning</a> Deepseek-ai et al. </li>
     
   
     
   
   </ul>

   <h3>üè∑ Responsible AI <a id="Responsible AI"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dinan2019build/">Build It Break It Fix It For Dialogue Safety: Robustness From Adversarial Human Attack</a> Emily Dinan, Samuel Humeau, Bharath Chintagunta, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2022when/">When To Make Exceptions: Exploring Language Models As Accounts Of Human Moral Judgment</a> Zhijing Jin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023safety/">Safety Assessment Of Chinese Large Language Models</a> Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kirk2023personalisation/">Personalisation Within Bounds: A Risk Taxonomy And Policy Framework For The Alignment Of Large Language Models With Personalised Feedback</a> Hannah Rose Kirk, Bertie Vidgen, Paul R√∂ttger, Scott A. Hale </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2024quality/">Quality Of Answers Of Generative Large Language Models Vs Peer Patients For Interpreting Lab Test Results For Lay Patients: Evaluation Study</a> Zhe He et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Scaling Laws <a id="Scaling Laws"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kaplan2020scaling/">Scaling Laws For Neural Language Models</a> Jared Kaplan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2021explanation/">An Explanation Of In-context Learning As Implicit Bayesian Inference</a> Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geiping2022training/">Cramming: Training A Language Model On A Single GPU In One Day</a> Jonas Geiping, Tom Goldstein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wei2022emergent/">Emergent Abilities Of Large Language Models</a> Jason Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alabdulmohsin2022revisiting/">Revisiting Neural Scaling Laws In Language And Vision</a> Ibrahim Alabdulmohsin, Behnam Neyshabur, Xiaohua Zhai </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hernandez2022scaling/">Scaling Laws And Interpretability Of Learning From Repeated Data</a> Danny Hernandez et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cherti2022reproducible/">Reproducible Scaling Laws For Contrastive Language-image Learning</a> Mehdi Cherti et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kojima2022large/">Large Language Models Are Zero-shot Reasoners</a> Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thoppilan2022language/">Lamda: Language Models For Dialog Applications</a> Romal Thoppilan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/antonello2023scaling/">Scaling Laws For Language Encoding Models In Fmri</a> Richard Antonello, Aditya Vaidya, Alexander G. Huth </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023explicit/">Navgpt: Explicit Reasoning In Vision-and-language Navigation With Large Language Models</a> Gengze Zhou, Yicong Hong, Qi Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nijkamp2023lessons/">Codegen2: Lessons For Training Llms On Programming And Natural Languages</a> Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xin2023survey/">A Survey Of Large Language Models</a> Wayne Xin Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/biderman2023emergent/">Emergent And Predictable Memorization In Large Language Models</a> Stella Biderman et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gholami2024ai/">AI And Memory Wall</a> Amir Gholami et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Security <a id="Security"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2017adversarial/">Adversarial Learning For Neural Dialogue Generation</a> Jiwei Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/minervini2018adversarially/">Adversarially Regularising Neural NLI Models To Integrate Logical Background Knowledge</a> Pasquale Minervini, Sebastian Riedel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tevet2018evaluating/">Evaluating Text Gans As Language Models</a> Guy Tevet, Gavriel Habib, Vered Shwartz, Jonathan Berant </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2018retrieval/">Retrieval-enhanced Adversarial Training For Neural Response Generation</a> Qingfu Zhu, Lei Cui, Weinan Zhang, Furu Wei, Ting Liu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hu2018attention/">Attention-guided Answer Distillation For Machine Reading Comprehension</a> Minghao Hu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/niu2018adversarial/">Adversarial Over-sensitivity And Over-stability Strategies For Dialogue Models</a> Tong Niu, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2018generating/">Generating Informative And Diverse Conversational Responses Via Adversarial Information Maximization</a> Yizhe Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2019robust/">Robust Neural Machine Translation With Doubly Adversarial Inputs</a> Yong Cheng, Lu Jiang, Wolfgang Macherey </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019say/">Say What I Want: Towards The Dark Side Of Neural Dialogue Models</a> Haochen Liu, Tyler Derr, Zitao Liu, Jiliang Tang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/si2019what/">What Does BERT Learn From Multiple-choice Reading Comprehension Datasets?</a> Chenglei Si, Shuohang Wang, Min-yen Kan, Jing Jiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2019enhanced/">Freelb: Enhanced Adversarial Training For Natural Language Understanding</a> Chen Zhu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/niven2019probing/">Probing Neural Network Comprehension Of Natural Language Arguments</a> Timothy Niven, Hung-yu Kao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sakaguchi2019adversarial/">Winogrande: An Adversarial Winograd Schema Challenge At Scale</a> Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clinchant2019use/">On The Use Of BERT For Neural Machine Translation</a> St√©phane Clinchant, Kweon Woo Jung, Vassilina Nikoulina </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zellers2019can/">Hellaswag: Can A Machine Really Finish Your Sentence?</a> Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi </li>
     
   
     
   
     
       <li> <a href="/publications/kong2019adversarial/">An Adversarial Approach To High-quality, Sentiment-controlled Neural Dialogue Generation</a> Xiang Kong, Bohan Li, Graham Neubig, Eduard Hovy, Yiming Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dinan2019build/">Build It Break It Fix It For Dialogue Safety: Robustness From Adversarial Human Attack</a> Emily Dinan, Samuel Humeau, Bharath Chintagunta, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wallace2019universal/">Universal Adversarial Triggers For Attacking And Analyzing NLP</a> Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hendrycks2020pretrained/">Pretrained Transformers Improve Out-of-distribution Robustness</a> Dan Hendrycks et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jones2020robust/">Robust Encodings: A Framework For Combating Adversarial Typos</a> Erik Jones, Robin Jia, Aditi Raghunathan, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2020generative/">Generative Data Augmentation For Commonsense Reasoning</a> Yiben Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020contextualized/">Contextualized Perturbation For Textual Adversarial Attack</a> Dianqi Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020adversarial/">Adversarial Training For Large Neural Language Models</a> Xiaodong Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2020character/">Charbert: Character-aware Pre-trained Language Model</a> Wentao Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/si2020better/">Better Robustness By More Coverage: Adversarial Training With Mixup Augmentation For Robust Fine-tuning</a> Chenglei Si et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wiegreffe2020measuring/">Measuring Association Between Labels And Free-text Rationales</a> Sarah Wiegreffe, Ana Marasoviƒá, Noah A. Smith </li>
     
   
     
       <li> <a href="/publications/zhang2020trojaning/">Trojaning Language Models For Fun And Profit</a> Xinyang Zhang, Zheng Zhang, Shouling Ji, Ting Wang </li>
     
   
     
   
     
       <li> <a href="/publications/cheng2020robust/">Advaug: Robust Adversarial Augmentation For Neural Machine Translation</a> Yong Cheng, Lu Jiang, Wolfgang Macherey, Jacob Eisenstein </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bartolo2020beat/">Beat The AI: Investigating Adversarial Human Annotation For Reading Comprehension</a> Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, Pontus Stenetorp </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jain2020contrastive/">Contrastive Code Representation Learning</a> Paras Jain et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020closer/">A Closer Look At The Robustness Of Vision-and-language Pre-trained Models</a> Linjie Li, Zhe Gan, Jingjing Liu </li>
     
   
     
       <li> <a href="/publications/tu2020empirical/">An Empirical Study On Robustness To Spurious Correlations Using Pre-trained Language Models</a> Lifu Tu, Garima Lalwani, Spandana Gella, He He </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qu2020contrast/">Coda: Contrast-enhanced And Diversity-promoting Data Augmentation For Natural Language Understanding</a> Yanru Qu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2020contrastive/">Contrastive Learning With Adversarial Perturbations For Conditional Text Generation</a> Seanie Lee, Dong Bok Lee, Sung Ju Hwang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/min2020syntactic/">Syntactic Data Augmentation Increases Robustness To Inference Heuristics</a> Junghyun Min, R. Thomas Mccoy, Dipanjan Das, Emily Pitler, Tal Linzen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020mitigating/">Mitigating Gender Bias For Neural Dialogue Generation With Adversarial Learning</a> Haochen Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/penha2021evaluating/">Evaluating The Robustness Of Retrieval Pipelines With Query Variation Generators</a> Gustavo Penha, Arthur C√¢mara, Claudia Hauff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021evaluation/">Dynaboard: An Evaluation-as-a-service Platform For Holistic Next-generation Benchmarking</a> Zhiyi Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021using/">Using Adversarial Attacks To Reveal The Statistical Bias In Machine Reading Comprehension Models</a> Jieyu Lin, Jiajie Zou, Nai Ding </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moradi2021evaluating/">Evaluating The Robustness Of Neural Language Models To Input Perturbations</a> Milad Moradi, Matthias Samwald </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hambardzumyan2021word/">WARP: Word-level Adversarial Reprogramming</a> Karen Hambardzumyan, Hrant Khachatrian, Jonathan May </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2021defending/">Defending Against Backdoor Attacks In Natural Language Generation</a> Xiaofei Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dong2021how/">How Should Pre-trained Language Models Be Fine-tuned Towards Adversarial Robustness?</a> Xinhsuai Dong, Luu Anh Tuan, Min Lin, Shuicheng Yan, Hanwang Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021adversarial/">Adversarial GLUE: A Multi-task Benchmark For Robustness Evaluation Of Language Models</a> Boxin Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maneriker2021improving/">Urltran: Improving Phishing URL Detection Using Transformers</a> Pranav Maneriker et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021adversarial/">Adversarial VQA: A New Benchmark For Evaluating The Robustness Of VQA Models</a> Linjie Li, Jie Lei, Zhe Gan, Jingjing Liu </li>
     
   
     
       <li> <a href="/publications/pan2021improved/">Improved Text Classification Via Contrastive Adversarial Training</a> Lin Pan, Chung-wei Hang, Avirup Sil, Saloni Potdar </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021evaluating/">Evaluating Large Language Models Trained On Code</a> Mark Chen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bartolo2021improving/">Improving Question Answering Model Robustness With Synthetic Adversarial Data Generation</a> Max Bartolo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2022are/">Are Large Pre-trained Language Models Leaking Your Personal Information?</a> Jie Huang, Hanyin Shao, Kevin Chen-chuan Chang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/menick2022teaching/">Teaching Language Models To Support Answers With Verified Quotes</a> Jacob Menick et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2022prompt/">Prompt Tuning For Generative Multimodal Pretrained Models</a> Hao Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sandoval2022lost/">Lost At C: A User Study On The Security Implications Of Large Language Model Code Assistants</a> Gustavo Sandoval et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/perez2022ignore/">Ignore Previous Prompt: Attack Techniques For Language Models</a> F√°bio Perez, Ian Ribeiro </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zoph2022st/">St-moe: Designing Stable And Transferable Sparse Expert Models</a> Barret Zoph et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/talmor2022commonsenseqa/">Commonsenseqa 2.0: Exposing The Limits Of AI Through Gamification</a> Alon Talmor et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2022shortcut/">Shortcut Learning Of Large Language Models In Natural Language Understanding</a> Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, Xia Hu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hartvigsen2022large/">Toxigen: A Large-scale Machine-generated Dataset For Adversarial And Implicit Hate Speech Detection</a> Thomas Hartvigsen et al. </li>
     
   
     
       <li> <a href="/publications/xu2022exploring/">Exploring The Universal Vulnerability Of Prompt-based Learning Paradigm</a> Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khoury2023how/">How Secure Is Code Generated By Chatgpt?</a> Rapha√´l Khoury, Anderson R. Avila, Jacob Brunelle, Baba Mamadou Camara </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/roy2023generating/">Generating Phishing Attacks Using Chatgpt</a> Sayak Saha Roy, Krishna Vamsi Naragam, Shirin Nilizadeh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lapid2023open/">Open Sesame! Universal Black Box Jailbreaking Of Large Language Models</a> Raz Lapid, Ron Langberg, Moshe Sipper </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/carlini2023are/">Are Aligned Neural Networks Adversarially Aligned?</a> Nicholas Carlini et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shapira2023clever/">Clever Hans Or Neural Theory Of Mind? Stress Testing Social Reasoning In Large Language Models</a> Natalie Shapira et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nasr2023scalable/">Scalable Extraction Of Training Data From (production) Language Models</a> Milad Nasr et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023chatgpt/">Chatgpt For Vulnerability Detection, Classification, And Repair: How Far Are We?</a> Michael Fu, Chakkrit Tantithamthavorn, Van Nguyen, Trung Le </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phute2023llm/">LLM Self Defense: By Self Examination, Llms Know They Are Being Tricked</a> Mansi Phute et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/greshake2023not/">Not What You've Signed Up For: Compromising Real-world Llm-integrated Applications With Indirect Prompt Injection</a> Kai Greshake et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hazell2023spear/">Spear Phishing With Large Language Models</a> Julian Hazell </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023robustness/">On The Robustness Of Chatgpt: An Adversarial And Out-of-distribution Perspective</a> Jindong Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023exploring/">Badgpt: Exploring Security Vulnerabilities Of Chatgpt Via Backdoor Attacks To Instructgpt</a> Jiawen Shi, Yixin Liu, Pan Zhou, Lichao Sun </li>
     
   
     
       <li> <a href="/publications/chen2023benchmarking/">Benchmarking Large Language Models In Retrieval-augmented Generation</a> Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun </li>
     
   
     
       <li> <a href="/publications/wu2023fake/">Fake News In Sheep's Clothing: Robust Fake News Detection Against Llm-empowered Style Attacks</a> Jiaying Wu, Jiafeng Guo, Bryan Hooi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2023llm/">LLM Lies: Hallucinations Are Not Bugs, But Features As Adversarial Examples</a> Jia-yu Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023privacy/">Privacy In Large Language Models: Attacks, Defenses And Future Directions</a> Haoran Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023safety/">Safety Assessment Of Chinese Large Language Models</a> Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shayegani2023survey/">Survey Of Vulnerabilities In Large Language Models Revealed By Adversarial Attacks</a> Erfan Shayegani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kang2023exploiting/">Exploiting Programmatic Behavior Of Llms: Dual-use Through Standard Security Attacks</a> Daniel Kang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chiang2023can/">Can Large Language Models Be An Alternative To Human Evaluations?</a> Cheng-han Chiang, Hung-yi Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tony2023dataset/">Llmseceval: A Dataset Of Natural Language Prompts For Security Evaluations</a> Catherine Tony, Markus Mutas, Nicol√°s E. D√≠az Ferreyra, Riccardo Scandariato </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2023how/">How Close Is Chatgpt To Human Experts? Comparison Corpus, Evaluation, And Detection</a> Biyang Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zou2023universal/">Universal And Transferable Adversarial Attacks On Aligned Language Models</a> Andy Zou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alkaswan2023open/">The (ab)use Of Open Source Code To Train Large Language Models</a> Ali Al-kaswan, Maliheh Izadi </li>
     
   
     
   
     
       <li> <a href="/publications/wei2023how/">Jailbroken: How Does LLM Safety Training Fail?</a> Alexander Wei, Nika Haghtalab, Jacob Steinhardt </li>
     
   
     
       <li> <a href="/publications/robey2023defending/">Smoothllm: Defending Large Language Models Against Jailbreaking Attacks</a> Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas </li>
     
   
     
       <li> <a href="/publications/wan2023poisoning/">Poisoning Language Models During Instruction Tuning</a> Alexander Wan, Eric Wallace, Sheng Shen, Dan Klein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023chatgpt/">In Chatgpt We Trust? Measuring And Characterizing The Reliability Of Chatgpt</a> Xinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang </li>
     
   
     
   
     
       <li> <a href="/publications/shen2023anything/">"do Anything Now": Characterizing And Evaluating In-the-wild Jailbreak Prompts On Large Language Models</a> Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023unveiling/">Unveiling Security, Privacy, And Ethical Concerns Of Chatgpt</a> Xiaodong Wu, Ran Duan, Jianbing Ni </li>
     
   
     
   
     
       <li> <a href="/publications/qi2023fine/">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</a> Xiangyu Qi et al. </li>
     
   
     
       <li> <a href="/publications/qi2023visual/">Visual Adversarial Examples Jailbreak Aligned Large Language Models</a> Xiangyu Qi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rawte2023troubling/">The Troubling Emergence Of Hallucination In Large Language Models -- An Extensive Definition, Quantification, And Prescriptive Remediations</a> Vipula Rawte et al. </li>
     
   
     
       <li> <a href="/publications/sadasivan2023can/">Can Ai-generated Text Be Reliably Detected?</a> Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhuo2023red/">Red Teaming Chatgpt Via Jailbreaking: Bias, Robustness, Reliability And Toxicity</a> Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing </li>
     
   
     
   
     
       <li> <a href="/publications/shen2023large/">Large Language Model Alignment: A Survey</a> Tianhao Shen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/soman2023observations/">Observations On Llms For Telecom Domain: Capabilities And Limitations</a> Sumit Soman, Ranjani H G </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2023survey/">A Survey On Large Language Model (LLM) Security And Privacy: The Good, The Bad, And The Ugly</a> Yifan Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023how/">How Robust Is GPT-3.5 To Predecessors? A Comprehensive Study On Language Understanding Tasks</a> Xuanting Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wolf2023fundamental/">Fundamental Limitations Of Alignment In Large Language Models</a> Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023evaluating/">On Evaluating Adversarial Robustness Of Large Vision-language Models</a> Yunqing Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/motlagh2024large/">Large Language Models In Cybersecurity: State-of-the-art</a> Farzad Nourmohammadzadeh Motlagh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/novelli2024generative/">Generative AI In EU Law: Liability, Privacy, Intellectual Property, And Cybersecurity</a> Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato, Luciano Floridi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zeng2024how/">How Johnny Can Persuade Llms To Jailbreak Them: Rethinking Persuasion To Challenge AI Safety By Humanizing Llms</a> Yi Zeng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Survey Paper <a id="Survey Paper"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/serban2016generative/">Generative Deep Neural Networks For Dialogue: A Short Review</a> Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, Joelle Pineau </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2017survey/">A Survey On Dialogue Systems: Recent Advances And New Frontiers</a> Hongshen Chen, Xiaorui Liu, Dawei Yin, Jiliang Tang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gehrmann2018end/">End-to-end Content And Plan Selection For Data-to-text Generation</a> Sebastian Gehrmann, Falcon Z. Dai, Henry Elder, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2018neural/">A Neural Interlingua For Multilingual Machine Translation</a> Yichao Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2019conditional/">Conditional Text Generation For Harmonious Human-machine Interaction</a> Bin Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sulubacak2019multimodal/">Multimodal Machine Translation Through Visuals And Speech</a> Umut Sulubacak et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2019review/">Review Conversational Reading Comprehension</a> Hu Xu, Bing Liu, Lei Shu, Philip S. Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/santhanam2019survey/">A Survey Of Natural Language Generation Techniques With A Focus On Dialogue Systems - Past, Present And Future Directions</a> Sashank Santhanam, Samira Shaikh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/csaky2019deep/">Deep Learning Based Chatbot Models</a> Richard Csaky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bugliarello2020multimodal/">Multimodal Pretraining Unmasked: A Meta-analysis And A Unified Framework Of Vision-and-language Berts</a> Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2020survey/">A Survey Of Knowledge-enhanced Text Generation</a> Wenhao Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020logical/">Logical Natural Language Generation From Open-domain Tables</a> Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020closer/">A Closer Look At The Robustness Of Vision-and-language Pre-trained Models</a> Linjie Li, Zhe Gan, Jingjing Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020machine/">Machine Reading Comprehension: The Role Of Contextualized Language Models And Beyond</a> Zhuosheng Zhang, Hai Zhao, Rui Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ganesh2020compressing/">Compressing Large-scale Transformer-based Models: A Case Study On BERT</a> Prakhar Ganesh et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020survey/">A Survey On Contextual Embeddings</a> Qi Liu, Matt J. Kusner, Phil Blunsom </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2020which/">Which *BERT? A Survey Organizing Contextualized Encoders</a> Patrick Xia, Shijie Wu, Benjamin Van Durme </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zaib2021short/">A Short Survey Of Pre-trained Language Models For Conversational AI-A Newage In NLP</a> Munazza Zaib, Quan Z. Sheng, Wei Emma Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/feng2021survey/">A Survey On Dialogue Summarization: Recent Advances And New Frontiers</a> Xiachong Feng, Xiaocheng Feng, Bing Qin </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sheng2021societal/">Societal Biases In Language Generation: Progress And Challenges</a> Emily Sheng, Kai-wei Chang, Premkumar Natarajan, Nanyun Peng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2021exploratory/">An Exploratory Study On Long Dialogue Summarization: What Works And What's Next</a> Yusen Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/narayanan2021efficient/">Efficient Large-scale Language Model Training On GPU Clusters Using Megatron-lm</a> Deepak Narayanan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2021scale/">Scale Efficiently: Insights From Pre-training And Fine-tuning Transformers</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hendrycks2021expert/">CUAD: An Expert-annotated NLP Dataset For Legal Contract Review</a> Dan Hendrycks, Collin Burns, Anya Chen, Spencer Ball </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021pretrained/">Pretrained Language Models For Text Generation: A Survey</a> Junyi Li, Tianyi Tang, Wayne Xin Zhao, Ji-rong Wen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kalyan2021ammus/">AMMUS : A Survey Of Transformer-based Pretrained Models In Natural Language Processing</a> Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, Sivanesan Sangeetha </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/min2021recent/">Recent Advances In Natural Language Processing Via Large Pre-trained Language Models: A Survey</a> Bonan Min et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2021pre/">Pre-train, Prompt, And Predict: A Systematic Survey Of Prompting Methods In Natural Language Processing</a> Pengfei Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/motger2021software/">Software-based Dialogue Systems: Survey, Taxonomy And Challenges</a> Quim Motger, Xavier Franch, Jordi Marco </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2022towards/">Towards Reasoning In Large Language Models: A Survey</a> Jie Huang, Kevin Chen-chuan Chang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/andreas2022language/">Language Models As Agent Models</a> Jacob Andreas </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022survey/">A Survey On Retrieval-augmented Text Generation</a> Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2022survey/">A Survey Of Vision-language Pre-trained Models</a> Yifan Du, Zikang Liu, Junyi Li, Wayne Xin Zhao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022survey/">A Survey Of Controllable Text Generation Using Transformer-based Pre-trained Language Models</a> Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022vision/">Vision-language Intelligence: Tasks, Representation Learning, And Large Models</a> Feng Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zan2022large/">Large Language Models Meet Nl2code: A Survey</a> Daoguang Zan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2022survey/">A Survey On Model Compression And Acceleration For Pretrained Language Models</a> Canwen Xu, Julian Mcauley </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guti%C3%A9rrez2022thinking/">Thinking About GPT-3 In-context Learning For Biomedical IE? Think Again</a> Bernal Jim√©nez Guti√©rrez et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/petrov2022systematic/">A Systematic Review And Replicability Study Of Bert4rec For Sequential Recommendation</a> Aleksandr Petrov, Craig Macdonald </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ji2022survey/">Survey Of Hallucination In Natural Language Generation</a> Ziwei Ji et al. </li>
     
   
     
       <li> <a href="/publications/qiao2022reasoning/">Reasoning With Language Model Prompting: A Survey</a> Shuofei Qiao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022delta/">Delta Tuning: A Comprehensive Study Of Parameter Efficient Methods For Pre-trained Language Models</a> Ning Ding et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/long2022vision/">Vision-and-language Pretrained Models: A Survey</a> Siqu Long, Feiqi Cao, Soyeon Caren Han, Haiqin Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gan2022vision/">Vision-language Pre-training: Basics, Recent Advances, And Future Trends</a> Zhe Gan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2022shortcut/">Shortcut Learning Of Large Language Models In Natural Language Understanding</a> Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, Xia Hu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mitchell2022debate/">The Debate Over Understanding In Ai's Large Language Models</a> Melanie Mitchell, David C. Krakauer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2022language/">Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey</a> Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos, Yulia Tsvetkov </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023retrieving/">Retrieving Multimodal Information For Augmented Generation: A Survey</a> Ruochen Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/schaeffer2023are/">Are Emergent Abilities Of Large Language Models A Mirage?</a> Rylan Schaeffer, Brando Miranda, Sanmi Koyejo </li>
     
   
     
       <li> <a href="/publications/tang2023science/">The Science Of Detecting Llm-generated Texts</a> Ruixiang Tang, Yu-neng Chuang, Xia Hu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mao2023survey/">Gpteval: A Survey On Assessments Of Chatgpt And GPT-4</a> Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, Erik Cambria </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/omar2023chatgpt/">Chatgpt Versus Traditional Question Answering For Knowledge Graphs: Current Status And Future Directions Towards Knowledge Graph Chatbots</a> Reham Omar, Omij Mangukiya, Panos Kalnis, Essam Mansour </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sohail2023decoding/">Decoding Chatgpt: A Taxonomy Of Existing Research, Current Challenges, And Possible Future Directions</a> Shahab Saquib Sohail et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/amani2023generative/">Generative AI Perceptions: A Survey To Measure The Perceptions Of Faculty, Staff, And Students On Generative AI Tools In Academia</a> Sara Amani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023instruction/">Instruction Tuning For Large Language Models: A Survey</a> Shengyu Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gozalobrizuela2023chatgpt/">Chatgpt Is Not All You Need. A State Of The Art Review Of Large Generative AI Models</a> Roberto Gozalo-brizuela, Eduardo C. Garrido-merchan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khraisha2023can/">Can Large Language Models Replace Humans In The Systematic Review Process? Evaluating Gpt-4's Efficacy In Screening And Extracting Data From Peer-reviewed And Grey Literature In Multiple Languages</a> Qusai Khraisha, Sophie Put, Johanna Kappenberg, Azza Warraitch, Kristin Hadfield </li>
     
   
     
   
     
       <li> <a href="/publications/wei2023evaluation/">Evaluation Of Chatgpt-generated Medical Responses: A Systematic Review And Meta-analysis</a> Qiuhong Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/staab2023beyond/">Beyond Memorization: Violating Privacy Via Inference With Large Language Models</a> Robin Staab, Mark Vero, Mislav Balunoviƒá, Martin Vechev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2023unifying/">Unifying Large Language Models And Knowledge Graphs: A Roadmap</a> Shirui Pan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023pre/">Pre-train, Prompt And Recommendation: A Comprehensive Survey Of Language Modelling Paradigm Adaptations In Recommender Systems</a> Peng Liu, Lemei Zhang, Jon Atle Gulla </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/motlagh2023impact/">The Impact Of Artificial Intelligence On The Evolution Of Digital Education: A Comparative Study Of Openai Text Generation Tools Including Chatgpt, Bing Chat, Bard, And Ernie</a> Negin Yazdani Motlagh, Matin Khajavi, Abbas Sharifi, Mohsen Ahmadi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/inie2023designing/">Designing Participatory AI: Creative Professionals' Worries And Expectations About Generative AI</a> Nanna Inie, Jeanette Falk, Steven Tanimoto </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parker2023large/">A Large Language Model Approach To Educational Survey Feedback Analysis</a> Michael J. Parker, Caitlin Anderson, Claire Stone, Yearim Oh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khondaker2023comprehensive/">Gptaraeval: A Comprehensive Evaluation Of Chatgpt On Arabic NLP</a> Md Tawkat Islam Khondaker, Abdul Waheed, El Moatez Billah Nagoudi, Muhammad Abdul-mageed </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ishmam2023from/">From Image To Language: A Critical Analysis Of Visual Question Answering (VQA) Approaches, Challenges, And Opportunities</a> Md Farhan Ishmam, Md Sakib Hossain Shovon, M. F. Mridha, Nilanjan Dey </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wong2023natural/">Natural Language Generation And Understanding Of Big Code For Ai-assisted Programming: A Review</a> Man Fai Wong, Shangxin Guo, Ching Nam Hang, Siu Wai Ho, Chee Wei Tan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2023bibliometric/">A Bibliometric Review Of Large Language Models Research From 2017 To 2023</a> Lizhou Fan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yan2023practical/">Practical And Ethical Challenges Of Large Language Models In Education: A Systematic Scoping Review</a> Lixiang Yan et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023parameter/">Parameter-efficient Fine-tuning Methods For Pretrained Language Models: A Critical Review And Assessment</a> Lingling Xu, Haoran Xie, Si-zhao Joe Qin, Xiaohui Tao, Fu Lee Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023survey/">A Survey On Large Language Models For Recommendation</a> Likang Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2023automatically/">Automatically Correcting Large Language Models: Surveying The Landscape Of Diverse Self-correction Strategies</a> Liangming Pan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023survey/">A Survey On Hallucination In Large Language Models: Principles, Taxonomy, Challenges, And Open Questions</a> Lei Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kalyan2023survey/">A Survey Of GPT-3 Family Large Language Models Including Chatgpt And GPT-4</a> Katikapalli Subramanyam Kalyan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023ai/">Ai-augmented Surveys: Leveraging Large Language Models And Surveys For Opinion Prediction</a> Junsol Kim, Byungkyu Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/niklaus2023multi/">LEXTREME: A Multi-lingual And Multi-task Benchmark For The Legal Domain</a> Joel Niklaus et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2023systematic/">A Systematic Survey Of Prompt Engineering On Vision-language Foundation Models</a> Jindong Gu et al. </li>
     
   
     
       <li> <a href="/publications/chen2023when/">When Large Language Models Meet Personalization: Perspectives Of Challenges And Opportunities</a> Jin Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xi2023rise/">The Rise And Potential Of Large Language Model Based Agents: A Survey</a> Zhiheng Xi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023how/">How Can Recommender Systems Benefit From Large Language Models: A Survey</a> Jianghao Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/omiye2023large/">Large Language Models In Medicine: The Potentials And Pitfalls</a> Jesutofunmi A. Omiye, Haiwen Gui, Shawheen J. Rezaei, James Zou, Roxana Daneshjou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/prather2023robots/">The Robots Are Here: Navigating The Generative AI Revolution In Computing Education</a> James Prather et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/joshi2023not/">"it's Not Like Jarvis, But It's Pretty Close!" -- Examining Chatgpt's Usage Among Undergraduate Students In Computer Science</a> Ishika Joshi, Ritvik Budhiraja, Harshal D Akolekar, Jagat Sesh Challa, Dhruv Kumar </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kirk2023personalisation/">Personalisation Within Bounds: A Risk Taxonomy And Policy Framework For The Alignment Of Large Language Models With Personalised Feedback</a> Hannah Rose Kirk, Bertie Vidgen, Paul R√∂ttger, Scott A. Hale </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023explainability/">Explainability For Large Language Models: A Survey</a> Haiyan Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mialon2023augmented/">Augmented Language Models: A Survey</a> Gr√©goire Mialon et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shayegani2023survey/">Survey Of Vulnerabilities In Large Language Models Revealed By Adversarial Attacks</a> Erfan Shayegani et al. </li>
     
   
     
       <li> <a href="/publications/ferrara2023should/">Should Chatgpt Be Biased? Challenges And Risks Of Bias In Large Language Models</a> Emilio Ferrara </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023large/">Large Language Models For Generative Information Extraction: A Survey</a> Derong Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023multimodal/">Multimodal Foundation Models: From Specialists To General-purpose Assistants</a> Chunyuan Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/leiter2023meta/">Chatgpt: A Meta-analysis After 2.5 Months</a> Christoph Leiter et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023one/">One Small Step For Generative AI, One Giant Leap For AGI: A Complete Survey On Chatgpt In AIGC Era</a> Chaoning Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023large/">Large Language Models On Graphs: A Comprehensive Survey</a> Bowen Jin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023short/">A Short Survey Of Viewing Large Language Models In Legal Aspect</a> Zhongxiang Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kucharavy2023fundamentals/">Fundamentals Of Generative Large Language Models And Perspectives In Cyber-defense</a> Andrei Kucharavy et al. </li>
     
   
     
       <li> <a href="/publications/olga2023generative/">Generative AI: Implications And Applications For Education</a> Anastasia Olnancy Olga et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sison2023more/">Chatgpt: More Than A Weapon Of Mass Deception, Ethical Challenges And Responses From The Human-centered Artificial Intelligence (HCAI) Perspective</a> Alejo Jose G. Sison, Marco Tulio Daza, Roberto Gozalo-brizuela, Eduardo C. Garrido-merch√°n </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2023vision/">Vision Language Models In Autonomous Driving: A Survey And Outlook</a> Xingcheng Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xin2023survey/">A Survey Of Large Language Models</a> Wayne Xin Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chang2023language/">Language Model Behavior: A Comprehensive Survey</a> Tyler A. Chang, Benjamin K. Bergen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023recommender/">Recommender Systems In The Era Of Large Language Models (llms)</a> Zihuai Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sumers2023cognitive/">Cognitive Architectures For Language Agents</a> Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L. Griffiths </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023large/">Large Language Model Alignment: A Survey</a> Tianhao Shen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2023comprehensive/">A Comprehensive Survey Of Ai-generated Content (AIGC): A History Of Generative AI From GAN To Chatgpt</a> Yihan Cao et al. </li>
     
   
     
       <li> <a href="/publications/liu2023summary/">Summary Of Chatgpt-related Research And Perspective Towards The Future Of Large Language Models</a> Yiheng Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yao2023survey/">A Survey On Large Language Model (LLM) Security And Privacy: The Good, The Bad, And The Ugly</a> Yifan Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023trustworthy/">Trustworthy Llms: A Survey And Guideline For Evaluating Large Language Models' Alignment</a> Yang Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023survey/">A Survey On Model Compression For Large Language Models</a> Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023aligning/">Aligning Large Language Models With Human: A Survey</a> Yufei Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yin2023survey/">A Survey On Multimodal Large Language Models</a> Shukang Yin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/tian2023opportunities/">Opportunities And Challenges For Chatgpt And Large Language Models In Biomedicine And Health</a> Shubo Tian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nazi2023large/">Large Language Models In Healthcare And Medical Domain: A Review</a> Zabir Al Nazi, Wei Peng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tonmoy2024comprehensive/">A Comprehensive Survey Of Hallucination Mitigation Techniques In Large Language Models</a> S. M Towhidul Islam Tonmoy et al. </li>
     
   
     
       <li> <a href="/publications/gallotta2024large/">Large Language Models And Games: A Survey And Roadmap</a> Roberto Gallotta et al. </li>
     
   
     
       <li> <a href="/publications/khojah2024beyond/">Beyond Code Generation: An Observational Study Of Chatgpt Usage In Software Engineering Practice</a> Ranim Khojah, Mazen Mohamad, Philipp Leitner, Francisco Gomes De Oliveira Neto </li>
     
   
     
   
     
       <li> <a href="/publications/kaur2024from/">From Text To Transformation: A Comprehensive Review Of Large Language Models' Versatility</a> Pravneet Kaur et al. </li>
     
   
     
       <li> <a href="/publications/sahoo2024systematic/">A Systematic Survey Of Prompt Engineering In Large Language Models: Techniques And Applications</a> Pranab Sahoo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maleki2024ai/">AI Hallucinations: A Misnomer Worth Clarifying</a> Negar Maleki, Balaji Padmanabhan, Kaushik Dutta </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2024survey/">A Survey Of Resource-efficient LLM And Multimodal Foundation Models</a> Mengwei Xu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ramos2024review/">A Review Of Large Language Models And Autonomous Agents In Chemistry</a> Mayk Caldas Ramos, Christopher J. Collison, Andrew D. White </li>
     
   
     
       <li> <a href="/publications/alamin2024history/">History Of Generative Artificial Intelligence (AI) Chatbots: Past, Present, And Future Development</a> Md. Al-amin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheong2024am/">(A)I Am Not A Lawyer, But...: Engaging Legal Experts Towards Responsible LLM Policies For Legal Advice</a> Inyoung Cheong, King Xia, K. J. Kevin Feng, Quan Ze Chen, Amy X. Zhang </li>
     
   
     
       <li> <a href="/publications/zhao2024revolutionizing/">Revolutionizing Finance With Llms: An Overview Of Applications And Insights</a> Huaqin Zhao et al. </li>
     
   
     
       <li> <a href="/publications/hartsock2024vision/">Vision-language Models For Medical Report Generation And Visual Question Answering: A Review</a> Iryna Hartsock, Ghulam Rasool </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/motlagh2024large/">Large Language Models In Cybersecurity: State-of-the-art</a> Farzad Nourmohammadzadeh Motlagh et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/frankford2024ai/">Ai-tutoring In Software Engineering Education</a> Eduard Frankford, Clemens Sauerwein, Patrick Bassner, Stephan Krusche, Ruth Breu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caffagni2024revolution/">The Revolution Of Multimodal Large Language Models: A Survey</a> Davide Caffagni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2024survey/">Survey On Large Language Model-enhanced Reinforcement Learning: Concept, Taxonomy, And Methods</a> Yuji Cao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hua2024large/">Large Language Models In Mental Health Care: A Scoping Review</a> Yining Hua et al. </li>
     
   
     
   
     
       <li> <a href="/publications/deldjoo2024review/">A Review Of Modern Recommender Systems Using Generative Models (gen-recsys)</a> Yashar Deldjoo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2024datasets/">Datasets For Large Language Models: A Comprehensive Survey</a> Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2024survey/">A Survey On RAG Meeting Llms: Towards Retrieval-augmented Large Language Models</a> Wenqi Fan et al. </li>
     
   
     
       <li> <a href="/publications/liang2024monitoring/">Monitoring Ai-modified Content At Scale: A Case Study On The Impact Of Chatgpt On AI Conference Peer Reviews</a> Weixin Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2024continual/">Continual Learning For Large Language Models: A Survey</a> Tongtong Wu et al. </li>
     
   
     
       <li> <a href="/publications/hagendorff2024mapping/">Mapping The Ethics Of Generative AI: A Comprehensive Scoping Review</a> Thilo Hagendorff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2024large/">Large Language Models For Data Annotation And Synthesis: A Survey</a> Zhen Tan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Tokenization <a id="Tokenization"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/niu2018adversarial/">Adversarial Over-sensitivity And Over-stability Strategies For Dialogue Models</a> Tong Niu, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/choe2019bridging/">Bridging The Gap For Tokenizer-free Language Models</a> Dokook Choe, Rami Al-rfou, Mandy Guo, Heeyoung Lee, Noah Constant </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cruzbenito2020automated/">Automated Source Code Generation And Auto-completion Using Deep Learning: Comparing And Discussing Current Language-model-related Approaches</a> Juan Cruz-benito, Sanjay Vishwakarma, Francisco Martin-fernandez, Ismael Faro </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/arora2020natural/">Inltk: Natural Language Toolkit For Indic Languages</a> Gaurav Arora </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rust2020how/">How Good Is Your Tokenizer? On The Monolingual Performance Of Multilingual Language Models</a> Phillip Rust, Jonas Pfeiffer, Ivan Vuliƒá, Sebastian Ruder, Iryna Gurevych </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2020detecting/">Detecting Hallucinated Content In Conditional Neural Sequence Generation</a> Chunting Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bostrom2020byte/">Byte Pair Encoding Is Suboptimal For Language Model Pretraining</a> Kaj Bostrom, Greg Durrett </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vannguyen2021light/">Trankit: A Light-weight Transformer-based Toolkit For Multilingual Natural Language Processing</a> Minh Van Nguyen, Viet Dac Lai, Amir Pouran Ben Veyseh, Thien Huu Nguyen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/clark2021pre/">CANINE: Pre-training An Efficient Tokenization-free Encoder For Language Representation</a> Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lowphansirikul2021pretraining/">Wangchanberta: Pretraining Transformer-based Thai Language Models</a> Lalita Lowphansirikul, Charin Polpanumas, Nawat Jantrakulchai, Sarana Nutanong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021what/">What Changes Can Large-scale Language Models Bring? Intensive Study On Hyperclova: Billions-scale Korean Generative Pretrained Transformers</a> Boseop Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nogueira2021investigating/">Investigating The Limitations Of Transformers With Simple Arithmetic Tasks</a> Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/robinson2022leveraging/">Leveraging Large Language Models For Multiple Choice Question Answering</a> Joshua Robinson, Christopher Michael Rytting, David Wingate </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gafni2022make/">Make-a-scene: Scene-based Text-to-image Generation With Human Priors</a> Oran Gafni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/borsos2022language/">Audiolm: A Language Modeling Approach To Audio Generation</a> Zal√°n Borsos et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gruver2023large/">Large Language Models Are Zero-shot Time Series Forecasters</a> Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/seenivasan2023end/">Surgicalgpt: End-to-end Language-vision GPT For Visual Question Answering In Surgery</a> Lalithkumar Seenivasan, Mobarakol Islam, Gokul Kannan, Hongliang Ren </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023video/">Video-llava: Learning United Visual Representation By Alignment Before Projection</a> Bin Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rozi%C3%A8re2023code/">Code Llama: Open Foundation Models For Code</a> Baptiste Rozi√®re et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/petrov2023language/">Language Model Tokenizers Introduce Unfairness Between Languages</a> Aleksandar Petrov, Emanuele La Malfa, Philip H. S. Torr, Adel Bibi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Tools <a id="Tools"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2016neural/">Neural Machine Translation Advised By Statistical Machine Translation</a> Xing Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2016user/">A User Simulator For Task-completion Dialogues</a> Xiujun Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2017question/">Question Answering And Question Generation As Dual Tasks</a> Duyu Tang, Nan Duan, Tao Qin, Zhao Yan, Ming Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/elliott2017imagination/">Imagination Improves Multimodal Translation</a> Desmond Elliott, √Åkos K√°d√°r </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/miller2017dialog/">Parlai: A Dialog Research Software Platform</a> Alexander H. Miller et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hieber2017toolkit/">Sockeye: A Toolkit For Neural Machine Translation</a> Felix Hieber et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alvarezmelis2017causal/">A Causal Framework For Explaining The Predictions Of Black-box Sequence-to-sequence Models</a> David Alvarez-melis, Tommi S. Jaakkola </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2017unified/">A Unified Query-based Generative Model For Question Generation And Question Answering</a> Linfeng Song, Zhiguo Wang, Wael Hamza </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2018automating/">Automating Reading Comprehension By Generating Question And Answer Pairs</a> Vishwajeet Kumar, Kireeti Boorla, Yogesh Meena, Ganesh Ramakrishnan, Yuan-fang Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cai2018skeleton/">Skeleton-to-response: Dialogue Generation Guided By Retrieval Memory</a> Deng Cai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tambwekar2018controllable/">Controllable Neural Story Plot Generation Via Reward Shaping</a> Pradyumna Tambwekar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/caccia2018language/">Language Gans Falling Short</a> Massimo Caccia et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/popel2018training/">Training Tips For The Transformer Model</a> Martin Popel, Ond≈ôej Bojar </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2018end/">Seq2rdf: An End-to-end Application For Deriving Triples From Natural Language Text</a> Yue Liu, Tongtao Zhang, Zhicheng Liang, Heng Ji, Deborah L. Mcguinness </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hashimoto2018retrieve/">A Retrieve-and-edit Framework For Predicting Structured Outputs</a> Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, Percy Liang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2018best/">The Best Of Both Worlds: Combining Recent Advances In Neural Machine Translation</a> Mia Xu Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2018multi/">Multi-task Learning With Sample Re-weighting For Machine Reading Comprehension</a> Yichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu, Jianfeng Gao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2018vision/">Vision-based Navigation With Language-based Assistance Via Imitation Learning With Indirect Intervention</a> Khanh Nguyen, Debadeepta Dey, Chris Brockett, Bill Dolan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vaswani2018neural/">Tensor2tensor For Neural Machine Translation</a> Ashish Vaswani et al. </li>
     
   
     
       <li> <a href="/publications/ram2018conversational/">Conversational AI: The Science Behind The Alexa Prize</a> Ashwin Ram et al. </li>
     
   
     
       <li> <a href="/publications/khatri2018advancing/">Advancing The State Of The Art In Open Domain Dialog Systems Through The Alexa Prize</a> Chandra Khatri et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kreutzer2018can/">Can Neural Machine Translation Be Improved With User Feedback?</a> Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, Stefan Riezler </li>
     
   
     
       <li> <a href="/publications/shah2018building/">Building A Conversational Agent Overnight With Dialogue Self-play</a> Pararth Shah et al. </li>
     
   
     
       <li> <a href="/publications/c%C3%B4t%C3%A92018learning/">Textworld: A Learning Environment For Text-based Games</a> Marc-alexandre C√¥t√© et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hoover2019visual/">Exbert: A Visual Analysis Tool To Explore Learned Representations In Transformers Models</a> Benjamin Hoover, Hendrik Strobelt, Sebastian Gehrmann </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/pei2019modular/">A Modular Task-oriented Dialogue System Using A Neural Mixture-of-experts</a> Jiahuan Pei, Pengjie Ren, Maarten De Rijke </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/byrne2019taskmaster/">Taskmaster-1: Toward A Realistic And Diverse Dialog Dataset</a> Bill Byrne et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qin2019entity/">Entity-consistent End-to-end Task-oriented Dialogue System With KB Retriever</a> Libo Qin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2019transferring/">Macnet: Transferring Knowledge From Machine Comprehension To Sequence-to-sequence Models</a> Boyuan Pan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2019multi/">12-in-1: Multi-task Vision And Language Representation Learning</a> Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee </li>
     
   
     
   
     
       <li> <a href="/publications/ghandeharioun2019approximating/">Approximating Interactive Human Evaluation With Self-play For Open-domain Dialog Systems</a> Asma Ghandeharioun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/salazar2019masked/">Masked Language Model Scoring</a> Julian Salazar, Davis Liang, Toan Q. Nguyen, Katrin Kirchhoff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2019synchronous/">Synchronous Bidirectional Neural Machine Translation</a> Long Zhou, Jiajun Zhang, Chengqing Zong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2019open/">UER: An Open-source Toolkit For Pre-training Models</a> Zhe Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thomason2019improving/">Improving Grounded Natural Language Understanding Through Human-robot Dialog</a> Jesse Thomason et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2019simple/">Simple And Effective Curriculum Pointer-generator Networks For Reading Comprehension Over Long Narratives</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2019evaluating/">MLQA: Evaluating Cross-lingual Extractive Question Answering</a> Patrick Lewis, Barlas Oƒüuz, Ruty Rinott, Sebastian Riedel, Holger Schwenk </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gehrmann2019statistical/">GLTR: Statistical Detection And Visualization Of Generated Text</a> Sebastian Gehrmann, Hendrik Strobelt, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kuchaiev2019toolkit/">Nemo: A Toolkit For Building AI Applications Using Neural Modules</a> Oleksii Kuchaiev et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2019language/">LAMOL: Language Modeling For Lifelong Language Learning</a> Fan-keng Sun, Cheng-hao Ho, Hung-yi Lee </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/svyatkovskiy2019ai/">Pythia: Ai-assisted Code Completion System</a> Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, Neel Sundaresan </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2019stickier/">Superglue: A Stickier Benchmark For General-purpose Language Understanding Systems</a> Alex Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/welleck2019non/">Non-monotonic Sequential Text Generation</a> Sean Welleck, Kiant√© Brantley, Hal Iii Daum√©, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019multiresolution/">Empdg: Multiresolution Interactive Empathetic Dialogue Generation</a> Qintong Li et al. </li>
     
   
     
       <li> <a href="/publications/das2019multi/">Multi-step Retriever-reader Interaction For Scalable Open-domain Question Answering</a> Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Andrew Mccallum </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kreutzer2019joey/">Joey NMT: A Minimalist NMT Toolkit For Novices</a> Julia Kreutzer, Jasmijn Bastings, Stefan Riezler </li>
     
   
     
       <li> <a href="/publications/zhang2019task/">Task-oriented Dialog Systems That Consider Multiple Appropriate Responses Under The Same Context</a> Yichi Zhang, Zhijian Ou, Zhou Yu </li>
     
   
     
   
     
       <li> <a href="/publications/jin2019multi/">MMM: Multi-stage Multi-task Learning For Multi-choice Reading Comprehension</a> Di Jin, Shuyang Gao, Jiun-yu Kao, Tagyoung Chung, Dilek Hakkani-tur </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2019modular/">Lingvo: A Modular And Scalable Framework For Sequence-to-sequence Modeling</a> Jonathan Shen et al. </li>
     
   
     
       <li> <a href="/publications/kang2019recommendation/">Recommendation As A Communication Game: Self-supervised Bot-play For Goal-oriented Dialogue</a> Dongyeop Kang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wallace2019allennlp/">Allennlp Interpret: A Framework For Explaining Predictions Of NLP Models</a> Eric Wallace et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020high/">Lightseq: A High Performance Inference Library For Transformers</a> Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2020long/">Long Range Arena: A Benchmark For Efficient Transformers</a> Yi Tay et al. </li>
     
   
     
   
     
       <li> <a href="/publications/su2020bert/">Bert-hlstms: BERT And Hierarchical Lstms For Visual Storytelling</a> Jing Su, Qingyun Dai, Frank Guerin, Mian Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2020recipes/">Recipes For Safety In Open-domain Chatbots</a> Jing Xu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/narang2020training/">WT5?! Training Text-to-text Models To Explain Their Predictions</a> Sharan Narang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2020rapidly/">Rapidly Bootstrapping A Question Answering Dataset For COVID-19</a> Raphael Tang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2020what/">What Disease Does This Patient Have? A Large-scale Open Domain Question Answering Dataset From Medical Exams</a> Di Jin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pfeiffer2020framework/">Adapterhub: A Framework For Adapting Transformers</a> Jonas Pfeiffer et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2020towards/">Towards Making The Most Of Context In Neural Machine Translation</a> Zaixiang Zheng, Xiang Yue, Shujian Huang, Jiajun Chen, Alexandra Birch </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/arora2020natural/">Inltk: Natural Language Toolkit For Indic Languages</a> Gaurav Arora </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2020incorporating/">Incorporating External Knowledge Through Pre-training For Natural Language To Code Generation</a> Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, Graham Neubig </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020hardware/">HAT: Hardware-aware Transformers For Efficient Natural Language Processing</a> Hanrui Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2020minimalist/">Mintl: Minimalist Transfer Learning For Task-oriented Dialogue Systems</a> Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ranasinghe2020transquest/">Transquest At WMT2020: Sentence-level Direct Assessment</a> Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2020plato/">PLATO-2: Towards Building An Open-domain Chatbot Via Curriculum Learning</a> Siqi Bao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2020controllable/">A Controllable Model Of Grounded Response Generation</a> Zeqiu Wu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shavrina2020russian/">Russiansuperglue: A Russian Language Understanding Evaluation Benchmark</a> Tatiana Shavrina et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moon2020situated/">Situated And Interactive Multimodal Conversations</a> Seungwhan Moon et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jain2020learning/">Learning To Faithfully Rationalize By Construction</a> Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, Byron C. Wallace </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/papangelis2020plato/">Plato Dialogue System: A Flexible Conversational AI Research Platform</a> Alexandros Papangelis et al. </li>
     
   
     
       <li> <a href="/publications/svyatkovskiy2020intellicode/">Intellicode Compose: Code Generation Using Transformer</a> Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, Neel Sundaresan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020vd/">VD-BERT: A Unified Vision And Dialog Transformer With BERT</a> Yue Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rastogi2020schema/">Schema-guided Dialogue State Tracking Task At DSTC8</a> Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, Pranav Khaitan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/karpukhin2020dense/">Dense Passage Retrieval For Open-domain Question Answering</a> Vladimir Karpukhin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tenney2020language/">The Language Interpretability Tool: Extensible, Interactive Visualizations And Analysis For NLP Models</a> Ian Tenney et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/le2020bi/">Bist: Bi-directional Spatio-temporal Reasoning For Video-grounded Dialogues</a> Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cai2020data/">Data Manipulation: Towards Effective Instance Learning For Neural Dialogue Generation Via Learning To Augment And Reweight</a> Hengyi Cai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rei2020neural/">COMET: A Neural Framework For MT Evaluation</a> Ricardo Rei, Craig Stewart, Ana C Farinha, Alon Lavie </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kale2020template/">Template Guided Text Generation For Task-oriented Dialogue</a> Mihir Kale, Abhinav Rastogi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yuan2021evaluating/">Bartscore: Evaluating Generated Text As Text Generation</a> Weizhe Yuan, Graham Neubig, Pengfei Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2021data/">MELM: Data Augmentation With Masked Entity Language Modeling For Low-resource NER</a> Ran Zhou et al. </li>
     
   
     
       <li> <a href="/publications/ding2021open/">Openprompt: An Open-source Framework For Prompt-learning</a> Ning Ding et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mitchell2021fast/">Fast Model Editing At Scale</a> Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D. Manning </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sheng2021revealing/">Revealing Persona Biases In Dialogue Systems</a> Emily Sheng, Josh Arnold, Zhou Yu, Kai-wei Chang, Nanyun Peng </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nagoudi2021text/">Arat5: Text-to-text Transformers For Arabic Language Generation</a> El Moatez Billah Nagoudi, Abdelrahim Elmadany, Muhammad Abdul-mageed </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021evaluation/">Dynaboard: An Evaluation-as-a-service Platform For Holistic Next-generation Benchmarking</a> Zhiyi Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021fast/">Fastmoe: A Fast Mixture-of-expert Training System</a> Jiaao He et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/meng2021coco/">COCO-LM: Correcting And Contrasting Text Sequences For Language Model Pretraining</a> Yu Meng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cho2021unifying/">Unifying Vision-and-language Tasks Via Text Generation</a> Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yamada2021efficient/">Efficient Passage Retrieval With Hashing For Open-domain Question Answering</a> Ikuya Yamada, Akari Asai, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2021chinese/">Psyqa: A Chinese Dataset For Generating Long Counseling Text For Mental Health Support</a> Hao Sun, Zhenru Lin, Chujie Zheng, Siyang Liu, Minlie Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2021scalable/">Scalable And Efficient Moe Training For Multitask Multilingual Models</a> Young Jin Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/plepi2021context/">Context Transformer With Stacked Pointer Networks For Conversational Question Answering Over Knowledge Graphs</a> Joan Plepi, Endri Kacupaj, Kuldeep Singh, Harsh Thakkar, Jens Lehmann </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/parvez2021retrieval/">Retrieval Augmented Code Generation And Summarization</a> Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-wei Chang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meister2021language/">Language Model Evaluation Beyond Perplexity</a> Clara Meister, Ryan Cotterell </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mao2021unified/">Unipelt: A Unified Framework For Parameter-efficient Language Model Tuning</a> Yuning Mao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2021multi/">Summ^n: A Multi-stage Summarization Framework For Long Input Dialogues And Documents</a> Yusen Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2021token/">Terapipe: Token-level Pipeline Parallelism For Training Large-scale Language Models</a> Zhuohan Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2021machine/">Codexglue: A Machine Learning Benchmark Dataset For Code Understanding And Generation</a> Shuai Lu et al. </li>
     
   
     
       <li> <a href="/publications/li2021supervision/">Supervision Exists Everywhere: A Data Efficient Contrastive Language-image Pre-training Paradigm</a> Yangguang Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dou2021is/">Is GPT-3 Text Indistinguishable From Human Text? Scarecrow: A Framework For Scrutinizing Machine Text</a> Yao Dou, Maxwell Forbes, Rik Koncel-kedziorski, Noah A. Smith, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/thulke2021efficient/">Efficient Retrieval Augmented Generation From Unstructured Knowledge For Task-oriented Dialog</a> David Thulke, Nico Daheim, Christian Dugast, Hermann Ney </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021encoder/">Deltalm: Encoder-decoder Pre-training For Language Generation And Translation By Augmenting Pretrained Multilingual Encoders</a> Shuming Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lelkes2021quiz/">Quiz-style Question Generation For News Stories</a> Adam D. Lelkes, Vinh Q. Tran, Cong Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021replication/">A Replication Study Of Dense Passage Retriever</a> Xueguang Ma, Kai Sun, Ronak Pradeep, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2021few/">Few-shot Conversational Dense Retrieval</a> Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, Zhiyuan Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kayser2021e/">E-vil: A Dataset And Benchmark For Natural Language Explanations In Vision-language Tasks</a> Maxime Kayser et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2022generating/">Re3: Generating Longer Stories With Recursive Reprompting And Revision</a> Kevin Yang, Yuandong Tian, Nanyun Peng, Dan Klein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2022vl/">Vl-checklist: Evaluating Pre-trained Vision-language Models With Objects, Attributes And Relations</a> Tiancheng Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/misra2022enabling/">Minicons: Enabling Flexible Behavioral And Representational Analyses Of Transformer Language Models</a> Kanishka Misra </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2022fast/">Flashattention: Fast And Memory-efficient Exact Attention With Io-awareness</a> Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R√© </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2022unifying/">Unifiedskg: Unifying And Multi-tasking Structured Knowledge Grounding With Text-to-text Language Models</a> Tianbao Xie et al. </li>
     
   
     
       <li> <a href="/publications/khot2022decomposed/">Decomposed Prompting: A Modular Approach For Solving Complex Tasks</a> Tushar Khot et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qian2022controllable/">Controllable Natural Language Generation With Contrastive Prefixes</a> Jing Qian, Li Dong, Yelong Shen, Furu Wei, Weizhu Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2022black/">Black-box Tuning For Language-model-as-a-service</a> Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2022integrating/">3DALL-E: Integrating Text-to-image AI In 3D Design Workflows</a> Vivian Liu, Jo Vermeulen, George Fitzmaurice, Justin Matejka </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhou2022teaching/">Teaching Algorithmic Reasoning Via In-context Learning</a> Hattie Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2022contrastive/">Contrastive Learning With Bidirectional Transformers For Sequential Recommendation</a> Hanwen Du et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dang2022beyond/">Beyond Text Generation: Supporting Writers With Continuous Automatic Text Summaries</a> Hai Dang, Karim Benharrak, Florian Lehmann, Daniel Buschek </li>
     
   
     
       <li> <a href="/publications/sandoval2022lost/">Lost At C: A User Study On The Security Implications Of Large Language Model Code Assistants</a> Gustavo Sandoval et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022complementary/">Dualprompt: Complementary Prompting For Rehearsal-free Continual Learning</a> Zifeng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2022recipe/">Vindlu: A Recipe For Effective Video-and-language Pretraining</a> Feng Cheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/aflalo2022vl/">Vl-interpret: An Interactive Visualization Tool For Interpreting Vision-language Transformers</a> Estelle Aflalo et al. </li>
     
   
     
   
     
       <li> <a href="/publications/nijkamp2022open/">Codegen: An Open Large Language Model For Code With Multi-turn Program Synthesis</a> Erik Nijkamp et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022self/">Self-adaptive In-context Learning: An Information Compression Perspective For In-context Example Selection And Ordering</a> Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022library/">LAVIS: A Library For Language-vision Intelligence</a> Dongxu Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2022dialogue/">Dialfred: Dialogue-enabled Agents For Embodied Instruction Following</a> Xiaofeng Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zan2022continual/">CERT: Continual Pre-training On Sketches For Library-oriented Code Generation</a> Daoguang Zan et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lynch2022interactive/">Interactive Language: Talking To Robots In Real Time</a> Corey Lynch et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022benchmark/">ELEVATER: A Benchmark And Toolkit For Evaluating Language-augmented Visual Models</a> Chunyuan Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bohnet2022attributed/">Attributed Question Answering: Evaluation And Modeling For Attributed Large Language Models</a> Bernd Bohnet et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2022competition/">Competition-level Code Generation With Alphacode</a> Yujia Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/athiwaratkun2022multi/">Multi-lingual Evaluation Of Code Generation Models</a> Ben Athiwaratkun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2022unified/">A Unified Multi-task Learning Framework For Multi-goal Conversational Recommender Systems</a> Yang Deng et al. </li>
     
   
     
       <li> <a href="/publications/ushio2022t/">T-NER: An All-round Python Library For Transformer-based Named Entity Recognition</a> Asahi Ushio, Jose Camacho-collados </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tiong2022plug/">Plug-and-play VQA: Zero-shot VQA By Conjoining Large Pretrained Models With Zero Training</a> Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven C. H. Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/talmor2022commonsenseqa/">Commonsenseqa 2.0: Exposing The Limits Of AI Through Gamification</a> Alon Talmor et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/prasad2022gradient/">Grips: Gradient-free, Edit-based Instruction Search For Prompting Large Language Models</a> Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sarkar2022what/">What Is It Like To Program With Artificial Intelligence?</a> Advait Sarkar et al. </li>
     
   
     
       <li> <a href="/publications/roberts2022scaling/">Scaling Up Models And Data With \(\texttt{t5x}\) And \(\texttt{seqio}\)</a> Adam Roberts et al. </li>
     
   
     
   
     
       <li> <a href="/publications/parisi2022tool/">TALM: Tool Augmented Language Models</a> Aaron Parisi, Yao Zhao, Noah Fiedel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/barei%C3%9F2022code/">Code Generation Tools (almost) For Free? A Study Of Few-shot, Pre-trained Language Models On Code</a> Patrick Barei√ü, Beatriz Souza, Marcelo D'amorim, Michael Pradel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khattab2022demonstrate/">Demonstrate-search-predict: Composing Retrieval And Language Models For Knowledge-intensive NLP</a> Omar Khattab et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bach2022integrated/">Promptsource: An Integrated Development Environment And Repository For Natural Language Prompts</a> Stephen H. Bach et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kang2022knowledge/">KALA: Knowledge-augmented Language Model Adaptation</a> Minki Kang, Jinheon Baek, Sung Ju Hwang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2022towards/">Towards A Unified Multi-dimensional Evaluator For Text Generation</a> Ming Zhong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2022open/">OPT: Open Pre-trained Transformer Language Models</a> Susan Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/miotto2022who/">Who Is GPT-3? An Exploration Of Personality, Values And Demographics</a> Maril√π Miotto, Nicola Rossberg, Bennett Kleinberg </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hou2022learning/">Learning Vector-quantized Item Representation For Transferable Sequential Recommenders</a> Yupeng Hou, Zhankui He, Julian Mcauley, Wayne Xin Zhao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/beurerkellner2022prompting/">Prompting Is Programming: A Query Language For Large Language Models</a> Luca Beurer-kellner, Marc Fischer, Martin Vechev </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2022fine/">FUM: Fine-grained And Fast User Modeling For News Recommendation</a> Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hartvigsen2022large/">Toxigen: A Large-scale Machine-generated Dataset For Adversarial And Implicit Hate Speech Detection</a> Thomas Hartvigsen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sarsa2022automatic/">Automatic Generation Of Programming Exercises And Code Explanations Using Large Language Models</a> Sami Sarsa, Paul Denny, Arto Hellas, Juho Leinonen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/thakur2022benchmarking/">Benchmarking Large Language Models For Automated Verilog RTL Code Generation</a> Shailja Thakur et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/smith2022using/">Using Deepspeed And Megatron To Train Megatron-turing NLG 530B, A Large-scale Generative Language Model</a> Shaden Smith et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022retrieval/">Reacc: A Retrieval-augmented Code Completion Framework</a> Shuai Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022synergizing/">React: Synergizing Reasoning And Acting In Language Models</a> Shunyu Yao et al. </li>
     
   
     
       <li> <a href="/publications/mirowski2022co/">Co-writing Screenplays And Theatre Scripts With Language Models: An Evaluation By Industry Professionals</a> Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, Richard Evans </li>
     
   
     
       <li> <a href="/publications/ramamurthy2022is/">Is Reinforcement Learning (not) For Natural Language Processing: Benchmarks, Baselines, And Building Blocks For Natural Language Policy Optimization</a> Rajkumar Ramamurthy et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/poldrack2023ai/">Ai-assisted Coding: Experiments With GPT-4</a> Russell A Poldrack, Thomas Lu, Ga≈°per Begu≈° </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2023does/">Does Synthetic Data Generation Of Llms Help Clinical Text Mining?</a> Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, Xia Hu </li>
     
   
     
   
     
       <li> <a href="/publications/yang2023teaching/">Gpt4tools: Teaching Large Language Model To Use Tools Via Self-instruction</a> Rui Yang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023chatgpt/">Chatgpt Vs. Google: A Comparative Study Of Search Performance And User Experience</a> Ruiyun Rayna Xu, Yue Katherine Feng, Hailiang Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chew2023llm/">Llm-assisted Content Analysis: Using Large Language Models To Support Deductive Coding</a> Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, Annice Kim </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/omar2023universal/">A Universal Question-answering Platform For Knowledge Graphs</a> Reham Omar, Ishika Dhall, Panos Kalnis, Essam Mansour </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023may/">Starcoder: May The Source Be With You!</a> Raymond Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/es2023automated/">Ragas: Automated Evaluation Of Retrieval Augmented Generation</a> Shahul Es, Jithin James, Luis Espinosa-anke, Steven Schockaert </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/communication2023multilingual/">Seamless: Multilingual Expressive And Streaming Speech Translation</a> Seamless Communication et al. </li>
     
   
     
   
     
       <li> <a href="/publications/amani2023generative/">Generative AI Perceptions: A Survey To Measure The Perceptions Of Faculty, Staff, And Students On Generative AI Tools In Academia</a> Sara Amani et al. </li>
     
   
     
       <li> <a href="/publications/suh2023structured/">Luminate: Structured Generation And Exploration Of Design Space With Large Language Models For Human-ai Co-creation</a> Sangho Suh, Meng Chen, Bryan Min, Toby Jia-jun Li, Haijun Xia </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hao2023augmenting/">Toolkengpt: Augmenting Frozen Language Models With Massive Tools Via Tool Embeddings</a> Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023pre/">Codegeex: A Pre-trained Model For Code Generation With Multilingual Benchmarking On Humaneval-x</a> Qinkai Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023augmenting/">Genegpt: Augmenting Large Language Models With Domain Tools For Improved Access To Biomedical Information</a> Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liao2023designerly/">Designerly Understanding: Information Needs For Model Transparency To Support Design Ideation For Ai-powered User Experience</a> Q. Vera Liao, Hariharan Subramonyam, Jennifer Wang, Jennifer Wortman Vaughan </li>
     
   
     
       <li> <a href="/publications/wu2023enabling/">Autogen: Enabling Next-gen LLM Applications Via Multi-agent Conversation</a> Qingyun Wu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023chat/">Chat-univi: Unified Visual Representation Empowers Large Language Models With Image And Video Understanding</a> Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, Li Yuan </li>
     
   
     
       <li> <a href="/publications/gao2023llama/">Llama-adapter V2: Parameter-efficient Visual Instruction Model</a> Peng Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/owoicho2023exploiting/">Exploiting Simulated User Feedback For Conversational Search: Ranking, Rewriting, And Beyond</a> Paul Owoicho, Ivan Sekuliƒá, Mohammad Aliannejadi, Jeffrey Dalton, Fabio Crestani </li>
     
   
     
   
     
       <li> <a href="/publications/lu2023plug/">Chameleon: Plug-and-play Compositional Reasoning With Large Language Models</a> Pan Lu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khattab2023compiling/">Dspy: Compiling Declarative Language Model Calls Into Self-improving Pipelines</a> Omar Khattab et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/m%C3%BCndler2023self/">Self-contradictory Hallucinations Of Large Language Models: Evaluation, Detection And Mitigation</a> Niels M√ºndler, Jingxuan He, Slobodan Jenko, Martin Vechev </li>
     
   
     
       <li> <a href="/publications/ding2023enhancing/">Enhancing Chat Language Models By Scaling High-quality Instructional Conversations</a> Ning Ding et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/motlagh2023impact/">The Impact Of Artificial Intelligence On The Evolution Of Digital Education: A Comparative Study Of Openai Text Generation Tools Including Chatgpt, Bing Chat, Bard, And Ernie</a> Negin Yazdani Motlagh, Matin Khajavi, Abbas Sharifi, Mohsen Ahmadi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dechoudhury2023benefits/">Benefits And Harms Of Large Language Models In Digital Mental Health</a> Munmun De Choudhury, Sachin R. Pendse, Neha Kumar </li>
     
   
     
       <li> <a href="/publications/khattak2023self/">Self-regulating Prompts: Foundational Model Adaptation Without Forgetting</a> Muhammad Uzair Khattak et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/reza2023rapid/">Abscribe: Rapid Exploration & Organization Of Multiple Writing Variations In Human-ai Co-writing Tasks Using Large Language Models</a> Mohi Reza et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023api/">Api-bank: A Comprehensive Benchmark For Tool-augmented Llms</a> Minghao Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/orenstrakh2023detecting/">Detecting Llm-generated Text In Computing Education: A Comparative Study For Chatgpt Cases</a> Michael Sheinman Orenstrakh, Oscar Karnalim, Carlos Anibal Suarez, Michael Liut </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023scaffolding/">Selenite: Scaffolding Online Sensemaking With Comprehensive Overviews Elicited From Large Language Models</a> Michael Xieyang Liu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/arefeen2023cost/">Leancontext: Cost-efficient Domain-specific Question Answering Using Llms</a> Md Adnan Arefeen, Biplob Debnath, Srimat Chakradhar </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sch%C3%A4fer2023empirical/">An Empirical Evaluation Of Using Large Language Models For Automated Unit Test Generation</a> Max Sch√§fer, Sarah Nadi, Aryaz Eghbali, Frank Tip </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cosler2023interactively/">Nl2spec: Interactively Translating Unstructured Natural Language To Temporal Logics With Large Language Models</a> Matthias Cosler, Christopher Hahn, Daniel Mendoza, Frederik Schmitt, Caroline Trippel </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/patil2023large/">Gorilla: Large Language Model Connected With Massive Apis</a> Shishir G. Patil, Tianjun Zhang, Xin Wang, Joseph E. Gonzalez </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yan2023generative/">Generative Artificial Intelligence In Learning Analytics: Contextualising Opportunities And Challenges Through The Learning Analytics Cycle</a> Lixiang Yan, Roberto Martinez-maldonado, Dragan Ga≈°eviƒá </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023reasoning/">Reasoning On Graphs: Faithful And Interpretable Large Language Model Reasoning</a> Linhao Luo, Yuan-fang Li, Gholamreza Haffari, Shirui Pan </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2023logic/">Logic-lm: Empowering Large Language Models With Symbolic Solvers For Faithful Logical Reasoning</a> Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang </li>
     
   
     
       <li> <a href="/publications/zheng2023judging/">Judging Llm-as-a-judge With Mt-bench And Chatbot Arena</a> Lianmin Zheng et al. </li>
     
   
     
       <li> <a href="/publications/xu2023comprehensive/">Superclue: A Comprehensive Chinese Large Language Model Benchmark</a> Liang Xu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guan2023leveraging/">Leveraging Pre-trained Large Language Models To Construct And Utilize World Models For Model-based Task Planning</a> Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023comprehensive/">Mvbench: A Comprehensive Multi-modal Video Understanding Benchmark</a> Kunchang Li et al. </li>
     
   
     
       <li> <a href="/publications/maaz2023video/">Video-chatgpt: Towards Detailed Video Understanding Via Large Vision And Language Models</a> Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023human/">VISAR: A Human-ai Argumentative Writing Assistant With Visual Programming And Rapid Draft Prototyping</a> Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, Toby Jia-jun Li </li>
     
   
     
       <li> <a href="/publications/yin2023language/">LAMM: Language-assisted Multi-modal Instruction-tuning Dataset, Framework, And Benchmark</a> Zhenfei Yin et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yager2023domain/">Domain-specific Chatbots For Science Using Embeddings</a> Kevin G. Yager </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2023effective/">Tallrec: An Effective And Efficient Tuning Framework To Align Large Language Model With Recommendation</a> Keqin Bao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/alizadeh2023llm/">LLM In A Flash: Efficient Large Language Model Inference With Limited Memory</a> Keivan Alizadeh et al. </li>
     
   
     
       <li> <a href="/publications/pandya2023automating/">Automating Customer Service Using Langchain: Building Custom Open-source GPT Chatbot For Organizations</a> Keivalya Pandya, Mehfuza Holia </li>
     
   
     
   
     
       <li> <a href="/publications/collins2023evaluating/">Evaluating Language Models For Mathematics Through Interactions</a> Katherine M. Collins et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/roth2023waffling/">Waffling Around For Performance: Visual Classification With Random Words And Broad Concepts</a> Karsten Roth et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chang2023how/">Chipgpt: How Far Are We From Natural Language Hardware Design</a> Kaiyan Chang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chang2023speechprompt/">Speechprompt V2: Prompt Tuning For Speech Classification Tasks</a> Kai-wei Chang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahuja2023multilingual/">MEGA: Multilingual Evaluation Of Generative AI</a> Kabir Ahuja et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2023llama/">Llama-reviewer: Advancing Code Review Automation With Large Language Models Through Parameter-efficient Fine-tuning</a> Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, Chun Zuo </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2023recommendation/">Recommendation As Instruction Following: A Large Language Model Empowered Recommendation Approach</a> Junjie Zhang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kasai2023evaluating/">Evaluating GPT-4 And Chatgpt On Japanese Medical Licensing Examinations</a> Jungo Kasai, Yuhei Kasai, Keisuke Sakaguchi, Yutaro Yamada, Dragomir Radev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023general/">Structgpt: A General Framework For Large Language Model To Reason Over Structured Data</a> Jinhao Jiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023retrieval/">Rella: Retrieval-enhanced Large Language Models For Lifelong Sequential Behavior Comprehension In Recommendation</a> Jianghao Lin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/han2023one/">Onellm: One Framework To Align All Modalities With Language</a> Jiaming Han et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2023graph/">Graphgpt: Graph Instruction Tuning For Large Language Models</a> Jiabin Tang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023evaluate/">Gptscore: Evaluate As You Desire</a> Jinlan Fu, See-kiong Ng, Zhengbao Jiang, Pengfei Liu </li>
     
   
     
   
     
       <li> <a href="/publications/haase2023artificial/">Artificial Muses: Generative Artificial Intelligence Chatbots Have Risen To Human-level Creativity</a> Jennifer Haase, Paul H. P. Hanel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gou2023large/">CRITIC: Large Language Models Can Self-correct With Tool-interactive Critiquing</a> Zhibin Gou et al. </li>
     
   
     
   
     
       <li> <a href="/publications/arawjo2023visual/">Chainforge: A Visual Toolkit For Prompt Engineering And LLM Hypothesis Testing</a> Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, Elena Glassman </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lauren%C3%A7on2023bigscience/">The Bigscience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset</a> Hugo Lauren√ßon et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023pre/">Missrec: Pre-training And Transferring Multi-modal Interest-aware Sequence Representation For Recommendation</a> Jinpeng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023generate/">Chatkbqa: A Generate-then-retrieve Framework For Knowledge Base Question Answering With Fine-tuned Large Language Models</a> Haoran Luo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023audioldm/">Audioldm 2: Learning Holistic Audio Generation With Self-supervised Pretraining</a> Haohe Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/shao2023closed/">Lmdrive: Closed-loop End-to-end Driving With Large Language Models</a> Hao Shao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fei2023reasoning/">Reasoning Implicit Sentiment With Chain-of-thought Prompting</a> Hao Fei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/todd2023level/">Level Generation Through Large Language Models</a> Graham Todd, Sam Earle, Muhammad Umair Nasir, Michael Cerny Green, Julian Togelius </li>
     
   
     
       <li> <a href="/publications/mialon2023augmented/">Augmented Language Models: A Survey</a> Gr√©goire Mialon et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2023chatgpt/">Chatgpt Chemistry Assistant For Text Mining And Prediction Of MOF Synthesis</a> Zhiling Zheng, Oufan Zhang, Christian Borgs, Jennifer T. Chayes, Omar M. Yaghi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023repository/">Repocoder: Repository-level Code Completion Through Iterative Retrieval And Generation</a> Fengji Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mollick2023assigning/">Assigning AI: Seven Approaches For Students, With Prompts</a> Ethan Mollick, Lilach Mollick </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023chatgpt/">Gptutor: A Chatgpt-powered Programming Tool For Code Explanation</a> Eason Chen, Ray Huang, Han-shin Chen, Yuen-hsien Tseng, Liang-yi Li </li>
     
   
     
       <li> <a href="/publications/sur%C3%ADs2023visual/">Vipergpt: Visual Inference Via Python Execution For Reasoning</a> D√≠dac Sur√≠s, Sachit Menon, Carl Vondrick </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2023llm/">Llm-blender: Ensembling Large Language Models With Pairwise Ranking And Generative Fusion</a> Dongfu Jiang, Xiang Ren, Bill Yuchen Lin </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/paul2023reasoning/">REFINER: Reasoning Feedback On Intermediate Representations</a> Debjit Paul et al. </li>
     
   
     
       <li> <a href="/publications/nam2023using/">Using An LLM To Help With Code Understanding</a> Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, Brad Myers </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/west2023ai/">AI And The FCI: Can Chatgpt Project An Understanding Of Introductory Physics?</a> Colin G. West </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/small2023opportunities/">Opportunities And Risks Of Llms For Scalable Deliberation With Polis</a> Christopher T. Small et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sima2023driving/">Drivelm: Driving With Graph Visual Question Answering</a> Chonghao Sima et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chan2023towards/">Chateval: Towards Better Llm-based Evaluators Through Multi-agent Debate</a> Chi-min Chan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/deng2023foundation/">K2: A Foundation Language Model For Geoscience Knowledge Understanding And Utilization</a> Cheng Deng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/rastogi2023supporting/">Supporting Human-ai Collaboration In Auditing Llms With Llms</a> Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, Harsha Nori, Saleema Amershi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/qian2023communicative/">Chatdev: Communicative Agents For Software Development</a> Chen Qian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tony2023dataset/">Llmseceval: A Dataset Of Natural Language Prompts For Security Evaluations</a> Catherine Tony, Markus Mutas, Nicol√°s E. D√≠az Ferreyra, Riccardo Scandariato </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023empowering/">LLM+P: Empowering Large Language Models With Optimal Planning Proficiency</a> Bo Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/paranjape2023automatic/">ART: Automatic Multi-step Reasoning And Tool-use For Large Language Models</a> Bhargavi Paranjape et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2023batch/">Batch Prompting: Efficient Inference With Large Language Model Apis</a> Zhoujun Cheng, Jungo Kasai, Tao Yu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sharma2023facilitating/">Facilitating Self-guided Mental Health Interventions Through Human-language Model Interaction: A Case Study Of Cognitive Restructuring</a> Ashish Sharma, Kevin Rushton, Inna Wanyin Lin, Theresa Nguyen, Tim Althoff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bran2023augmenting/">Chemcrow: Augmenting Large-language Models With Chemistry Tools</a> Andres M Bran et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023generative/">On Generative Agents In Recommendation</a> An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, Tat-seng Chua </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/awadalla2023open/">Openflamingo: An Open-source Framework For Training Large Autoregressive Vision-language Models</a> Anas Awadalla et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/khademi2023can/">Can Chatgpt And Bard Generate Aligned Assessment Items? A Reliability Analysis Against Human Performance</a> Abdolvahab Khademi </li>
     
   
     
       <li> <a href="/publications/kocaballi2023conversational/">Conversational Ai-powered Design: Chatgpt As Designer, User, And Product</a> A. Baki Kocaballi </li>
     
   
     
       <li> <a href="/publications/he2023exploring/">Exploring Human-like Translation Strategy With Large Language Models</a> Zhiwei He et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guan2023mitigating/">Mitigating Large Language Model Hallucinations Via Autonomous Knowledge Graph-based Retrofitting</a> Xinyan Guan et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023rethinking/">Rethinking The Evaluation For Conversational Recommendation In The Era Of Large Language Models</a> Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, Ji-rong Wen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liang2023can/">Can Large Language Models Provide Useful Feedback On Research Papers? A Large-scale Empirical Analysis</a> Weixin Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2023mm/">Mm-vet: Evaluating Large Multimodal Models For Integrated Capabilities</a> Weihao Yu et al. </li>
     
   
     
       <li> <a href="/publications/wei2023large/">Llmrec: Large Language Models With Graph Augmentation For Recommendation</a> Wei Wei et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sadasivan2023can/">Can Ai-generated Text Be Reliably Detected?</a> Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi </li>
     
   
     
       <li> <a href="/publications/dibia2023tool/">LIDA: A Tool For Automatic Generation Of Grammar-agnostic Visualizations And Infographics Using Large Language Models</a> Victor Dibia </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liventsev2023fully/">Fully Autonomous Programming With Large Language Models</a> Vadim Liventsev, Anastasiia Grishina, Aki H√§rm√§, Leon Moonen </li>
     
   
     
   
     
       <li> <a href="/publications/bezirhan2023automated/">Automated Reading Passage Generation With Openai's Large Language Model</a> Ummugul Bezirhan, Matthias Von Davier </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phung2023automating/">Automating Human Tutor-style Programming Feedback: Leveraging GPT-4 Tutor Model For Hint Generation And GPT-3.5 Student Model For Hint Validation</a> Tung Phung et al. </li>
     
   
     
   
     
       <li> <a href="/publications/phung2023generative/">Generative AI For Programming Education: Benchmarking Chatgpt, GPT-4, And Human Tutors</a> Tung Phung et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lai2023psy/">Psy-llm: Scaling Up Global Mental Health Psychological Services With Ai-based Large Language Models</a> Tin Lai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schick2023language/">Toolformer: Language Models Can Teach Themselves To Use Tools</a> Timo Schick et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liang2023encouraging/">Encouraging Divergent Thinking In Large Language Models Through Multi-agent Debate</a> Tian Liang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023better/">Better To Ask In English: Cross-lingual Evaluation Of Large Language Models For Healthcare Queries</a> Yiqiao Jin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2023can/">Can Chatgpt Replace Traditional KBQA Models? An In-depth Analysis Of The Question Answering Performance Of The GPT LLM Family</a> Yiming Tan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wen2023knowledge/">Mindmap: Knowledge Graph Prompting Sparks Graph Of Thoughts In Large Language Models</a> Yilin Wen, Zifeng Wang, Jimeng Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bang2023multimodal/">A Multitask, Multilingual, Multimodal Evaluation Of Chatgpt On Reasoning, Hallucination, And Interactivity</a> Yejin Bang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2023beyond/">Beyond Chain-of-thought, Effective Graph-of-thought Reasoning In Language Models</a> Yao Yao, Zuchao Li, Hai Zhao </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023enhanced/">Llavar: Enhanced Visual Instruction Tuning For Text-rich Image Understanding</a> Yanzhe Zhang et al. </li>
     
   
     
       <li> <a href="/publications/li2023llama/">Llama-vid: An Image Is Worth 2 Tokens In Large Language Models</a> Yanwei Li, Chengyao Wang, Jiaya Jia </li>
     
   
     
   
     
       <li> <a href="/publications/dubois2023simulation/">Alpacafarm: A Simulation Framework For Methods That Learn From Human Feedback</a> Yann Dubois et al. </li>
     
   
     
   
     
       <li> <a href="/publications/feng2023chatting/">Chatpose: Chatting About 3D Human Pose</a> Yao Feng et al. </li>
     
   
     
       <li> <a href="/publications/liu2023g/">G-eval: NLG Evaluation Using GPT-4 With Better Human Alignment</a> Yang Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2023large/">Recmind: Large Language Model Powered Agent For Recommendation</a> Yancheng Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ding2023integrating/">Integrating Action Knowledge And Llms For Task Planning And Situation Handling In Open Worlds</a> Yan Ding et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2023can/">Can Chatgpt Pass The Vietnamese National High School Graduation Examination?</a> Xuan-quy Dao, Ngoc-bich Le, Xuan-dung Phan, Bac-bien Ngo </li>
     
   
     
   
     
       <li> <a href="/publications/zang2023contextual/">Contextual Object Detection With Multimodal Large Language Models</a> Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, Chen Change Loy </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dan2023large/">Educhat: A Large-scale Language Model-based Chatbot System For Intelligent Education</a> Yuhao Dan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhuang2023dataset/">Toolqa: A Dataset For LLM Question Answering With External Tools</a> Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, Chao Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2023towards/">Gpt4aigchip: Towards Next-generation AI Accelerator Design Automation Via Large Language Models</a> Yonggan Fu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2023chat/">Chat-rec: Towards Interactive And Explainable Llms-augmented Recommender System</a> Yunfan Gao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qin2023facilitating/">Toolllm: Facilitating Large Language Models To Master 16000+ Real-world Apis</a> Yujia Qin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023how/">How Far Can Camels Go? Exploring The State Of Instruction Tuning On Open Resources</a> Yizhong Wang et al. </li>
     
   
     
       <li> <a href="/publications/ren2023representation/">Representation Learning With Large Language Models For Recommendation</a> Xubin Ren et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/herbold2023write/">AI, Write An Essay For Me: A Large-scale Comparison Of Human-written Versus Chatgpt-generated Essays</a> Steffen Herbold, Annette Hautli-janisz, Ute Heuer, Zlata Kikteva, Alexander Trautsch </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/biderman2023suite/">Pythia: A Suite For Analyzing Large Language Models Across Training And Scaling</a> Stella Biderman et al. </li>
     
   
     
       <li> <a href="/publications/dao2023performance/">Performance Comparison Of Large Language Models On VNHSGE English Dataset: Openai Chatgpt, Microsoft Bing Chat, And Google Bard</a> Xuan-quy Dao </li>
     
   
     
       <li> <a href="/publications/hong2023meta/">Metagpt: Meta Programming For A Multi-agent Collaborative Framework</a> Sirui Hong et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ott2023central/">Thoughtsource: A Central Hub For Large Language Model Reasoning Data</a> Simon Ott et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023radiology/">R2gengpt: Radiology Report Generation With Frozen Llms</a> Zhanyu Wang, Lingqiao Liu, Lei Wang, Luping Zhou </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023c/">C-eval: A Multi-level Multi-discipline Chinese Evaluation Suite For Foundation Models</a> Yuzhen Huang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/abbasiantaeb2023let/">Let The Llms Talk: Simulating Human-to-human Conversational QA Via Zero-shot Llm-to-llm Interactions</a> Zahra Abbasiantaeb, Yifei Yuan, Evangelos Kanoulas, Mohammad Aliannejadi </li>
     
   
     
       <li> <a href="/publications/wen2023hard/">Hard Prompts Made Easy: Gradient-based Discrete Optimization For Prompt Tuning And Discovery</a> Yuxin Wen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wei2023copiloting/">Copiloting The Copilots: Fusing Large Language Models With Completion Engines For Automated Program Repair</a> Yuxiang Wei, Chunqiu Steven Xia, Lingming Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2024large/">Large Language Models Meet Collaborative Filtering: An Efficient All-round Llm-based Recommender System</a> Sein Kim et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/khojah2024beyond/">Beyond Code Generation: An Observational Study Of Chatgpt Usage In Software Engineering Practice</a> Ranim Khojah, Mazen Mohamad, Philipp Leitner, Francisco Gomes De Oliveira Neto </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bassner2024ai/">Iris: An Ai-driven Virtual Tutor For Computer Science Education</a> Patrick Bassner, Eduard Frankford, Stephan Krusche </li>
     
   
     
       <li> <a href="/publications/dhillon2024shaping/">Shaping Human-ai Collaboration: Varied Scaffolding Levels In Co-writing With Language Models</a> Paramveer S. Dhillon et al. </li>
     
   
     
       <li> <a href="/publications/shaer2024ai/">Ai-augmented Brainwriting: Investigating The Use Of Llms In Group Ideation</a> Orit Shaer, Angelora Cooper, Osnat Mokryn, Andrew L. Kun, Hagit Ben Shoshan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hedderich2024piece/">A Piece Of Theatre: Investigating How Teachers Design LLM Chatbots To Assist Adolescent Cyberbullying Education</a> Michael A. Hedderich et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ramos2024review/">A Review Of Large Language Models And Autonomous Agents In Chemistry</a> Mayk Caldas Ramos, Christopher J. Collison, Andrew D. White </li>
     
   
     
   
     
       <li> <a href="/publications/dahl2024large/">Large Legal Fictions: Profiling Legal Hallucinations In Large Language Models</a> Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E. Ho </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kazemitabaar2024evaluating/">Codeaid: Evaluating A Classroom Deployment Of An Llm-based Programming Assistant That Balances Student And Educator Needs</a> Majeed Kazemitabaar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2024boosting/">Boosting Continual Learning Of Vision-language Models Via Mixture-of-experts Adapters</a> Jiazuo Yu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cheong2024am/">(A)I Am Not A Lawyer, But...: Engaging Legal Experts Towards Responsible LLM Policies For Legal Advice</a> Inyoung Cheong, King Xia, K. J. Kevin Feng, Quan Ze Chen, Amy X. Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lei2024materials/">Materials Science In The Era Of Large Language Models: A Perspective</a> Ge Lei, Ronan Docherty, Samuel J. Cooper </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frankford2024ai/">Ai-tutoring In Software Engineering Education</a> Eduard Frankford, Clemens Sauerwein, Patrick Bassner, Stephan Krusche, Ruth Breu </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2024chemical/">Chemllm: A Chemical Large Language Model</a> Di Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2024deepseek/">Deepseek-coder: When The Large Language Model Meets Programming -- The Rise Of Code Intelligence</a> Daya Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/anderson2024homogenization/">Homogenization Effects Of Large Language Models On Human Creative Ideation</a> Barrett R. Anderson, Jash Hemant Shah, Max Kreminski </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2024unified/">Llamafactory: Unified Efficient Fine-tuning Of 100+ Language Models</a> Yaowei Zheng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chiang2024chatbot/">Chatbot Arena: An Open Platform For Evaluating Llms By Human Preference</a> Wei-lin Chiang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/glm2024family/">Chatglm: A Family Of Large Language Models From GLM-130B To GLM-4 All Tools</a> Team Glm et al. </li>
     
   
     
       <li> <a href="/publications/tu2024towards/">Towards Conversational Diagnostic AI</a> Tao Tu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2024quality/">Quality Of Answers Of Generative Large Language Models Vs Peer Patients For Interpreting Lab Test Results For Lay Patients: Evaluation Study</a> Zhe He et al. </li>
     
   
     
       <li> <a href="/publications/chen2024how/">How Far Are We To GPT-4V? Closing The Gap To Commercial Multimodal Models With Open-source Suites</a> Zhe Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Training Techniques <a id="Training Techniques"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2016latent/">Latent Attention For If-then Program Synthesis</a> Xinyun Chen, Chang Liu, Richard Shin, Dawn Song, Mingcheng Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/stahlberg2018simple/">Simple Fusion: Return Of The Language Model</a> Felix Stahlberg, James Cross, Veselin Stoyanov </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019enabling/">Enabling Robots To Understand Incomplete Natural Language Instructions Using Commonsense Reasoning</a> Haonan Chen, Hao Tan, Alan Kuntz, Mohit Bansal, Ron Alterovitz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019gmail/">Gmail Smart Compose: Real-time Assisted Writing</a> Mia Xu Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2019transformers/">Transformers Without Tears: Improving The Normalization Of Self-attention</a> Toan Q. Nguyen, Julian Salazar </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/duckworth2019parallel/">Parallel Scheduled Sampling</a> Daniel Duckworth, Arvind Neelakantan, Ben Goodrich, Lukasz Kaiser, Samy Bengio </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/petroni2019language/">Language Models As Knowledge Bases?</a> Fabio Petroni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lan2019lite/">ALBERT: A Lite BERT For Self-supervised Learning Of Language Representations</a> Zhenzhong Lan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kong2019adversarial/">An Adversarial Approach To High-quality, Sentiment-controlled Neural Dialogue Generation</a> Xiang Kong, Bohan Li, Graham Neubig, Eduard Hovy, Yiming Yang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shazeer2019fast/">Fast Transformer Decoding: One Write-head Is All You Need</a> Noam Shazeer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mihaylova2019scheduled/">Scheduled Sampling For Transformers</a> Tsvetomila Mihaylova, Andr√© F. T. Martins </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/voskarides2020query/">Query Resolution For Conversational Search With Limited Supervision</a> Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, Maarten De Rijke </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yang2020generative/">Generative Data Augmentation For Commonsense Reasoning</a> Yiben Yang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vanbiljon2020optimal/">On Optimal Transformer Depth For Low-resource Language Translation</a> Elan Van Biljon, Arnu Pretorius, Julia Kreutzer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/puri2020training/">Training Question Answering Models From Synthetic Data</a> Raul Puri, Ryan Spring, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020rethinking/">Rethinking The Value Of Transformer Components</a> Wenxuan Wang, Zhaopeng Tu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/araabi2020optimizing/">Optimizing Transformer For Low-resource Neural Machine Translation</a> Ali Araabi, Christof Monz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/roberts2020how/">How Much Knowledge Can You Pack Into The Parameters Of A Language Model?</a> Adam Roberts, Colin Raffel, Noam Shazeer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2020contrastive/">Contrastive Triple Extraction With Generative Transformer</a> Hongbin Ye et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cai2020data/">Data Manipulation: Towards Effective Instance Learning For Neural Dialogue Generation Via Learning To Augment And Reweight</a> Hengyi Cai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vari%C5%A12021sequence/">Sequence Length Is A Domain: Length-based Overfitting In Transformer Models</a> Du≈°an Vari≈°, Ond≈ôej Bojar </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/narayanan2021efficient/">Efficient Large-scale Language Model Training On GPU Clusters Using Megatron-lm</a> Deepak Narayanan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/santhanam2021effective/">Colbertv2: Effective And Efficient Retrieval Via Lightweight Late Interaction</a> Keshav Santhanam, Omar Khattab, Jon Saad-falcon, Christopher Potts, Matei Zaharia </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kocetkov2022tb/">The Stack: 3 TB Of Permissively Licensed Source Code</a> Denis Kocetkov et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bavarian2022efficient/">Efficient Training Of Language Models To Fill In The Middle</a> Mohammad Bavarian et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shen2023mixture/">Mixture-of-experts Meets Instruction Tuning:a Winning Combination For Large Language Models</a> Sheng Shen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/todd2023level/">Level Generation Through Large Language Models</a> Graham Todd, Sam Earle, Muhammad Umair Nasir, Michael Cerny Green, Julian Togelius </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023transforming/">NL2TL: Transforming Natural Languages To Temporal Logics Using Large Language Models</a> Yongchao Chen, Rujul Gandhi, Yang Zhang, Chuchu Fan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/groeneveld2024accelerating/">Olmo: Accelerating The Science Of Language Models</a> Dirk Groeneveld et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Transformer <a id="Transformer"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2016end/">End-to-end Answer Chunk Extraction And Ranking For Reading Comprehension</a> Yang Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/meng2016interactive/">Interactive Attention For Neural Machine Translation</a> Fandong Meng, Zhengdong Lu, Hang Li, Qun Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/seo2016bidirectional/">Bidirectional Attention Flow For Machine Comprehension</a> Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/firat2016multi/">Multi-way, Multilingual Neural Machine Translation With A Shared Attention Mechanism</a> Orhan Firat, Kyunghyun Cho, Yoshua Bengio </li>
     
   
     
       <li> <a href="/publications/caglayan2016multimodal/">Multimodal Attention For Neural Machine Translation</a> Ozan Caglayan, Lo√Øc Barrault, Fethi Bougares </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xing2016topic/">Topic Aware Neural Response Generation</a> Chen Xing et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2016context/">A Context-aware Attention Network For Interactive Question Answering</a> Huayu Li, Martin Renqiang Min, Yong Ge, Asim Kadav </li>
     
   
     
       <li> <a href="/publications/liu2016neural/">Neural Machine Translation With Supervised Attention</a> Lemao Liu, Masao Utiyama, Andrew Finch, Eiichiro Sumita </li>
     
   
     
   
     
       <li> <a href="/publications/sordoni2016iterative/">Iterative Alternating Neural Attention For Machine Reading</a> Alessandro Sordoni, Philip Bachman, Adam Trischler, Yoshua Bengio </li>
     
   
     
       <li> <a href="/publications/eriguchi2016tree/">Tree-to-sequence Attentional Neural Machine Translation</a> Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2017table/">Table-to-text Generation By Structure-aware Seq2seq Learning</a> Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, Zhifang Sui </li>
     
   
     
       <li> <a href="/publications/hu2017reinforced/">Reinforced Mnemonic Reader For Machine Reading Comprehension</a> Minghao Hu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chaplot2017gated/">Gated-attention Architectures For Task-oriented Language Grounding</a> Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ahmed2017weighted/">Weighted Transformer Network For Machine Translation</a> Karim Ahmed, Nitish Shirish Keskar, Richard Socher </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hieber2017toolkit/">Sockeye: A Toolkit For Neural Machine Translation</a> Felix Hieber et al. </li>
     
   
     
       <li> <a href="/publications/daniluk2017frustratingly/">Frustratingly Short Attention Spans In Neural Language Modeling</a> Micha≈Ç Daniluk, Tim Rockt√§schel, Johannes Welbl, Sebastian Riedel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vaswani2017attention/">Attention Is All You Need</a> Ashish Vaswani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2017syntax/">Syntax-directed Attention For Neural Machine Translation</a> Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2017flexible/">Flexible And Creative Chinese Poetry Generation Using Neural Memory</a> Jiyuan Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/delbrouck2017multimodal/">Multimodal Compact Bilinear Pooling For Multimodal Neural Machine Translation</a> Jean-benoit Delbrouck, Stephane Dupont </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2017best/">Best Of Both Worlds: Transferring Knowledge From Discriminative Learning To A Generative Visual Dialog Model</a> Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2017are/">Are You Talking To Me? Reasoned Visual Dialog Generation Through Adversarial Learning</a> Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, Anton Van Den Hengel </li>
     
   
     
       <li> <a href="/publications/gu2017non/">Non-autoregressive Neural Machine Translation</a> Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, Richard Socher </li>
     
   
     
       <li> <a href="/publications/libovick%C3%BD2017attention/">Attention Strategies For Multi-source Sequence-to-sequence Learning</a> Jind≈ôich Libovick√Ω, Jind≈ôich Helcl </li>
     
   
     
   
     
       <li> <a href="/publications/shao2017generating/">Generating High-quality And Informative Conversation Responses With Sequence-to-sequence Models</a> Louis Shao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/devlin2018pre/">BERT: Pre-training Of Deep Bidirectional Transformers For Language Understanding</a> Jacob Devlin, Ming-wei Chang, Kenton Lee, Kristina Toutanova </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dziri2018augmenting/">Augmenting Neural Response Generation With Context-aware Topical Attention</a> Nouha Dziri, Ehsan Kamalloo, Kory W. Mathewson, Osmar Zaiane </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vlasov2018few/">Few-shot Generalization Across Dialogue Tasks</a> Vladimir Vlasov, Akela Drissner-schmid, Alan Nichol </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gr%C3%B6nroos2018memad/">The Memad Submission To The WMT18 Multimodal Translation Task</a> Stig-arne Gr√∂nroos et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fan2018hierarchical/">Hierarchical Neural Story Generation</a> Angela Fan, Mike Lewis, Yann Dauphin </li>
     
   
     
       <li> <a href="/publications/kim2018multimodal/">Multimodal Dual Attention Memory For Video Story Question Answering</a> Kyung-min Kim, Seong-ho Choi, Jin-hwa Kim, Byoung-tak Zhang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2018contextualized/">Sdnet: Contextualized Attention-based Deep Network For Conversational Question Answering</a> Chenguang Zhu, Michael Zeng, Xuedong Huang </li>
     
   
     
       <li> <a href="/publications/madotto2018effectively/">Mem2seq: Effectively Incorporating Knowledge Bases Into End-to-end Task-oriented Dialog Systems</a> Andrea Madotto, Chien-sheng Wu, Pascale Fung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2018task/">A Task In A Suit And A Tie: Paraphrase Generation With Semantic Augmentation</a> Su Wang, Rahul Gupta, Nancy Chang, Jason Baldridge </li>
     
   
     
       <li> <a href="/publications/popel2018training/">Training Tips For The Transformer Model</a> Martin Popel, Ond≈ôej Bojar </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2018end/">Seq2rdf: An End-to-end Application For Deriving Triples From Natural Language Text</a> Yue Liu, Tongtao Zhang, Zhicheng Liang, Heng Ji, Deborah L. Mcguinness </li>
     
   
     
   
     
       <li> <a href="/publications/zhong2018affect/">An Affect-rich Neural Conversational Model With Biased Attention And Weighted Cross-entropy Loss</a> Peixiang Zhong, Di Wang, Chunyan Miao </li>
     
   
     
       <li> <a href="/publications/bapna2018training/">Training Deeper Neural Machine Translation Models With Transparent Attention</a> Ankur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao, Yonghui Wu </li>
     
   
     
   
     
       <li> <a href="/publications/fan2018can/">"bilingual Expert" Can Find Translation Errors</a> Kai Fan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2018best/">The Best Of Both Worlds: Combining Recent Advances In Neural Machine Translation</a> Mia Xu Chen et al. </li>
     
   
     
       <li> <a href="/publications/kitaev2018multilingual/">Multilingual Constituency Parsing With Self-attention And Pre-training</a> Nikita Kitaev, Steven Cao, Dan Klein </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dou2018exploiting/">Exploiting Deep Representations For Neural Machine Translation</a> Zi-yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, Tong Zhang </li>
     
   
     
       <li> <a href="/publications/yu2018combining/">Qanet: Combining Local Convolution With Global Self-attention For Reading Comprehension</a> Adams Wei Yu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bauer2018commonsense/">Commonsense For Generative Multi-hop Question Answering Tasks</a> Lisa Bauer, Yicheng Wang, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/alrfou2018character/">Character-level Language Modeling With Deeper Self-attention</a> Rami Al-rfou, Dokook Choe, Noah Constant, Mandy Guo, Llion Jones </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shuster2018engaging/">Engaging Image Captioning Via Personality</a> Kurt Shuster, Samuel Humeau, Hexiang Hu, Antoine Bordes, Jason Weston </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2018multi/">Multi-cast Attention Networks For Retrieval-based Question Answering And Response Prediction</a> Yi Tay, Luu Anh Tuan, Siu Cheung Hui </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vaswani2018neural/">Tensor2tensor For Neural Machine Translation</a> Ashish Vaswani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/niu2018recursive/">Recursive Visual Attention In Visual Dialog</a> Yulei Niu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/choi2018fine/">Fine-grained Attention Mechanism For Neural Machine Translation</a> Heeyoul Choi, Kyunghyun Cho, Yoshua Bengio </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2018improving/">Improving The Transformer Translation Model With Document-level Context</a> Jiacheng Zhang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wen2018sequence/">Sequence-to-sequence Learning For Task-oriented Dialogue With Dialogue State Representation</a> Haoyang Wen, Yijia Liu, Wanxiang Che, Libo Qin, Ting Liu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/garg2019transfer/">TANDA: Transfer And Adapt Pre-trained Transformer Models For Answer Sentence Selection</a> Siddhant Garg, Thuy Vu, Alessandro Moschitti </li>
     
   
     
       <li> <a href="/publications/vanaken2019how/">How Does BERT Answer Questions? A Layer-wise Analysis Of Transformer Representations</a> Betty Van Aken, Benjamin Winter, Alexander L√∂ser, Felix A. Gers </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rothe2019leveraging/">Leveraging Pre-trained Checkpoints For Sequence Generation Tasks</a> Sascha Rothe, Shashi Narayan, Aliaksei Severyn </li>
     
   
     
   
     
       <li> <a href="/publications/hoover2019visual/">Exbert: A Visual Analysis Tool To Explore Learned Representations In Transformers Models</a> Benjamin Hoover, Hendrik Strobelt, Sebastian Gehrmann </li>
     
   
     
       <li> <a href="/publications/lu2019pretraining/">Vilbert: Pretraining Task-agnostic Visiolinguistic Representations For Vision-and-language Tasks</a> Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee </li>
     
   
     
       <li> <a href="/publications/zhang2019synchronous/">Synchronous Bidirectional Inference For Neural Sequence Generation</a> Jiajun Zhang, Long Zhou, Yang Zhao, Chengqing Zong </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2019robust/">Robust Neural Machine Translation With Doubly Adversarial Inputs</a> Yong Cheng, Lu Jiang, Wolfgang Macherey </li>
     
   
     
       <li> <a href="/publications/cornia2019training/">Smart: Training Shallow Memory-aware Transformers For Robotic Explainability</a> Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara </li>
     
   
     
   
     
       <li> <a href="/publications/dathathri2019plug/">Plug And Play Language Models: A Simple Approach To Controlled Text Generation</a> Sumanth Dathathri et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shoeybi2019megatron/">Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism</a> Mohammad Shoeybi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/qin2019entity/">Entity-consistent End-to-end Task-oriented Dialogue System With KB Retriever</a> Libo Qin et al. </li>
     
   
     
       <li> <a href="/publications/hu2019iterative/">Iterative Answer Prediction With Pointer-augmented Multimodal Transformers For Textvqa</a> Ronghang Hu, Amanpreet Singh, Trevor Darrell, Marcus Rohrbach </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/michel2019are/">Are Sixteen Heads Really Better Than One?</a> Paul Michel, Omer Levy, Graham Neubig </li>
     
   
     
       <li> <a href="/publications/ferreira2019neural/">Neural Data-to-text Generation: A Comparison Between Pipeline And End-to-end Architectures</a> Thiago Castro Ferreira, Chris Van Der Lee, Emiel Van Miltenburg, Emiel Krahmer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/su2019vl/">VL-BERT: Pre-training Of Generic Visual-linguistic Representations</a> Weijie Su et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/le2019multimodal/">Multimodal Transformer Networks For End-to-end Video-grounded Dialogue Systems</a> Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C. H. Hoi </li>
     
   
     
   
     
       <li> <a href="/publications/sukhbaatar2019adaptive/">Adaptive Attention Span In Transformers</a> Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019text/">Text Summarization With Pretrained Encoders</a> Yang Liu, Mirella Lapata </li>
     
   
     
   
     
       <li> <a href="/publications/liu2019multi/">MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models</a> Linqing Liu, Huan Wang, Jimmy Lin, Richard Socher, Caiming Xiong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/neelakantan2019neural/">Neural Assistant: Joint Action Prediction, Response Generation, And Latent Knowledge Reasoning</a> Arvind Neelakantan et al. </li>
     
   
     
       <li> <a href="/publications/yu2019multimodal/">Multimodal Unified Attention Networks For Vision-and-language Interactions</a> Zhou Yu, Yuhao Cui, Jun Yu, Dacheng Tao, Qi Tian </li>
     
   
     
       <li> <a href="/publications/cohan2019pretrained/">Pretrained Language Models For Sequential Sentence Classification</a> Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, Daniel S. Weld </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019simple/">Visualbert: A Simple And Performant Baseline For Vision And Language</a> Liunian Harold Li, Mark Yatskar, Da Yin, Cho-jui Hsieh, Kai-wei Chang </li>
     
   
     
       <li> <a href="/publications/sukhbaatar2019augmenting/">Augmenting Self-attention With Persistent Memory</a> Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, Armand Joulin </li>
     
   
     
       <li> <a href="/publications/dai2019transformer/">Transformer-xl: Attentive Language Models Beyond A Fixed-length Context</a> Zihang Dai et al. </li>
     
   
     
   
     
       <li> <a href="/publications/toneva2019interpreting/">Interpreting And Improving Natural-language Processing (in Machines) With Natural Language-processing (in The Brain)</a> Mariya Toneva, Leila Wehbe </li>
     
   
     
       <li> <a href="/publications/zheng2019dynamic/">Dynamic Past And Future For Neural Machine Translation</a> Zaixiang Zheng, Shujian Huang, Zhaopeng Tu, Xin-yu Dai, Jiajun Chen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2019text/">Text Infilling</a> Wanrong Zhu, Zhiting Hu, Eric Xing </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2019transformers/">Transformers Without Tears: Improving The Normalization Of Self-attention</a> Toan Q. Nguyen, Julian Salazar </li>
     
   
     
       <li> <a href="/publications/zha2019context/">Context-aware Visual Policy Network For Fine-grained Image Captioning</a> Zheng-jun Zha, Daqing Liu, Hanwang Zhang, Yongdong Zhang, Feng Wu </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2019efficient/">Efficient Attention Mechanism For Visual Dialog That Can Handle All The Interactions Between Multiple Inputs</a> Van-quang Nguyen, Masanori Suganuma, Takayuki Okatani </li>
     
   
     
       <li> <a href="/publications/fan2019reducing/">Reducing Transformer Depth On Demand With Structured Dropout</a> Angela Fan, Edouard Grave, Armand Joulin </li>
     
   
     
       <li> <a href="/publications/wen2019adapting/">Adapting And Evaluating A Deep Learning Language Model For Clinical Why-question Answering</a> Andrew Wen, Mohamed Y. Elwazir, Sungrim Moon, Jungwei Fan </li>
     
   
     
       <li> <a href="/publications/klein2019learning/">Learning To Answer By Learning To Ask: Getting The Best Of GPT-2 And BERT Worlds</a> Tassilo Klein, Moin Nabi </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hoang2019efficient/">Efficient Adaptation Of Pretrained Transformers For Abstractive Summarization</a> Andrew Hoang, Antoine Bosselut, Asli Celikyilmaz, Yejin Choi </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhong2019coarse/">Coarse-grain Fine-grain Coattention Network For Multi-evidence Question Answering</a> Victor Zhong, Caiming Xiong, Nitish Shirish Keskar, Richard Socher </li>
     
   
     
       <li> <a href="/publications/yang2019towards/">Towards Making The Most Of BERT In Neural Machine Translation</a> Jiacheng Yang et al. </li>
     
   
     
       <li> <a href="/publications/vig2019multiscale/">A Multiscale Visualization Of Attention In The Transformer Model</a> Jesse Vig </li>
     
   
     
       <li> <a href="/publications/zhou2019synchronous/">Synchronous Bidirectional Neural Machine Translation</a> Long Zhou, Jiajun Zhang, Chengqing Zong </li>
     
   
     
   
     
       <li> <a href="/publications/chen2019bert/">BERT For Joint Intent Classification And Slot Filling</a> Qian Chen, Zhu Zhuo, Wen Wang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/vashishth2019attention/">Attention Interpretability Across NLP Tasks</a> Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, Manaal Faruqui </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rae2019compressive/">Compressive Transformers For Long-range Sequence Modelling</a> Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/maruf2019selective/">Selective Attention For Context-aware Neural Machine Translation</a> Sameen Maruf, Andr√© F. T. Martins, Gholamreza Haffari </li>
     
   
     
   
     
       <li> <a href="/publications/lee2019what/">What Would Elsa Do? Freezing Layers During Transformer Fine-tuning</a> Jaejun Lee, Raphael Tang, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2019simple/">Simple And Effective Curriculum Pointer-generator Networks For Reading Comprehension Over Long Narratives</a> Yi Tay et al. </li>
     
   
     
       <li> <a href="/publications/gu2019levenshtein/">Levenshtein Transformer</a> Jiatao Gu, Changhan Wang, Jake Zhao </li>
     
   
     
       <li> <a href="/publications/ye2019mask/">Align, Mask And Select: A Simple Method For Incorporating Commonsense Knowledge Into Language Representation Models</a> Zhi-xiu Ye, Qian Chen, Wen Wang, Zhen-hua Ling </li>
     
   
     
   
     
       <li> <a href="/publications/jain2019attention/">Attention Is Not Explanation</a> Sarthak Jain, Byron C. Wallace </li>
     
   
     
       <li> <a href="/publications/zhu2019enhanced/">Freelb: Enhanced Adversarial Training For Natural Language Understanding</a> Chen Zhu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/khandelwal2019sample/">Sample Efficient Text Summarization Using A Single Pre-trained Transformer</a> Urvashi Khandelwal, Kevin Clark, Dan Jurafsky, Lukasz Kaiser </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/trivedi2019repurposing/">Repurposing Entailment For Multi-hop Question Answering Tasks</a> Harsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, Niranjan Balasubramanian </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019distilling/">Distilling Knowledge Learned In BERT For Text Generation</a> Yen-chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu </li>
     
   
     
       <li> <a href="/publications/alberti2019fusion/">Fusion Of Detected Objects In Text For Visual Question Answering</a> Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter </li>
     
   
     
       <li> <a href="/publications/donahue2019improving/">Lakhnes: Improving Multi-instrumental Music Generation With Cross-domain Pre-training</a> Chris Donahue, Huanru Henry Mao, Yiting Ethan Li, Garrison W. Cottrell, Julian Mcauley </li>
     
   
     
       <li> <a href="/publications/sankar2019do/">Do Neural Dialog Systems Use The Conversation History Effectively? An Empirical Study</a> Chinnadhurai Sankar, Sandeep Subramanian, Christopher Pal, Sarath Chandar, Yoshua Bengio </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/naseem2019rewarding/">Rewarding Smatch: Transition-based AMR Parsing With Reinforcement Learning</a> Tahira Naseem et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sun2019how/">How To Fine-tune BERT For Text Classification?</a> Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang </li>
     
   
     
       <li> <a href="/publications/zenkel2019adding/">Adding Interpretable Attention To Neural Translation Models Improves Word Alignment</a> Thomas Zenkel, Joern Wuebker, John Denero </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2019pay/">Pay Less Attention With Lightweight And Dynamic Convolutions</a> Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, Michael Auli </li>
     
   
     
       <li> <a href="/publications/stahlberg2019nmt/">On NMT Search Errors And Model Errors: Cat Got Your Tongue?</a> Felix Stahlberg, Bill Byrne </li>
     
   
     
       <li> <a href="/publications/sun2019sequential/">Bert4rec: Sequential Recommendation With Bidirectional Encoder Representations From Transformer</a> Fei Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tan2019learning/">LXMERT: Learning Cross-modality Encoder Representations From Transformers</a> Hao Tan, Mohit Bansal </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/qu2019attentive/">Attentive History Selection For Conversational Question Answering</a> Chen Qu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/junczysdowmunt2019microsoft/">Microsoft Translator At WMT 2019: Towards Large-scale Document-level Neural Machine Translation</a> Marcin Junczys-dowmunt </li>
     
   
     
   
     
       <li> <a href="/publications/glass2019span/">Span Selection Pre-training For Question Answering</a> Michael Glass et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/whang2019effective/">An Effective Domain Adaptive Post-training Method For BERT In Response Selection</a> Taesun Whang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/gu2019insertion/">Insertion-based Decoding With Automatically Inferred Generation Order</a> Jiatao Gu, Qi Liu, Kyunghyun Cho </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wolf2019transfer/">Transfertransfo: A Transfer Learning Approach For Neural Network Based Conversational Agents</a> Thomas Wolf, Victor Sanh, Julien Chaumond, Clement Delangue </li>
     
   
     
   
     
       <li> <a href="/publications/cornia2019meshed/">Meshed-memory Transformer For Image Captioning</a> Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara </li>
     
   
     
   
     
       <li> <a href="/publications/hao2019modeling/">Modeling Recurrence For Transformer</a> Jie Hao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lakew2019controlling/">Controlling The Output Length Of Neural Machine Translation</a> Surafel Melaku Lakew, Mattia Di Gangi, Marcello Federico </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2019contextualized/">Contextualized Sparse Representations For Real-time Open-domain Question Answering</a> Jinhyuk Lee, Minjoon Seo, Hannaneh Hajishirzi, Jaewoo Kang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/landi2019multimodal/">Multimodal Attention Networks For Low-level Vision-and-language Navigation</a> Federico Landi, Lorenzo Baraldi, Marcella Cornia, Massimiliano Corsini, Rita Cucchiara </li>
     
   
     
       <li> <a href="/publications/htut2019do/">Do Attention Heads In BERT Track Syntactic Dependencies?</a> Phu Mon Htut, Jason Phang, Shikha Bordia, Samuel R. Bowman </li>
     
   
     
       <li> <a href="/publications/raffel2019exploring/">Exploring The Limits Of Transfer Learning With A Unified Text-to-text Transformer</a> Colin Raffel et al. </li>
     
   
     
       <li> <a href="/publications/so2019evolved/">The Evolved Transformer</a> David R. So, Chen Liang, Quoc V. Le </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jiao2019distilling/">Tinybert: Distilling BERT For Natural Language Understanding</a> Xiaoqi Jiao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/dong2019unified/">Unified Language Model Pre-training For Natural Language Understanding And Generation</a> Li Dong et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2019bp/">Bp-transformer: Modelling Long-range Context Via Binary Partitioning</a> Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, Zheng Zhang </li>
     
   
     
       <li> <a href="/publications/pruthi2019learning/">Learning To Deceive With Attention-based Explanations</a> Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, Zachary C. Lipton </li>
     
   
     
       <li> <a href="/publications/gavrilov2019self/">Self-attentive Model For Headline Generation</a> Daniil Gavrilov, Pavel Kalaidin, Valentin Malykh </li>
     
   
     
       <li> <a href="/publications/zhang2019pre/">PEGASUS: Pre-training With Extracted Gap-sentences For Abstractive Summarization</a> Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J. Liu </li>
     
   
     
   
     
       <li> <a href="/publications/hrinchuk2019correction/">Correction Of Automatic Speech Recognition With Transformer Sequence-to-sequence Model</a> Oleksii Hrinchuk, Mariya Popova, Boris Ginsburg </li>
     
   
     
       <li> <a href="/publications/zhang2019sg/">Sg-net: Syntax-guided Machine Reading Comprehension</a> Zhuosheng Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kovaleva2019revealing/">Revealing The Dark Secrets Of BERT</a> Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2019semantically/">Semantically Conditioned Dialog Response Generation Via Hierarchical Disentangled Self-attention</a> Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/conneau2019unsupervised/">Unsupervised Cross-lingual Representation Learning At Scale</a> Alexis Conneau et al. </li>
     
   
     
   
     
       <li> <a href="/publications/baevski2019cloze/">Cloze-driven Pretraining Of Self-attention Networks</a> Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael Auli </li>
     
   
     
   
     
       <li> <a href="/publications/edunov2019pre/">Pre-trained Language Model Representations For Language Generation</a> Sergey Edunov, Alexei Baevski, Michael Auli </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/martin2019tasty/">Camembert: A Tasty French Language Model</a> Louis Martin et al. </li>
     
   
     
       <li> <a href="/publications/irie2019language/">Language Modeling With Deep Transformers</a> Kazuki Irie, Albert Zeyer, Ralf Schl√ºter, Hermann Ney </li>
     
   
     
       <li> <a href="/publications/stern2019insertion/">Insertion Transformer: Flexible Sequence Generation Via Insertion Operations</a> Mitchell Stern, William Chan, Jamie Kiros, Jakob Uszkoreit </li>
     
   
     
       <li> <a href="/publications/mccarley2019structured/">Structured Pruning Of A Bert-based Question Answering Model</a> J. S. Mccarley, Rishav Chakravarti, Avirup Sil </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/vig2019visualizing/">Visualizing Attention In Transformer-based Language Representation Models</a> Jesse Vig </li>
     
   
     
       <li> <a href="/publications/wang2019tree/">Tree Transformer: Integrating Tree Structures Into Self-attention</a> Yau-shian Wang, Hung-yi Lee, Yun-nung Chen </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2019unified/">Unified Vision-language Pre-training For Image Captioning And VQA</a> Luowei Zhou et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kanade2019learning/">Learning And Evaluating Contextual Embedding Of Source Code</a> Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi </li>
     
   
     
       <li> <a href="/publications/ran2019option/">Option Comparison Network For Multiple-choice Reading Comprehension</a> Qiu Ran, Peng Li, Weiwei Hu, Jie Zhou </li>
     
   
     
       <li> <a href="/publications/vlasov2019dialogue/">Dialogue Transformers</a> Vladimir Vlasov, Johannes E. M. Mosig, Alan Nichol </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/henderson2019efficient/">Convert: Efficient And Accurate Conversational Representations From Transformers</a> Matthew Henderson et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2019large/">Dialogpt: Large-scale Generative Pre-training For Conversational Response Generation</a> Yizhe Zhang et al. </li>
     
   
     
       <li> <a href="/publications/li2019story/">Story Ending Prediction By Transferable BERT</a> Zhongyang Li, Xiao Ding, Ting Liu </li>
     
   
     
   
     
       <li> <a href="/publications/bao2019non/">Non-autoregressive Transformer By Position Learning</a> Yu Bao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zafrir2019quantized/">Q8BERT: Quantized 8bit BERT</a> Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat </li>
     
   
     
       <li> <a href="/publications/kreutzer2019joey/">Joey NMT: A Minimalist NMT Toolkit For Novices</a> Julia Kreutzer, Jasmijn Bastings, Stefan Riezler </li>
     
   
     
   
     
       <li> <a href="/publications/qiao2019understanding/">Understanding The Behaviors Of BERT In Ranking</a> Yifan Qiao, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019unicoder/">Unicoder-vl: A Universal Encoder For Vision And Language By Cross-modal Pre-training</a> Gen Li et al. </li>
     
   
     
       <li> <a href="/publications/bansal2019learning/">Learning To Few-shot Learn Across Diverse Natural Language Classification Tasks</a> Trapit Bansal, Rishikesh Jha, Andrew Mccallum </li>
     
   
     
       <li> <a href="/publications/qiu2019blockwise/">Blockwise Self-attention For Long Document Understanding</a> Jiezhong Qiu et al. </li>
     
   
     
       <li> <a href="/publications/hao2019multi/">Multi-granularity Self-attention For Neural Machine Translation</a> Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, Zhaopeng Tu </li>
     
   
     
       <li> <a href="/publications/choe2019bridging/">Bridging The Gap For Tokenizer-free Language Models</a> Dokook Choe, Rami Al-rfou, Mandy Guo, Heeyoung Lee, Noah Constant </li>
     
   
     
   
     
       <li> <a href="/publications/sundararaman2019syntax/">Syntax-infused Transformer And BERT Models For Machine Translation And Natural Language Understanding</a> Dhanasekar Sundararaman et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2019incremental/">Incremental Transformer With Deliberation Decoder For Document Grounded Conversations</a> Zekang Li et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2019explicit/">Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection</a> Guangxiang Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/cai2019graph/">Graph Transformer For Graph-to-sequence Learning</a> Deng Cai, Wai Lam </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2019linguistic/">Linguistic Knowledge And Transferability Of Contextual Representations</a> Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, Noah A. Smith </li>
     
   
     
       <li> <a href="/publications/zhao2019parallel/">MUSE: Parallel Multi-scale Attention For Sequence To Sequence Learning</a> Guangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, Liangchen Luo </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2019modeling/">Modeling Graph Structure In Transformer For Better Amr-to-text Generation</a> Jie Zhu et al. </li>
     
   
     
       <li> <a href="/publications/bao2019pre/">PLATO: Pre-trained Dialogue Generation Model With Discrete Latent Variable</a> Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang </li>
     
   
     
       <li> <a href="/publications/voita2019bottom/">The Bottom-up Evolution Of Representations In The Transformer: A Study With Machine Translation And Language Modeling Objectives</a> Elena Voita, Rico Sennrich, Ivan Titov </li>
     
   
     
       <li> <a href="/publications/voita2019analyzing/">Analyzing Multi-head Self-attention: Specialized Heads Do The Heavy Lifting, The Rest Can Be Pruned</a> Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov </li>
     
   
     
       <li> <a href="/publications/pino2019harnessing/">Harnessing Indirect Training Data For End-to-end Automatic Speech Translation: Tricks Of The Trade</a> Juan Pino et al. </li>
     
   
     
   
     
       <li> <a href="/publications/csaky2019deep/">Deep Learning Based Chatbot Models</a> Richard Csaky </li>
     
   
     
   
     
       <li> <a href="/publications/jean2019context/">Context-aware Learning For Neural Machine Translation</a> S√©bastien Jean, Kyunghyun Cho </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2019tensorized/">A Tensorized Transformer For Language Modeling</a> Xindian Ma et al. </li>
     
   
     
       <li> <a href="/publications/platanios2019competence/">Competence-based Curriculum Learning For Neural Machine Translation</a> Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas Poczos, Tom M. Mitchell </li>
     
   
     
   
     
       <li> <a href="/publications/parisotto2019stabilizing/">Stabilizing Transformers For Reinforcement Learning</a> Emilio Parisotto et al. </li>
     
   
     
       <li> <a href="/publications/shazeer2019fast/">Fast Transformer Decoding: One Write-head Is All You Need</a> Noam Shazeer </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dinan2019second/">The Second Conversational Intelligence Challenge (convai2)</a> Emily Dinan et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/press2019improving/">Improving Transformer Models By Reordering Their Sublayers</a> Ofir Press, Noah A. Smith, Omer Levy </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/houlsby2019parameter/">Parameter-efficient Transfer Learning For NLP</a> Neil Houlsby et al. </li>
     
   
     
       <li> <a href="/publications/ziegler2019encoder/">Encoder-agnostic Adaptation For Conditional Language Generation</a> Zachary M. Ziegler, Luke Melas-kyriazi, Sebastian Gehrmann, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mihaylova2019scheduled/">Scheduled Sampling For Transformers</a> Tsvetomila Mihaylova, Andr√© F. T. Martins </li>
     
   
     
   
     
       <li> <a href="/publications/prato2019fully/">Fully Quantized Transformer For Machine Translation</a> Gabriele Prato, Ella Charlaix, Mehdi Rezagholizadeh </li>
     
   
     
       <li> <a href="/publications/zhang2019detecting/">Recosa: Detecting The Relevant Contexts With Self-attention For Multi-turn Dialogue Generation</a> Hainan Zhang, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng </li>
     
   
     
   
     
       <li> <a href="/publications/malmi2019high/">Encode, Tag, Realize: High-precision Text Editing</a> Eric Malmi, Sebastian Krause, Sascha Rothe, Daniil Mirylenka, Aliaksei Severyn </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/keskar2019conditional/">CTRL: A Conditional Transformer Language Model For Controllable Generation</a> Nitish Shirish Keskar, Bryan Mccann, Lav R. Varshney, Caiming Xiong, Richard Socher </li>
     
   
     
   
     
       <li> <a href="/publications/noever2020chess/">The Chess Transformer: Mastering Play Using Generative Language Models</a> David Noever, Matt Ciolino, Josh Kalin </li>
     
   
     
   
     
       <li> <a href="/publications/ma2020xlm/">XLM-T: Scaling Up Multilingual Machine Translation With Pretrained Cross-lingual Transformer Encoders</a> Shuming Ma et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020high/">Lightseq: A High Performance Inference Library For Transformers</a> Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei Li </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/song2020kvl/">KVL-BERT: Knowledge Enhanced Visual-and-linguistic BERT For Visual Commonsense Reasoning</a> Dandan Song, Siyi Ma, Zhanchen Sun, Sicheng Yang, Lejian Liao </li>
     
   
     
   
     
       <li> <a href="/publications/hendrycks2020pretrained/">Pretrained Transformers Improve Out-of-distribution Robustness</a> Dan Hendrycks et al. </li>
     
   
     
       <li> <a href="/publications/lukovnikov2020pretrained/">Pretrained Transformers For Simple Question Answering Over Knowledge Graphs</a> D. Lukovnikov, A. Fischer, J. Lehmann </li>
     
   
     
   
     
       <li> <a href="/publications/cruzbenito2020automated/">Automated Source Code Generation And Auto-completion Using Deep Learning: Comparing And Discussing Current Language-model-related Approaches</a> Juan Cruz-benito, Sanjay Vishwakarma, Francisco Martin-fernandez, Ismael Faro </li>
     
   
     
       <li> <a href="/publications/rosset2020knowledge/">Knowledge-aware Language Model Pretraining</a> Corby Rosset et al. </li>
     
   
     
   
     
       <li> <a href="/publications/clement2020multi/">Pymt5: Multi-mode Translation Of Natural Language And Python Code With Transformers</a> Colin B. Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, Neel Sundaresan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yu2020assessing/">Assessing Phrasal Representation And Composition In Transformers</a> Lang Yu, Allyson Ettinger </li>
     
   
     
   
     
       <li> <a href="/publications/jain2020indic/">Indic-transformers: An Analysis Of Transformer Language Models For Indian Languages</a> Kushal Jain, Adwait Deshpande, Kumar Shridhar, Felix Laumann, Ayushman Dash </li>
     
   
     
   
     
       <li> <a href="/publications/tafjord2020generating/">Proofwriter: Generating Implications, Proofs, And Abductive Statements Over Natural Language</a> Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/saharia2020non/">Non-autoregressive Machine Translation With Latent Alignments</a> Chitwan Saharia, William Chan, Saurabh Saxena, Mohammad Norouzi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/tay2020long/">Long Range Arena: A Benchmark For Efficient Transformers</a> Yi Tay et al. </li>
     
   
     
   
     
       <li> <a href="/publications/su2020bert/">Bert-hlstms: BERT And Hierarchical Lstms For Visual Storytelling</a> Jing Su, Qingyun Dai, Frank Guerin, Mian Zhou </li>
     
   
     
       <li> <a href="/publications/radiyadixit2020how/">How Fine Can Fine-tuning Be? Learning Efficient Language Models</a> Evani Radiya-dixit, Xin Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020when/">When Do You Need Billions Of Words Of Pretraining Data?</a> Yian Zhang, Alex Warstadt, Haau-sing Li, Samuel R. Bowman </li>
     
   
     
       <li> <a href="/publications/li2020bridging/">Bridging Text And Video: A Universal Multimodal Transformer For Video-audio Scene-aware Dialog</a> Zekang Li et al. </li>
     
   
     
       <li> <a href="/publications/voskarides2020query/">Query Resolution For Conversational Search With Limited Supervision</a> Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, Maarten De Rijke </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ainslie2020encoding/">ETC: Encoding Long And Structured Inputs In Transformers</a> Joshua Ainslie et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lin2020variational/">Variational Transformers For Diverse Response Generation</a> Zhaojiang Lin, Genta Indra Winata, Peng Xu, Zihan Liu, Pascale Fung </li>
     
   
     
       <li> <a href="/publications/kale2020text/">Text-to-text Pre-training For Data-to-text Tasks</a> Mihir Kale, Abhinav Rastogi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ekstedt2020transformer/">Turngpt: A Transformer-based Language Model For Predicting Turn-taking In Spoken Dialog</a> Erik Ekstedt, Gabriel Skantze </li>
     
   
     
       <li> <a href="/publications/sun2020compact/">Mobilebert: A Compact Task-agnostic BERT For Resource-limited Devices</a> Zhiqing Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sinha2020unnatural/">Unnatural Language Inference</a> Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, Adina Williams </li>
     
   
     
       <li> <a href="/publications/pradeep2020scientific/">Scientific Claim Verification With VERT5ERINI</a> Ronak Pradeep, Xueguang Ma, Rodrigo Nogueira, Jimmy Lin </li>
     
   
     
   
     
       <li> <a href="/publications/vanbiljon2020optimal/">On Optimal Transformer Depth For Low-resource Language Translation</a> Elan Van Biljon, Arnu Pretorius, Julia Kreutzer </li>
     
   
     
   
     
       <li> <a href="/publications/sood2020improving/">Improving Natural Language Processing Tasks With Human Gaze-guided Neural Attention</a> Ekta Sood, Simon Tannert, Philipp Mueller, Andreas Bulling </li>
     
   
     
       <li> <a href="/publications/wang2020encoding/">Encoding Syntactic Knowledge In Transformer Encoder For Intent Detection And Slot Filling</a> Jixuan Wang, Kai Wei, Martin Radfar, Weiwei Zhang, Clement Chung </li>
     
   
     
   
     
       <li> <a href="/publications/shazeer2020talking/">Talking-heads Attention</a> Noam Shazeer, Zhenzhong Lan, Youlong Cheng, Nan Ding, Le Hou </li>
     
   
     
       <li> <a href="/publications/xiao2020ernie/">ERNIE-GEN: An Enhanced Multi-flow Pre-training And Fine-tuning Framework For Natural Language Generation</a> Dongling Xiao et al. </li>
     
   
     
       <li> <a href="/publications/tay2020rethinking/">Synthesizer: Rethinking Self-attention In Transformer Models</a> Yi Tay et al. </li>
     
   
     
       <li> <a href="/publications/lepikhin2020scaling/">Gshard: Scaling Giant Models With Conditional Computation And Automatic Sharding</a> Dmitry Lepikhin et al. </li>
     
   
     
       <li> <a href="/publications/shen2020simple/">A Simple But Tough-to-beat Data Augmentation Approach For Natural Language Understanding And Generation</a> Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, Weizhu Chen </li>
     
   
     
   
     
       <li> <a href="/publications/tang2020rapidly/">Rapidly Bootstrapping A Question Answering Dataset For COVID-19</a> Raphael Tang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2020cross/">Imagebert: Cross-modal Pre-training With Large-scale Weak-supervised Image-text Data</a> Di Qi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jin2020simple/">A Simple Baseline To Semi-supervised Domain Adaptation For Machine Translation</a> Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2020discourse/">Dialogbert: Discourse-aware Response Generation Via Learning To Recover And Rank Utterances</a> Xiaodong Gu, Kang Min Yoo, Jung-woo Ha </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020very/">Very Deep Transformers For Neural Machine Translation</a> Xiaodong Liu, Kevin Duh, Liyuan Liu, Jianfeng Gao </li>
     
   
     
   
     
       <li> <a href="/publications/cao2020behind/">Behind The Scene: Revealing The Secrets Of Pre-trained Vision-and-language Models</a> Jize Cao et al. </li>
     
   
     
       <li> <a href="/publications/koutsikakis2020greek/">GREEK-BERT: The Greeks Visiting Sesame Street</a> John Koutsikakis, Ilias Chalkidis, Prodromos Malakasiotis, Ion Androutsopoulos </li>
     
   
     
       <li> <a href="/publications/mao2020exploring/">Dialoguetrm: Exploring The Intra- And Inter-modal Emotional Behaviors In The Conversation</a> Yuzhao Mao et al. </li>
     
   
     
       <li> <a href="/publications/guo2020sequence/">Sequence-level Mixed Sample Data Augmentation</a> Demi Guo, Yoon Kim, Alexander M. Rush </li>
     
   
     
   
     
       <li> <a href="/publications/pfeiffer2020framework/">Adapterhub: A Framework For Adapting Transformers</a> Jonas Pfeiffer et al. </li>
     
   
     
       <li> <a href="/publications/sun2020contextualized/">Colake: Contextualized Language And Knowledge Embedding</a> Tianxiang Sun et al. </li>
     
   
     
       <li> <a href="/publications/liu2020reading/">Rikinet: Reading Wikipedia Pages For Natural Question Answering</a> Dayiheng Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zandie2020multi/">Emptransfo: A Multi-head Transformer Architecture For Creating Empathetic Dialog Systems</a> Rohola Zandie, Mohammad H. Mahoor </li>
     
   
     
       <li> <a href="/publications/zhu2020vision/">Vision-dialog Navigation By Exploring Cross-modal Memory</a> Yi Zhu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2020funnel/">Funnel-transformer: Filtering Out Sequential Redundancy For Efficient Language Processing</a> Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le </li>
     
   
     
   
     
       <li> <a href="/publications/ke2020rethinking/">Rethinking Positional Encoding In Language Pre-training</a> Guolin Ke, Di He, Tie-yan Liu </li>
     
   
     
       <li> <a href="/publications/penha2020what/">What Does BERT Know About Books, Movies And Music? Probing BERT For Conversational Recommendation</a> Gustavo Penha, Claudia Hauff </li>
     
   
     
       <li> <a href="/publications/kim2020length/">Length-adaptive Transformer: Train Once With Length Drop, Use Anytime With Search</a> Gyuwan Kim, Kyunghyun Cho </li>
     
   
     
       <li> <a href="/publications/wang2020confidence/">Confidence-aware Non-repetitive Multimodal Transformers For Textcaps</a> Zhaokai Wang, Renda Bao, Qi Wu, Si Liu </li>
     
   
     
   
     
       <li> <a href="/publications/zheng2020towards/">Towards Making The Most Of Context In Neural Machine Translation</a> Zaixiang Zheng, Xiang Yue, Shujian Huang, Jiajun Chen, Alexandra Birch </li>
     
   
     
   
     
       <li> <a href="/publications/wang2020rethinking/">Rethinking The Value Of Transformer Components</a> Wenxuan Wang, Zhaopeng Tu </li>
     
   
     
       <li> <a href="/publications/wang2020multi/">Minilmv2: Multi-head Self-attention Relation Distillation For Compressing Pretrained Transformers</a> Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, Furu Wei </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gontier2020measuring/">Measuring Systematic Generalization In Neural Proof Generation With Transformers</a> Nicolas Gontier, Koustuv Sinha, Siva Reddy, Christopher Pal </li>
     
   
     
   
     
       <li> <a href="/publications/devries2020as/">As Good As New. How To Successfully Recycle English GPT-2 To Make Models For Other Languages</a> Wietse De Vries, Malvina Nissim </li>
     
   
     
   
     
       <li> <a href="/publications/antoun2020pre/">Aragpt2: Pre-trained Transformer For Arabic Language Generation</a> Wissam Antoun, Fady Baly, Hazem Hajj </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2020auto/">Auto-captions On GIF: A Large-scale Video-sentence Dataset For Vision-language Pre-training</a> Yingwei Pan et al. </li>
     
   
     
       <li> <a href="/publications/ding2020ernie/">Ernie-doc: A Retrospective Long-document Modeling Transformer</a> Siyu Ding et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2020deep/">Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers</a> Wenhui Wang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2020multilingual/">Multilingual Denoising Pre-training For Neural Machine Translation</a> Yinhan Liu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bao2020pseudo/">Unilmv2: Pseudo-masked Language Models For Unified Language Model Pre-training</a> Hangbo Bao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020logical/">Logical Natural Language Generation From Open-domain Tables</a> Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, William Yang Wang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bhojanapalli2020low/">Low-rank Bottleneck In Multi-head Attention Models</a> Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar </li>
     
   
     
       <li> <a href="/publications/lei2020memory/">MART: Memory-augmented Recurrent Transformer For Coherent Video Paragraph Captioning</a> Jie Lei et al. </li>
     
   
     
       <li> <a href="/publications/wang2020hardware/">HAT: Hardware-aware Transformers For Efficient Natural Language Processing</a> Hanrui Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fei2020retrofitting/">Retrofitting Structure-aware Transformer Language Model For End Tasks</a> Hao Fei, Yafeng Ren, Donghong Ji </li>
     
   
     
       <li> <a href="/publications/qi2020predicting/">Prophetnet: Predicting Future N-gram For Sequence-to-sequence Pre-training</a> Weizhen Qi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2020enhance/">Enhance Multimodal Transformer With External Label And In-domain Pretrain: Hateful Meme Challenge Winning Solution</a> Ron Zhu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/longpre2020how/">How Effective Is Task-agnostic Data Augmentation For Pretrained Transformers?</a> Shayne Longpre, Yu Wang, Christopher Dubois </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tambe2020sentence/">Edgebert: Sentence-level Energy Optimizations For Latency-aware Multi-task NLP Inference</a> Thierry Tambe et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wiegreffe2020measuring/">Measuring Association Between Labels And Free-text Rationales</a> Sarah Wiegreffe, Ana Marasoviƒá, Noah A. Smith </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/cheng2020robust/">Advaug: Robust Adversarial Augmentation For Neural Machine Translation</a> Yong Cheng, Lu Jiang, Wolfgang Macherey, Jacob Eisenstein </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2020modifying/">Modifying Memories In Transformer Models</a> Chen Zhu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ribeiro2020investigating/">Investigating Pretrained Language Models For Graph-to-text Generation</a> Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Sch√ºtze, Iryna Gurevych </li>
     
   
     
       <li> <a href="/publications/li2020object/">Oscar: Object-semantics Aligned Pre-training For Vision-language Tasks</a> Xiujun Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/qu2020open/">Open-retrieval Conversational Question Answering</a> Chen Qu et al. </li>
     
   
     
       <li> <a href="/publications/feng2020pre/">Codebert: A Pre-trained Model For Programming And Natural Languages</a> Zhangyin Feng et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shleifer2020pre/">Pre-trained Summarization Distillation</a> Sam Shleifer, Alexander M. Rush </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chu2020multi/">Multi-step Joint-modality Attention Network For Scene-aware Dialogue System</a> Yun-wei Chu, Kuan-yen Lin, Chao-chun Hsu, Lun-wei Ku </li>
     
   
     
   
     
       <li> <a href="/publications/wang2020fairseq/">Fairseq S2T: Fast Speech-to-text Modeling With Fairseq</a> Changhan Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/bunk2020lightweight/">DIET: Lightweight Language Understanding For Dialogue Systems</a> Tanja Bunk, Daksh Varshneya, Vladimir Vlasov, Alan Nichol </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/seo2020look/">Look Before You Speak: Visually Contextualized Utterances</a> Paul Hongsuck Seo, Arsha Nagrani, Cordelia Schmid </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020train/">Train Large, Then Compress: Rethinking Model Size For Efficient Training And Inference Of Transformers</a> Zhuohan Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2020mixup/">Mixup-transformer: Dynamic Data Augmentation For NLP Tasks</a> Lichao Sun et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2020text/">TIME: Text And Image Mutual-translation Adversarial Networks</a> Bingchen Liu, Kunpeng Song, Yizhe Zhu, Gerard De Melo, Ahmed Elgammal </li>
     
   
     
       <li> <a href="/publications/li2020efficient/">Efficient Transformer-based Large Scale Language Representations Using Hardware-friendly Block Structured Pruning</a> Bingbing Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bi2020pre/">PALM: Pre-training An Autoencoding&autoregressive Language Model For Context-conditioned Generation</a> Bin Bi et al. </li>
     
   
     
   
     
       <li> <a href="/publications/pudipeddi2020training/">Training Large Neural Networks With Constant Memory Using A New Execution Algorithm</a> Bharadwaj Pudipeddi, Maral Mesmakhosroshahi, Jinwen Xi, Sujeeth Bharadwaj </li>
     
   
     
       <li> <a href="/publications/zaheer2020big/">Big Bird: Transformers For Longer Sequences</a> Manzil Zaheer et al. </li>
     
   
     
       <li> <a href="/publications/vanaken2020hidden/">Visbert: Hidden-state Visualizations For Transformers</a> Betty Van Aken, Benjamin Winter, Alexander L√∂ser, Felix A. Gers </li>
     
   
     
       <li> <a href="/publications/ranasinghe2020transquest/">Transquest At WMT2020: Sentence-level Direct Assessment</a> Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020mapping/">Mapping Natural Language Instructions To Mobile UI Action Sequences</a> Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, Jason Baldridge </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/desai2020calibration/">Calibration Of Pre-trained Transformers</a> Shrey Desai, Greg Durrett </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2020efficient/">SPARTA: Efficient Open-domain Question Answering Via Sparse Transformer Matching Retrieval</a> Tiancheng Zhao, Xiaopeng Lu, Kyusong Lee </li>
     
   
     
       <li> <a href="/publications/peng2020building/">SOLOIST: Building Task Bots At Scale With Transfer Learning And Machine Teaching</a> Baolin Peng et al. </li>
     
   
     
       <li> <a href="/publications/ezencan2020comparison/">A Comparison Of LSTM And BERT For Small Corpus</a> Aysu Ezen-can </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020constrained/">POINTER: Constrained Progressive Text Generation Via Insertion-based Generative Pre-training</a> Yizhe Zhang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yang2020intent/">IART: Intent-aware Response Ranking With Transformers In Information-seeking Conversation Systems</a> Liu Yang et al. </li>
     
   
     
       <li> <a href="/publications/xu2020multi/">Layoutlmv2: Multi-modal Pre-training For Visually-rich Document Understanding</a> Yang Xu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/majumdar2020improving/">Improving Vision-and-language Navigation With Image-text Pairs From The Web</a> Arjun Majumdar et al. </li>
     
   
     
       <li> <a href="/publications/cohan2020document/">SPECTER: Document-level Representation Learning Using Citation-informed Transformers</a> Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S. Weld </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020closer/">A Closer Look At The Robustness Of Vision-and-language Pre-trained Models</a> Linjie Li, Zhe Gan, Jingjing Liu </li>
     
   
     
   
     
       <li> <a href="/publications/kim2020code/">Code Prediction By Feeding Trees To Transformers</a> Seohyun Kim, Jinman Zhao, Yuchi Tian, Satish Chandra </li>
     
   
     
       <li> <a href="/publications/xue2020massively/">Mt5: A Massively Multilingual Pre-trained Text-to-text Transformer</a> Linting Xue et al. </li>
     
   
     
       <li> <a href="/publications/yang2020just/">Just Ask: Learning To Answer Questions From Millions Of Narrated Videos</a> Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid </li>
     
   
     
       <li> <a href="/publications/wu2020controllable/">A Controllable Model Of Grounded Response Generation</a> Zeqiu Wu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lauscher2020from/">From Zero To Hero: On The Limitations Of Zero-shot Cross-lingual Transfer With Multilingual Transformers</a> Anne Lauscher, Vinit Ravishankar, Ivan Vuliƒá, Goran Glava≈° </li>
     
   
     
       <li> <a href="/publications/shavrina2020russian/">Russiansuperglue: A Russian Language Understanding Evaluation Benchmark</a> Tatiana Shavrina et al. </li>
     
   
     
       <li> <a href="/publications/zhao2020segment/">SEAL: Segment-wise Extractive-abstractive Long-form Text Summarization</a> Yao Zhao, Mohammad Saleh, Peter J. Liu </li>
     
   
     
   
     
       <li> <a href="/publications/gupta2020global/">GMAT: Global Memory Augmentation For Transformers</a> Ankit Gupta, Jonathan Berant </li>
     
   
     
       <li> <a href="/publications/qu2020contrast/">Coda: Contrast-enhanced And Diversity-promoting Data Augmentation For Natural Language Understanding</a> Yanru Qu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fan2020addressing/">Addressing Some Limitations Of Transformers With Feedback Memory</a> Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar </li>
     
   
     
       <li> <a href="/publications/soldaini2020cascade/">The Cascade Transformer: An Application For Efficient Answer Sentence Selection</a> Luca Soldaini, Alessandro Moschitti </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020unqovering/">Unqovering Stereotyping Biases Via Underspecified Questions</a> Tao Li, Tushar Khot, Daniel Khashabi, Ashish Sabharwal, Vivek Srikumar </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/shakeri2020end/">End-to-end Synthetic Data Generation For Domain Adaptation Of Question Answering Systems</a> Siamak Shakeri et al. </li>
     
   
     
   
     
       <li> <a href="/publications/r%C3%BCckl%C3%A92020efficiency/">Adapterdrop: On The Efficiency Of Adapters In Transformers</a> Andreas R√ºckl√© et al. </li>
     
   
     
   
     
       <li> <a href="/publications/clark2020transformers/">Transformers As Soft Reasoners Over Language</a> Peter Clark, Oyvind Tafjord, Kyle Richardson </li>
     
   
     
       <li> <a href="/publications/lee2020contrastive/">Contrastive Learning With Adversarial Perturbations For Conditional Text Generation</a> Seanie Lee, Dong Bok Lee, Sung Ju Hwang </li>
     
   
     
       <li> <a href="/publications/ganesh2020compressing/">Compressing Large-scale Transformer-based Models: A Case Study On BERT</a> Prakhar Ganesh et al. </li>
     
   
     
   
     
       <li> <a href="/publications/marasovi%C4%872020natural/">Natural Language Rationales With Full-stack Visual Reasoning: From Pixels To Semantic Frames To Commonsense Graphs</a> Ana Marasoviƒá et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2020empirical/">An Empirical Investigation Of Pre-trained Transformer Language Models For Open-domain Dialogue Generation</a> Piji Li </li>
     
   
     
       <li> <a href="/publications/mehta2020deep/">Delight: Deep And Light-weight Transformer</a> Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/talmor2020leap/">Leap-of-thought: Teaching Pre-trained Models To Systematically Reason Over Implicit Knowledge</a> Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant </li>
     
   
     
       <li> <a href="/publications/chan2020self/">Cocon: A Self-supervised Approach For Controlled Text Generation</a> Alvin Chan, Yew-soon Ong, Bill Pung, Aston Zhang, Jie Fu </li>
     
   
     
       <li> <a href="/publications/lin2020conversational/">Conversational Question Reformulation Via Sequence-to-sequence Architectures And Pretrained Language Models</a> Sheng-chieh Lin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mosbach2020stability/">On The Stability Of Fine-tuning BERT: Misconceptions, Explanations, And Strong Baselines</a> Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lopez2020simplifying/">Simplifying Paragraph-level Question Generation Via Transformer Language Models</a> Luis Enrico Lopez, Diane Kathryn Cruz, Jan Christian Blaise Cruz, Charibeth Cheng </li>
     
   
     
       <li> <a href="/publications/prasanna2020when/">When BERT Plays The Lottery, All Tickets Are Winning</a> Sai Prasanna, Anna Rogers, Anna Rumshisky </li>
     
   
     
       <li> <a href="/publications/araabi2020optimizing/">Optimizing Transformer For Low-resource Neural Machine Translation</a> Ali Araabi, Christof Monz </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/svyatkovskiy2020intellicode/">Intellicode Compose: Code Generation Using Transformer</a> Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, Neel Sundaresan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kant2020spatially/">Spatially Aware Multimodal Transformers For Textvqa</a> Yash Kant et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/mager2020gpt/">Gpt-too: A Language-model-first Approach For Amr-to-text Generation</a> Manuel Mager et al. </li>
     
   
     
       <li> <a href="/publications/cao2020decomposing/">Deformer: Decomposing Pre-trained Transformers For Faster Question Answering</a> Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian </li>
     
   
     
       <li> <a href="/publications/wu2020lite/">Lite Transformer With Long-short Range Attention</a> Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, Song Han </li>
     
   
     
       <li> <a href="/publications/wang2020vd/">VD-BERT: A Unified Vision And Dialog Transformer With BERT</a> Yue Wang et al. </li>
     
   
     
       <li> <a href="/publications/mohankumar2020towards/">Towards Transparent And Explainable Attention Models</a> Akash Kumar Mohankumar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2020ma/">MA-DST: Multi-attention Based Scalable Dialog State Tracking</a> Adarsh Kumar, Peter Ku, Anuj Kumar Goyal, Angeliki Metallinou, Dilek Hakkani-tur </li>
     
   
     
   
     
       <li> <a href="/publications/kim2020sequential/">Sequential Latent Knowledge Selection For Knowledge-grounded Dialogue</a> Byeongchang Kim, Jaewoo Ahn, Gunhee Kim </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2020efficient/">Earlybert: Efficient BERT Training Via Early-bird Lottery Tickets</a> Xiaohan Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2020self/">Linformer: Self-attention With Linear Complexity</a> Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma </li>
     
   
     
       <li> <a href="/publications/nie2020contextualized/">Coregen: Contextualized Code Representation Learning For Commit Message Generation</a> Lun Yiu Nie et al. </li>
     
   
     
   
     
       <li> <a href="/publications/marino2020integrating/">KRISP: Integrating Implicit And Symbolic Knowledge For Open-domain Knowledge-based VQA</a> Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, Marcus Rohrbach </li>
     
   
     
       <li> <a href="/publications/nogueira2020document/">Document Ranking With A Pretrained Sequence-to-sequence Model</a> Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin </li>
     
   
     
   
     
       <li> <a href="/publications/cho2020x/">X-LXMERT: Paint, Caption And Answer Questions With Multi-modal Transformers</a> Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geng2020dynamic/">Dynamic Graph Representation Learning For Video Dialog Via Multi-modal Shuffled Transformers</a> Shijie Geng et al. </li>
     
   
     
       <li> <a href="/publications/vuli%C4%872020probing/">Probing Pretrained Language Models For Lexical Semantics</a> Ivan Vuliƒá, Edoardo Maria Ponti, Robert Litschko, Goran Glava≈°, Anna Korhonen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/beltagy2020long/">Longformer: The Long-document Transformer</a> Iz Beltagy, Matthew E. Peters, Arman Cohan </li>
     
   
     
   
     
       <li> <a href="/publications/zhou2020pre/">Pre-training Text-to-text Transformers For Concept-centric Common Sense</a> Wangchunshu Zhou et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2020accelerating/">Accelerating Training Of Transformer-based Language Models With Progressive Layer Dropping</a> Minjia Zhang, Yuxiong He </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kasai2020non/">Non-autoregressive Machine Translation With Disentangled Context Transformer</a> Jungo Kasai, James Cross, Marjan Ghazvininejad, Jiatao Gu </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2020multimodal/">DSTC8-AVSD: Multimodal Semantic Transformer Network With Retrieval Style Word Generator</a> Hwanhee Lee et al. </li>
     
   
     
       <li> <a href="/publications/chung2020rethinking/">Rethinking Embedding Coupling In Pre-trained Language Models</a> Hyung Won Chung, Thibault F√©vry, Henry Tsai, Melvin Johnson, Sebastian Ruder </li>
     
   
     
       <li> <a href="/publications/ahmad2020transformer/">A Transformer-based Approach For Source Code Summarization</a> Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-wei Chang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2020recurrent/">A Recurrent Vision-and-language BERT For Navigation</a> Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-opazo, Stephen Gould </li>
     
   
     
   
     
       <li> <a href="/publications/zhang2020distillation/">Ternarybert: Distillation-aware Ultra-low Bit BERT</a> Wei Zhang et al. </li>
     
   
     
       <li> <a href="/publications/ye2020contrastive/">Contrastive Triple Extraction With Generative Transformer</a> Hongbin Ye et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sajjad2020effect/">On The Effect Of Dropping Layers Of Pre-trained Transformer Models</a> Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Preslav Nakov </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bostrom2020byte/">Byte Pair Encoding Is Suboptimal For Language Model Pretraining</a> Kaj Bostrom, Greg Durrett </li>
     
   
     
   
     
       <li> <a href="/publications/you2020hard/">Hard-coded Gaussian Attention For Neural Machine Translation</a> Weiqiu You, Simeng Sun, Mohit Iyyer </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2020trans/">TRANS-BLSTM: Transformer With Bidirectional LSTM For Language Understanding</a> Zhiheng Huang, Peng Xu, Davis Liang, Ajay Mishra, Bing Xiang </li>
     
   
     
       <li> <a href="/publications/xu2020edit/">EDITOR: An Edit-based Transformer With Repositioning For Neural Machine Translation With Soft Lexical Constraints</a> Weijia Xu, Marine Carpuat </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bird2020chatbot/">Chatbot Interaction With Artificial Intelligence: Human Data Augmentation With T5 And Language Transformer Ensemble For Text Classification</a> Jordan J. Bird, Anik√≥ Ek√°rt, Diego R. Faria </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kumar2020data/">Data Augmentation Using Pre-trained Transformer Models</a> Varun Kumar, Ashutosh Choudhary, Eunah Cho </li>
     
   
     
   
     
       <li> <a href="/publications/xin2020dynamic/">Deebert: Dynamic Early Exiting For Accelerating BERT Inference</a> Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2020robust/">Robust Conversational AI With Grounded Text Generation</a> Jianfeng Gao et al. </li>
     
   
     
       <li> <a href="/publications/xu2021human/">Human Parity On Commonsenseqa: Augmenting Self-attention With External Attention</a> Yichong Xu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/goyal2021larger/">Larger-scale Transformers For Multilingual Masked Language Modeling</a> Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau </li>
     
   
     
   
     
       <li> <a href="/publications/bao2021unified/">Vlmo: Unified Vision-language Pre-training With Mixture-of-modality-experts</a> Hangbo Bao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2021end/">E2E-VLP: End-to-end Vision-language Pre-training Enhanced By Visual Learning</a> Haiyang Xu et al. </li>
     
   
     
       <li> <a href="/publications/zhang2021ernie/">Ernie-vilg: Unified Generative Pre-training For Bidirectional Vision-language Generation</a> Han Zhang et al. </li>
     
   
     
       <li> <a href="/publications/zhang2021hierarchical/">Hierarchical Task Learning From Language Instructions With Unified Transformers And Self-monitoring</a> Yichi Zhang, Joyce Chai </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2021one/">One Chatbot Per Person: Creating Personalized Chatbots Based On Implicit User Profiles</a> Zhengyi Ma, Zhicheng Dou, Yutao Zhu, Hanxun Zhong, Ji-rong Wen </li>
     
   
     
       <li> <a href="/publications/bao2021g/">G-transformer For Document-level Machine Translation</a> Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, Weihua Luo </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chrysostomou2021improving/">Improving The Faithfulness Of Attention-based Explanations With Task-specific Information For Text Classification</a> George Chrysostomou, Nikolaos Aletras </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/webersinke2021pretrained/">Climatebert: A Pretrained Language Model For Climate-related Text</a> Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler, Markus Leippold </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2021improving/">Improving Stack Overflow Question Title Generation With Copying Enhanced Codebert Model And Bi-modal Information</a> Fengji Zhang et al. </li>
     
   
     
       <li> <a href="/publications/jin2021good/">A Good Prompt Is Worth Millions Of Parameters: Low-resource Prompt-based Learning For Vision-language Models</a> Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, Xiang Ren </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nooralahzadeh2021progressive/">Progressive Transformer-based Generation Of Radiology Reports</a> Farhad Nooralahzadeh, Nicolas Perez Gonzalez, Thomas Frauenfelder, Koji Fujimoto, Michael Krauthammer </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rahimi2021explaining/">Explaining Documents' Relevance To Search Queries</a> Razieh Rahimi, Youngwoo Kim, Hamed Zamani, James Allan </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kharitonov2021text/">Text-free Prosody-aware Generative Spoken Language Modeling</a> Eugene Kharitonov et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021lightweight/">Lightner: A Lightweight Tuning Paradigm For Low-resource NER Via Pluggable Prompting</a> Xiang Chen et al. </li>
     
   
     
       <li> <a href="/publications/kong2021bidirectional/">BLT: Bidirectional Layout Transformer For Controllable Layout Generation</a> Xiang Kong et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mu2021self/">SLIP: Self-supervision Meets Language-image Pre-training</a> Norman Mu, Alexander Kirillov, David Wagner, Saining Xie </li>
     
   
     
       <li> <a href="/publications/nagoudi2021text/">Arat5: Text-to-text Transformers For Arabic Language Generation</a> El Moatez Billah Nagoudi, Abdelrahim Elmadany, Muhammad Abdul-mageed </li>
     
   
     
       <li> <a href="/publications/zaken2021simple/">Bitfit: Simple Parameter-efficient Fine-tuning For Transformer-based Masked Language-models</a> Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg </li>
     
   
     
       <li> <a href="/publications/hu2021low/">Lora: Low-rank Adaptation Of Large Language Models</a> Edward J. Hu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2021augmenting/">Augmenting Sequential Recommendation With Pseudo-prior Items Via Reversely Pre-training Transformer</a> Zhiwei Liu, Ziwei Fan, Yu Wang, Philip S. Yu </li>
     
   
     
       <li> <a href="/publications/vari%C5%A12021sequence/">Sequence Length Is A Domain: Length-based Overfitting In Transformer Models</a> Du≈°an Vari≈°, Ond≈ôej Bojar </li>
     
   
     
   
     
       <li> <a href="/publications/li2021align/">Align And Prompt: Video-and-language Pre-training With Entity Prompts</a> Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, Steven C. H. Hoi </li>
     
   
     
       <li> <a href="/publications/zhang2021exploratory/">An Exploratory Study On Long Dialogue Summarization: What Works And What's Next</a> Yusen Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/pan2021contrastive/">Contrastive Learning For Many-to-many Multilingual Neural Machine Translation</a> Xiao Pan, Mingxuan Wang, Liwei Wu, Lei Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2021tr/">TR-BERT: Dynamic Token Reduction For Accelerating BERT Inference</a> Deming Ye, Yankai Lin, Yufei Huang, Maosong Sun </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kacupaj2021conversational/">Conversational Question Answering Over Knowledge Graphs With Transformer And Graph Attention Networks</a> Endri Kacupaj et al. </li>
     
   
     
       <li> <a href="/publications/tay2021scale/">Scale Efficiently: Insights From Pre-training And Fine-tuning Transformers</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/santhanam2021rome/">Rome Was Built In 1776: A Case Study On Factual Correctness In Knowledge-grounded Response Generation</a> Sashank Santhanam et al. </li>
     
   
     
       <li> <a href="/publications/peer2021greedy/">Greedy-layer Pruning: Speeding Up Transformer Models For Natural Language Processing</a> David Peer, Sebastian Stabinger, Stefan Engl, Antonio Rodriguez-sanchez </li>
     
   
     
       <li> <a href="/publications/so2021searching/">Primer: Searching For Efficient Transformers For Language Modeling</a> David R. So et al. </li>
     
   
     
       <li> <a href="/publications/hendrycks2021expert/">CUAD: An Expert-annotated NLP Dataset For Legal Contract Review</a> Dan Hendrycks, Collin Burns, Anya Chen, Spencer Ball </li>
     
   
     
       <li> <a href="/publications/yogatama2021adaptive/">Adaptive Semiparametric Language Models</a> Dani Yogatama, Cyprien De Masson D'autume, Lingpeng Kong </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2021knowledge/">Knowledge Neurons In Pretrained Transformers</a> Damai Dai et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yang2021causal/">Causal Attention For Vision-language Tasks</a> Xu Yang, Hanwang Zhang, Guojun Qi, Jianfei Cai </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2021mastering/">Cogview: Mastering Text-to-image Generation Via Transformers</a> Ming Ding et al. </li>
     
   
     
   
     
       <li> <a href="/publications/moradi2021gpt/">GPT-3 Models Are Poor Few-shot Learners In The Biomedical Domain</a> Milad Moradi, Kathrin Blagec, Florian Haberl, Matthias Samwald </li>
     
   
     
   
     
       <li> <a href="/publications/ni2021sentence/">Sentence-t5: Scalable Sentence Encoders From Pre-trained Text-to-text Models</a> Jianmo Ni et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2021unified/">UFO: A Unified Transformer For Vision-language Representation Learning</a> Jianfeng Wang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/mao2021dynamic/">DYLE: Dynamic Latent Extraction For Abstractive Long-input Summarization</a> Ziming Mao et al. </li>
     
   
     
       <li> <a href="/publications/straka2021czech/">Robeczech: Czech Roberta, A Monolingual Contextualized Language Representation Model</a> Milan Straka, Jakub N√°plava, Jana Strakov√°, David Samuel </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/he2021fast/">Fastmoe: A Fast Mixture-of-expert Training System</a> Jiaao He et al. </li>
     
   
     
       <li> <a href="/publications/chen2021simple/">Hiddencut: Simple Data Augmentation For Natural Language Understanding With Better Generalization</a> Jiaao Chen, Dinghan Shen, Weizhu Chen, Diyi Yang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2021violet/">VIOLET : End-to-end Video-language Transformers With Masked Visual-token Modeling</a> Tsu-jui Fu et al. </li>
     
   
     
       <li> <a href="/publications/dejong2021mention/">Mention Memory: Incorporating Textual Knowledge Into Transformers Through Entity Mention Attention</a> Michiel De Jong, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, William Cohen </li>
     
   
     
       <li> <a href="/publications/vannguyen2021light/">Trankit: A Light-weight Transformer-based Toolkit For Multilingual Natural Language Processing</a> Minh Van Nguyen, Viet Dac Lai, Amir Pouran Ben Veyseh, Thien Huu Nguyen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tang2021clip/">Clip4caption: CLIP For Video Caption</a> Mingkang Tang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/rae2021scaling/">Scaling Language Models: Methods, Analysis & Insights From Training Gopher</a> Jack W. Rae et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2021diagnosing/">Diagnosing Vision-and-language Navigation: What Really Matters</a> Wanrong Zhu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/xue2021advancing/">Advancing High-resolution Video-language Representation With Large-scale Video Transcriptions</a> Hongwei Xue et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gheini2021cross/">Cross-attention Is All You Need: Adapting Pretrained Transformers For Machine Translation</a> Mozhdeh Gheini, Xiang Ren, Jonathan May </li>
     
   
     
       <li> <a href="/publications/chefer2021generic/">Generic Attention-model Explainability For Interpreting Bi-modal And Encoder-decoder Transformers</a> Hila Chefer, Shir Gur, Lior Wolf </li>
     
   
     
       <li> <a href="/publications/zeng2021pangu/">Pangu-\(Œ±\): Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation</a> Wei Zeng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xia2021using/">Using Prior Knowledge To Guide Bert's Attention In Semantic Textual Matching Tasks</a> Tingyu Xia, Yue Wang, Yuan Tian, Yi Chang </li>
     
   
     
       <li> <a href="/publications/borgeaud2021improving/">Improving Language Models By Retrieving From Trillions Of Tokens</a> Sebastian Borgeaud et al. </li>
     
   
     
   
     
       <li> <a href="/publications/lin2021chinese/">M6: A Chinese Multimodal Pretrainer</a> Junyang Lin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/li2021dialogue/">Dialogue History Matters! Personalized Response Selectionin Multi-turn Retrieval-based Chatbots</a> Juntao Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/singh2021nlp/">The NLP Cookbook: Modern Recipes For Transformer Based Deep Learning Architectures</a> Sushant Singh, Ausif Mahmood </li>
     
   
     
       <li> <a href="/publications/chen2021data/">Visualgpt: Data-efficient Adaptation Of Pretrained Language Models For Image Captioning</a> Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny </li>
     
   
     
       <li> <a href="/publications/eisenschlos2021multi/">MATE: Multi-view Attention For Table Transformer Efficiency</a> Julian Martin Eisenschlos, Maharshi Gor, Thomas M√ºller, William W. Cohen </li>
     
   
     
   
     
       <li> <a href="/publications/p%C3%A9rez2021pre/">Robertuito: A Pre-trained Language Model For Social Media Text In Spanish</a> Juan Manuel P√©rez, Dami√°n A. Furman, Laura Alonso Alemany, Franco Luque </li>
     
   
     
   
     
       <li> <a href="/publications/clark2021pre/">CANINE: Pre-training An Efficient Tokenization-free Encoder For Language Representation</a> Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dou2021empirical/">An Empirical Study Of Training End-to-end Vision-and-language Transformers</a> Zi-yi Dou et al. </li>
     
   
     
       <li> <a href="/publications/plepi2021context/">Context Transformer With Stacked Pointer Networks For Conversational Question Answering Over Knowledge Graphs</a> Joan Plepi, Endri Kacupaj, Kuldeep Singh, Harsh Thakkar, Jens Lehmann </li>
     
   
     
       <li> <a href="/publications/xing2021km/">KM-BART: Knowledge Enhanced Multimodal BART For Visual Commonsense Generation</a> Yiran Xing et al. </li>
     
   
     
       <li> <a href="/publications/kulkarni2021learning/">Learning Rich Representation Of Keyphrases From Text</a> Mayank Kulkarni, Debanjan Mahata, Ravneet Arora, Rajarshi Bhowmik </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rohde2021hierarchical/">Hierarchical Learning For Generation With Long Source Sequences</a> Tobias Rohde, Xiaoxia Wu, Yinhan Liu </li>
     
   
     
       <li> <a href="/publications/kalyan2021ammus/">AMMUS : A Survey Of Transformer-based Pretrained Models In Natural Language Processing</a> Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, Sivanesan Sangeetha </li>
     
   
     
   
     
       <li> <a href="/publications/pearce2021comparative/">A Comparative Study Of Transformer-based Language Models On Extractive Question Answering</a> Kate Pearce, Tiffany Zhan, Aneesh Komanduri, Justin Zhan </li>
     
   
     
   
     
       <li> <a href="/publications/cobbe2021training/">Training Verifiers To Solve Math Word Problems</a> Karl Cobbe et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021end/">Swinbert: End-to-end Transformers With Sparse Attention For Video Captioning</a> Kevin Lin et al. </li>
     
   
     
       <li> <a href="/publications/lu2021pretrained/">Pretrained Transformers As Universal Computation Engines</a> Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch </li>
     
   
     
       <li> <a href="/publications/frank2021vision/">Vision-and-language Or Vision-for-language? On Cross-modal Influence In Multimodal Transformers</a> Stella Frank, Emanuele Bugliarello, Desmond Elliott </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2021seeing/">Seeing Out Of The Box: End-to-end Pre-training For Vision-language Representation Learning</a> Zhicheng Huang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lowphansirikul2021pretraining/">Wangchanberta: Pretraining Transformer-based Thai Language Models</a> Lalita Lowphansirikul, Charin Polpanumas, Nawat Jantrakulchai, Sarana Nutanong </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/manakul2021long/">Long-span Summarization Via Local Attention And Content Selection</a> Potsawee Manakul, Mark J. F. Gales </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021scene/">SGEITL: Scene Graph Enhanced Image-text Learning For Visual Commonsense Reasoning</a> Zhecan Wang et al. </li>
     
   
     
       <li> <a href="/publications/sun2021pre/">Lightningdot: Pre-training Visual-semantic Embeddings For Real-time Image-text Retrieval</a> Siqi Sun et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021additive/">Fastformer: Additive Attention Can Be All You Need</a> Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, Xing Xie </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2021multimodal/">Multimodal Transformer With Variable-length Memory For Vision-and-language Navigation</a> Chuang Lin et al. </li>
     
   
     
       <li> <a href="/publications/tay2021are/">Are Pre-trained Convolutions Better Than Pre-trained Transformers?</a> Yi Tay et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2021new/">Iconqa: A New Benchmark For Abstract Diagram Understanding And Visual Language Reasoning</a> Pan Lu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bondarenko2021understanding/">Understanding And Overcoming The Challenges Of Efficient Transformer Quantization</a> Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort </li>
     
   
     
       <li> <a href="/publications/zuo2021taming/">Taming Sparsely Activated Transformer With Stochastic Experts</a> Simiao Zuo et al. </li>
     
   
     
       <li> <a href="/publications/wu2021visual/">N\"UWA: Visual Synthesis Pre-training For Neural Visual World Creation</a> Chenfei Wu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ju2021prompting/">Prompting Visual-language Models For Efficient Video Understanding</a> Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, Weidi Xie </li>
     
   
     
   
     
       <li> <a href="/publications/xiao2021pre/">Lawformer: A Pre-trained Language Model For Chinese Legal Long Documents</a> Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, Maosong Sun </li>
     
   
     
       <li> <a href="/publications/liu2021non/">Non-invasive Self-attention For Side Information Fusion In Sequential Recommendation</a> Chang Liu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/fang2021compressing/">Compressing Visual-linguistic Model Via Knowledge Distillation</a> Zhiyuan Fang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schramowski2021large/">Large Pre-trained Language Models Contain Human-like Biases Of What Is Right And Wrong To Do</a> Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A. Rothkopf, Kristian Kersting </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/min2021recent/">Recent Advances In Natural Language Processing Via Large Pre-trained Language Models: A Survey</a> Bonan Min et al. </li>
     
   
     
       <li> <a href="/publications/li2021token/">Terapipe: Token-level Pipeline Parallelism For Training Large-scale Language Models</a> Zhuohan Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/he2021nlp/">Generate, Annotate, And Learn: NLP With Synthetic Text</a> Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, Mohammad Norouzi </li>
     
   
     
       <li> <a href="/publications/mccoy2021how/">How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty In Text Generation Using RAVEN</a> R. Thomas Mccoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz </li>
     
   
     
       <li> <a href="/publications/kim2021what/">What Changes Can Large-scale Language Models Bring? Intensive Study On Hyperclova: Billions-scale Korean Generative Pretrained Transformers</a> Boseop Kim et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ma2021linear/">Luna: Linear Unified Nested Attention</a> Xuezhe Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/prabhumoye2021focused/">Focused Attention Improves Document-grounded Generation</a> Shrimai Prabhumoye, Kazuma Hashimoto, Yingbo Zhou, Alan W Black, Ruslan Salakhutdinov </li>
     
   
     
       <li> <a href="/publications/gao2021code/">Code Structure Guided Transformer For Source Code Summarization</a> Shuzheng Gao et al. </li>
     
   
     
       <li> <a href="/publications/caciularu2021cross/">CDLM: Cross-document Language Modeling</a> Avi Caciularu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sharma2021towards/">Towards Facilitating Empathic Conversations In Online Mental Health Support: A Reinforcement Learning Approach</a> Ashish Sharma, Inna W. Lin, Adam S. Miner, David C. Atkins, Tim Althoff </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2021history/">History Aware Multimodal Transformer For Vision-and-language Navigation</a> Shizhe Chen, Pierre-louis Guhur, Cordelia Schmid, Ivan Laptev </li>
     
   
     
       <li> <a href="/publications/karmakar2021what/">What Do Pre-trained Code Models Know About Code?</a> Anjan Karmakar, Romain Robbes </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lazaridou2021mind/">Mind The Gap: Assessing Temporal Generalization In Neural Language Models</a> Angeliki Lazaridou et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/coenen2021human/">Wordcraft: A Human-ai Collaborative Editor For Story Writing</a> Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, Ann Yuan </li>
     
   
     
       <li> <a href="/publications/narayan2021planning/">Planning With Learned Entity Prompts For Abstractive Summarization</a> Shashi Narayan et al. </li>
     
   
     
       <li> <a href="/publications/maneriker2021improving/">Urltran: Improving Phishing URL Detection Using Transformers</a> Pranav Maneriker et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2021identifier/">Codet5: Identifier-aware Unified Pre-trained Encoder-decoder Models For Code Understanding And Generation</a> Yue Wang, Weishi Wang, Shafiq Joty, Steven C. H. Hoi </li>
     
   
     
       <li> <a href="/publications/kao2021optimized/">FLAT: An Optimized Dataflow For Mitigating Attention Bottlenecks</a> Sheng-chun Kao, Suvinay Subramanian, Gaurav Agrawal, Amir Yazdanbakhsh, Tushar Krishna </li>
     
   
     
       <li> <a href="/publications/biten2021layout/">Latr: Layout-aware Transformer For Scene-text VQA</a> Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, R. Manmatha </li>
     
   
     
   
     
       <li> <a href="/publications/pashevich2021episodic/">Episodic Transformer For Vision-and-language Navigation</a> Alexander Pashevich, Cordelia Schmid, Chen Sun </li>
     
   
     
   
     
       <li> <a href="/publications/suglia2021embodied/">Embodied BERT: A Transformer Model For Embodied, Language-guided Visual Task Completion</a> Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme </li>
     
   
     
       <li> <a href="/publications/ma2021encoder/">Deltalm: Encoder-decoder Pre-training For Language Generation And Translation By Augmenting Pretrained Multilingual Encoders</a> Shuming Ma et al. </li>
     
   
     
       <li> <a href="/publications/khare2021multimodal/">MMBERT: Multimodal BERT Pretraining For Improved Medical VQA</a> Yash Khare et al. </li>
     
   
     
       <li> <a href="/publications/chi2021multilingual/">MT6: Multilingual Pretrained Text-to-text Transformer With Translation Pairs</a> Zewen Chi et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lelkes2021quiz/">Quiz-style Question Generation For News Stories</a> Adam D. Lelkes, Vinh Q. Tran, Cong Yu </li>
     
   
     
       <li> <a href="/publications/uchendu2021benchmark/">TURINGBENCH: A Benchmark Environment For Turing Test In The Age Of Neural Text Generation</a> Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, Dongwon Lee </li>
     
   
     
       <li> <a href="/publications/moudgil2021scene/">SOAT: A Scene- And Object-aware Transformer For Vision-and-language Navigation</a> Abhinav Moudgil, Arjun Majumdar, Harsh Agrawal, Stefan Lee, Dhruv Batra </li>
     
   
     
   
     
       <li> <a href="/publications/atri2021leveraging/">See, Hear, Read: Leveraging Multimodality With Guided Attention For Abstractive Text Summarization</a> Yash Kumar Atri, Shraman Pramanick, Vikram Goyal, Tanmoy Chakraborty </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/kaliamoorthi2021distilling/">Distilling Large Language Models Into Tiny And Effective Students Using Pqrnn</a> Prabhu Kaliamoorthi, Aditya Siddhant, Edward Li, Melvin Johnson </li>
     
   
     
   
     
       <li> <a href="/publications/zafrir2021prune/">Prune Once For All: Sparse Pre-trained Language Models</a> Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, Moshe Wasserblat </li>
     
   
     
   
     
       <li> <a href="/publications/qiu2021vt/">VT-CLIP: Enhancing Vision-language Models With Visual-guided Texts</a> Longtian Qiu et al. </li>
     
   
     
       <li> <a href="/publications/lei2021when/">When Attention Meets Fast Recurrence: Training Language Models With Reduced Compute</a> Tao Lei </li>
     
   
     
       <li> <a href="/publications/phan2021multi/">Cotext: Multi-task Learning With Code-text Transformer</a> Long Phan et al. </li>
     
   
     
       <li> <a href="/publications/phan2021text/">Scifive: A Text-to-text Transformer Model For Biomedical Literature</a> Long N. Phan et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2021linear/">Linear-time Self Attention With Codeword Histogram For Efficient Recommendation</a> Yongji Wu et al. </li>
     
   
     
       <li> <a href="/publications/xue2021towards/">Byt5: Towards A Token-free Future With Pre-trained Byte-to-byte Models</a> Linting Xue et al. </li>
     
   
     
   
     
       <li> <a href="/publications/pan2021improved/">Improved Text Classification Via Contrastive Adversarial Training</a> Lin Pan, Chung-wei Hang, Avirup Sil, Saloni Potdar </li>
     
   
     
       <li> <a href="/publications/cheng2021effective/">An Effective Non-autoregressive Model For Spoken Language Understanding</a> Lizhi Cheng, Weijia Jia, Wenmian Yang </li>
     
   
     
       <li> <a href="/publications/gui2021knowledge/">KAT: A Knowledge Augmented Transformer For Vision-and-language</a> Liangke Gui et al. </li>
     
   
     
       <li> <a href="/publications/huang2021unifying/">Unifying Multimodal Transformer For Bi-directional Image And Text Generation</a> Yupan Huang, Hongwei Xue, Bei Liu, Yutong Lu </li>
     
   
     
       <li> <a href="/publications/chen2021fine/">Fine-grained Style Control In Transformer-based Text-to-speech Synthesis</a> Li-wei Chen, Alexander Rudnicky </li>
     
   
     
   
     
       <li> <a href="/publications/shleifer2021improved/">Normformer: Improved Transformer Pretraining With Extra Normalization</a> Sam Shleifer, Jason Weston, Myle Ott </li>
     
   
     
       <li> <a href="/publications/yao2021fine/">FILIP: Fine-grained Interactive Language-image Pre-training</a> Lewei Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xie2021explanation/">An Explanation Of In-context Learning As Implicit Bayesian Inference</a> Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma </li>
     
   
     
       <li> <a href="/publications/kim2021learned/">Learned Token Pruning For Transformers</a> Sehoon Kim et al. </li>
     
   
     
       <li> <a href="/publications/fang2021transformer/">Transformer-based Conditional Variational Autoencoder For Controllable Story Generation</a> Le Fang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/li2021personalized/">Personalized Transformer For Explainable Recommendation</a> Lei Li, Yongfeng Zhang, Li Chen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nogueira2021investigating/">Investigating The Limitations Of Transformers With Simple Arithmetic Tasks</a> Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin </li>
     
   
     
       <li> <a href="/publications/guo2021efficient/">Longt5: Efficient Text-to-text Transformer For Long Sequences</a> Mandy Guo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2021pre/">Condenser: A Pre-training Architecture For Dense Retrieval</a> Luyu Gao, Jamie Callan </li>
     
   
     
       <li> <a href="/publications/topal2021exploring/">Exploring Transformers In Natural Language Generation: GPT, BERT, And Xlnet</a> M. Onat Topal, Anil Bas, Imke Van Heerden </li>
     
   
     
       <li> <a href="/publications/kim2021i/">I-BERT: Integer-only BERT Quantization</a> Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer </li>
     
   
     
   
     
       <li> <a href="/publications/nye2021show/">Show Your Work: Scratchpads For Intermediate Computation With Language Models</a> Maxwell Nye et al. </li>
     
   
     
       <li> <a href="/publications/li2021text/">Text Compression-aided Transformer Encoding</a> Zuchao Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/kim2021self/">Self-guided Contrastive Learning For BERT Sentence Representations</a> Taeuk Kim, Kang Min Yoo, Sang-goo Lee </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/sohn2022visual/">Visual Prompt Tuning For Generative Transfer Learning</a> Kihyuk Sohn et al. </li>
     
   
     
   
     
       <li> <a href="/publications/meng2022mass/">Mass-editing Memory In A Transformer</a> Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David Bau </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022think/">Think Global, Act Local: Dual-scale Graph Transformer For Vision-and-language Navigation</a> Shizhe Chen, Pierre-louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev </li>
     
   
     
   
     
       <li> <a href="/publications/sun2022length/">A Length-extrapolatable Transformer</a> Yutao Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dettmers2022matrix/">Llm.int8(): 8-bit Matrix Multiplication For Transformers At Scale</a> Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer </li>
     
   
     
   
     
       <li> <a href="/publications/misra2022enabling/">Minicons: Enabling Flexible Behavioral And Representational Analyses Of Transformer Language Models</a> Kanishka Misra </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022what/">What Language Model Architecture And Pretraining Objective Work Best For Zero-shot Generalization?</a> Thomas Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2022adaptive/">Adamct: Adaptive Mixture Of Cnn-transformer For Sequential Recommendation</a> Juyong Jiang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/wang2022foundation/">Omnivl:one Foundation Model For Image-language And Video-language Tasks</a> Junke Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hoffmann2022training/">Training Compute-optimal Large Language Models</a> Jordan Hoffmann et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/geiping2022training/">Cramming: Training A Language Model On A Single GPU In One Day</a> Jonas Geiping, Tom Goldstein </li>
     
   
     
       <li> <a href="/publications/dao2022fast/">Flashattention: Fast And Memory-efficient Exact Attention With Io-awareness</a> Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R√© </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/schneider2022towards/">Towards Trustworthy Autograding Of Short, Multi-lingual, Multi-type Answers</a> Johannes Schneider, Robin Richner, Micha Riser </li>
     
   
     
       <li> <a href="/publications/zhang2022pretraining/">Coditt5: Pretraining For Source Code And Natural Language Editing</a> Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, Milos Gligoric </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2022efficient/">Zeroquant: Efficient And Affordable Post-training Quantization For Large-scale Transformers</a> Zhewei Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2022empirical/">An Empirical Study Of End-to-end Video-language Transformers With Masked Visual Modeling</a> Tsu-jui Fu et al. </li>
     
   
     
       <li> <a href="/publications/qi2022integrating/">RASAT: Integrating Relational Structures Into Pretrained Seq2seq Model For Text-to-sql</a> Jiexing Qi et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2022generative/">Generative Spoken Dialogue Language Modeling</a> Tu Anh Nguyen et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ahmed2022few/">Few-shot Training Llms For Project-specific Code-summarization</a> Toufique Ahmed, Premkumar Devanbu </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022simple/">Lilt: A Simple Yet Effective Language-independent Layout Transformer For Structured Document Understanding</a> Jiapeng Wang, Lianwen Jin, Kai Ding </li>
     
   
     
       <li> <a href="/publications/lu2022unified/">Unified-io: A Unified Model For Vision, Language, And Multi-modal Tasks</a> Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022generative/">GIT: A Generative Image-to-text Transformer For Vision And Language</a> Jianfeng Wang et al. </li>
     
   
     
       <li> <a href="/publications/borisov2022language/">Language Models Are Realistic Tabular Data Generators</a> Vadim Borisov, Kathrin Se√üler, Tobias Leemann, Martin Pawelczyk, Gjergji Kasneci </li>
     
   
     
       <li> <a href="/publications/yu2022contrastive/">Coca: Contrastive Captioners Are Image-text Foundation Models</a> Jiahui Yu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/yu2022scaling/">Scaling Autoregressive Models For Content-rich Text-to-image Generation</a> Jiahui Yu et al. </li>
     
   
     
       <li> <a href="/publications/yang2022grouping/">Gtrans: Grouping And Fusing Transformer Layers For Neural Machine Translation</a> Jian Yang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022end/">End-to-end Transformer Based Model For Image Captioning</a> Yiyu Wang, Jungang Xu, Yingfei Sun </li>
     
   
     
   
     
       <li> <a href="/publications/li2022clinical/">Clinical-longformer And Clinical-bigbird: Transformers For Long Clinical Sequences</a> Yikuan Li, Ramsey M. Wehbe, Faraz S. Ahmad, Hanyin Wang, Yuan Luo </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022tune/">Tune-a-video: One-shot Tuning Of Image Diffusion Models For Text-to-video Generation</a> Jay Zhangjie Wu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/delarosa2022efficient/">BERTIN: Efficient Pre-training Of A Spanish Language Model Using Perplexity Sampling</a> Javier De La Rosa et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/smith2022coda/">Coda-prompt: Continual Decomposed Attention-based Prompting For Rehearsal-free Continual Learning</a> James Seale Smith et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cho2022dall/">Dall-eval: Probing The Reasoning Skills And Social Biases Of Text-to-image Generation Models</a> Jaemin Cho, Abhay Zala, Mohit Bansal </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cao2022model/">A Model-agnostic Data Manipulation Method For Persona-based Dialogue Generation</a> Yu Cao, Wei Bi, Meng Fang, Shuming Shi, Dacheng Tao </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2022vision/">VLT: Vision-language Transformer And Query Generation For Referring Segmentation</a> Henghui Ding, Chang Liu, Suchen Wang, Xudong Jiang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/du2022survey/">A Survey Of Vision-language Pre-trained Models</a> Yifan Du, Zikang Liu, Junyi Li, Wayne Xin Zhao </li>
     
   
     
       <li> <a href="/publications/jiang2022pseudo/">Pseudo-q: Generating Pseudo Language Queries For Visual Grounding</a> Haojun Jiang, Yuanze Lin, Dongchen Han, Shiji Song, Gao Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hua2022transformer/">Transformer Quality In Linear Time</a> Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc V. Le </li>
     
   
     
       <li> <a href="/publications/du2022contrastive/">Contrastive Learning With Bidirectional Transformers For Sequential Recommendation</a> Hanwen Du et al. </li>
     
   
     
       <li> <a href="/publications/zhang2022survey/">A Survey Of Controllable Text Generation Using Transformer-based Pre-trained Language Models</a> Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song </li>
     
   
     
   
     
       <li> <a href="/publications/bao2022vl/">Vl-beit: Generative Vision-language Pretraining</a> Hangbo Bao, Wenhui Wang, Li Dong, Furu Wei </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022multimodal/">Murag: Multimodal Retrieval-augmented Generator For Open Question Answering Over Images And Text</a> Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen </li>
     
   
     
       <li> <a href="/publications/park2022lut/">LUT-GEMM: Quantized Matrix Multiplication Based On Luts For Efficient Inference In Large-scale Generative Language Models</a> Gunho Park et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022uni/">Uni-perceiver V2: A Generalist Model For Large-scale Vision And Vision-language Tasks</a> Hao Li et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/perez2022ignore/">Ignore Previous Prompt: Attack Techniques For Language Models</a> F√°bio Perez, Ian Ribeiro </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ke2022hierarchical/">Hitskt: A Hierarchical Transformer Model For Session-aware Knowledge Tracing</a> Fucai Ke et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/moiseev2022structured/">SKILL: Structured Knowledge Infusion For Large Language Models</a> Fedor Moiseev, Zhe Dong, Enrique Alfonseca, Martin Jaggi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2022hybrid/">Hybrid Transformer With Multi-level Fusion For Multimodal Knowledge Graph Completion</a> Xiang Chen et al. </li>
     
   
     
       <li> <a href="/publications/aflalo2022vl/">Vl-interpret: An Interactive Visualization Tool For Interpreting Vision-language Transformers</a> Estelle Aflalo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frantar2022accurate/">GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers</a> Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh </li>
     
   
     
   
     
       <li> <a href="/publications/kurtic2022optimal/">The Optimal BERT Surgeon: Scalable And Accurate Second-order Pruning For Large Language Models</a> Eldar Kurtic et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/he2022space/">SPACE-3: Unified Dialog Model Pre-training For Task-oriented Dialog Understanding And Generation</a> Wanwei He et al. </li>
     
   
     
       <li> <a href="/publications/chen2022jointly/">Pali: A Jointly-scaled Multilingual Language-image Model</a> Xi Chen et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/hong2022large/">Cogvideo: Large-scale Pretraining For Text-to-video Generation Via Transformers</a> Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/gao2022multi/">MIST: Multi-modal Iterative Spatial-temporal Transformer For Long-form Video Question Answering</a> Difei Gao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hutchins2022block/">Block-recurrent Transformers</a> Delesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, Behnam Neyshabur </li>
     
   
     
   
     
       <li> <a href="/publications/gong2022future/">Future Transformer For Long-term Action Anticipation</a> Dayoung Gong, Joonseok Lee, Manjin Kim, Seong Jong Ha, Minsu Cho </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fu2022hungry/">Hungry Hungry Hippos: Towards Language Modeling With State Space Models</a> Daniel Y. Fu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2022why/">Why Can GPT Learn In-context? Language Models Implicitly Perform Gradient Descent As Meta-optimizers</a> Damai Dai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022effective/">Mplug: Effective And Efficient Vision-language Learning By Cross-modal Skip-connections</a> Chenliang Li et al. </li>
     
   
     
   
     
       <li> <a href="/publications/tu2022visual/">Visual Query Tuning: Towards Effective Usage Of Intermediate Representations For Parameter And Memory Efficient Transfer Learning</a> Cheng-hao Tu, Zheda Mai, Wei-lun Chao </li>
     
   
     
       <li> <a href="/publications/hsu2022language/">Language Model Compression With Weighted Low-rank Factorization</a> Yen-chang Hsu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/olsson2022learning/">In-context Learning And Induction Heads</a> Catherine Olsson et al. </li>
     
   
     
       <li> <a href="/publications/xu2022survey/">A Survey On Model Compression And Acceleration For Pretrained Language Models</a> Canwen Xu, Julian Mcauley </li>
     
   
     
       <li> <a href="/publications/oh2022why/">Why Does Surprisal From Larger Transformer-based Language Models Provide A Poorer Fit To Human Reading Times?</a> Byung-doh Oh, William Schuler </li>
     
   
     
       <li> <a href="/publications/anil2022exploring/">Exploring Length Generalization In Large Language Models</a> Cem Anil et al. </li>
     
   
     
   
     
       <li> <a href="/publications/sun2022long/">Long-form Video-language Pre-training With Multimodal Temporal Contrastive Learning</a> Yuchong Sun et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ni2022expanding/">Expanding Language-image Pretrained Models For General Video Recognition</a> Bolin Ni et al. </li>
     
   
     
       <li> <a href="/publications/lin2022vision/">ADAPT: Vision-language Navigation With Modality-aligned Action Prompts</a> Bingqian Lin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/workshop2022open/">BLOOM: A 176b-parameter Open-access Multilingual Language Model</a> Bigscience Workshop et al. </li>
     
   
     
       <li> <a href="/publications/zhang2022revisiting/">Revisiting End-to-end Speech-to-text Translation From Scratch</a> Biao Zhang, Barry Haddow, Rico Sennrich </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022competition/">Competition-level Code Generation With Alphacode</a> Yujia Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zoph2022st/">St-moe: Designing Stable And Transferable Sparse Expert Models</a> Barret Zoph et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/he2022prompt/">Hyperprompt: Prompt-based Task-conditioning Of Transformers</a> Yun He et al. </li>
     
   
     
   
     
       <li> <a href="/publications/bulatov2022recurrent/">Recurrent Memory Transformer</a> Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ushio2022t/">T-NER: An All-round Python Library For Transformer-based Named Entity Recognition</a> Asahi Ushio, Jose Camacho-collados </li>
     
   
     
       <li> <a href="/publications/huang2022pre/">Layoutlmv3: Pre-training For Document AI With Unified Text And Image Masking</a> Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei </li>
     
   
     
   
     
       <li> <a href="/publications/bucker2022reshaping/">Reshaping Robot Trajectories Using Natural Language Commands: A Study Of Multi-modal Data Alignment Using Transformers</a> Arthur Bucker et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jiang2022general/">VIMA: General Robot Manipulation With Multimodal Prompts</a> Yunfan Jiang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/zhai2022high/">Bytetransformer: A High-performance Transformer Boosted For Variable-length Inputs</a> Yujia Zhai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/cui2022democratizing/">Democratizing Contrastive Language-image Pre-training: A CLIP Benchmark Of Data, Model, And Supervision</a> Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, Jing Shao </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wan2022what/">What Do They Capture? -- A Structural Analysis Of Pre-trained Language Models For Source Code</a> Yao Wan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/leviathan2022fast/">Fast Inference From Transformers Via Speculative Decoding</a> Yaniv Leviathan, Matan Kalman, Yossi Matias </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/dou2022coarse/">Coarse-to-fine Vision-language Pre-training With Fusion In The Backbone</a> Zi-yi Dou et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/guo2022retrieval/">Retrieval Augmentation Of Large Language Models For Lay Language Generation</a> Yue Guo, Wei Qiu, Gondy Leroy, Sheng Wang, Trevor Cohen </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2022mixture/">Adamix: Mixture-of-adaptations For Parameter-efficient Model Tuning</a> Yaqing Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/petrov2022systematic/">A Systematic Review And Replicability Study Of Bert4rec For Sequential Recommendation</a> Aleksandr Petrov, Craig Macdonald </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kamath2022new/">A New Path: Scaling Vision-and-language Navigation With Synthetic Instructions And Imitation Learning</a> Aishwarya Kamath et al. </li>
     
   
     
       <li> <a href="/publications/maharana2022storydall/">Storydall-e: Adapting Pretrained Text-to-image Transformers For Story Continuation</a> Adyasha Maharana, Darryl Hannan, Mohit Bansal </li>
     
   
     
   
     
       <li> <a href="/publications/haviv2022transformer/">Transformer Language Models Without Positional Encodings Still Learn Positional Information</a> Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/srivastava2022beyond/">Beyond The Imitation Game: Quantifying And Extrapolating The Capabilities Of Language Models</a> Aarohi Shammie Srivastava et al. </li>
     
   
     
       <li> <a href="/publications/parisi2022tool/">TALM: Tool Augmented Language Models</a> Aaron Parisi, Yao Zhao, Noah Fiedel </li>
     
   
     
       <li> <a href="/publications/chowdhery2022scaling/">Palm: Scaling Language Modeling With Pathways</a> Aakanksha Chowdhery et al. </li>
     
   
     
   
     
       <li> <a href="/publications/asai2022parameter/">ATTEMPT: Parameter-efficient Multi-task Tuning Via Attentional Mixtures Of Soft Prompts</a> Akari Asai, Mohammadreza Salehi, Matthew E. Peters, Hannaneh Hajishirzi </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2022memorizing/">Memorizing Transformers</a> Yuhuai Wu, Markus N. Rabe, Delesley Hutchins, Christian Szegedy </li>
     
   
     
       <li> <a href="/publications/wang2022image/">Image As A Foreign Language: Beit Pretraining For All Vision And Vision-language Tasks</a> Wenhui Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/ji2022survey/">Survey Of Hallucination In Natural Language Generation</a> Ziwei Ji et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/schramowski2022can/">Can Machines Help Us Answering Question 16 In Datasheets, And In Turn Reflecting On Inappropriate Content?</a> Patrick Schramowski, Christopher Tauchmann, Kristian Kersting </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gafni2022make/">Make-a-scene: Scene-based Text-to-image Generation With Human Priors</a> Oran Gafni et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mees2022what/">What Matters In Language Conditioned Robotic Imitation Learning Over Unstructured Data</a> Oier Mees, Lukas Hermann, Wolfram Burgard </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ratner2022parallel/">Parallel Context Windows For Large Language Models</a> Nir Ratner et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/long2022vision/">Vision-and-language Pretrained Models: A Survey</a> Siqu Long, Feiqi Cao, Soyeon Caren Han, Haiqin Yang </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/muennighoff2022gpt/">SGPT: GPT Sentence Embeddings For Semantic Search</a> Niklas Muennighoff </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zheng2022prompt/">Prompt Vision Transformer For Domain Generalization</a> Zangwei Zheng, Xiangyu Yue, Kai Wang, Yang You </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022accelerating/">Accelerating Attention Through Gradient-based Learned Runtime Pruning</a> Zheng Li, Soroush Ghodrati, Amir Yazdanbakhsh, Hadi Esmaeilzadeh, Mingu Kang </li>
     
   
     
       <li> <a href="/publications/varia2022instruction/">Instruction Tuning For Few-shot Aspect-based Sentiment Analysis</a> Siddharth Varia et al. </li>
     
   
     
       <li> <a href="/publications/eddine2022pretrained/">Arabart: A Pretrained Arabic Sequence-to-sequence Model For Abstractive Summarization</a> Moussa Kamal Eddine, Nadi Tomeh, Nizar Habash, Joseph Le Roux, Michalis Vazirgiannis </li>
     
   
     
   
     
       <li> <a href="/publications/geva2022transformer/">Transformer Feed-forward Layers Build Predictions By Promoting Concepts In The Vocabulary Space</a> Mor Geva, Avi Caciularu, Kevin Ro Wang, Yoav Goldberg </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chan2022data/">Data Distributional Properties Drive Emergent In-context Learning In Transformers</a> Stephanie C. Y. Chan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ding2022faster/">Cogview2: Faster And Better Text-to-image Generation Via Hierarchical Transformers</a> Ming Ding, Wendi Zheng, Wenyi Hong, Jie Tang </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/yasunaga2022retrieval/">Retrieval-augmented Multimodal Language Modeling</a> Michihiro Yasunaga et al. </li>
     
   
     
       <li> <a href="/publications/zhao2022educational/">Educational Question Generation Of Children Storybooks Via Question Type Distribution Learning And Event-centric Summarization</a> Zhenjie Zhao et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/glass2022generate/">Re2g: Retrieve, Rerank, Generate</a> Michael Glass et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/tschannen2022image/">CLIPPO: Image-and-language Understanding From Pixels Only</a> Michael Tschannen, Basil Mustafa, Neil Houlsby </li>
     
   
     
       <li> <a href="/publications/wu2022efficient/">An Efficient Memory-augmented Transformer For Knowledge-intensive NLP Tasks</a> Yuxiang Wu et al. </li>
     
   
     
       <li> <a href="/publications/qu2022simple/">Siri: A Simple Selective Retraining Mechanism For Transformer-based Visual Grounding</a> Mengxue Qu et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/jia2022visual/">Visual Prompt Tuning</a> Menglin Jia et al. </li>
     
   
     
       <li> <a href="/publications/zhang2022open/">OPT: Open Pre-trained Transformer Language Models</a> Susan Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2022pre/">Retromae: Pre-training Retrieval-oriented Language Models Via Masked Auto-encoder</a> Shitao Xiao, Zheng Liu, Yingxia Shao, Zhao Cao </li>
     
   
     
   
     
       <li> <a href="/publications/hu2022empowering/">Empowering Language Models With Knowledge Graph Reasoning For Question Answering</a> Ziniu Hu et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ivgi2022efficient/">Efficient Long-text Understanding With Short-text Models</a> Maor Ivgi, Uri Shaham, Jonathan Berant </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/barraco2022mean/">Camel: Mean Teacher Learning For Image Captioning</a> Manuele Barraco et al. </li>
     
   
     
       <li> <a href="/publications/schuster2022confident/">Confident Adaptive Language Modeling</a> Tal Schuster et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bonifacio2022data/">Inpars: Data Augmentation For Information Retrieval Using Large Language Models</a> Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Rodrigo Nogueira </li>
     
   
     
       <li> <a href="/publications/wang2022super/">Super-naturalinstructions: Generalization Via Declarative Instructions On 1600+ NLP Tasks</a> Yizhong Wang et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/srinivasan2022continual/">Climb: A Continual Learning Benchmark For Vision-and-language Tasks</a> Tejas Srinivasan et al. </li>
     
   
     
       <li> <a href="/publications/phan2022pretrained/">Vit5: Pretrained Text-to-text Transformer For Vietnamese Language Generation</a> Long Phan, Hieu Tran, Hieu Nguyen, Trieu H. Trinh </li>
     
   
     
       <li> <a href="/publications/wei2022multimodality/">MVP: Multimodality-guided Visual Pre-training</a> Longhui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, Qi Tian </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/qi2022fine/">FUM: Fine-grained And Fast User Modeling For News Recommendation</a> Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2022personalized/">Personalized Prompt Learning For Explainable Recommendation</a> Lei Li, Yongfeng Zhang, Li Chen </li>
     
   
     
       <li> <a href="/publications/tunstall2022efficient/">Efficient Few-shot Learning Without Prompts</a> Lewis Tunstall et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ravi2022vlc/">VLC-BERT: Visual Question Answering With Contextualized Commonsense Knowledge</a> Sahithya Ravi, Aditya Chinchure, Leonid Sigal, Renjie Liao, Vered Shwartz </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/smith2022using/">Using Deepspeed And Megatron To Train Megatron-turing NLG 530B, A Large-scale Generative Language Model</a> Shaden Smith et al. </li>
     
   
     
       <li> <a href="/publications/thoppilan2022language/">Lamda: Language Models For Dialog Applications</a> Romal Thoppilan et al. </li>
     
   
     
   
     
       <li> <a href="/publications/villegas2022variable/">Phenaki: Variable Length Video Generation From Open Domain Textual Description</a> Ruben Villegas et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2022generative/">Biogpt: Generative Pre-trained Transformer For Biomedical Text Generation And Mining</a> Renqian Luo et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lu2022retrieval/">Reacc: A Retrieval-augmented Code Completion Framework</a> Shuai Lu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chandel2022training/">Training And Evaluating A Jupyter Notebook Data Science Assistant</a> Shubham Chandel, Colin B. Clement, Guillermo Serrato, Neel Sundaresan </li>
     
   
     
       <li> <a href="/publications/chu2022meta/">Meta Policy Learning For Cold-start Conversational Recommendation</a> Zhendong Chu, Hongning Wang, Yun Xiao, Bo Long, Lingfei Wu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/siriwardhana2022improving/">Improving The Domain Adaptation Of Retrieval Augmented Generation (RAG) Models For Open Domain Question Answering</a> Shamane Siriwardhana et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shahriar2023have/">Let's Have A Chat! A Conversation With Chatgpt: Technology, Applications, And Limitations</a> Sakib Shahriar, Kadhim Hayawi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/eldan2023how/">Tinystories: How Small Can Language Models Be And Still Speak Coherent English?</a> Ronen Eldan, Yuanzhi Li </li>
     
   
     
       <li> <a href="/publications/anil2023palm/">Palm 2 Technical Report</a> Rohan Anil et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/antonello2023scaling/">Scaling Laws For Language Encoding Models In Fmri</a> Richard Antonello, Aditya Vaidya, Alexander G. Huth </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sohail2023decoding/">Decoding Chatgpt: A Taxonomy Of Existing Research, Current Challenges, And Possible Future Directions</a> Shahab Saquib Sohail et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/communication2023multilingual/">Seamless: Multilingual Expressive And Streaming Speech Translation</a> Seamless Communication et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mitrovi%C4%872023chatgpt/">Chatgpt Or Human? Detect And Explain. Explaining Decisions Of Machine Learning Model For Detecting Short Chatgpt-generated Text</a> Sandra Mitroviƒá, Davide Andreoletti, Omran Ayoub </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rajput2023recommender/">Recommender Systems With Generative Retrieval</a> Shashank Rajput et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023llama/">Llama-adapter: Efficient Fine-tuning Of Language Models With Zero-init Attention</a> Renrui Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/phung2023grounded/">Grounded Text-to-image Synthesis With Attention Refocusing</a> Quynh Phung, Songwei Ge, Jia-bin Huang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023contrastive/">Medcpt: Contrastive Pre-trained Transformers With Large-scale Pubmed Search Logs For Zero-shot Biomedical Information Retrieval</a> Qiao Jin et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sridhar2023harnessing/">Harnessing Llms In Curricular Design: Using GPT-4 To Support Authoring Of Learning Objectives</a> Pragnya Sridhar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhu2023pre/">3d-vista: Pre-trained Transformer For 3D Vision And Text Alignment</a> Ziyu Zhu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023git/">Git-mol: A Multi-modal Large Language Model For Molecular Science With Graph, Image, And Text</a> Pengfei Liu, Yiming Ren, Jun Tao, Zhixiang Ren </li>
     
   
     
   
     
       <li> <a href="/publications/cai2023do/">Do Large Language Models Resemble Humans In Language Use?</a> Zhenguang G. Cai, Xufeng Duan, David A. Haslett, Shuqi Wang, Martin J. Pickering </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/openai2023gpt/">GPT-4 Technical Report</a> Openai et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dziri2023faith/">Faith And Fate: Limits Of Transformers On Compositionality</a> Nouha Dziri et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/nguyen2023multimodal/">Openvivqa: Task, Dataset, And Multimodal Fusion Models For Visual Question Answering In Vietnamese</a> Nghia Hieu Nguyen, Duong T. D. Vo, Kiet Van Nguyen, Ngan Luu-thuy Nguyen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/liu2023text/">Text Matching Improves Sequential Recommendation By Reducing Popularity Biases</a> Zhenghao Liu et al. </li>
     
   
     
       <li> <a href="/publications/dehghani2023scaling/">Scaling Vision Transformers To 22 Billion Parameters</a> Mostafa Dehghani et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/poli2023hyena/">Hyena Hierarchy: Towards Larger Convolutional Language Models</a> Michael Poli et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/rafiepour2023cnn/">CTRAN: Cnn-transformer-based Network For Natural Language Understanding</a> Mehrdad Rafiepour, Javad Salimi Sartakhti </li>
     
   
     
       <li> <a href="/publications/hasan2023zero/">Zero- And Few-shot Prompting With Llms: A Comparative Study With Fine-tuned Models For Bangla Sentiment Analysis</a> Md. Arid Hasan et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wong2023natural/">Natural Language Generation And Understanding Of Big Code For Ai-assisted Programming: A Review</a> Man Fai Wong, Shangxin Guo, Ching Nam Hang, Siu Wai Ho, Chee Wei Tan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2023parameter/">Parameter-efficient Fine-tuning Methods For Pretrained Language Models: A Critical Review And Assessment</a> Lingling Xu, Haoran Xie, Si-zhao Joe Qin, Xiaohui Tao, Fu Lee Wang </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/brocki2023deep/">Deep Learning Mental Health Dialogue System</a> Lennart Brocki, George C. Dyer, Anna G≈Çadka, Neo Christopher Chung </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kheiri2023exploiting/">Sentimentgpt: Exploiting GPT For Advanced Sentiment Analysis And Its Departure From Current Machine Learning</a> Kiana Kheiri, Hamid Karimi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kalyan2023survey/">A Survey Of GPT-3 Family Large Language Models Including Chatgpt And GPT-4</a> Katikapalli Subramanyam Kalyan </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023blip/">BLIP-2: Bootstrapping Language-image Pre-training With Frozen Image Encoders And Large Language Models</a> Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xiao2023contrastive/">Contrastive Video Question Answering Via Video Graph Transformer</a> Junbin Xiao et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ainslie2023training/">GQA: Training Generalized Multi-query Transformer Models From Multi-head Checkpoints</a> Joshua Ainslie et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/niklaus2023multi/">LEXTREME: A Multi-lingual And Multi-task Benchmark For The Legal Domain</a> Joel Niklaus et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023graphix/">Graphix-t5: Mixing Pre-trained Transformers With Graph-aware Layers For Text-to-sql Parsing</a> Jinyang Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ding2023scaling/">Longnet: Scaling Transformers To 1,000,000,000 Tokens</a> Jiayu Ding et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023unlearn/">Unlearn What You Want To Forget: Efficient Unlearning For Llms</a> Jiaao Chen, Diyi Yang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mu2023learning/">Learning To Compress Prompts With Gist Tokens</a> Jesse Mu, Xiang Lisa Li, Noah Goodman </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/yao2023llm/">LLM Lies: Hallucinations Are Not Bugs, But Features As Adversarial Examples</a> Jia-yu Yao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/koco%C5%842023jack/">Chatgpt: Jack Of All Trades, Master Of None</a> Jan Koco≈Ñ et al. </li>
     
   
     
       <li> <a href="/publications/copet2023simple/">Simple And Controllable Music Generation</a> Jade Copet et al. </li>
     
   
     
   
     
       <li> <a href="/publications/jahan2023evaluation/">Evaluation Of Chatgpt On Biomedical Tasks: A Zero-shot Comparison With Fine-tuned Generative Transformers</a> Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/savelka2023large/">Large Language Models (GPT) Struggle To Answer Multiple-choice Questions About Code</a> Jaromir Savelka, Arav Agarwal, Christopher Bogart, Majd Sakr </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chang2023text/">Muse: Text-to-image Generation Via Masked Generative Transformers</a> Huiwen Chang et al. </li>
     
   
     
   
     
       <li> <a href="/publications/touvron2023open/">Llama: Open And Efficient Foundation Language Models</a> Hugo Touvron et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ye2023ip/">Ip-adapter: Text Compatible Image Prompt Adapter For Text-to-image Diffusion Models</a> Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, Wei Yang </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wang2023pre/">Missrec: Pre-training And Transferring Multi-modal Interest-aware Sequence Representation For Recommendation</a> Jinpeng Wang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/huang2023chatgpt/">Chatgpt For Shaping The Future Of Dentistry: The Potential Of Multi-modal Large Language Model</a> Hanyao Huang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2023explainability/">Explainability For Large Language Models: A Survey</a> Haiyan Zhao et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liao2023gpt/">GPT-4 Enhanced Multimodal Grounding For Autonomous Driving: Leveraging Cross-modal Attention With Large Language Models</a> Haicheng Liao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/song2023from/">Moviechat: From Dense Token To Sparse Memory For Long Video Understanding</a> Enxin Song et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/frantar2023massive/">Sparsegpt: Massive Language Models Can Be Accurately Pruned In One-shot</a> Elias Frantar, Dan Alistarh </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lee2023read/">Read-only Prompt Optimization For Vision-language Few-shot Learning</a> Dongjun Lee et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kim2023solar/">SOLAR 10.7B: Scaling Large Language Models With Simple Yet Effective Depth Up-scaling</a> Dahyun Kim et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jeong2023study/">A Study On The Implementation Of Generative AI Services Using An Enterprise Data-based LLM Application Architecture</a> Cheonsu Jeong </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/wu2023visual/">Visual Chatgpt: Talking, Drawing And Editing With Visual Foundation Models</a> Chenfei Wu et al. </li>
     
   
     
   
     
       <li> <a href="/publications/han2023effective/">E^2VPT: An Effective And Efficient Approach For Visual Prompt Tuning</a> Cheng Han et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023graph/">Graph Transformer For Recommendation</a> Chaoliu Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lund2023chatgpt/">Chatgpt And A New Academic Reality: Artificial Intelligence-written Research Papers And The Ethics Of The Large Language Models In Scholarly Publishing</a> Brady Lund et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/peng2023reinventing/">RWKV: Reinventing Rnns For The Transformer Era</a> Bo Peng et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bulatov2023scaling/">Scaling Transformer To 1M Tokens And Beyond With RMT</a> Aydar Bulatov, Yuri Kuratov, Yermek Kapushev, Mikhail S. Burtsev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2023action/">ADAPT: Action-aware Driving Caption Transformer</a> Bu Jin et al. </li>
     
   
     
   
     
       <li> <a href="/publications/klenitskiy2023turning/">Turning Dross Into Gold Loss: Is Bert4rec Really Better Than Sasrec?</a> Anton Klenitskiy, Alexey Vasilev </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/hendy2023how/">How Good Are GPT Models At Machine Translation? A Comprehensive Evaluation</a> Amr Hendy et al. </li>
     
   
     
       <li> <a href="/publications/kazemnejad2023impact/">The Impact Of Positional Encoding On Length Generalization In Transformers</a> Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, Siva Reddy </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bahrini2023threats/">Chatgpt: Applications, Opportunities, And Threats</a> Aram Bahrini et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gu2023linear/">Mamba: Linear-time Sequence Modeling With Selective State Spaces</a> Albert Gu, Tri Dao </li>
     
   
     
       <li> <a href="/publications/jiang2023mistral/">Mistral 7B</a> Albert Q. Jiang et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/kalai2023calibrated/">Calibrated Language Models Must Hallucinate</a> Adam Tauman Kalai, Santosh S. Vempala </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/garcia2023unreasonable/">The Unreasonable Effectiveness Of Few-shot Learning For Machine Translation</a> Xavier Garcia et al. </li>
     
   
     
   
     
       <li> <a href="/publications/chen2023pali/">Pali-3 Vision Language Models: Smaller, Faster, Stronger</a> Xi Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dai2023towards/">Instructblip: Towards General-purpose Vision-language Models With Instruction Tuning</a> Wenliang Dai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/shi2023retrieval/">REPLUG: Retrieval-augmented Black-box Language Models</a> Weijia Shi et al. </li>
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xin2023survey/">A Survey Of Large Language Models</a> Wayne Xin Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/bezirhan2023automated/">Automated Reading Passage Generation With Openai's Large Language Model</a> Ummugul Bezirhan, Matthias Von Davier </li>
     
   
     
       <li> <a href="/publications/chang2023language/">Language Model Behavior: A Comprehensive Survey</a> Tyler A. Chang, Benjamin K. Bergen </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2023flashattention/">Flashattention-2: Faster Attention With Better Parallelism And Work Partitioning</a> Tri Dao </li>
     
   
     
   
     
       <li> <a href="/publications/ahmed2023automatic/">Automatic Semantic Augmentation Of Language Model Prompts (for Code Summarization)</a> Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, Earl T. Barr </li>
     
   
     
   
     
       <li> <a href="/publications/hariri2023unlocking/">Unlocking The Potential Of Chatgpt: A Comprehensive Exploration Of Its Applications, Advantages, Limitations, And Future Directions In Natural Language Processing</a> Walid Hariri </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gong2023multimodal/">Multimodal-gpt: A Vision And Language Model For Dialogue With Humans</a> Tao Gong et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023comparative/">A Comparative Study Of Pretrained Language Models For Long Clinical Text</a> Yikuan Li, Ramsey M. Wehbe, Faraz S. Ahmad, Hanyin Wang, Yuan Luo </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fathullah2023prompting/">Prompting Large Language Models With Speech Recognition Abilities</a> Yassir Fathullah et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhang2023training/">Controlvideo: Training-free Controllable Text-to-video Generation</a> Yabo Zhang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/li2023textbooks/">Textbooks Are All You Need II: Phi-1.5 Technical Report</a> Yuanzhi Li et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/luo2023open/">Biomedgpt: Open Multimodal Generative Pre-trained Transformer For Biomedicine</a> Yizhen Luo et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/sun2023retentive/">Retentive Network: A Successor To Transformer For Large Language Models</a> Yutao Sun et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2023extending/">Extending Context Window Of Large Language Models Via Positional Interpolation</a> Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian </li>
     
   
     
   
     
       <li> <a href="/publications/gunasekar2023textbooks/">Textbooks Are All You Need</a> Suriya Gunasekar et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/fang2023eva/">EVA-02: A Visual Representation For Neon Genesis</a> Yuxin Fang et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/jin2024hidden/">Hidden Flaws Behind Expert-level Accuracy Of Multimodal GPT-4 Vision In Medicine</a> Qiao Jin et al. </li>
     
   
     
       <li> <a href="/publications/kaur2024from/">From Text To Transformation: A Comprehensive Review Of Large Language Models' Versatility</a> Pravneet Kaur et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lieber2024hybrid/">Jamba: A Hybrid Transformer-mamba Language Model</a> Opher Lieber et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/xu2024survey/">A Survey Of Resource-efficient LLM And Multimodal Foundation Models</a> Mengwei Xu et al. </li>
     
   
     
       <li> <a href="/publications/haque2024exploring/">Exploring Chatgpt And Its Impact On Society</a> Md. Asraful Haque, Shuai Li </li>
     
   
     
   
     
       <li> <a href="/publications/alamin2024history/">History Of Generative Artificial Intelligence (AI) Chatbots: Past, Present, And Future Development</a> Md. Al-amin et al. </li>
     
   
     
   
     
   
     
       <li> <a href="/publications/beck2024extended/">Xlstm: Extended Long Short-term Memory</a> Maximilian Beck et al. </li>
     
   
     
   
     
       <li> <a href="/publications/liu2024linear/">Linrec: Linear Attention Mechanism For Long-term Sequential Recommender Systems</a> Langming Liu et al. </li>
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/chen2024pixart/">Pixart-\sigma: Weak-to-strong Training Of Diffusion Transformer For 4K Text-to-image Generation</a> Junsong Chen et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/zhao2024revolutionizing/">Revolutionizing Finance With Llms: An Overview Of Applications And Insights</a> Huaqin Zhao et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gemmateam2024gemma/">Gemma 2: Improving Open Language Models At A Practical Size</a> Gemma Team et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/gholami2024ai/">AI And Memory Wall</a> Amir Gholami et al. </li>
     
   
     
   
     
       <li> <a href="/publications/ai2024open/">Yi: Open Foundation Models By 01.AI</a> 01. Ai et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/mo2024large/">Large Language Model (LLM) AI Text Generation Detection Based On Transformer Deep Learning Algorithm</a> Yuhong Mo, Hao Qin, Yushan Dong, Ziyi Zhu, Zhenglin Li </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/dao2024transformers/">Transformers Are Ssms: Generalized Models And Efficient Algorithms Through Structured State Space Duality</a> Tri Dao, Albert Gu </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/ma2024era/">The Era Of 1-bit Llms: All Large Language Models Are In 1.58 Bits</a> Shuming Ma et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Uncategorized <a id="Uncategorized"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lin2023generating/">Generating With Confidence: Uncertainty Quantification For Black-box Large Language Models</a> Zhen Lin, Shubhendu Trivedi, Jimeng Sun </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>

   <h3>üè∑ Vector Indexing <a id="Vector Indexing"></a></h3>
   <ul>
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
       <li> <a href="/publications/lewis2020retrieval/">Retrieval-augmented Generation For Knowledge-intensive NLP Tasks</a> Patrick Lewis et al. </li>
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
     
   
   </ul>





    </div>

    <!-- 
    <script>
      document.addEventListener('DOMContentLoaded', function () {
        let isBot = true;

        // Check for mouse movement or keyboard interaction
        document.addEventListener('mousemove', function () {
          isBot = false;
        });
        document.addEventListener('keydown', function () {
          isBot = false;
        });

        // Redirect or take action if no user interaction is detected
        setTimeout(function () {
          if (isBot) {
            // Redirect to a challenge page or show a message
            window.location.href = '/challenge-page.html'; // Update with your actual challenge page
          }
        }, 5000); // Adjust the time delay as needed
      });
    </script>
    -->
  </body>
</html>
