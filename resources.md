---
layout: default
title: Resources on Large Language Models
---

## ü§ñüìö Andrew Ng‚Äôs Latest LLM & Generative AI Courses

- [**Generative AI for Everyone**](https://www.deeplearning.ai/courses/generative-ai-for-everyone/): Non-technical introduction to generative AI and large language models, covering prompt engineering, business applications, and strategy. Taught by **Andrew Ng**.  [oai_citation:0‚Ä°deeplearning.ai](https://www.deeplearning.ai/courses/generative-ai-for-everyone/?utm_source=chatgpt.com)

- [**AI Python for Beginners**](https://www.deeplearning.ai/short-courses/ai-python-for-beginners/): Learn Python API calls, chatbots, debugging, and LLM integrations. Taught by **Andrew Ng**.

- [**LangChain for LLM Application Development**](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/): Build intelligent LLM apps featuring chains, memory, and QA using LangChain. Co-taught by **Andrew Ng** and Harrison Chase.

- [**ChatGPT Prompt Engineering for Developers**](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/): Techniques for crafting effective prompts and building bots with the OpenAI API. Co-taught by **Andrew Ng** and Isa Fulford.

- [**Building Systems with the ChatGPT API**](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/): Develop end-to-end LLM workflows and integrations using the ChatGPT API. Co-taught by **Andrew Ng** and Isa Fulford.

## üÜï New Short Courses on DeepLearning.AI

- [**Orchestrating Workflows for GenAI Applications**](https://www.deeplearning.ai/short-courses/orchestrating-workflows-for-genai-applications/): Learn to turn a GenAI or RAG prototype into a production-ready, automated pipeline using Apache Airflow. (by Astronomer)

- [**DSPy: Build and Optimize Agentic Apps**](https://www.deeplearning.ai/short-courses/dspy-build-and-optimize-agentic-apps/): Build, debug, and optimize AI agents using DSPy and MLflow. (by Databricks)

- [**Reinforcement Fine-Tuning LLMs with GRPO**](https://www.deeplearning.ai/short-courses/reinforcement-fine-tuning-llms-grpo/): Improve LLM reasoning and performance with reinforcement learning using GRPO (Group Relative Policy Optimization). (by Predibase)

- [**MCP: Build Rich-Context AI Apps with Anthropic**](https://www.deeplearning.ai/short-courses/mcp-build-rich-context-ai-apps-with-anthropic/): Build AI apps that access tools, data, and prompts using Anthropic's Model Context Protocol. (by Anthropic)

- [**Building AI Voice Agents for Production**](https://www.deeplearning.ai/short-courses/building-ai-voice-agents-for-production/): Build responsive, human-like AI voice applications. (by LiveKit and RealAvatar)

- [**LLMs as Operating Systems: Agent Memory**](https://www.deeplearning.ai/short-courses/llms-as-operating-systems-agent-memory/): Build memory-augmented systems with MemGPT agents. (by Letta)

- [**Building Code Agents with Hugging Face smolagents**](https://www.deeplearning.ai/short-courses/building-code-agents-with-hugging-face-smolagents/): Build agents that write and execute code using Hugging Face's smolagents framework. (by Hugging Face)

- [**Building AI Browser Agents**](https://www.deeplearning.ai/short-courses/building-ai-browser-agents/): Build browser agents that navigate and interact with websites reliably. (by AGI Inc)

- [**Getting Structured LLM Output**](https://www.deeplearning.ai/short-courses/getting-structured-llm-output/): Generate structured output to power robust production LLM applications. (by DotTxt)

- [**Vibe Coding 101 with Replit**](https://www.deeplearning.ai/short-courses/vibe-coding-101-with-replit/): Learn to build and deploy AI coding agents in a web-based IDE. (by Replit)

- [**Long-Term Agentic Memory with LangGraph**](https://www.deeplearning.ai/short-courses/long-term-agentic-memory-with-langgraph/): Build long-memory agents using LangGraph and LangMem. (by LangChain)

- [**Event-Driven Agentic Document Workflows**](https://www.deeplearning.ai/short-courses/event-driven-agentic-document-workflows/): Process documents and fill forms using agent workflows with RAG. (by LlamaIndex)

- [**Build Apps with Windsurf‚Äôs AI Coding Agents**](https://www.deeplearning.ai/short-courses/build-apps-with-windsurf-ai-coding-agents/): Debug and deploy applications with Windsurf‚Äôs AI-powered IDE. (by Windsurf)

- [**Evaluating AI Agents**](https://www.deeplearning.ai/short-courses/evaluating-ai-agents/): Evaluate, improve, and iterate on AI agents using structured assessments. (by Arize AI)

- [**Attention in Transformers: Concepts and Code in PyTorch**](https://www.deeplearning.ai/short-courses/attention-in-transformers-concepts-and-code-in-pytorch/): Implement the attention mechanism in PyTorch and understand its impact. (by StatQuest)

- [**How Transformer LLMs Work**](https://www.deeplearning.ai/short-courses/how-transformer-llms-work/): A visual and code-based introduction to the architecture behind modern LLMs. (by Jay Alammar & Maarten Grootendorst)

- [**Building Towards Computer Use with Anthropic**](https://www.deeplearning.ai/short-courses/building-towards-computer-use-with-anthropic/): Learn how AI assistants can perform real tasks on computers. (by Anthropic)

- [**Build Long-Context AI Apps with Jamba**](https://www.deeplearning.ai/short-courses/build-long-context-ai-apps-with-jamba/): Create apps that handle long documents using the Jamba model. (by AI21 Labs)

- [**Reasoning with o1**](https://www.deeplearning.ai/short-courses/reasoning-with-o1/): Learn how to use and prompt OpenAI's o1 model for reasoning tasks. (by OpenAI)

- [**Collaborative Writing and Coding with OpenAI Canvas**](https://www.deeplearning.ai/short-courses/collaborative-writing-and-coding-with-openai-canvas/): Collaborate with AI to write and code using OpenAI Canvas. (by OpenAI)
  
## üìùüîó Blog Posts & Useful Links

- [**LLM Powered Autonomous Agents**](https://lilianweng.github.io/posts/2023-06-23-agent/): A deep dive into how large language models are powering the next generation of autonomous agents, enabling systems to perform complex tasks with minimal human input.

- [**Google "We Have No Moat, And Neither Does OpenAI"**](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither): Leaked internal Google document discussing the competitive landscape of AI and arguing that neither Google nor OpenAI have sustainable competitive advantages in the long term.

- [**Prompt Engineering**](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/): An introduction to prompt engineering techniques, providing guidelines on how to effectively interact with large language models to obtain the best results.

- [**How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources**](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1): This article investigates how GPT models acquire their emergent abilities, tracing them back to the training data and architectures used.

- [**Why did all of the public reproduction of GPT-3 fail?**](https://jingfengyang.github.io/gpt): This post explores the difficulties and challenges researchers faced when attempting to reproduce the capabilities of GPT-3, offering insights into why these efforts largely fell short.

- [**Alpaca: Synthetic Data for LLMs**](https://crfm.stanford.edu/2023/03/13/alpaca.html): Stanford's approach to generating synthetic data for fine-tuning large language models using OpenAI's API.

- [**Evol-Instruct: Improving Dataset Quality**](https://arxiv.org/abs/2304.12244): Techniques for enhancing instruction datasets with evolved synthetic data.

- [**Orca: High-Quality Data Generation**](https://arxiv.org/abs/2306.02707): Orca paper explaining how to generate better synthetic data through instruction following and feedback models.

- [**Scaling Laws for LLMs**](https://arxiv.org/pdf/2001.08361.pdf): A study on scaling laws, which predict LLM performance based on model and dataset size.

- [**Chinchilla's Wild Implications**](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications): Insights into how the scaling laws affect LLMs' computational efficiency.

- [**TinyLlama**](https://github.com/jzhang38/TinyLlama): A project focused on training a Llama model from scratch, providing insights into pre-training LLMs.

- [**BigBench: LLM Benchmarking**](https://github.com/google/BIG-bench): A large-scale benchmark for evaluating LLM capabilities across various tasks.

- [**Training a Causal Language Model from Scratch**](https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt): Hugging Face tutorial on pre-training GPT-2 from scratch using the transformers library.

- [**LLMDataHub: Curated Datasets for LLMs**](https://github.com/Zjh-819/LLMDataHub): Collection of datasets for pre-training, fine-tuning, and RLHF of large language models.

- [**Perplexity in LLMs**](https://huggingface.co/docs/transformers/perplexity): Hugging Face guide on measuring model perplexity for text generation tasks.

- [**Karpathy's Zero to Hero: GPT**](https://www.youtube.com/watch?v=kCc8FmEb1nY): A 2-hour course by Andrej Karpathy on building a GPT model from scratch, focusing on tokenization and transformer fundamentals.

- [**Karpathy's Intro to Tokenization**](https://www.youtube.com/watch?v=jV0GMp8mb5g): A detailed introduction to tokenization for LLMs, explaining how text is processed into tokens for transformer models.

## Podcasts with Deep Learning Superstars

- [**The AI Alignment Podcast**](https://futureoflife.org/ai-alignment-podcast/): Conversations with leading AI researchers and thinkers like Stuart Russell, Yoshua Bengio, and more, covering cutting-edge research in AI alignment and deep learning.

- [**Lex Fridman Podcast**](https://lexfridman.com/podcast/): Features interviews with AI pioneers like Yann LeCun, Geoffrey Hinton, Demis Hassabis, and Andrej Karpathy, discussing AI, deep learning, and the future of technology.

- [**Machine Learning Street Talk**](https://www.youtube.com/c/MachineLearningStreetTalk): In-depth discussions with AI researchers such as Yannic Kilcher and Connor Leahy, tackling topics in AI ethics, deep learning, and more.

- [**The Gradient Podcast**](https://thegradient.pub/category/podcast/): Interviews with researchers and practitioners in AI, deep learning, and NLP, including guests like Fei-Fei Li and Sebastian Ruder.

- [**TWIML AI Podcast**](https://twimlai.com/tag/podcast/): Host Sam Charrington interviews top minds in AI and machine learning, such as Andrew Ng and Ian Goodfellow, diving deep into industry trends and research breakthroughs.

- [**Data Skeptic**](https://dataskeptic.com/podcast): A podcast covering data science, machine learning, and AI, featuring leading experts from academia and industry, like Charles Isbell and Dario Amodei.
  
## üéìüß† Courses on Large Language Models (LLMs)

Below is a collection of university and online courses that offer a deep dive into the concepts, tools, and applications of Large Language Models (LLMs). These courses range from theoretical foundations to practical applications in business and data science.

### University Courses

1. [**Stanford University - TECH 16: Large Language Models for Business with Python**](https://continuingstudies.stanford.edu/courses/professional-and-personal-development/large-language-models-for-business-with-python/20232_TECH-16): This course covers the use of LLMs in business applications, with a focus on practical programming with Python. Students learn how to integrate LLMs into business processes to drive innovation and efficiency.

2. [**ETH Z√ºrich - 263-5354-00L: Large Language Models**](https://rycolab.io/classes/llm-s23/): Focused on the theoretical underpinnings and current developments of LLMs, this course covers a broad range of topics from model training to application.

3. [**University of Toronto - COMP790-101: Large Language Models**](https://github.com/craffel/llm-seminar): This seminar-style course reviews the latest research on LLMs, covering both foundational knowledge and emerging trends in their development.

### Online Courses

1. [**Coursera - Natural Language Processing with Transformers**](https://www.coursera.org/learn/transformers): This course introduces transformers, which are the foundation of modern LLMs. It focuses on using transformers for various NLP tasks such as text classification, summarization, and translation.

2. [**DataCamp - Transformer Models for NLP**](https://www.datacamp.com/courses/transformer-models-for-nlp): Learn how to leverage transformer models to perform advanced natural language processing tasks with hands-on coding exercises in Python.

3. [**Udemy - GPT-3 and OpenAI API: A Guide for Building LLM-Powered Applications**](https://www.udemy.com/course/gpt3-openai-api/): This course provides practical insights into using GPT-3 and OpenAI‚Äôs API to build applications that utilize LLMs, with a focus on creating conversational agents and content generation.

4. [**DeepLearning.AI - Generative AI with Large Language Models**](https://www.deeplearning.ai/courses/generative-ai-with-llms/): This course from DeepLearning.AI covers the key concepts of generative AI, with a particular focus on LLMs. It includes hands-on practice in fine-tuning LLMs, prompt engineering, and applying these models to real-world use cases.
  
## Tools & Packages

- [**LangChain**](https://python.langchain.com/docs/get_started/introduction): A framework for building LLM-powered applications with modular integrations, memory, and chaining prompts.

- [**LlamaIndex**](https://gpt-index.readthedocs.io/en/latest/): Connects LLMs with external data like documents and databases, ideal for knowledge-augmented applications.

- [**Dyson**](https://github.com/turing-technologies/dyson): Enables dynamic instruction tuning and fine-tuning of LLMs with custom prompts and instructions.

- [**LangGraph**](https://github.com/langchain-ai/langgraph): Integrates LLMs with graph-based data, enhancing structured data querying and reasoning.

- [**DeepSpeed**](https://www.deepspeed.ai/): Optimizes large model training with techniques like ZeRO, quantization, and memory efficiency.

- [**Hugging Face Transformers**](https://huggingface.co/docs/transformers): Provides tools for using, fine-tuning, and deploying transformer models like GPT and BERT.

- [**OpenRouter**](https://openrouter.ai/): An open-source alternative for routing prompts through multiple LLM APIs like GPT-4 and Claude.

- [**Guidance**](https://github.com/guidance-ai/guidance): A library to guide and structure LLM outputs programmatically for complex tasks.

- [**Haystack**](https://haystack.deepset.ai/overview/intro): A framework for building scalable LLM-powered search and retrieval systems, including RAG pipelines.

- [**FastRAG**](https://github.com/IntelLabs/fastRAG): Efficient framework for low-latency, scalable Retrieval-Augmented Generation (RAG) pipelines.

- [**DSPy**](https://github.com/stanfordnlp/dspy): A library that allows you to optimize prompts and LLM outputs through programmatic evaluation.

### üß∞üì¶ Books

1. [**Deep Learning**](https://www.deeplearningbook.org/) by Ian Goodfellow, Yoshua Bengio, and Aaron Courville: A foundational book that covers the principles of deep learning. It provides theoretical insights and practical applications, making it essential for understanding the building blocks of LLMs.

2. [**Natural Language Processing with Transformers**](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) by Lewis Tunstall, Leandro von Werra, and Thomas Wolf: This book offers a practical guide to using transformer models for NLP tasks, with a focus on tools like Hugging Face's libraries. It‚Äôs a great resource for anyone working with modern LLMs.

3. [**Transformers for Natural Language Processing**](https://www.packtpub.com/en-us/product/transformers-for-natural-language-processing-9781800565791?srsltid=AfmBOooDVYPilamoz-i2YbaiP8K2QrslJGiP-QAy37HSb-iR0WdbsSdy) by Denis Rothman: This book provides an in-depth look at transformer models, from BERT to GPT-3, and explains how to implement them for a variety of NLP tasks.

4. [**GPT-3: Building Innovative NLP Products Using Large Language Models**](https://www.amazon.co.uk/Gpt-3-Building-Innovative-Products-Language/dp/1098113624) by Sandra Kublik, Shubham Saboo, and Dhaval Pattani: A hands-on guide for building applications using GPT-3, covering everything from prompt engineering to integrating GPT-3 into real-world products.

5. [**Neural Networks and Deep Learning**](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen: A classic introduction to neural networks and deep learning, providing a step-by-step guide to building and understanding deep models, which serve as the foundation for LLMs.

6. [**Hands-On Large Language Models: Language Understanding and Generation**](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/): provides practical tools for using LLMs in tasks like copywriting, summarization, and semantic search. It covers transformer architecture, generative models, and fine-tuning techniques to optimize LLMs for specific applications.

### üóÇÔ∏èüîç Other Collections of Related Work

- [**Awesome-LLM: a curated list of Large Language Mode**](https://github.com/Hannibal046/Awesome-LLM): A comprehensive and well-maintained repository that curates resources, papers, tools, and frameworks related to Large Language Models (LLMs). It covers a wide range of topics including model architectures, training techniques, and applications.

Please, feel free to submit a [web form](contributing.html) to add more links in this page.
