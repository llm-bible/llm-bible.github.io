---
layout: default
title: Resources on Large Language Models
---

## Blog Posts & Useful Links

- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/): A deep dive into how large language models are powering the next generation of autonomous agents, enabling systems to perform complex tasks with minimal human input.

- [Google "We Have No Moat, And Neither Does OpenAI"](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither): Leaked internal Google document discussing the competitive landscape of AI and arguing that neither Google nor OpenAI have sustainable competitive advantages in the long term.

- [Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/): An introduction to prompt engineering techniques, providing guidelines on how to effectively interact with large language models to obtain the best results.

- [How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1): This article investigates how GPT models acquire their emergent abilities, tracing them back to the training data and architectures used.

- [Why did all of the public reproduction of GPT-3 fail?](https://jingfengyang.github.io/gpt): This post explores the difficulties and challenges researchers faced when attempting to reproduce the capabilities of GPT-3, offering insights into why these efforts largely fell short.

- [Alpaca: Synthetic Data for LLMs](https://crfm.stanford.edu/2023/03/13/alpaca.html): Stanford's approach to generating synthetic data for fine-tuning large language models using OpenAI's API.

- [Evol-Instruct: Improving Dataset Quality](https://arxiv.org/abs/2304.12244): Techniques for enhancing instruction datasets with evolved synthetic data.

- [Orca: High-Quality Data Generation](https://arxiv.org/abs/2306.02707): Orca paper explaining how to generate better synthetic data through instruction following and feedback models.

- [Scaling Laws for LLMs](https://arxiv.org/pdf/2001.08361.pdf): A study on scaling laws, which predict LLM performance based on model and dataset size.

- [Chinchilla's Wild Implications](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications): Insights into how the scaling laws affect LLMs' computational efficiency.

- [TinyLlama](https://github.com/jzhang38/TinyLlama): A project focused on training a Llama model from scratch, providing insights into pre-training LLMs.

- [BigBench: LLM Benchmarking](https://github.com/google/BIG-bench): A large-scale benchmark for evaluating LLM capabilities across various tasks.

- [Training a Causal Language Model from Scratch](https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt): Hugging Face tutorial on pre-training GPT-2 from scratch using the transformers library.

- [LLMDataHub: Curated Datasets for LLMs](https://github.com/Zjh-819/LLMDataHub): Collection of datasets for pre-training, fine-tuning, and RLHF of large language models.

- [Perplexity in LLMs](https://huggingface.co/docs/transformers/perplexity): Hugging Face guide on measuring model perplexity for text generation tasks.

- [Karpathy's Zero to Hero: GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY): A 2-hour course by Andrej Karpathy on building a GPT model from scratch, focusing on tokenization and transformer fundamentals.

- [Karpathy's Intro to Tokenization](https://www.youtube.com/watch?v=jV0GMp8mb5g): A detailed introduction to tokenization for LLMs, explaining how text is processed into tokens for transformer models.

## Podcasts with Deep Learning Superstars

- [**The AI Alignment Podcast**](https://futureoflife.org/ai-alignment-podcast/): Conversations with leading AI researchers and thinkers like Stuart Russell, Yoshua Bengio, and more, covering cutting-edge research in AI alignment and deep learning.

- [**Lex Fridman Podcast**](https://lexfridman.com/podcast/): Features interviews with AI pioneers like Yann LeCun, Geoffrey Hinton, Demis Hassabis, and Andrej Karpathy, discussing AI, deep learning, and the future of technology.

- [**Machine Learning Street Talk**](https://www.youtube.com/c/MachineLearningStreetTalk): In-depth discussions with AI researchers such as Yannic Kilcher and Connor Leahy, tackling topics in AI ethics, deep learning, and more.

- [**The Gradient Podcast**](https://thegradient.pub/category/podcast/): Interviews with researchers and practitioners in AI, deep learning, and NLP, including guests like Fei-Fei Li and Sebastian Ruder.

- [**TWIML AI Podcast**](https://twimlai.com/tag/podcast/): Host Sam Charrington interviews top minds in AI and machine learning, such as Andrew Ng and Ian Goodfellow, diving deep into industry trends and research breakthroughs.

- [**Data Skeptic**](https://dataskeptic.com/podcast): A podcast covering data science, machine learning, and AI, featuring leading experts from academia and industry, like Charles Isbell and Dario Amodei.
  
## Courses on Large Language Models (LLMs)

Below is a collection of university and online courses that offer a deep dive into the concepts, tools, and applications of Large Language Models (LLMs). These courses range from theoretical foundations to practical applications in business and data science.

### University Courses

1. [Stanford University - TECH 16: Large Language Models for Business with Python](https://continuingstudies.stanford.edu/courses/professional-and-personal-development/large-language-models-for-business-with-python/20232_TECH-16): This course covers the use of LLMs in business applications, with a focus on practical programming with Python. Students learn how to integrate LLMs into business processes to drive innovation and efficiency.

2. [ETH Zürich - 263-5354-00L: Large Language Models](https://rycolab.io/classes/llm-s23/): Focused on the theoretical underpinnings and current developments of LLMs, this course covers a broad range of topics from model training to application.

3. [University of Toronto - COMP790-101: Large Language Models](https://github.com/craffel/llm-seminar): This seminar-style course reviews the latest research on LLMs, covering both foundational knowledge and emerging trends in their development.

### Online Courses

1. [Coursera - Natural Language Processing with Transformers](https://www.coursera.org/learn/transformers): This course introduces transformers, which are the foundation of modern LLMs. It focuses on using transformers for various NLP tasks such as text classification, summarization, and translation.

2. [DataCamp - Transformer Models for NLP](https://www.datacamp.com/courses/transformer-models-for-nlp): Learn how to leverage transformer models to perform advanced natural language processing tasks with hands-on coding exercises in Python.

3. [Udemy - GPT-3 and OpenAI API: A Guide for Building LLM-Powered Applications](https://www.udemy.com/course/gpt3-openai-api/): This course provides practical insights into using GPT-3 and OpenAI’s API to build applications that utilize LLMs, with a focus on creating conversational agents and content generation.

4. [DeepLearning.AI - Generative AI with Large Language Models](https://www.deeplearning.ai/courses/generative-ai-with-llms/): This course from DeepLearning.AI covers the key concepts of generative AI, with a particular focus on LLMs. It includes hands-on practice in fine-tuning LLMs, prompt engineering, and applying these models to real-world use cases.
  
## Tools & Packages

- [LangChain](https://python.langchain.com/docs/get_started/introduction): A framework for building LLM-powered applications with modular integrations, memory, and chaining prompts.

- [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/): Connects LLMs with external data like documents and databases, ideal for knowledge-augmented applications.

- [Dyson](https://github.com/turing-technologies/dyson): Enables dynamic instruction tuning and fine-tuning of LLMs with custom prompts and instructions.

- [LangGraph](https://github.com/langchain-ai/langgraph): Integrates LLMs with graph-based data, enhancing structured data querying and reasoning.

- [DeepSpeed](https://www.deepspeed.ai/): Optimizes large model training with techniques like ZeRO, quantization, and memory efficiency.

- [Hugging Face Transformers](https://huggingface.co/docs/transformers): Provides tools for using, fine-tuning, and deploying transformer models like GPT and BERT.

- [OpenRouter](https://openrouter.ai/): An open-source alternative for routing prompts through multiple LLM APIs like GPT-4 and Claude.

- [Guidance](https://github.com/guidance-ai/guidance): A library to guide and structure LLM outputs programmatically for complex tasks.

- [Haystack](https://haystack.deepset.ai/overview/intro): A framework for building scalable LLM-powered search and retrieval systems, including RAG pipelines.

- [FastRAG](https://github.com/IntelLabs/fastRAG): Efficient framework for low-latency, scalable Retrieval-Augmented Generation (RAG) pipelines.

- [DSPy](https://github.com/stanfordnlp/dspy): A library that allows you to optimize prompts and LLM outputs through programmatic evaluation.

### Books

1. [Deep Learning](https://www.deeplearningbook.org/) by Ian Goodfellow, Yoshua Bengio, and Aaron Courville: A foundational book that covers the principles of deep learning. It provides theoretical insights and practical applications, making it essential for understanding the building blocks of LLMs.

2. [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) by Lewis Tunstall, Leandro von Werra, and Thomas Wolf: This book offers a practical guide to using transformer models for NLP tasks, with a focus on tools like Hugging Face's libraries. It’s a great resource for anyone working with modern LLMs.

3. [Transformers for Natural Language Processing](https://www.packtpub.com/en-us/product/transformers-for-natural-language-processing-9781800565791?srsltid=AfmBOooDVYPilamoz-i2YbaiP8K2QrslJGiP-QAy37HSb-iR0WdbsSdy) by Denis Rothman: This book provides an in-depth look at transformer models, from BERT to GPT-3, and explains how to implement them for a variety of NLP tasks.

4. [GPT-3: Building Innovative NLP Products Using Large Language Models](https://www.amazon.co.uk/Gpt-3-Building-Innovative-Products-Language/dp/1098113624) by Sandra Kublik, Shubham Saboo, and Dhaval Pattani: A hands-on guide for building applications using GPT-3, covering everything from prompt engineering to integrating GPT-3 into real-world products.

5. [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen: A classic introduction to neural networks and deep learning, providing a step-by-step guide to building and understanding deep models, which serve as the foundation for LLMs.

6. [Hands-On Large Language Models: Language Understanding and Generation ](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/): provides practical tools for using LLMs in tasks like copywriting, summarization, and semantic search. It covers transformer architecture, generative models, and fine-tuning techniques to optimize LLMs for specific applications.

### Other Collections of Related Work

1. [Awesome-LLM: a curated list of Large Language Mode](https://github.com/Hannibal046/Awesome-LLM): A comprehensive and well-maintained repository that curates resources, papers, tools, and frameworks related to Large Language Models (LLMs). It covers a wide range of topics including model architectures, training techniques, and applications.

Please, feel free to submit a [web form](contributing.html) to add more links in this page.
