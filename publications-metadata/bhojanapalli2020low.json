[["michel2019are", "Are Sixteen Heads Really Better Than One?"], ["kovaleva2019revealing", "Revealing The Dark Secrets Of BERT"], ["voita2019analyzing", "Analyzing Multi-head Self-attention: Specialized Heads Do The Heavy Lifting, The Rest Can Be Pruned"], ["ainslie2023training", "GQA: Training Generalized Multi-query Transformer Models From Multi-head Checkpoints"]]