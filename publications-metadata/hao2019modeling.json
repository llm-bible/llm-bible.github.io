[["lei2021when", "When Attention Meets Fast Recurrence: Training Language Models With Reduced Compute"], ["sun2023retentive", "Retentive Network: A Successor To Transformer For Large Language Models"], ["vaswani2017attention", "Attention Is All You Need"], ["ahmed2017weighted", "Weighted Transformer Network For Machine Translation"]]