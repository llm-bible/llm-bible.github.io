[["tambe2020sentence", "Edgebert: Sentence-level Energy Optimizations For Latency-aware Multi-task NLP Inference"], ["xu2022survey", "A Survey On Model Compression And Acceleration For Pretrained Language Models"], ["zadeh2020quantizing", "GOBO: Quantizing Attention-based NLP Models For Low Latency And Energy Efficient Inference"], ["ma2024era", "The Era Of 1-bit Llms: All Large Language Models Are In 1.58 Bits"]]