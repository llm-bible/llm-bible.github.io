[["narayanan2021efficient", "Efficient Large-scale Language Model Training On GPU Clusters Using Megatron-lm"], ["zeng2021pangu", "Pangu-\\(\u03b1\\): Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation"], ["li2021token", "Terapipe: Token-level Pipeline Parallelism For Training Large-scale Language Models"], ["pourreza2023din", "DIN-SQL: Decomposed In-context Learning Of Text-to-sql With Self-correction"]]