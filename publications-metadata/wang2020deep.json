[["wang2020multi", "Minilmv2: Multi-head Self-attention Relation Distillation For Compressing Pretrained Transformers"], ["wu2021one", "One Teacher Is Enough? Pre-trained Language Model Distillation From Multiple Teachers"], ["liu2019multi", "MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models"], ["kim2016sequence", "Sequence-level Knowledge Distillation"]]