[["yao2022efficient", "Zeroquant: Efficient And Affordable Post-training Quantization For Large-scale Transformers"], ["bondarenko2021understanding", "Understanding And Overcoming The Challenges Of Efficient Transformer Quantization"], ["huang2024pushing", "Billm: Pushing The Limit Of Post-training Quantization For Llms"], ["frantar2022accurate", "GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers"]]