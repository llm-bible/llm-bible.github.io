[["clark2020pre", "ELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators"], ["chung2020rethinking", "Rethinking Embedding Coupling In Pre-trained Language Models"], ["chi2021xlm", "XLM-E: Cross-lingual Language Model Pre-training Via ELECTRA"], ["goyal2021larger", "Larger-scale Transformers For Multilingual Masked Language Modeling"]]