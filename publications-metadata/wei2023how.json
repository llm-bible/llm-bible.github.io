[["sun2023safety", "Safety Assessment Of Chinese Large Language Models"], ["qi2023fine", "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"], ["zeng2024how", "How Johnny Can Persuade Llms To Jailbreak Them: Rethinking Persuasion To Challenge AI Safety By Humanizing Llms"], ["shayegani2023survey", "Survey Of Vulnerabilities In Large Language Models Revealed By Adversarial Attacks"]]