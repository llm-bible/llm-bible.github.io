[["dao2023flashattention", "Flashattention-2: Faster Attention With Better Parallelism And Work Partitioning"], ["poli2023hyena", "Hyena Hierarchy: Towards Larger Convolutional Language Models"], ["ma2021linear", "Luna: Linear Unified Nested Attention"], ["hua2022transformer", "Transformer Quality In Linear Time"]]