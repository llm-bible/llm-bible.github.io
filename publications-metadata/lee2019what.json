[["sukhbaatar2019augmenting", "Augmenting Self-attention With Persistent Memory"], ["dou2018exploiting", "Exploiting Deep Representations For Neural Machine Translation"], ["sajjad2020effect", "On The Effect Of Dropping Layers Of Pre-trained Transformer Models"], ["radiyadixit2020how", "How Fine Can Fine-tuning Be? Learning Efficient Language Models"]]