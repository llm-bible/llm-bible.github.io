[["dong2019unified", "Unified Language Model Pre-training For Natural Language Understanding And Generation"], ["devlin2018pre", "BERT: Pre-training Of Deep Bidirectional Transformers For Language Understanding"], ["xiao2020ernie", "ERNIE-GEN: An Enhanced Multi-flow Pre-training And Fine-tuning Framework For Natural Language Generation"], ["bi2020pre", "PALM: Pre-training An Autoencoding&autoregressive Language Model For Context-conditioned Generation"]]