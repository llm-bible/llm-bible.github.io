[["shayegani2023survey", "Survey Of Vulnerabilities In Large Language Models Revealed By Adversarial Attacks"], ["greshake2023not", "Not What You've Signed Up For: Compromising Real-world Llm-integrated Applications With Indirect Prompt Injection"], ["carlini2023are", "Are Aligned Neural Networks Adversarially Aligned?"], ["kang2023exploiting", "Exploiting Programmatic Behavior Of Llms: Dual-use Through Standard Security Attacks"]]