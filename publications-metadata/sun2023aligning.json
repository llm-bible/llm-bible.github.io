[["rafailov2023direct", "Direct Preference Optimization: Your Language Model Is Secretly A Reward Model"], ["wu2023fine", "Fine-grained Human Feedback Gives Better Rewards For Language Model Training"], ["zheng2023secrets", "Secrets Of RLHF In Large Language Models Part I: PPO"], ["k\u00f6pf2023openassistant", "Openassistant Conversations -- Democratizing Large Language Model Alignment"]]