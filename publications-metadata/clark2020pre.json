[["chi2021xlm", "XLM-E: Cross-lingual Language Model Pre-training Via ELECTRA"], ["voita2019bottom", "The Bottom-up Evolution Of Representations In The Transformer: A Study With Machine Translation And Language Modeling Objectives"], ["eisenschlos2019efficient", "Multifit: Efficient Multi-lingual Language Model Fine-tuning"], ["su2021improving", "Tacl: Improving BERT Pre-training With Token-aware Contrastive Learning"]]