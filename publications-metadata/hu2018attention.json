[["liu2019multi", "MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models"], ["lu2022ernie", "Ernie-search: Bridging Cross-encoder With Dual-encoder Via Self On-the-fly Distillation For Dense Passage Retrieval"], ["kim2016sequence", "Sequence-level Knowledge Distillation"], ["wu2021one", "One Teacher Is Enough? Pre-trained Language Model Distillation From Multiple Teachers"]]