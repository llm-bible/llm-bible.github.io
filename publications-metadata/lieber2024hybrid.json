[["kim2021scalable", "Scalable And Efficient Moe Training For Multitask Multilingual Models"], ["rajbhandari2022deepspeed", "Deepspeed-moe: Advancing Mixture-of-experts Inference And Training To Power Next-generation AI Scale"], ["gu2023linear", "Mamba: Linear-time Sequence Modeling With Selective State Spaces"], ["he2021fast", "Fastmoe: A Fast Mixture-of-expert Training System"]]