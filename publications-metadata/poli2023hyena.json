[["ding2023scaling", "Longnet: Scaling Transformers To 1,000,000,000 Tokens"], ["vari\u01612021sequence", "Sequence Length Is A Domain: Length-based Overfitting In Transformer Models"], ["gu2023linear", "Mamba: Linear-time Sequence Modeling With Selective State Spaces"], ["zaheer2020big", "Big Bird: Transformers For Longer Sequences"]]