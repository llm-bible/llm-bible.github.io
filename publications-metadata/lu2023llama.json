[["xu2023parameter", "Parameter-efficient Fine-tuning Methods For Pretrained Language Models: A Critical Review And Assessment"], ["rozi\u00e8re2023code", "Code Llama: Open Foundation Models For Code"], ["hu2023llm", "Llm-adapters: An Adapter Family For Parameter-efficient Fine-tuning Of Large Language Models"], ["wang2022mixture", "Adamix: Mixture-of-adaptations For Parameter-efficient Model Tuning"]]