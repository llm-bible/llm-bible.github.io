[["bondarenko2021understanding", "Understanding And Overcoming The Challenges Of Efficient Transformer Quantization"], ["xiao2022accurate", "Smoothquant: Accurate And Efficient Post-training Quantization For Large Language Models"], ["frantar2022accurate", "GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers"], ["kim2023memory", "Memory-efficient Fine-tuning Of Compressed Large Language Models Via Sub-4-bit Integer Quantization"]]