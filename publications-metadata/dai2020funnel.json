[["kim2021learned", "Learned Token Pruning For Transformers"], ["ding2023scaling", "Longnet: Scaling Transformers To 1,000,000,000 Tokens"], ["jiang2023redundancy", "Mixphm: Redundancy-aware Parameter-efficient Tuning For Low-resource Visual Question Answering"], ["lu2021pretrained", "Pretrained Transformers As Universal Computation Engines"]]