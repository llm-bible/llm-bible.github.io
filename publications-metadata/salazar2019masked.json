[["shin2020eliciting", "Autoprompt: Eliciting Knowledge From Language Models With Automatically Generated Prompts"], ["su2021improving", "Tacl: Improving BERT Pre-training With Token-aware Contrastive Learning"], ["wettig2022should", "Should You Mask 15% In Masked Language Modeling?"], ["zhao2020masking", "Masking As An Efficient Alternative To Finetuning For Pretrained Language Models"]]