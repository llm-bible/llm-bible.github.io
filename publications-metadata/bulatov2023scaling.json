[["bulatov2022recurrent", "Recurrent Memory Transformer"], ["ding2023scaling", "Longnet: Scaling Transformers To 1,000,000,000 Tokens"], ["cherti2022reproducible", "Reproducible Scaling Laws For Contrastive Language-image Learning"], ["tay2021scale", "Scale Efficiently: Insights From Pre-training And Fine-tuning Transformers"]]