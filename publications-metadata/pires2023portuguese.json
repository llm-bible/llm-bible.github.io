[["rust2020how", "How Good Is Your Tokenizer? On The Monolingual Performance Of Multilingual Language Models"], ["wang2020pretrain", "To Pretrain Or Not To Pretrain: Examining The Benefits Of Pretraining On Resource Rich Tasks"], ["singh2020are", "Are We Pretraining It Right? Digging Deeper Into Visio-linguistic Pretraining"], ["razeghi2022impact", "Impact Of Pretraining Term Frequencies On Few-shot Reasoning"]]