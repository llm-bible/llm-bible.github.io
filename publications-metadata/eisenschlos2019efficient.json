[["longpre2020how", "How Effective Is Task-agnostic Data Augmentation For Pretrained Transformers?"], ["ma2021encoder", "Deltalm: Encoder-decoder Pre-training For Language Generation And Translation By Augmenting Pretrained Multilingual Encoders"], ["clark2020pre", "ELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators"], ["zan2022continual", "CERT: Continual Pre-training On Sketches For Library-oriented Code Generation"]]