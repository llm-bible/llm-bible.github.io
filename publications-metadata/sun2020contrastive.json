[["fu2020lrc", "LRC-BERT: Latent-representation Contrastive Knowledge Distillation For Natural Language Understanding"], ["wu2021one", "One Teacher Is Enough? Pre-trained Language Model Distillation From Multiple Teachers"], ["lu2022ernie", "Ernie-search: Bridging Cross-encoder With Dual-encoder Via Self On-the-fly Distillation For Dense Passage Retrieval"], ["yang2019model", "Model Compression With Two-stage Multi-teacher Knowledge Distillation For Web Question Answering System"]]