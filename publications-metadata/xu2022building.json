[["dou2021empirical", "An Empirical Study Of Training End-to-end Vision-and-language Transformers"], ["ma2021encoder", "Deltalm: Encoder-decoder Pre-training For Language Generation And Translation By Augmenting Pretrained Multilingual Encoders"], ["hao2022language", "Language Models Are General-purpose Interfaces"], ["tan2019learning", "LXMERT: Learning Cross-modality Encoder Representations From Transformers"]]