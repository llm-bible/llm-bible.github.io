[["logan2021cutting", "Cutting Down On Prompts And Parameters: Simple Few-shot Learning With Language Models"], ["hsieh2023distilling", "Distilling Step-by-step! Outperforming Larger Language Models With Less Training Data And Smaller Model Sizes"], ["lukovnikov2020pretrained", "Pretrained Transformers For Simple Question Answering Over Knowledge Graphs"], ["chen2019distilling", "Distilling Knowledge Learned In BERT For Text Generation"]]