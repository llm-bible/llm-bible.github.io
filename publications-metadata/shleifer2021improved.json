[["nguyen2019transformers", "Transformers Without Tears: Improving The Normalization Of Self-attention"], ["cao2020decomposing", "Deformer: Decomposing Pre-trained Transformers For Faster Question Answering"], ["radiyadixit2020how", "How Fine Can Fine-tuning Be? Learning Efficient Language Models"], ["sukhbaatar2019augmenting", "Augmenting Self-attention With Persistent Memory"]]