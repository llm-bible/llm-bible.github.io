[["bhojanapalli2020low", "Low-rank Bottleneck In Multi-head Attention Models"], ["michel2019are", "Are Sixteen Heads Really Better Than One?"], ["htut2019do", "Do Attention Heads In BERT Track Syntactic Dependencies?"], ["voita2019analyzing", "Analyzing Multi-head Self-attention: Specialized Heads Do The Heavy Lifting, The Rest Can Be Pruned"]]