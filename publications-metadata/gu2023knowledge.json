[["liang2020towards", "Mixkd: Towards Efficient Distillation Of Large-scale Language Models"], ["jiao2019distilling", "Tinybert: Distilling BERT For Natural Language Understanding"], ["liu2019multi", "MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models"], ["you2020knowledge", "Knowledge Distillation For Improved Accuracy In Spoken Question Answering"]]