[["dai2021knowledge", "Knowledge Neurons In Pretrained Transformers"], ["gekhman2024does", "Does Fine-tuning Llms On New Knowledge Encourage Hallucinations?"], ["qi2022integrating", "RASAT: Integrating Relational Structures Into Pretrained Seq2seq Model For Text-to-sql"], ["petroni2020how", "How Context Affects Language Models' Factual Predictions"]]