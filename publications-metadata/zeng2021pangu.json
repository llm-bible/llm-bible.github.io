[["narayanan2021efficient", "Efficient Large-scale Language Model Training On GPU Clusters Using Megatron-lm"], ["li2021token", "Terapipe: Token-level Pipeline Parallelism For Training Large-scale Language Models"], ["zhang2020large", "CPM: A Large-scale Generative Chinese Pre-trained Language Model"], ["shoeybi2019megatron", "Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism"]]