[["bhojanapalli2020low", "Low-rank Bottleneck In Multi-head Attention Models"], ["michel2019are", "Are Sixteen Heads Really Better Than One?"], ["kovaleva2019revealing", "Revealing The Dark Secrets Of BERT"], ["voita2019analyzing", "Analyzing Multi-head Self-attention: Specialized Heads Do The Heavy Lifting, The Rest Can Be Pruned"]]