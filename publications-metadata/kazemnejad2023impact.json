[["haviv2022transformer", "Transformer Language Models Without Positional Encodings Still Learn Positional Information"], ["ke2020rethinking", "Rethinking Positional Encoding In Language Pre-training"], ["irie2019language", "Language Modeling With Deep Transformers"], ["anil2022exploring", "Exploring Length Generalization In Large Language Models"]]