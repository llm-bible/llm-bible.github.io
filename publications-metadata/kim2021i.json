[["bondarenko2021understanding", "Understanding And Overcoming The Challenges Of Efficient Transformer Quantization"], ["yao2022efficient", "Zeroquant: Efficient And Affordable Post-training Quantization For Large-scale Transformers"], ["zadeh2020quantizing", "GOBO: Quantizing Attention-based NLP Models For Low Latency And Energy Efficient Inference"], ["frantar2022accurate", "GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers"]]