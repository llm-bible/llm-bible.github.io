[["rajbhandari2022deepspeed", "Deepspeed-moe: Advancing Mixture-of-experts Inference And Training To Power Next-generation AI Scale"], ["shen2023scaling", "Scaling Vision-language Models With Sparse Mixture Of Experts"], ["artetxe2021efficient", "Efficient Large Scale Language Modeling With Mixtures Of Experts"], ["he2021fast", "Fastmoe: A Fast Mixture-of-expert Training System"]]