[["lewis2019denoising", "BART: Denoising Sequence-to-sequence Pre-training For Natural Language Generation, Translation, And Comprehension"], ["lin2020pre", "Pre-training Multilingual Neural Machine Translation By Leveraging Alignment Information"], ["siddhant2022towards", "Towards The Next 1000 Languages In Multilingual Machine Translation: Exploring The Synergy Between Supervised And Self-supervised Learning"], ["ma2020xlm", "XLM-T: Scaling Up Multilingual Machine Translation With Pretrained Cross-lingual Transformer Encoders"]]