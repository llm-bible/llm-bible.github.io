[["clark2021pre", "CANINE: Pre-training An Efficient Tokenization-free Encoder For Language Representation"], ["bostrom2020byte", "Byte Pair Encoding Is Suboptimal For Language Model Pretraining"], ["rust2020how", "How Good Is Your Tokenizer? On The Monolingual Performance Of Multilingual Language Models"], ["lowphansirikul2021pretraining", "Wangchanberta: Pretraining Transformer-based Thai Language Models"]]