[["ganesh2020compressing", "Compressing Large-scale Transformer-based Models: A Case Study On BERT"], ["zhang2020distillation", "Ternarybert: Distillation-aware Ultra-low Bit BERT"], ["wang2020deep", "Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers"], ["wu2021one", "One Teacher Is Enough? Pre-trained Language Model Distillation From Multiple Teachers"]]