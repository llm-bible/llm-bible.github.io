[["wu2021one", "One Teacher Is Enough? Pre-trained Language Model Distillation From Multiple Teachers"], ["dou2021empirical", "An Empirical Study Of Training End-to-end Vision-and-language Transformers"], ["sun2020contrastive", "Contrastive Distillation On Intermediate Representations For Language Model Compression"], ["li2021scheduled", "Scheduled Sampling In Vision-language Pretraining With Decoupled Encoder-decoder Network"]]