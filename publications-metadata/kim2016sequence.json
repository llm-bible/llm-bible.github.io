[["liu2019multi", "MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models"], ["lu2022ernie", "Ernie-search: Bridging Cross-encoder With Dual-encoder Via Self On-the-fly Distillation For Dense Passage Retrieval"], ["peer2021greedy", "Greedy-layer Pruning: Speeding Up Transformer Models For Natural Language Processing"], ["wang2020deep", "Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers"]]