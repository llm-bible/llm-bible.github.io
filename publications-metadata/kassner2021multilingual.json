[["wu2020are", "Are All Languages Created Equal In Multilingual BERT?"], ["wu2019surprising", "Beto, Bentz, Becas: The Surprising Cross-lingual Effectiveness Of BERT"], ["qin2020cosda", "Cosda-ml: Multi-lingual Code-switching Data Augmentation For Zero-shot Cross-lingual NLP"], ["kaliamoorthi2021distilling", "Distilling Large Language Models Into Tiny And Effective Students Using Pqrnn"]]