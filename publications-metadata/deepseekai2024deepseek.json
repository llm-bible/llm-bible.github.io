[["zhang2023heavy", "H\\(_2\\)O: Heavy-hitter Oracle For Efficient Generative Inference Of Large Language Models"], ["xiao2023efficient", "Efficient Streaming Language Models With Attention Sinks"], ["kim2021scalable", "Scalable And Efficient Moe Training For Multitask Multilingual Models"], ["rajbhandari2022deepspeed", "Deepspeed-moe: Advancing Mixture-of-experts Inference And Training To Power Next-generation AI Scale"]]