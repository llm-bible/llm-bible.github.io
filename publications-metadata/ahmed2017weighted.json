[["vaswani2017attention", "Attention Is All You Need"], ["wu2019pay", "Pay Less Attention With Lightweight And Dynamic Convolutions"], ["hao2019modeling", "Modeling Recurrence For Transformer"], ["yu2018combining", "Qanet: Combining Local Convolution With Global Self-attention For Reading Comprehension"]]