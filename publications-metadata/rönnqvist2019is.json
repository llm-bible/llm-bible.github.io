[["rust2020how", "How Good Is Your Tokenizer? On The Monolingual Performance Of Multilingual Language Models"], ["ma2019universal", "Universal Text Representation From BERT: An Empirical Study"], ["tang2020multilingual", "Multilingual Translation With Extensible Multilingual Pretraining And Finetuning"], ["zhou2021universal", "UC2: Universal Cross-lingual Cross-modal Vision-and-language Pre-training"]]