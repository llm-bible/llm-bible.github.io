[["shridhar2022distilling", "Distilling Reasoning Capabilities Into Smaller Language Models"], ["wu2021one", "One Teacher Is Enough? Pre-trained Language Model Distillation From Multiple Teachers"], ["kim2023cot", "The Cot Collection: Improving Zero-shot And Few-shot Learning Of Language Models Via Chain-of-thought Fine-tuning"], ["fu2022complexity", "Complexity-based Prompting For Multi-step Reasoning"]]