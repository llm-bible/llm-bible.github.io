[["tan2019learning", "LXMERT: Learning Cross-modality Encoder Representations From Transformers"], ["chen2019universal", "UNITER: Universal Image-text Representation Learning"], ["xia2020cross", "XGPT: Cross-modal Generative Pre-training For Image Captioning"], ["wettig2022should", "Should You Mask 15% In Masked Language Modeling?"]]