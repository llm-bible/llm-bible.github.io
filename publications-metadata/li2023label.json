[["hsieh2023distilling", "Distilling Step-by-step! Outperforming Larger Language Models With Less Training Data And Smaller Model Sizes"], ["chung2023increasing", "Increasing Diversity While Maintaining Accuracy: Text Data Generation With Large Language Models And Human Interventions"], ["chen2021knowledge", "Knowprompt: Knowledge-aware Prompt-tuning With Synergistic Optimization For Relation Extraction"], ["rozi\u00e8re2023code", "Code Llama: Open Foundation Models For Code"]]