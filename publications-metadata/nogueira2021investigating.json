[["haviv2022transformer", "Transformer Language Models Without Positional Encodings Still Learn Positional Information"], ["wang2020rethinking", "Rethinking The Value Of Transformer Components"], ["clark2021pre", "CANINE: Pre-training An Efficient Tokenization-free Encoder For Language Representation"], ["kazemnejad2023impact", "The Impact Of Positional Encoding On Length Generalization In Transformers"]]