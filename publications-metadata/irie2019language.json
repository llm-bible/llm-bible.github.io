[["haviv2022transformer", "Transformer Language Models Without Positional Encodings Still Learn Positional Information"], ["ke2020rethinking", "Rethinking Positional Encoding In Language Pre-training"], ["kazemnejad2023impact", "The Impact Of Positional Encoding On Length Generalization In Transformers"], ["yu2023low", "Low-rank Adaptation Of Large Language Model Rescoring For Parameter-efficient Speech Recognition"]]