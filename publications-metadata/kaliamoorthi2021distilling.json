[["wang2020deep", "Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers"], ["hsieh2023distilling", "Distilling Step-by-step! Outperforming Larger Language Models With Less Training Data And Smaller Model Sizes"], ["shleifer2020pre", "Pre-trained Summarization Distillation"], ["liu2019multi", "MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models"]]