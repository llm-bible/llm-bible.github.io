[["rahman2020improved", "An Improved Attention For Visual Question Answering"], ["choi2018fine", "Fine-grained Attention Mechanism For Neural Machine Translation"], ["haviv2022transformer", "Transformer Language Models Without Positional Encodings Still Learn Positional Information"], ["liu2017phase", "Phase Conductor On Multi-layered Attentions For Machine Comprehension"]]