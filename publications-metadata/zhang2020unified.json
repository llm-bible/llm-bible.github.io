[["fang2019towards", "Towards Transfer Learning For End-to-end Speech Synthesis From Deep Pre-trained Language Models"], ["jiao2019distilling", "Tinybert: Distilling BERT For Natural Language Understanding"], ["zhang2020distillation", "Ternarybert: Distillation-aware Ultra-low Bit BERT"], ["liu2019multi", "MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models"]]