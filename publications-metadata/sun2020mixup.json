[["si2020better", "Better Robustness By More Coverage: Adversarial Training With Mixup Augmentation For Robust Fine-tuning"], ["kumar2020data", "Data Augmentation Using Pre-trained Transformer Models"], ["longpre2020how", "How Effective Is Task-agnostic Data Augmentation For Pretrained Transformers?"], ["qu2020contrast", "Coda: Contrast-enhanced And Diversity-promoting Data Augmentation For Natural Language Understanding"]]