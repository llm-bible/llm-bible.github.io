[["shen2023anything", "\"do Anything Now\": Characterizing And Evaluating In-the-wild Jailbreak Prompts On Large Language Models"], ["liu2023jailbreaking", "Jailbreaking Chatgpt Via Prompt Engineering: An Empirical Study"], ["wei2023how", "Jailbroken: How Does LLM Safety Training Fail?"], ["shayegani2023survey", "Survey Of Vulnerabilities In Large Language Models Revealed By Adversarial Attacks"]]