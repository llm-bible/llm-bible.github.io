[["wang2022what", "What Language Model Architecture And Pretraining Objective Work Best For Zero-shot Generalization?"], ["wang2023multitask", "Multitask Prompt Tuning Enables Parameter-efficient Transfer Learning"], ["ni2020learning", "M3P: Learning Universal Representations Via Multitask Multilingual Multimodal Pre-training"], ["muennighoff2022crosslingual", "Crosslingual Generalization Through Multitask Finetuning"]]