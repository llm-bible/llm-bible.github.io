[["lee2019what", "What Would Elsa Do? Freezing Layers During Transformer Fine-tuning"], ["shleifer2021improved", "Normformer: Improved Transformer Pretraining With Extra Normalization"], ["shi2023towards", "Towards Efficient Fine-tuning Of Pre-trained Code Models: An Experimental Study And Beyond"], ["sajjad2020effect", "On The Effect Of Dropping Layers Of Pre-trained Transformer Models"]]