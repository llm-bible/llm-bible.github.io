[["zeng2021pangu", "Pangu-\\(\u03b1\\): Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation"], ["li2021token", "Terapipe: Token-level Pipeline Parallelism For Training Large-scale Language Models"], ["rajbhandari2019memory", "Zero: Memory Optimizations Toward Training Trillion Parameter Models"], ["shoeybi2019megatron", "Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism"]]