[["voita2019analyzing", "Analyzing Multi-head Self-attention: Specialized Heads Do The Heavy Lifting, The Rest Can Be Pruned"], ["bhojanapalli2020low", "Low-rank Bottleneck In Multi-head Attention Models"], ["kovaleva2019revealing", "Revealing The Dark Secrets Of BERT"], ["htut2019do", "Do Attention Heads In BERT Track Syntactic Dependencies?"]]