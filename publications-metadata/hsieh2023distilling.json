[["shleifer2020pre", "Pre-trained Summarization Distillation"], ["kaliamoorthi2021distilling", "Distilling Large Language Models Into Tiny And Effective Students Using Pqrnn"], ["chung2022scaling", "Scaling Instruction-finetuned Language Models"], ["liu2019multi", "MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models"]]