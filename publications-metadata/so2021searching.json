[["lei2021when", "When Attention Meets Fast Recurrence: Training Language Models With Reduced Compute"], ["razumovskaia2021crossing", "Crossing The Conversational Chasm: A Primer On Natural Language Processing For Multilingual Task-oriented Dialogue Systems"], ["cao2020decomposing", "Deformer: Decomposing Pre-trained Transformers For Faster Question Answering"], ["wang2020high", "Lightseq: A High Performance Inference Library For Transformers"]]