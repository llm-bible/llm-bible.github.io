[["song2022clip", "CLIP Models Are Few-shot Learners: Empirical Studies On VQA And Visual Entailment"], ["fan2023improving", "Improving CLIP Training With Language Rewrites"], ["cui2022democratizing", "Democratizing Contrastive Language-image Pre-training: A CLIP Benchmark Of Data, Model, And Supervision"], ["wang2022clip", "CLIP-TD: CLIP Targeted Distillation For Vision-language Tasks"]]