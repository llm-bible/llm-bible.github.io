[["lu2022ernie", "Ernie-search: Bridging Cross-encoder With Dual-encoder Via Self On-the-fly Distillation For Dense Passage Retrieval"], ["liu2019multi", "MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models"], ["fu2020lrc", "LRC-BERT: Latent-representation Contrastive Knowledge Distillation For Natural Language Understanding"], ["nogueira2020document", "Document Ranking With A Pretrained Sequence-to-sequence Model"]]