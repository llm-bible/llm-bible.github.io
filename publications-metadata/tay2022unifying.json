[["chung2022scaling", "Scaling Instruction-finetuned Language Models"], ["wang2022what", "What Language Model Architecture And Pretraining Objective Work Best For Zero-shot Generalization?"], ["mao2021unified", "Unipelt: A Unified Framework For Parameter-efficient Language Model Tuning"], ["longpre2023flan", "The Flan Collection: Designing Data And Methods For Effective Instruction Tuning"]]