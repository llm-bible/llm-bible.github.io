[["turc2019well", "Well-read Students Learn Better: On The Importance Of Pre-training Compact Models"], ["yang2019model", "Model Compression With Two-stage Multi-teacher Knowledge Distillation For Web Question Answering System"], ["wang2020deep", "Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers"], ["xia2022structured", "Structured Pruning Learns Compact And Accurate Models"]]