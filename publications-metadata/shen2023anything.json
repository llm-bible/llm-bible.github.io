[["liu2023jailbreaking", "Jailbreaking Chatgpt Via Prompt Engineering: An Empirical Study"], ["zeng2024how", "How Johnny Can Persuade Llms To Jailbreak Them: Rethinking Persuasion To Challenge AI Safety By Humanizing Llms"], ["wei2023how", "Jailbroken: How Does LLM Safety Training Fail?"], ["qi2023visual", "Visual Adversarial Examples Jailbreak Aligned Large Language Models"]]