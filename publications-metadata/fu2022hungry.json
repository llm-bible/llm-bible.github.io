[["dao2024transformers", "Transformers Are Ssms: Generalized Models And Efficient Algorithms Through Structured State Space Duality"], ["gu2023linear", "Mamba: Linear-time Sequence Modeling With Selective State Spaces"], ["ding2023scaling", "Longnet: Scaling Transformers To 1,000,000,000 Tokens"], ["poli2023hyena", "Hyena Hierarchy: Towards Larger Convolutional Language Models"]]