[["wang2020high", "Lightseq: A High Performance Inference Library For Transformers"], ["kim2021learned", "Learned Token Pruning For Transformers"], ["ding2023scaling", "Longnet: Scaling Transformers To 1,000,000,000 Tokens"], ["bulatov2022recurrent", "Recurrent Memory Transformer"]]