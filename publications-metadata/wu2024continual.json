[["scialom2022fine", "Fine-tuned Language Models Are Continual Learners"], ["smith2022coda", "Coda-prompt: Continual Decomposed Attention-based Prompting For Rehearsal-free Continual Learning"], ["luo2023empirical", "An Empirical Study Of Catastrophic Forgetting In Large Language Models During Continual Fine-tuning"], ["zheng2023preventing", "Preventing Zero-shot Transfer Degradation In Continual Learning Of Vision-language Models"]]