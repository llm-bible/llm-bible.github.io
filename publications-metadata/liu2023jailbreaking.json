[["shen2023anything", "\"do Anything Now\": Characterizing And Evaluating In-the-wild Jailbreak Prompts On Large Language Models"], ["zeng2024how", "How Johnny Can Persuade Llms To Jailbreak Them: Rethinking Persuasion To Challenge AI Safety By Humanizing Llms"], ["robey2023defending", "Smoothllm: Defending Large Language Models Against Jailbreaking Attacks"], ["wei2023how", "Jailbroken: How Does LLM Safety Training Fail?"]]