[["bulatov2022recurrent", "Recurrent Memory Transformer"], ["ding2023scaling", "Longnet: Scaling Transformers To 1,000,000,000 Tokens"], ["wang2020self", "Linformer: Self-attention With Linear Complexity"], ["ma2021linear", "Luna: Linear Unified Nested Attention"]]