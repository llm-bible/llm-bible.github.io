[["qi2020cross", "Imagebert: Cross-modal Pre-training With Large-scale Weak-supervised Image-text Data"], ["wettig2022should", "Should You Mask 15% In Masked Language Modeling?"], ["li2019unicoder", "Unicoder-vl: A Universal Encoder For Vision And Language By Cross-modal Pre-training"], ["zhou2021universal", "UC2: Universal Cross-lingual Cross-modal Vision-and-language Pre-training"]]