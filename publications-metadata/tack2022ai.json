[["li2024unsupervised", "Promptkd: Unsupervised Prompt Distillation For Vision-language Models"], ["wu2021one", "One Teacher Is Enough? Pre-trained Language Model Distillation From Multiple Teachers"], ["abbasiantaeb2023let", "Let The Llms Talk: Simulating Human-to-human Conversational QA Via Zero-shot Llm-to-llm Interactions"], ["ho2022large", "Large Language Models Are Reasoning Teachers"]]