[["liu2019multi", "MKD: A Multi-task Knowledge Distillation Approach For Pretrained Language Models"], ["kim2016sequence", "Sequence-level Knowledge Distillation"], ["yang2019model", "Model Compression With Two-stage Multi-teacher Knowledge Distillation For Web Question Answering System"], ["shleifer2020pre", "Pre-trained Summarization Distillation"]]