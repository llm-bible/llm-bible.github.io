[["zhang2022open", "OPT: Open Pre-trained Transformer Language Models"], ["eisenschlos2019efficient", "Multifit: Efficient Multi-lingual Language Model Fine-tuning"], ["radiyadixit2020how", "How Fine Can Fine-tuning Be? Learning Efficient Language Models"], ["wang2020deep", "Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers"]]