[["voita2019bottom", "The Bottom-up Evolution Of Representations In The Transformer: A Study With Machine Translation And Language Modeling Objectives"], ["chefer2021generic", "Generic Attention-model Explainability For Interpreting Bi-modal And Encoder-decoder Transformers"], ["sun2023retentive", "Retentive Network: A Successor To Transformer For Large Language Models"], ["sukhbaatar2019augmenting", "Augmenting Self-attention With Persistent Memory"]]