[["clark2020pre", "ELECTRA: Pre-training Text Encoders As Discriminators Rather Than Generators"], ["dou2018exploiting", "Exploiting Deep Representations For Neural Machine Translation"], ["fan2020addressing", "Addressing Some Limitations Of Transformers With Feedback Memory"], ["vanaken2019how", "How Does BERT Answer Questions? A Layer-wise Analysis Of Transformer Representations"]]