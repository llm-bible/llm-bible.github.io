[["vaswani2017attention", "Attention Is All You Need"], ["bapna2018training", "Training Deeper Neural Machine Translation Models With Transparent Attention"], ["wang2020hardware", "HAT: Hardware-aware Transformers For Efficient Natural Language Processing"], ["wu2020lite", "Lite Transformer With Long-short Range Attention"]]