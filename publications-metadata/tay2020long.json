[["chefer2021generic", "Generic Attention-model Explainability For Interpreting Bi-modal And Encoder-decoder Transformers"], ["hua2022transformer", "Transformer Quality In Linear Time"], ["poli2023hyena", "Hyena Hierarchy: Towards Larger Convolutional Language Models"], ["peng2023reinventing", "RWKV: Reinventing Rnns For The Transformer Era"]]