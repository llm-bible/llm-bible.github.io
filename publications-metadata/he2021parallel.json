[["lewis2019denoising", "BART: Denoising Sequence-to-sequence Pre-training For Natural Language Generation, Translation, And Comprehension"], ["zhang2020constrained", "POINTER: Constrained Progressive Text Generation Via Insertion-based Generative Pre-training"], ["kasai2020non", "Non-autoregressive Machine Translation With Disentangled Context Transformer"], ["wang2021can", "Can Generative Pre-trained Language Models Serve As Knowledge Bases For Closed-book QA?"]]