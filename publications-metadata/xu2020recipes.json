[["sun2023safety", "Safety Assessment Of Chinese Large Language Models"], ["wei2023how", "Jailbroken: How Does LLM Safety Training Fail?"], ["qi2023fine", "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"], ["dinan2019build", "Build It Break It Fix It For Dialogue Safety: Robustness From Adversarial Human Attack"]]