[["singh2020are", "Are We Pretraining It Right? Digging Deeper Into Visio-linguistic Pretraining"], ["lan2019lite", "ALBERT: A Lite BERT For Self-supervised Learning Of Language Representations"], ["wang2018can", "Can You Tell Me How To Get Past Sesame Street? Sentence-level Pretraining Beyond Language Modeling"], ["baevski2019cloze", "Cloze-driven Pretraining Of Self-attention Networks"]]