[["xue2020massively", "Mt5: A Massively Multilingual Pre-trained Text-to-text Transformer"], ["ainslie2023training", "GQA: Training Generalized Multi-query Transformer Models From Multi-head Checkpoints"], ["zhu2020incorporating", "Incorporating BERT Into Neural Machine Translation"], ["malmi2019high", "Encode, Tag, Realize: High-precision Text Editing"]]