[["dao2022fast", "Flashattention: Fast And Memory-efficient Exact Attention With Io-awareness"], ["gupta2020global", "GMAT: Global Memory Augmentation For Transformers"], ["bulatov2022recurrent", "Recurrent Memory Transformer"], ["poli2023hyena", "Hyena Hierarchy: Towards Larger Convolutional Language Models"]]