[["bulatov2022recurrent", "Recurrent Memory Transformer"], ["christopoulou2022pangu", "Pangu-coder: Program Synthesis With Function-level Language Modeling"], ["ding2023scaling", "Longnet: Scaling Transformers To 1,000,000,000 Tokens"], ["gupta2020global", "GMAT: Global Memory Augmentation For Transformers"]]