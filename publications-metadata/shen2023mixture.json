[["rajbhandari2022deepspeed", "Deepspeed-moe: Advancing Mixture-of-experts Inference And Training To Power Next-generation AI Scale"], ["kim2021scalable", "Scalable And Efficient Moe Training For Multitask Multilingual Models"], ["shen2023scaling", "Scaling Vision-language Models With Sparse Mixture Of Experts"], ["dai2023towards", "Instructblip: Towards General-purpose Vision-language Models With Instruction Tuning"]]