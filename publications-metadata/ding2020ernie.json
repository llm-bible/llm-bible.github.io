[["hao2019modeling", "Modeling Recurrence For Transformer"], ["lei2021when", "When Attention Meets Fast Recurrence: Training Language Models With Reduced Compute"], ["qiu2019blockwise", "Blockwise Self-attention For Long Document Understanding"], ["xu2020multi", "Layoutlmv2: Multi-modal Pre-training For Visually-rich Document Understanding"]]