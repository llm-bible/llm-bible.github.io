[["vari\u01612021sequence", "Sequence Length Is A Domain: Length-based Overfitting In Transformer Models"], ["peng2023reinventing", "RWKV: Reinventing Rnns For The Transformer Era"], ["cao2020decomposing", "Deformer: Decomposing Pre-trained Transformers For Faster Question Answering"], ["shi2023towards", "Towards Efficient Fine-tuning Of Pre-trained Code Models: An Experimental Study And Beyond"]]