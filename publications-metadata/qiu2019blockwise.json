[["gupta2020global", "GMAT: Global Memory Augmentation For Transformers"], ["htut2019do", "Do Attention Heads In BERT Track Syntactic Dependencies?"], ["ding2020ernie", "Ernie-doc: A Retrospective Long-document Modeling Transformer"], ["lan2019lite", "ALBERT: A Lite BERT For Self-supervised Learning Of Language Representations"]]