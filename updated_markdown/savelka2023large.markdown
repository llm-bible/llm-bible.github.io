---
layout: publication
title: 'Large Language Models (GPT) Struggle To Answer Multiple-choice Questions About Code'
authors: Jaromir Savelka, Arav Agarwal, Christopher Bogart, Majd Sakr
conference: "Arxiv"
year: 2023
citations: 42
bibkey: savelka2023large
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2303.08033'}
tags: ['Interpretability and Explainability', 'Transformer', 'RAG', 'Model Architecture', 'GPT', 'Merging', 'Reinforcement Learning', 'Pretraining Methods']
---
We analyzed effectiveness of three generative pre-trained transformer (GPT)
models in answering multiple-choice question (MCQ) assessments, often involving
short snippets of code, from introductory and intermediate programming courses
at the postsecondary level. This emerging technology stirs countless
discussions of its potential uses (e.g., exercise generation, code explanation)
as well as misuses in programming education (e.g., cheating). However, the
capabilities of GPT models and their limitations to reason about and/or analyze
code in educational settings have been under-explored. We evaluated several
OpenAI's GPT models on formative and summative MCQ assessments from three
Python courses (530 questions). We found that MCQs containing code snippets are
not answered as successfully as those that only contain natural language. While
questions requiring to fill-in a blank in the code or completing a natural
language statement about the snippet are handled rather successfully, MCQs that
require analysis and/or reasoning about the code (e.g., what is true/false
about the snippet, or what is its output) appear to be the most challenging.
These findings can be leveraged by educators to adapt their instructional
practices and assessments in programming courses, so that GPT becomes a
valuable assistant for a learner as opposed to a source of confusion and/or
potential hindrance in the learning process.
