---
layout: publication
title: 'Level Generation Through Large Language Models'
authors: Graham Todd, Sam Earle, Muhammad Umair Nasir, Michael Cerny Green, Julian Togelius
conference: "FDG 2023 Proceedings of the 18th International Conference on the Foundations of Digital Games"
year: 2023
citations: 43
bibkey: todd2023level
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2302.05817'}
tags: ['RAG', 'Tools', 'Training Techniques']
---
Large Language Models (LLMs) are powerful tools, capable of leveraging their
training on natural language to write stories, generate code, and answer
questions. But can they generate functional video game levels? Game levels,
with their complex functional constraints and spatial relationships in more
than one dimension, are very different from the kinds of data an LLM typically
sees during training. Datasets of game levels are also hard to come by,
potentially taxing the abilities of these data-hungry models. We investigate
the use of LLMs to generate levels for the game Sokoban, finding that LLMs are
indeed capable of doing so, and that their performance scales dramatically with
dataset size. We also perform preliminary experiments on controlling LLM level
generators and discuss promising areas for future work.
