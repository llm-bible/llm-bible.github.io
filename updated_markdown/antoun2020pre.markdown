---
layout: publication
title: 'Aragpt2: Pre-trained Transformer For Arabic Language Generation'
authors: Wissam Antoun, Fady Baly, Hazem Hajj
conference: "Arxiv"
year: 2020
citations: 48
bibkey: antoun2020pre
additional_links:
  - {name: "Paper", url: 'https://arxiv.org/abs/2012.15520'}
tags: ['Language Modeling', 'Transformer', 'RAG', 'GPT', 'Applications', 'Model Architecture', 'Pretraining Methods']
---
Recently, pre-trained transformer-based architectures have proven to be very
efficient at language modeling and understanding, given that they are trained
on a large enough corpus. Applications in language generation for Arabic are
still lagging in comparison to other NLP advances primarily due to the lack of
advanced Arabic language generation models. In this paper, we develop the first
advanced Arabic language generation model, AraGPT2, trained from scratch on a
large Arabic corpus of internet text and news articles. Our largest model,
AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic
language model available. The Mega model was evaluated and showed success on
different tasks including synthetic news generation, and zero-shot question
answering. For text generation, our best model achieves a perplexity of 29.8 on
held-out Wikipedia articles. A study conducted with human evaluators showed the
significant success of AraGPT2-mega in generating news articles that are
difficult to distinguish from articles written by humans. We thus develop and
release an automatic discriminator model with a 98% percent accuracy in
detecting model-generated text. The models are also publicly available, hoping
to encourage new research directions and applications for Arabic NLP.
